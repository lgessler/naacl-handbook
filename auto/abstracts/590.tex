Noting that world knowledge continuously evolves over time, large language models (LLMs) need to be properly adjusted by performing the "knowledge editing", which involves updating outdated information or correcting false information. To achieve  reliable and "massive" editing capabilities in terms of $\textit{generalization}$ and $\textit{specificity}$, this paper proposes a unified knowledge editing method called in-$\textbf{CO}$ntext retrieval-augmented $\textbf{M}$ass-$\textbf{E}$diting $\textbf{M}$emory (COMEM), which combines two types of editing approaches: parameter updating and in-context knowledge editing (IKE). In particular, COMEM incorporates $\textit{retrieval-augmented IKE}$, a novel extension of IKE designed for  massive editing tasks, based on an $\textit{updating}$-aware demonstration construction. Experimental results on the zsRE and CounterFact datasets demonstrate that COMEM outperforms all existing methods, achieving state-of-the-art performance. Our code is available at https://github.com/JoveReCode/COMEM.git.