Effectively training language models on long inputs poses many technical challenges. As a cost consideration, languages models are pre- trained on a fixed sequence length before being adapted to longer sequences. We explore var- ious methods for adapting models to longer inputs by training on segmented sequences and an interpolation-based method for extending absolute positional embeddings. We develop a training procedure to extend the input con- text size of pretrained models with no architec- tural changes and no additional memory costs than training on the original input lengths. By sub-sampling segments from long inputs while maintaining their original position the model is able to learn new positional interactions. Our method benefits both models trained with abso- lute positional embeddings, by extending their input contexts, as well as popular relative posi- tional embedding methods showing a reduced perplexity on sequences longer than they were trained on. We demonstrate our method can extend input contexts by a factor of 4Ã— while improving perplexity.