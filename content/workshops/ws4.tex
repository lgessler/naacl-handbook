\begin{wsschedulenolist}
{Workshop on Online Abuse and Harms}
{4}
{ws4}
{ws4}
{Don Alberto 3}
{https://www.workshopononlineabuse.com/}
    Digital technologies have brought many benefits for society, transforming how people connect, communicate and interact with each other. However, they have also enabled abusive and harmful content such as hate speech and harassment to reach large audiences, and for their negative effects to be amplified. The sheer amount of content shared online means that abuse and harm can only be tackled at scale with the help of computational tools. However, detecting and moderating online abuse and harms is a difficult task, with many technical, social, legal and ethical challenges. The Workshop on Online Abuse and Harms (WOAH) invites paper submissions from a wide range of fields, including natural language processing, machine learning, computational social sciences, law, politics, psychology, sociology and cultural studies. We explicitly encourage interdisciplinary submissions, technical as well as non-technical submissions, and submissions that focus on under-resourced languages. We also invite non-archival submissions and civil society reports. The topics covered by WOAH include, but are not limited to:

\begin{enumerate}
    \item New models or methods for detecting abusive and harmful online content, including misinformation;
    \item Biases and limitations of existing detection models or datasets for abusive and harmful online content, particularly those in commercial use;
    \item New datasets and taxonomies for online abuse and harms;
    \item New evaluation metrics and procedures for the detection of harmful content;
    \item Dynamics of online abuse and harms, as well as their impact on different communities
    \item Social, legal, and ethical implications of detecting, monitoring and moderating online abuse
\end{enumerate}

In addition, we invite submissions related to the theme for this eighth edition of WOAH, which will be online harms in the age of large language models. Highly capable Large Language Models (LLMs) are now widely deployed and easily accessible by millions across the globe. Without proper safeguards, these LLMs will readily follow malicious instructions and generate toxic content. Even the safest LLMs can be exploited by bad actors for harmful purposes. With this theme, we invite submissions that explore the implications of LLMs for the creation, dissemination and detection of harmful online content. We are interested in how to stop LLMs from following malicious instructions and generating toxic content, but also how they could be used to improve content moderation and enable countermeasures like personalised counterspeech. To support our theme, we have invited an interdisciplinary line-up of high-profile speakers across academia, industry and public policy.
\end{wsschedulenolist}
