- abstract: Large language models (LLMs) have demonstrated powerful capabilities in
    natural language processing, yet their vast number of parameters  poses challenges
    for deployment and inference efficiency. Structured model pruning emerges as a
    viable approach to reduce model size and accelerate inference, without requiring
    specialized operators and libraries for deployment. However, structured pruning
    often severely weakens the model's capability.Despite repetitive fine-tuning can
    restore the capability to a certain extent, it impairs LLMs' utility as versatile
    problem solvers.To address this issue, we propose a novel structured pruning algorithm
    tailored for LLMs. It derives the importance of different components, namely rows
    and columns in parameter matrices, based on intermediate data dependencies. Then
    it removes coupled components across different layers simultaneously and preserves
    dependency relationships within remaining parameters, avoiding significant performance
    degradation. The pruned model requires only few epochs of fine-tuning to restore
    its performance,  ensuring the model's ability to generalize.Empirical evaluations
    on LLaMA, Vicuna, and ChatGLM3 demonstrate our algorithm's efficacy, yielding
    20% parameter reduction while retaining at least 94.4% of original performance
    metrics.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Efficient/Low-Resource Methods for NLP
  authors:
  - emails: '****@mail.ustc.edu.cn'
    first_name: Honghe
    google_scholar_id: https://scholar.google.com/citations?user=WPooT04AAAAJ
    institution: University of Science and Technology of China
    last_name: Zhang
    name: Honghe Zhang
    username: ~Honghe_Zhang1
  - emails: '****@mail.ustc.edu.cn'
    first_name: XiaolongShi
    institution: University of Science and Technology of China
    last_name: XiaolongShi
    name: XiaolongShi
    orcid: https://orcid.org/0009-0001-3516-8765
    username: ~XiaolongShi1
  - dblp_id: https://dblp.uni-trier.de/pid/66/7761-1
    emails: '****@ustc.edu.cn'
    first_name: Jingwei
    homepage: https://faculty.ustc.edu.cn/sunjingwei
    institution: University of Science and Technology of China
    last_name: Sun
    name: Jingwei Sun
    username: ~Jingwei_Sun3
  - dblp_id: https://dblp.org/pid/44/1372
    emails: '****@ustc.edu.cn'
    first_name: Guangzhong
    institution: University of Science and Technology of China
    last_name: Sun
    name: Guangzhong Sun
    username: ~Guangzhong_Sun1
  decision: toFindings
  end_page: 12
  file: 2.pdf
  id: 2
  num_pages: 12
  openreview_id: LbGDWbOzrT
  pdf_file: 471ec069c8e4e6a7ca146fd7b02df071ffa1cacb.pdf
  start_page: 1
  title: Structured Pruning for Large Language Models Using Coupled Components Elimination
    and Minor Fine-tuning
- abstract: 'Knowledge Distillation (KD) is a predominant approach for BERT compression.

    Previous KD-based methods focus on designing extra alignment losses for the student
    model to mimic the behavior of the teacher model.

    These methods transfer the knowledge in an indirect way.

    In this paper, we propose a novel Weight-Inherited Distillation (WID), which directly
    transfers knowledge from the teacher.

    WID does not require any additional alignment loss and trains a compact student
    by inheriting the weights, showing a new perspective of knowledge distillation.

    Specifically, we design the row compactors and column compactors as mappings and
    then compress the weights via structural re-parameterization.

    Experimental results on the GLUE and SQuAD benchmarks show that WID outperforms
    previous state-of-the-art KD-based baselines.

    Further analysis indicates that WID can also learn the attention patterns from
    the teacher model without any alignment loss on attention distributions.

    The code is available at https://github.com/wutaiqiang/WID-NAACL2024.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Efficient/Low-Resource Methods for NLP
  authors:
  - dblp_id: https://dblp.org/pid/303/5950
    emails: '****@connect.hku.hk'
    first_name: Taiqiang
    google_scholar_id: https://scholar.google.com/citations?user=mCtvn50AAAAJ&hl=zh-CN&oi=ao
    homepage: https://wutaiqiang.github.io
    last_name: Wu
    name: Taiqiang Wu
    orcid: https://orcid.org/0000-0002-3664-3513
    semantic_scholar_id: https://www.semanticscholar.org/author/Taiqiang-Wu/2137407647
    username: ~Taiqiang_Wu1
  - emails: '****@qq.com'
    first_name: Cheng
    homepage: https://github.com/hhou435
    last_name: Hou
    name: Cheng Hou
    username: ~Cheng_Hou1
  - emails: '****@mails.tsinghua.edu.cn'
    first_name: Shanshan
    last_name: Lao
    name: Shanshan Lao
    username: ~Shanshan_Lao1
  - dblp_id: https://dblp.org/pid/130/6398
    emails: '****@mails.tsinghua.edu.cn'
    first_name: Jiayi
    google_scholar_id: https://scholar.google.com/citations?hl=zh-CN&user=qWKYwpQAAAAJ
    last_name: Li
    name: Jiayi Li
    username: ~Jiayi_Li2
  - dblp_id: https://dblp.org/pid/88/3656
    emails: '****@eee.hku.hk'
    first_name: Ngai
    google_scholar_id: https://scholar.google.com/citations?user=PM_uMYIAAAAJ&hl=en
    homepage: https://www.eee.hku.hk/~nwong/
    institution: The University of Hong Kong
    last_name: Wong
    name: Ngai Wong
    orcid: https://orcid.org/0000-0002-3026-0108
    username: ~Ngai_Wong1
  - emails: '****@ruc.edu.cn'
    first_name: Zhe
    google_scholar_id: https://scholar.google.com.hk/citations?hl=zh-CN&user=Xh7oU4kAAAAJ
    last_name: Zhao
    name: Zhe Zhao
    username: ~Zhe_Zhao1
  - dblp_id: https://dblp.org/pid/30/3847
    emails: '****@sz.tsinghua.edu.cn'
    first_name: Yujiu
    google_scholar_id: https://scholar.google.com/citations?user=4gH3sxsAAAAJ&hl=zh-TW
    homepage: https://sites.google.com/view/iigroup-thu
    institution: Graduate School at Shenzhen,Tsinghua University
    last_name: Yang
    name: Yujiu Yang
    orcid: https://orcid.org/0000-0002-6427-1024
    semantic_scholar_id: https://www.semanticscholar.org/author/Yujiu-Yang/3001727
    username: ~Yujiu_Yang2
  decision: toFindings
  end_page: 28
  file: 4.pdf
  id: 4
  num_pages: 16
  openreview_id: F4ZPhEGFnW
  pdf_file: a32f0efc64e9fe8e7ff3cf981d9c92777ea7b76f.pdf
  start_page: 13
  title: Weight-Inherited Distillation for Task-Agnostic BERT Compression
- abstract: Cybersecurity information is often technically complex and relayed through
    unstructured text, making automation of cyber threat intelligence highly challenging.
    For such text domains that involve high levels of expertise, pretraining on in-domain
    corpora has been a popular method for language models to obtain domain expertise.
    However, cybersecurity texts often contain non-linguistic elements (such as URLs
    and hash values) that could be unsuitable with the established pretraining methodologies.
    Previous work in other domains have removed or filtered such text as noise, but
    the effectiveness of these methods have not been investigated, especially in the
    cybersecurity domain. We experiment with different pretraining methodologies to
    account for non-linguistic elements (NLEs) and evaluate their effectiveness through
    downstream tasks and probing tasks. Our proposed strategy, a combination of selective
    MLM and jointly training NLE token classification, outperforms the commonly taken
    approach of replacing NLEs. We use our domain-customized methodology to train
    CyBERTuned, a cybersecurity domain language model that outperforms other cybersecurity
    PLMs on most tasks.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - dblp_id: https://dblp.org/pid/294/0262
    emails: '****@gmail.com'
    first_name: Eugene
    institution: S2W Inc.
    last_name: Jang
    name: Eugene Jang
    semantic_scholar_id: https://www.semanticscholar.org/author/Eugene-Jang/2101320851
    username: ~Eugene_Jang1
  - emails: '****@iu.edu'
    first_name: Jian
    google_scholar_id: https://scholar.google.com/citations?user=_nZtGvYAAAAJ&hl=en
    homepage: https://cuijian0819.github.io
    last_name: Cui
    name: Jian Cui
    username: ~Jian_Cui4
  - emails: '****@s2w.inc'
    first_name: Dayeon
    institution: S2W Inc.
    last_name: Yim
    name: Dayeon Yim
    username: ~Dayeon_Yim1
  - emails: '****@kaist.ac.kr'
    first_name: Youngjin
    google_scholar_id: https://scholar.google.com/citations?user=Sa87FgYAAAAJ&hl=en
    institution: Korea Advanced Institute of Science & Technology
    last_name: Jin
    name: Youngjin Jin
    username: ~Youngjin_Jin1
  - dblp_id: https://dblp.org/pid/47/9656
    emails: '****@gmail.com'
    first_name: Jin-Woo
    last_name: Chung
    name: Jin-Woo Chung
    username: ~Jin-Woo_Chung1
  - emails: '****@kaist.ac.kr'
    first_name: Seungwon
    google_scholar_id: https://scholar.google.com.tw/citations?user=DbAmqd8AAAAJ
    homepage: http://nss.kaist.ac.kr/?page_id=29
    institution: Korea Advanced Institute of Science & Technology
    last_name: Shin
    name: Seungwon Shin
    username: ~Seungwon_Shin1
  - emails: '****@gmail.com'
    first_name: Yongjae
    google_scholar_id: https://scholar.google.com/citations?user=bvvWuJcAAAAJ
    institution: S2W Inc.
    last_name: Lee
    name: Yongjae Lee
    orcid: https://orcid.org/0009-0006-5838-1186
    semantic_scholar_id: https://www.semanticscholar.org/author/Yongjae-Lee/2109263592
    username: ~Yongjae_Lee1
  decision: toFindings
  end_page: 42
  file: 7.pdf
  id: 7
  num_pages: 14
  openreview_id: p6DV71uHvn
  pdf_file: 64660f3286cac9fc97e9ae9a4feccd6deb4f2e74.pdf
  start_page: 29
  title: 'Ignore Me But Don''t Replace Me: Utilizing Non-Linguistic Elements for Pretraining
    on the Cybersecurity Domain'
- abstract: "Existing dense retrieval systems utilize the same model architecture\
    \ for encoding both the passages and the queries, even though queries are much\
    \ shorter and simpler than passages. \nThis leads to high latency of the query\
    \ encoding, which is performed online and therefore might impact user experience.\
    \ \nWe show that combining a standard large passage encoder with a small efficient\
    \ query encoder can provide significant latency drops with only a small decrease\
    \ in quality. \nWe offer a pretraining and training solution for multiple small\
    \ query encoder architectures. \nUsing a small transformer architecture we are\
    \ able to decrease latency by up to $\\sim12\\times$, while $MRR@10$ on the MS\
    \ MARCO dev set only decreases from $38.2$ to $36.2$. \nIf this solution does\
    \ not reach the desired latency requirements, we propose an efficient RNN as the\
    \ query encoder, which processes the query prefix incrementally and only infers\
    \ the last word after the query is issued. \nThis shortens latency by $\\sim38\\\
    times$ with only a minor drop in quality, reaching $35.5$ $MRR@10$ score."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Retrieval and Text Mining
  authors:
  - dblp_id: https://dblp.org/pid/71/9893.html
    emails: '****@gmail.com'
    first_name: Nachshon
    institution: Amazon
    last_name: Cohen
    name: Nachshon Cohen
    orcid: https://orcid.org/my-orcid?orcid=0000-0001-8302-2739
    semantic_scholar_id: https://www.semanticscholar.org/author/Nachshon-Cohen/2065787365
    username: ~Nachshon_Cohen1
  - dblp_id: https://dblp.org/pid/230/8587
    emails: '****@gmail.com'
    first_name: Yaron
    institution: Amazon
    last_name: Fairstein
    name: Yaron Fairstein
    semantic_scholar_id: https://www.semanticscholar.org/author/Yaron-Fairstein/52223185
    username: ~Yaron_Fairstein1
  - dblp_id: https://dblp.org/pid/264/5003
    emails: '****@gmail.com'
    first_name: Guy
    institution: Amazon
    last_name: Kushilevitz
    name: Guy Kushilevitz
    semantic_scholar_id: https://www.semanticscholar.org/author/Guy-Kushilevitz/1667801612
    username: ~Guy_Kushilevitz1
  decision: toFindings
  end_page: 50
  file: 17.pdf
  id: 17
  num_pages: 8
  openreview_id: wK9qvzQw1K
  pdf_file: d9c34874be894dc63e551997b40f61c166887a49.pdf
  start_page: 43
  title: Extremely efficient online query encoding for dense retrieval
- abstract: 'Large Language Models (LLMs) have exhibited impressive generation capabilities,
    but they suffer from hallucinations when solely relying on their internal knowledge,
    especially when answering questions that require less commonly known information.
    Retrievalaugmented LLMs have emerged as a potential solution to ground LLMs in
    external knowledge. Nonetheless, recent approaches have primarily emphasized retrieval
    from unstructured text corpora, owing to its seamless integration into prompts.
    When using structured data such as knowledge graphs, most methods simplify it
    into natural text, neglecting the underlying structures. Moreover, a significant
    gap in the current landscape is the absence of a realistic benchmark for evaluating
    the effectiveness of grounding LLMs on heterogeneous knowledge sources (e.g.,
    knowledge base and text). To fill this gap, we have curated a comprehensive dataset
    that poses two unique challenges: (1) Two-hop multi-source questions that require
    retrieving information from both open-domain structured and unstructured knowledge
    sources; retrieving information from structured knowledge sources is a critical
    component in correctly answering the questions. (2) Generation of symbolic queries
    (e.g., SPARQL for Wikidata) is a key requirement, which adds another layer of
    challenge. Our dataset is created using a combination of automatic generation
    through predefined reasoning chains and human annotation. We also introduce a
    novel approach that leverages multiple retrieval tools, including text passage
    retrieval and symbolic language-assisted retrieval. Our model outperforms previous
    approaches by a significant margin, demonstrating its effectiveness in addressing
    the above-mentioned reasoning challenges.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Generation
  authors:
  - dblp_id: https://dblp.org/pid/41/10049-6.html
    emails: '****@gmail.com'
    first_name: Wenting
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=aySy_OMAAAAJ
    last_name: Zhao
    name: Wenting Zhao
    username: ~Wenting_Zhao4
  - dblp_id: https://dblp.org/pid/96/2615-6
    emails: '****@salesforce.com'
    first_name: Ye
    google_scholar_id: https://scholar.google.com/citations?user=QMKD6YMAAAAJ&hl=en
    institution: SalesForce.com
    last_name: Liu
    name: Ye Liu
    username: ~Ye_Liu4
  - dblp_id: https://dblp.org/pid/88/7733
    emails: '****@salesforce.com'
    first_name: Tong
    institution: Salesforce Research
    last_name: Niu
    name: Tong Niu
    username: ~Tong_Niu1
  - dblp_id: https://dblp.uni-trier.de/pid/167/0275
    emails: '****@hust.edu.cn'
    first_name: Yao
    google_scholar_id: https://scholar.google.com/citations?user=c3MtqtMAAAAJ&hl=en
    homepage: http://wanyao.me
    institution: Huazhong University of Science and Technology
    last_name: Wan
    name: Yao Wan
    username: ~Yao_Wan2
  - dblp_id: https://dblp.org/pid/y/PhilipSYu
    emails: '****@uic.edu'
    first_name: Philip
    google_scholar_id: https://scholar.google.com/citations?user=D0lL1r0AAAAJ&hl=zh-CN
    homepage: https://cs.uic.edu/profiles/philip-yu/
    institution: University of Illinois, Chicago
    last_name: Yu
    middle_name: S.
    name: Philip S. Yu
    orcid: https://orcid.org/0000-0002-3491-5968
    username: ~Philip_S._Yu1
  - dblp_id: https://dblp.org/pid/62/2078
    emails: '****@salesforce.com'
    first_name: Shafiq
    google_scholar_id: https://scholar.google.com/citations?user=hR249csAAAAJ&hl=en
    homepage: https://raihanjoty.github.io/
    institution: SalesForce.com and Nanyang Technological University
    last_name: Joty
    name: Shafiq Joty
    username: ~Shafiq_Joty1
  - dblp_id: http://dblp.uni-trier.de/pers/hd/z/Zhou:Yingbo
    emails: '****@salesforce.com'
    first_name: Yingbo
    google_scholar_id: https://scholar.google.com/citations?user=H_6RQ7oAAAAJ&hl=en
    institution: Salesforce Research
    last_name: Zhou
    name: Yingbo Zhou
    username: ~Yingbo_Zhou1
  - emails: '****@gmail.com'
    first_name: Semih
    google_scholar_id: https://scholar.google.com/citations?user=krh3p8AAAAAJ&hl=en
    institution: SalesForce.com
    last_name: Yavuz
    name: Semih Yavuz
    username: ~Semih_Yavuz1
  decision: toFindings
  end_page: 68
  file: 32.pdf
  id: 32
  num_pages: 18
  openreview_id: 5GU7iWePKD
  pdf_file: 1b69dbefd587da605e790d2e74544fe57a603dab.pdf
  start_page: 51
  title: 'DIVKNOWQA: Assessing the Reasoning Ability of LLMs via Open-Domain Question
    Answering over Knowledge Base and Text'
- abstract: Geometric knowledge graph embedding models (gKGEs) have shown great potential
    for knowledge graph completion (KGC), i.e., automatically predicting missing triples.
    However, contemporary gKGEs require high embedding dimensionalities or complex
    embedding spaces for good KGC performance, drastically limiting their space and
    time efficiency. Facing these challenges, we propose SpeedE, a lightweight Euclidean
    gKGE that (1) provides strong inference capabilities, (2) is competitive with
    state-of-the-art gKGEs, even significantly outperforming them on YAGO3-10 and
    WN18RR, and (3) dramatically increases their efficiency, in particular, needing
    solely a fifth of the training time and a fourth of the parameters of the state-of-the-art
    ExpressivE model on WN18RR to reach the same KGC performance.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Efficient/Low-Resource Methods for NLP
  authors:
  - dblp_id: https://dblp.org/pid/33/2524-2
    emails: '****@tuwien.ac.at'
    first_name: Aleksandar
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=N1GQTqkAAAAJ&view_op=list_works&gmla=AHoSzlXmAsBNTNeFBwf887dqSRjQQAjV23JE6AOjK-6Am9TP0Uv341Z5b7UA30NhFbsM6EjGGNYiHkWx-vMQonSW
    homepage: https://informatics.tuwien.ac.at/people/aleksandar-pavlovic
    last_name: "Pavlovi\u0107"
    name: "Aleksandar Pavlovi\u0107"
    orcid: https://orcid.org/0000-0001-6887-9515
    semantic_scholar_id: https://www.semanticscholar.org/author/Aleksandar-Pavlovic/94276658
    username: "~Aleksandar_Pavlovi\u01071"
  - dblp_id: https://dblp.org/pid/20/9117
    emails: '****@dbai.tuwien.ac.at'
    first_name: Emanuel
    institution: TU Wien (Vienna University of Technology)
    last_name: Sallinger
    name: Emanuel Sallinger
    username: ~Emanuel_Sallinger1
  decision: toFindings
  end_page: 92
  file: 38.pdf
  id: 38
  num_pages: 24
  openreview_id: 7pCiEDIHcj
  pdf_file: cbb125f605deaf3119521b98be7675ae2cc35407.pdf
  start_page: 69
  title: 'SpeedE: Euclidean Geometric Knowledge Graph Embedding Strikes Back'
- abstract: Real-world sequential decision making is characterized by sparse rewards
    and large decision spaces, posing significant difficulty for experiential learning
    systems like $\textit{tabula rasa}$ reinforcement learning (RL) agents. Large
    Language Models (LLMs), with a wealth of world knowledge, can help RL agents learn
    quickly and adapt to distribution shifts. In this work, we introduce Language
    Guided Exploration (LGE) framework, which uses a pre-trained language model (called
    GUIDE ) to provide decision-level guidance to an RL agent (called EXPLORER ).
    We observe that on ScienceWorld (Wang et al., 2022), a challenging text environment,
    LGE outperforms vanilla RL agents significantly and also outperforms other sophisticated
    methods like Behaviour Cloning and Text Decision Transformer.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - dblp_id: https://dblp.org/pid/230/3698
    emails: '****@umass.edu'
    first_name: Hitesh
    google_scholar_id: https://scholar.google.co.in/citations?user=negBG5QAAAAJ&hl=en
    homepage: https://hitzkrieg.github.io/website/
    last_name: Golchha
    name: Hitesh Golchha
    username: ~Hitesh_Golchha1
  - emails: '****@umass.edu'
    first_name: Sahil
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=9_nm8KgAAAAJ&scilu=&scisig=AOScLA0AAAAAZSjJYOUN5DbMyzHi6y1G_-Ynb2k&gmla=AJ1KiT0V7u-FsCF-u262-KWOQ4ORFug71F1ATqDoLuIeJtPmrkND-fMZ2Cm3vKQGfyVRNMHWQQ_oyTI2A-oZHAY5CPiEySDjikd_AO8&sciund=11516717298977621805
    homepage: https://ysahil97.github.io
    last_name: Yerawar
    name: Sahil Yerawar
    username: ~Sahil_Yerawar1
  - dblp_id: https://dblp.org/pid/274/7280
    emails: '****@cs.umass.edu'
    first_name: Dhruvesh
    google_scholar_id: https://scholar.google.com/citations?user=6F2CvwoAAAAJ
    homepage: http://dhruveshp.com
    institution: College of Information and Computer Science, University of Massachusetts,
      Amherst
    last_name: Patel
    name: Dhruvesh Patel
    orcid: https://orcid.org/0000-0003-3062-2292
    semantic_scholar_id: https://www.semanticscholar.org/author/Dhruvesh-Patel/152206573
    username: ~Dhruvesh_Patel1
  - dblp_id: https://dblp.org/pid/181/9448
    emails: '****@seas.upenn.edu'
    first_name: Soham
    google_scholar_id: https://scholar.google.com/citations?user=nOsmu8UAAAAJ&hl=en&oi=ao
    homepage: https://sdan2.github.io/
    institution: International Business Machines
    last_name: Dan
    name: Soham Dan
    username: ~Soham_Dan1
  - dblp_id: https://dblp.org/pid/178/2877
    emails: '****@gmail.com'
    first_name: Keerthiram
    institution: International Business Machines
    last_name: Murugesan
    name: Keerthiram Murugesan
    username: ~Keerthiram_Murugesan1
  decision: toFindings
  end_page: 102
  file: 41.pdf
  id: 41
  num_pages: 10
  openreview_id: FrY8OdyqP3
  pdf_file: 3743367193de4fc0e75000d30c2f114be81b466c.pdf
  start_page: 93
  title: Language Guided Exploration for RL Agents in Text Environments
- abstract: "The Uniform Information Density (UID) principle posits that humans prefer\
    \ to spread information evenly during language production. We examine if this\
    \ UID principle can help capture differences between Large Language Models (LLMs)-generated\
    \ and human-generated texts. We propose GPT-who, the first psycholinguistically-inspired\
    \ domain-agnostic statistical detector. This detector employs UID-based features\n\
    to model the unique statistical signature of each LLM and human author for accurate\
    \ detection. \nWe evaluate our method using 4 large-scale benchmark datasets and\
    \ find that GPT-who outperforms state-of-the-art detectors (both statistical-\
    \ \\& non-statistical) such as GLTR, GPTZero, DetectGPT, OpenAI detector, and\
    \ ZeroGPT by over $20$\\% across domains.\nIn addition to better performance,\
    \ \nit is computationally inexpensive and utilizes an interpretable representation\
    \ of text articles. We find that GPT-who can distinguish texts generated by very\
    \ sophisticated LLMs, even when the overlying text is indiscernible.\nUID-based\
    \ measures for all datasets and code are available at https://github.com/saranya-venkatraman/gpt-who."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Linguistic theories, Cognitive Modeling and Psycholinguistics
  authors:
  - emails: '****@psu.edu'
    first_name: Saranya
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=qrvxwt4AAAAJ
    institution: Pennsylvania State University
    last_name: Venkatraman
    name: Saranya Venkatraman
    username: ~Saranya_Venkatraman1
  - dblp_id: https://dblp.uni-trier.de/pid/244/0488
    emails: '****@gmail.com'
    first_name: Adaku
    google_scholar_id: https://scholar.google.com/citations?user=A4be1l4AAAAJ&hl=en
    homepage: https://adauchendu.github.io/
    institution: MIT Lincoln Laboratory, Massachusetts Institute of Technology
    last_name: Uchendu
    name: Adaku Uchendu
    semantic_scholar_id: https://www.semanticscholar.org/search?q=%22Adaku%20Uchendu%22&sort=relevance
    username: ~Adaku_Uchendu1
  - dblp_id: https://dblp.org/pid/l/DongwonLee
    emails: '****@psu.edu'
    first_name: Dongwon
    google_scholar_id: https://scholar.google.com/citations?user=MzL-WnEAAAAJ&hl=en
    homepage: http://pike.psu.edu/dongwon
    institution: The Pennsylvania State University
    last_name: Lee
    name: Dongwon Lee
    username: ~Dongwon_Lee1
  decision: toFindings
  end_page: 115
  file: 42.pdf
  id: 42
  num_pages: 13
  openreview_id: i5OFmyTWJq
  pdf_file: b3225a2b0e491e8196f8f837807372e8db17edac.pdf
  start_page: 103
  title: 'GPT-who: An Information Density-based Machine-Generated Text Detector'
- abstract: Encoder-decoder transformer models have achieved great success on various
    vision-language (VL) and language tasks, but they suffer from high inference latency.
    Typically, the decoder takes up most of the latency because of the auto-regressive
    decoding. To accelerate the inference, we propose an approach of performing Dynamic
    Early Exit on Decoder (DEED). We build a multi-exit encoder-decoder transformer
    model which is trained with deep supervision so that each of its decoder layers
    is capable of generating plausible predictions. In addition, we leverage simple
    yet practical techniques, including shared generation head and adaptation modules,
    to keep accuracy when exiting at shallow decoder layers. Based on the multi-exit
    model, we perform step-level dynamic early exit during inference, where the model
    may decide to use fewer decoder layers based on its confidence of the current
    layer at each individual decoding step. Considering different number of decoder
    layers may be used at different decoding steps, we compute deeper-layer decoder
    features of previous decoding steps just-in-time, which ensures the features from
    different decoding steps are semantically aligned. We evaluate our approach with
    three state-of-the-art encoder-decoder transformer models on various VL and language
    tasks. We show our approach can reduce overall inference latency by 20%-74% with
    comparable or even higher accuracy compared to baselines.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multimodality and Language Grounding to Vision, Robotics and Beyond
  authors:
  - emails: '****@gmail.com'
    first_name: Peng
    google_scholar_id: https://scholar.google.com/citations?user=h_oYR-IAAAAJ&hl=en
    homepage: http://pengtang.xyz/
    institution: Amazon
    last_name: Tang
    name: Peng Tang
    username: ~Peng_Tang1
  - dblp_id: https://dblp.org/pers/z/Zhu:Pengkai.html
    emails: '****@bu.edu'
    first_name: Pengkai
    google_scholar_id: https://scholar.google.com/citations?user=cWvu-I8AAAAJ&hl=en
    institution: Boston University
    last_name: Zhu
    name: Pengkai Zhu
    username: ~Pengkai_Zhu2
  - emails: '****@ucsd.edu'
    first_name: Tian
    homepage: https://hikaru-nara.github.io
    last_name: Li
    name: Tian Li
    username: ~Tian_Li2
  - dblp_id: https://dblp.org/pid/206/7473
    emails: '****@amazon.com'
    first_name: Srikar
    google_scholar_id: https://scholar.google.com/citations?user=KQLmaxgAAAAJ&hl=en
    institution: Amazon
    last_name: Appalaraju
    name: Srikar Appalaraju
    semantic_scholar_id: https://www.semanticscholar.org/author/Srikar-Appalaraju/26316634
    username: ~Srikar_Appalaraju2
  - dblp_id: https://dblp.org/pid/43/2321
    emails: '****@amazon.com'
    first_name: Vijay
    google_scholar_id: https://scholar.google.com/citations?user=n9fRgvkAAAAJ&hl=en
    institution: Amazon
    last_name: Mahadevan
    name: Vijay Mahadevan
    username: ~Vijay_Mahadevan3
  - dblp_id: https://dblp.org/pid/m/RManmatha
    emails: '****@cs.umass.edu'
    first_name: R.
    google_scholar_id: https://scholar.google.com/citations?user=_0aMq28AAAAJ&hl=en&oi=ao
    homepage: https://www.cics.umass.edu/faculty/directory/manmatha_r
    institution: Amazon
    last_name: Manmatha
    name: R. Manmatha
    username: ~R._Manmatha1
  decision: toFindings
  end_page: 131
  file: 57.pdf
  id: 57
  num_pages: 16
  openreview_id: CMcU9P3LJK
  pdf_file: 915ca95be5d5a77685caa20a4ced27b1f3455254.pdf
  start_page: 116
  title: 'DEED: Dynamic Early Exit on Decoder for Accelerating Encoder-Decoder Transformer
    Models'
- abstract: "An ideal length-extrapolatable Transformer language model can handle\
    \ sequences longer than the training length without any fine-tuning. \nSuch long-context\
    \ utilization capability relies heavily on a flexible positional embedding design.\
    \ \nUpon investigating the flexibility of existing large pre-trained Transformer\
    \ language models, we find that the T5 family deserves a closer look, as its positional\
    \ embeddings capture rich and flexible attention patterns. \nHowever, T5 suffers\
    \ from the dispersed attention issue: the longer the input sequence, the flatter\
    \ the attention distribution. To alleviate the issue, we propose two attention\
    \ alignment strategies via temperature scaling. \nOur findings show improvement\
    \ on the long-context utilization capability of T5 on language modeling, retrieval,\
    \ multi-document question answering, and code completion tasks without any fine-tuning.\
    \ This suggests that a flexible positional embedding design and attention alignment\
    \ can go a long way toward Transformer length extrapolation. The code is released\
    \ at: \\url{https://github.com/chijames/T5-Attention-Alignment}"
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - dblp_id: https://dblp.org/pid/207/7824
    emails: '****@andrew.cmu.edu'
    first_name: Ta-Chung
    google_scholar_id: https://scholar.google.com.tw/citations?user=ZqpdQOoAAAAJ&hl=en
    institution: Carnegie Mellon University
    last_name: Chi
    name: Ta-Chung Chi
    semantic_scholar_id: https://www.semanticscholar.org/author/Ta-Chung-Chi/27531332
    username: ~Ta-Chung_Chi1
  - dblp_id: https://dblp.org/pid/213/0948
    emails: '****@alumni.princeton.edu'
    first_name: Ting-Han
    google_scholar_id: https://scholar.google.com/citations?user=1mQ3kTEAAAAJ&hl=en
    last_name: Fan
    name: Ting-Han Fan
    semantic_scholar_id: https://www.semanticscholar.org/author/Ting-Han-Fan/32037089
    username: ~Ting-Han_Fan1
  - dblp_id: https://dblp.org/pid/29/5401
    emails: '****@cs.cmu.edu'
    first_name: Alexander
    google_scholar_id: https://scholar.google.com/citations?user=axOnEnQAAAAJ&hl=en
    homepage: http://www.cs.cmu.edu/~air/
    institution: Carnegie Mellon University and Carnegie Mellon University
    last_name: Rudnicky
    name: Alexander Rudnicky
    orcid: https://orcid.org/0000-0003-3896-9397
    semantic_scholar_id: https://www.semanticscholar.org/author/A.-Rudnicky/3156164
    username: ~Alexander_Rudnicky1
  decision: toFindings
  end_page: 148
  file: 63.pdf
  id: 63
  num_pages: 17
  openreview_id: 9QxcLrRDrn
  pdf_file: 1077be077a8c11c101b9561bd9b9759061d92fb3.pdf
  start_page: 132
  title: Attention Alignment and Flexible Positional Embeddings Improve Transformer
    Length Extrapolation
- abstract: Alignment serves as an important step to steer large language models (LLMs)
    towards human preferences. In this paper, we propose an automatic way to construct
    contrastive data for LLM, using preference pairs from multiple models of varying
    strengths (e.g., InstructGPT, ChatGPT and GPT-4). We compare the contrastive techniques
    of SLiC and DPO to SFT baselines and find that DPO provides a step-function improvement
    even after continuing SFT saturates. We also explore a data curriculum learning
    scheme for contrastive post-training, which starts by learning from "easier" pairs
    and transitioning to "harder" ones, which further improves alignment. Finally,
    we scale up our experiments to train with more data and larger models like Orca.
    Remarkably, our automatic contrastive post-training further improves the performance
    of Orca, already a state-of-the-art instruction learning model tuned with GPT-4
    outputs, to outperform ChatGPT.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Efficient/Low-Resource Methods for NLP
  authors:
  - dblp_id: https://dblp.org/pid/234/8009
    emails: '****@ucsd.edu'
    first_name: Canwen
    google_scholar_id: https://scholar.google.com/citations?user=oopKCDMAAAAJ
    homepage: https://canwenxu.net
    last_name: Xu
    name: Canwen Xu
    username: ~Canwen_Xu1
  - emails: '****@aol.com'
    first_name: Corby
    google_scholar_id: https://scholar.google.com/citations?user=Y2YBgCsAAAAJ&hl=en
    homepage: http://corbyrosset.com/
    last_name: Rosset
    name: Corby Rosset
    username: ~Corby_Rosset2
  - emails: '****@cs.washington.edu'
    first_name: Ethan
    google_scholar_id: https://scholar.google.com/citations?user=oL7MlaIAAAAJ&hl=en&oi=ao
    homepage: https://echau18.gitlab.io
    institution: Microsoft
    last_name: Chau
    middle_name: C.
    name: Ethan C. Chau
    semantic_scholar_id: https://www.semanticscholar.org/author/Ethan-C.-Chau/1974400874
    username: ~Ethan_C._Chau1
  - dblp_id: https://dblp.org/pid/127/0394
    emails: '****@delcorro.ai'
    first_name: Luciano
    google_scholar_id: https://scholar.google.de/citations?user=vJfDxrIAAAAJ&hl=en
    homepage: http://people.mpi-inf.mpg.de/~corrogg/
    institution: Microsoft Research
    last_name: Corro
    middle_name: Del
    name: Luciano Del Corro
    semantic_scholar_id: https://www.semanticscholar.org/author/Luciano-Del-Corro/1875906
    username: ~Luciano_Del_Corro1
  - emails: '****@microsoft.com'
    first_name: Shweti
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=e_XZNYgAAAAJ
    institution: Microsoft
    last_name: Mahajan
    name: Shweti Mahajan
    username: ~Shweti_Mahajan1
  - dblp_id: https://dblp.org/pid/29/3483
    emails: '****@cs.ucsd.edu'
    first_name: Julian
    google_scholar_id: https://scholar.google.com/citations?user=icbo4M0AAAAJ&hl=en
    homepage: http://cseweb.ucsd.edu/~jmcauley/
    institution: University of California, San Diego, University of California, San
      Diego
    last_name: McAuley
    name: Julian McAuley
    username: ~Julian_McAuley1
  - dblp_id: https://dblp.org/pid/n/JenniferNeville
    emails: '****@purdue.edu'
    first_name: Jennifer
    institution: Purdue University and Purdue University
    last_name: Neville
    name: Jennifer Neville
    username: ~Jennifer_Neville1
  - dblp_id: https://dblp.org/pid/147/9148
    emails: '****@microsoft.com'
    first_name: Ahmed
    google_scholar_id: https://scholar.google.com/citations?user=sNGk-9MAAAAJ&hl=en
    homepage: https://www.microsoft.com/en-us/research/people/hassanam/publications/
    institution: Microsoft Research
    last_name: Awadallah
    middle_name: Hassan
    name: Ahmed Hassan Awadallah
    username: ~Ahmed_Hassan_Awadallah1
  - dblp_id: https://dblp.org/pid/168/8201
    emails: '****@gmail.com'
    first_name: Nikhil
    google_scholar_id: https://scholar.google.com/citations?user=GhqD_rwAAAAJ&hl=en
    institution: Microsoft
    last_name: Rao
    name: Nikhil Rao
    username: ~Nikhil_Rao1
  decision: toFindings
  end_page: 162
  file: 64.pdf
  id: 64
  num_pages: 14
  openreview_id: udVI9TTa0q
  pdf_file: 465668c725d0654487c28423dff2f78c1b60f706.pdf
  start_page: 149
  title: Automatic Pair Construction for Contrastive Post-training
- abstract: Fact-checking is an essential task in NLP that is commonly utilized to
    validate the factual accuracy of a piece of text. Previous approaches mainly involve
    the resource-intensive process of fine-tuning pre-trained language models on specific
    datasets. In addition, there is a notable gap in datasets that focus on fact-checking
    texts generated by large language models (LLMs). In this paper, we introduce Self-Checker,
    a plug-and-play framework that harnesses LLMs for efficient and rapid fact-checking
    in a few-shot manner. We also present the BingCheck dataset, specifically designed
    for fact-checking texts generated by LLMs. Empirical results demonstrate the potential
    of Self-Checker in the use of LLMs for fact-checking. Compared to state-of-the-art
    fine-tuned models, there is still significant room for improvement, indicating
    that adopting LLMs could be a promising direction for future fact-checking research.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - emails: '****@iastate.edu'
    first_name: Miaoran
    institution: Iowa State University
    last_name: Li
    name: Miaoran Li
    orcid: https://orcid.org/0000-0003-2445-5731
    username: ~Miaoran_Li1
  - dblp_id: https://dblp.org/pid/144/2759
    emails: '****@gmail.com'
    first_name: Baolin
    google_scholar_id: https://scholar.google.com/citations?user=u1CNjgwAAAAJ&hl=zh-CN
    last_name: Peng
    name: Baolin Peng
    username: ~Baolin_Peng2
  - dblp_id: https://dblp.org/pid/05/3289
    emails: '****@acm.org'
    first_name: Michel
    google_scholar_id: https://scholar.google.com/citations?user=rs1M7CAAAAAJ&hl=en
    homepage: http://research.microsoft.com/~mgalley
    institution: Microsoft
    last_name: Galley
    name: Michel Galley
    orcid: https://orcid.org/0000-0002-3310-1831
    semantic_scholar_id: https://www.semanticscholar.org/author/Michel-Galley/1947267
    username: ~Michel_Galley1
  - dblp_id: https://dblp.org/pid/92/5339
    emails: '****@microsoft.com'
    first_name: Jianfeng
    homepage: https://www.microsoft.com/en-us/research/people/jfgao/
    institution: Microsoft Research
    last_name: Gao
    name: Jianfeng Gao
    username: ~Jianfeng_Gao1
  - emails: '****@uri.edu'
    first_name: Zhu
    last_name: Zhang
    name: Zhu Zhang
    username: ~Zhu_Zhang5
  decision: toFindings
  end_page: 181
  file: 70.pdf
  id: 70
  num_pages: 19
  openreview_id: uLC1AGRQxq
  pdf_file: 2defb5f64a1f91906a5a8ce117e4912ca5aa8797.pdf
  start_page: 163
  title: 'Self-Checker: Plug-and-Play Modules for Fact-Checking with Large Language
    Models'
- abstract: Morphological modeling in neural machine translation (NMT) is a promising
    approach to achieving open-vocabulary machine translation for morphologically-rich
    languages. However, existing methods such as sub-word tokenization and character-based
    models are limited to the surface forms of the words. In this work, we propose
    a framework-solution for modeling complex morphology in low-resource settings.
    A two-tier transformer architecture is chosen to encode morphological information
    at the inputs. At the target-side output, a multi-task multi-label training scheme
    coupled with a beam search-based decoder are found to improve machine translation
    performance. An attention augmentation scheme to the transformer model is proposed
    in a generic form to allow integration of pre-trained language models and also
    facilitate modeling of word order relationships between the source and target
    languages. Several data augmentation techniques are evaluated and shown to increase
    translation performance in low-resource settings. We evaluate our proposed solution
    on Kinyarwanda $\leftrightarrow$ English translation using public-domain parallel
    text. Our final models achieve competitive performance in relation to large multi-lingual
    models. We hope that our results will motivate more use of explicit morphological
    information and the proposed model and data augmentations in low-resource NMT.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Translation
  authors:
  - dblp_id: https://dblp.org/pid/278/2652
    emails: '****@umass.edu'
    first_name: Antoine
    google_scholar_id: https://scholar.google.com/citations?user=NESE5TIAAAAJ
    last_name: Nzeyimana
    name: Antoine Nzeyimana
    orcid: https://orcid.org/0000-0003-4567-3471
    semantic_scholar_id: https://www.semanticscholar.org/author/Antoine-Nzeyimana/51994077
    username: ~Antoine_Nzeyimana1
  decision: toFindings
  end_page: 195
  file: 77.pdf
  id: 77
  num_pages: 14
  openreview_id: SPYqKyE4eU
  pdf_file: 2b29d50fb8593852f476b4340d3c0e620ca28cef.pdf
  start_page: 182
  title: Low-resource neural machine translation with morphological modeling
- abstract: To achieve state-of-the-art performance, one still needs to train NER
    models on large-scale, high-quality annotated data, an asset that is both costly
    and time-intensive to accumulate. In contrast, real-world applications often resort
    to massive low-quality labeled data through non-expert annotators via crowdsourcing
    and external knowledge bases via distant supervision as a cost-effective alternative.
    However, these annotation methods result in noisy labels, which in turn lead to
    a notable decline in performance. Hence, we propose to denoise the noisy NER data
    with guidance from a small set of clean instances.  Along with the main NER model
    we train a discriminator model and use its outputs to recalibrate the sample weights.
    The discriminator is capable of detecting both span and category errors with different
    discriminative prompts. Results on public crowdsourcing and distant supervision
    datasets show that the proposed method can consistently improve performance with
    a small guidance set.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Extraction
  authors:
  - dblp_id: https://dblp.org/pid/236/6321
    emails: '****@virginia.edu'
    first_name: Zhendong
    homepage: https://zdchu.github.io/
    institution: University of Virginia
    last_name: Chu
    name: Zhendong Chu
    username: ~Zhendong_Chu1
  - dblp_id: https://dblp.org/pid/08/7975
    emails: '****@gmail.com'
    first_name: Ruiyi
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=-seCWbAAAAAJ
    homepage: http://zhangry868.github.io/
    institution: Adobe Systems
    last_name: Zhang
    name: Ruiyi Zhang
    username: ~Ruiyi_Zhang3
  - dblp_id: https://dblp.org/pid/32/1593-1
    emails: '****@gmail.com'
    first_name: Tong
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=6-ARmXsAAAAJ&view_op=list_works&sortby=pubdate
    institution: Adobe Research
    last_name: Yu
    name: Tong Yu
    semantic_scholar_id: https://www.semanticscholar.org/author/Tong-Yu/1500399016
    username: ~Tong_Yu3
  - emails: '****@adobe.com'
    first_name: Rajiv
    google_scholar_id: https://scholar.google.com/
    last_name: Jain
    name: Rajiv Jain
    username: ~Rajiv_Jain1
  - dblp_id: https://dblp.org/pid/27/6671
    emails: '****@adobe.com'
    first_name: Vlad
    google_scholar_id: https://scholar.google.com/citations?user=oyWpVa8AAAAJ&hl=en
    homepage: https://research.adobe.com/person/vlad-morariu/
    institution: Adobe
    last_name: Morariu
    middle_name: I
    name: Vlad I Morariu
    semantic_scholar_id: https://www.semanticscholar.org/author/Vlad-I.-Morariu/2852035
    username: ~Vlad_I_Morariu1
  - dblp_id: https://dblp.uni-trier.de/pers/g/Gu:Jiuxiang.html
    emails: '****@adobe.com'
    first_name: Jiuxiang
    google_scholar_id: https://scholar.google.com.sg/citations?user=zPxKV9EAAAAJ&hl=en
    homepage: http://gujiuxiang.com
    institution: Adobe Systems
    last_name: Gu
    name: Jiuxiang Gu
    username: ~Jiuxiang_Gu2
  - dblp_id: https://dblp.org/pid/58/896
    emails: '****@adobe.com'
    first_name: Ani
    google_scholar_id: https://scholar.google.com/citations?user=vXQdb5kAAAAJ&hl=en
    institution: Adobe Research
    last_name: Nenkova
    name: Ani Nenkova
    semantic_scholar_id: https://www.semanticscholar.org/author/A.-Nenkova/3115414
    username: ~Ani_Nenkova1
  decision: toFindings
  end_page: 210
  file: 78.pdf
  id: 78
  num_pages: 15
  openreview_id: yXAiHVPlpu
  pdf_file: cdeb01fd47445b3d2d6cf850041a88b2ccd44abe.pdf
  start_page: 196
  title: 'Self-Cleaning: Improving a Named Entity Recognizer Trained on Noisy Data
    with a Few Clean Instances'
- abstract: The success of Natural Language Understanding (NLU) benchmarks in various
    languages, such as GLUE for English, CLUE for Chinese, KLUE for Korean, and IndoNLU
    for Indonesian, has facilitated the evaluation of new NLU models across a wide
    range of tasks. To establish a standardized set of benchmarks for Vietnamese NLU,
    we introduce the first Vietnamese Language Understanding Evaluation (VLUE) benchmark.
    The VLUE benchmark encompasses five datasets covering different NLU tasks, including
    text classification, span extraction, and natural language understanding. To provide
    an insightful overview of the current state of Vietnamese NLU, we then evaluate
    seven state-of-the-art pre-trained models, including both multilingual and Vietnamese
    monolingual models, on our proposed VLUE benchmark. Furthermore, we present CafeBERT,
    a new state-of-the-art pre-trained model that achieves superior results across
    all tasks in the VLUE benchmark. Our model combines the proficiency of a multilingual
    pre-trained model with Vietnamese linguistic knowledge. CafeBERT is developed
    based on the XLM-RoBERTa model, with an additional pretraining step utilizing
    a significant amount of Vietnamese textual data to enhance its adaptation to the
    Vietnamese language. For the purpose of future research, CafeBERT is made publicly
    available for research purposes.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - dblp_id: https://dblp.org/pid/293/6723
    emails: '****@gmail.com'
    first_name: Phong
    google_scholar_id: https://scholar.google.com/citations?authuser=1&user=SMzQsQMAAAAJ
    institution: The UIT NLP Group and Zalo
    last_name: Do
    middle_name: Nguyen-Thuan
    name: Phong Nguyen-Thuan Do
    username: ~Phong_Nguyen-Thuan_Do1
  - dblp_id: https://dblp.org/pid/317/1208
    emails: '****@denison.edu'
    first_name: Son
    google_scholar_id: https://scholar.google.com/citations?user=De7jXQUAAAAJ&hl=en
    homepage: https://sontran.vercel.app
    last_name: Tran
    middle_name: Quoc
    name: Son Quoc Tran
    semantic_scholar_id: https://www.semanticscholar.org/author/Son-Quoc-Tran/2066625858
    username: ~Son_Quoc_Tran1
  - emails: '****@gm.uit.edu.vn'
    first_name: Phu
    google_scholar_id: https://scholar.google.com/citations?user=dlVDIrsAAAAJ&hl=en
    last_name: Hoang
    middle_name: Gia
    name: Phu Gia Hoang
    semantic_scholar_id: https://www.semanticscholar.org/author/Phu-Gia-Hoang/2077433112
    username: ~Phu_Gia_Hoang1
  - dblp_id: https://dblp.org/pid/174/4526
    emails: '****@uit.edu.vn'
    first_name: Kiet
    google_scholar_id: https://scholar.google.com.vn/citations?user=v3RSwOkAAAAJ&hl=en
    homepage: https://sites.google.com/uit.edu.vn/kietnv
    institution: University of Information Technology, VNU-HCM
    last_name: Nguyen
    middle_name: Van
    name: Kiet Van Nguyen
    orcid: https://orcid.org/0000-0002-8456-2742
    semantic_scholar_id: https://www.semanticscholar.org/author/Kiet-Van-Nguyen/10346850
    username: ~Kiet_Van_Nguyen1
  - emails: '****@uit.edu.vn'
    first_name: Ngan
    google_scholar_id: https://scholar.google.com/schhp?hl=vi
    homepage: http://nlp.uit.edu.vn/
    last_name: Nguyen
    middle_name: Luu-Thuy
    name: Ngan Luu-Thuy Nguyen
    orcid: https://orcid.org/0000-0003-3931-849X
    username: ~Ngan_Luu-Thuy_Nguyen1
  decision: toFindings
  end_page: 222
  file: 83.pdf
  id: 83
  num_pages: 12
  openreview_id: uTl4iNHgHo
  pdf_file: 075ea376e13542aa6e22ae9f1a6625315d182999.pdf
  start_page: 211
  title: 'VLUE: A New Benchmark and Multi-task Knowledge Transfer Learning for Vietnamese
    Natural Language Understanding'
- abstract: "Fine-tuning pre-trained language models (LMs) is essential for enhancing\
    \ their capabilities.\nExisting techniques commonly fine-tune on input-output\
    \ pairs (e.g., instruction tuning) or with numerical rewards that gauge the output\
    \ quality (e.g., RLHF). We explore LMs' potential to **le**arn from **t**extual\
    \ **i**nteractions (**LETI**) that not only check their correctness with *binary\
    \ labels* but also pinpoint and explain errors in their outputs through *textual\
    \ feedback*.\nOur focus is the code generation task, where the model produces\
    \ code based on natural language instructions. \nThis setting invites a natural\
    \ and scalable way to acquire textual feedback: the error messages and stack traces\
    \ from code execution using a Python interpreter. LETI iteratively fine-tunes\
    \ the model, using the LM objective, on a concatenation of natural language instructions,\
    \ LM-generated programs, and textual feedback. Prepended to this fine-tuning text,\
    \ a binary reward token is used to differentiate correct and buggy solutions.\n\
    LETI requires *no* ground-truth outputs for training and even outperforms a fine-tuned\
    \ baseline that does. \nLETI not only improves the performance of LMs on a code\
    \ generation dataset MBPP, but also generalizes to other datasets. \nTrained on\
    \ MBPP, it achieves comparable or better performance than the base LMs on unseen\
    \ problems in HumanEval. \nFurthermore, compared to binary feedback, we observe\
    \ that textual feedback leads to improved generation quality and sample efficiency,\
    \ achieving the same performance with fewer than half of the gradient steps.\n\
    LETI is equally applicable in natural language tasks when they can be formulated\
    \ as code generation, which we empirically verified on event argument extraction."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - dblp_id: https://dblp.org/pid/264/9892
    emails: '****@illinois.edu'
    first_name: Xingyao
    google_scholar_id: https://scholar.google.com/citations?user=F7qq3YcAAAAJ&hl=en
    homepage: https://xingyaoww.github.io/
    institution: Department of Computer Science, University of Illinois Urbana-Champaign
    last_name: Wang
    name: Xingyao Wang
    orcid: https://orcid.org/0000-0002-3483-8624
    semantic_scholar_id: https://www.semanticscholar.org/author/Xingyao-Wang/2144803999
    username: ~Xingyao_Wang1
  - emails: '****@illinois.edu'
    first_name: Hao
    google_scholar_id: https://scholar.google.com/citations?view_op=list_works&hl=en&user=sl4q0WkAAAAJ
    homepage: https://haopeng-nlp.github.io/
    institution: Department of Computer Science,  University of Illinois Urbana-Champaign
    last_name: Peng
    name: Hao Peng
    username: ~Hao_Peng4
  - dblp_id: https://dblp.org/pid/18/10505
    emails: '****@illinois.edu'
    first_name: Reyhaneh
    google_scholar_id: https://scholar.google.com/citations?user=9gmW8MYAAAAJ&hl=en
    homepage: https://reyhaneh.cs.illinois.edu/
    institution: Department of Computer Science
    last_name: Jabbarvand
    name: Reyhaneh Jabbarvand
    username: ~Reyhaneh_Jabbarvand1
  - emails: '****@illinois.edu'
    first_name: Heng
    google_scholar_id: https://scholar.google.com/citations?user=z7GCqT4AAAAJ&hl=en
    homepage: http://blender.cs.illinois.edu/hengji.html
    institution: University of Illinois, Urbana-Champaign
    last_name: Ji
    name: Heng Ji
    username: ~Heng_Ji3
  decision: toFindings
  end_page: 239
  file: 84.pdf
  id: 84
  num_pages: 17
  openreview_id: U5EgAamZWZ
  pdf_file: f62c0f5f85c3042977ea7a1ad0e9cdef1bd08566.pdf
  start_page: 223
  title: 'LETI: Learning to Generate from Textual Interactions'
- abstract: The pre-trained language model (PLM) has achieved significant success
    in the field of knowledge graph completion (KGC) by effectively modeling entity
    and relation descriptions. In recent studies, the research in this field has been
    categorized into methods based on word matching and sentence matching, with the
    former significantly lags behind. However, there is a critical issue in word matching
    methods, which is that these methods fail to obtain satisfactory single embedding
    representations for entities.To address this issue and enhance entity representation,
    we propose the Bilateral Masking with prompt for Knowledge Graph Completion (BMKGC)
    approach.Our methodology employs prompts to narrow the distance between the predicted
    entity and the known entity. Additionally, the BMKGC model incorporates a bi-encoder
    architecture, enabling simultaneous predictions at both the head and tail. Furthermore,
    we propose a straightforward technique to augment positive samples, mitigating
    the problem of degree bias present in knowledge graphs and thereby improving the
    model's robustness. Experimental results conclusively demonstrate that BMKGC achieves
    state-of-the-art performance on the WN18RR dataset.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Extraction
  authors:
  - emails: '****@stu.ahu.edu.cn'
    first_name: Yonghui
    homepage: https://github.com/Everyth1ng-kyh
    last_name: Kong
    name: yonghui kong
    username: ~yonghui_kong1
  - dblp_id: https://dblp.uni-trier.de/pid/240/7820
    emails: '****@ahu.edu.cn'
    first_name: Cunhang
    google_scholar_id: https://scholar.google.com/citations?user=QbnlF74AAAAJ&hl=zh-CN
    institution: 'School of Computer Science and Technology, Anhui University, Hefei
      230601, China '
    last_name: Fan
    name: Cunhang Fan
    semantic_scholar_id: https://www.semanticscholar.org/author/Cunhang-Fan/117639306
    username: ~Cunhang_Fan1
  - emails: '****@stu.ahu.edu.cn'
    first_name: Yujie
    homepage: https://github.com/Eutopun
    last_name: Chen
    name: Yujie Chen
    username: ~Yujie_Chen1
  - dblp_id: https://dblp.org/pid/71/208-14.html
    emails: '****@mail.tsinghua.edu.cn'
    first_name: Shuai
    last_name: Zhang
    name: Shuai Zhang
    semantic_scholar_id: https://www.semanticscholar.org/author/Shuai-Zhang/2108432483?sort=influence
    username: ~Shuai_Zhang21
  - emails: '****@ahu.edu.cn'
    first_name: Zhao
    homepage: https://ieeexplore.ieee.org/author/37532072800
    institution: School of Computer Science and Technology, Anhui University, Hefei
      230601, China
    last_name: Lv
    name: Zhao Lv
    username: ~Zhao_Lv1
  - dblp_id: https://dblp.org/pid/46/2916
    emails: '****@tsinghua.edu.cn'
    first_name: Jianhua
    institution: Tsinghua University
    last_name: Tao
    name: Jianhua Tao
    username: ~Jianhua_Tao1
  decision: toFindings
  end_page: 249
  file: 86.pdf
  id: 86
  num_pages: 10
  openreview_id: K6mCHNKVK5
  pdf_file: 70340d4a5ba9499fce1905a86a9dc580105ec497.pdf
  start_page: 240
  title: Bilateral Masking with prompt for Knowledge Graph Completion
- abstract: Generative language models are usually pre-trained on large text corpus
    via predicting the next token (i.e., sub-word/word/phrase) given the previous
    ones. Recent works have demonstrated the impressive performance of large generative
    language models on downstream tasks. However, existing generative language models
    generally neglect an inherent challenge in text corpus during training, i.e.,
    the imbalance between frequent tokens and infrequent ones. It can lead a language
    model to be dominated by common and easy-to-learn tokens, thereby overlooking
    the infrequent and difficult-to-learn ones. To alleviate that, we propose a **MiLe
    Loss** function for **mi**tigating the bias of **le**arning difficulties with
    tokens.  During training, it can dynamically assess the learning difficulty of
    a to-be-learned token, according to the information entropy of the corresponding
    predicted probability distribution over the vocabulary. Then it scales the training
    loss adaptively, trying to lead the model to focus more on the difficult-to-learn
    tokens. On the Pile dataset, we train generative language models at different
    scales of 468M, 1.2B, and 6.7B parameters. Experiments reveal that models incorporating
    the proposed MiLe Loss can gain consistent performance improvement on downstream
    benchmarks.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Learning for NLP
  authors:
  - emails: '****@iie.ac.cn'
    first_name: Zhenpeng
    homepage: https://github.com/Saggressive
    last_name: Su
    name: Zhenpeng Su
    username: ~Zhenpeng_Su1
  - dblp_id: https://dblp.org/pid/78/9911
    emails: '****@tsinghua.org.cn'
    first_name: Zijia
    google_scholar_id: https://scholar.google.com/citations?user=ghUYrHkAAAAJ&hl=zh-CN&oi=ao
    homepage: https://sites.google.com/site/linzijia72/
    institution: Kuaishou Technology
    last_name: Lin
    name: Zijia Lin
    username: ~Zijia_Lin1
  - emails: '****@gmail.com'
    first_name: Baixue
    homepage: https://scholar.google.com/citations?user=dLNRRc8AAAAJ&hl=zh-CN
    last_name: Baixue
    name: Baixue
    username: ~Baixue1
  - emails: '****@gmail.com'
    first_name: Hui
    google_scholar_id: https://scholar.google.com/citations?user=erpvWcIAAAAJ&hl=zh-CN
    homepage: https://huichen24.github.io/
    institution: Tsinghua University, Tsinghua University
    last_name: Chen
    name: Hui Chen
    username: ~Hui_Chen7
  - emails: '****@iie.ac.cn'
    first_name: Songlin
    homepage: http://people.ucas.ac.cn/~0000967?language=en
    last_name: Hu
    name: Songlin Hu
    username: ~Songlin_Hu2
  - dblp_id: https://dblp.org/pid/69/5011-19
    emails: '****@iie.ac.cn'
    first_name: Wei
    homepage: http://people.ucas.ac.cn/~iiezhouwei
    last_name: Zhou
    name: Wei Zhou
    username: ~Wei_Zhou5
  - dblp_id: https://dblp.org/pid/51/740
    emails: '****@tsinghua.edu.cn'
    first_name: Guiguang
    google_scholar_id: https://scholar.google.com.tw/citations?user=B7F3yt4AAAAJ
    homepage: http://ise.thss.tsinghua.edu.cn/MIG/dgg.html
    institution: Tsinghua University
    last_name: Ding
    name: Guiguang Ding
    username: ~Guiguang_Ding1
  - emails: '****@iie.ac.cn'
    first_name: Xing
    google_scholar_id: https://scholar.google.com.hk/citations?user=ZKd3UjkAAAAJ&hl=zh-CN
    homepage: https://scholar.google.com.hk/citations?user=ZKd3UjkAAAAJ&hl=zh-CN
    last_name: W
    name: Xing W
    username: ~Xing_W1
  decision: toFindings
  end_page: 262
  file: 92.pdf
  id: 92
  num_pages: 13
  openreview_id: dmc9v9sFVY
  pdf_file: 1045f476ff844dc6967a47230782eb373efc04b2.pdf
  start_page: 250
  title: 'MiLe Loss: a New Loss for Mitigating the Bias of Learning Difficulties in
    Generative Language Models'
- abstract: Addressing the challenge of automated geometry math problem-solving in
    artificial intelligence (AI) involves understanding multi-modal information and
    mathematics. \textcolor{black}{Current methods struggle with accurately interpreting
    geometry diagrams, which hinders effective problem-solving. To tackle this issue,
    we present the \textbf{G}eometry problem s\textbf{O}lver with natural \textbf{L}anguage
    \textbf{D}escription (GOLD) model. GOLD enhances the extraction of geometric relations
    by separately processing symbols and geometric primitives within the diagram.
    Subsequently, it converts the extracted relations into natural language descriptions,
    efficiently utilizing large language models to solve geometry math problems.}
    Experiments show that the GOLD model outperforms the Geoformer model, the previous
    best method on the UniGeo dataset, by achieving accuracy improvements of 12.7\%
    and 42.1\% in calculation and proving subsets. Additionally, it surpasses the
    former best model on the PGPS9K and Geometry3K datasets, PGPSNet, by obtaining
    accuracy enhancements of 1.8\% and 3.2\%, respectively.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multimodality and Language Grounding to Vision, Robotics and Beyond
  authors:
  - emails: '****@strath.ac.uk'
    first_name: Jiaxin
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=zQ4pLNEAAAAJ
    homepage: https://knightzhang625.github.io
    institution: University of Strathclyde
    last_name: Zhang
    name: Jiaxin Zhang
    orcid: https://orcid.org/0000-0001-7355-7975
    username: ~Jiaxin_Zhang7
  - dblp_id: https://dblp.org/pid/87/6501
    emails: '****@strath.ac.uk'
    first_name: Yashar
    google_scholar_id: https://scholar.google.co.uk/citations?user=BaFcnWIAAAAJ&hl=en
    homepage: https://www.strath.ac.uk/staff/moshfeghiyashardr/
    institution: University of Strathclyde
    last_name: Moshfeghi
    name: Yashar Moshfeghi
    username: ~Yashar_Moshfeghi1
  decision: toFindings
  end_page: 278
  file: 101.pdf
  id: 101
  num_pages: 16
  openreview_id: mPEfLoTPki
  pdf_file: e25e92f553acdc87c93af4431f6bf3ae5ef1ef57.pdf
  start_page: 263
  title: 'GOLD: Geometry Problem Solver with Natural Language Description'
- abstract: We introduce RoDia, the first dataset for Romanian dialect identification
    from speech. The RoDia dataset includes a varied compilation of speech samples
    from five distinct regions of Romania, covering both urban and rural environments,
    totaling 2 hours of manually annotated speech data. Along with our dataset, we
    introduce a set of competitive models to be used as baselines for future research.
    The top scoring model achieves a macro F1 score of 59.83% and a micro F1 score
    of 62.08%, indicating that the task is challenging. We thus believe that RoDia
    is a valuable resource that will stimulate research aiming to address the challenges
    of Romanian dialect identification. We release our dataset at https://github.com/codrut2/RoDia.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - emails: '****@gmail.com'
    first_name: Rotaru
    last_name: "Codru\u021B"
    name: "Rotaru Codru\u021B"
    username: "~Rotaru_Codru\u021B1"
  - dblp_id: https://dblp.org/pid/253/8663
    emails: '****@yahoo.ro'
    first_name: Nicolae
    google_scholar_id: https://scholar.google.com/citations?user=W_1liH0AAAAJ&hl=ro&oi=ao
    last_name: Ristea
    middle_name: Catalin
    name: Nicolae Catalin Ristea
    username: ~Nicolae_Catalin_Ristea1
  - dblp_id: https://dblp.org/pid/120/9006
    emails: '****@gmail.com'
    first_name: Radu
    google_scholar_id: https://scholar.google.com/citations?user=qVbwC6QAAAAJ&hl=en
    homepage: http://raduionescu.herokuapp.com
    institution: Universitatea Bucuresti
    last_name: Ionescu
    middle_name: Tudor
    name: Radu Tudor Ionescu
    username: ~Radu_Tudor_Ionescu1
  decision: toFindings
  end_page: 286
  file: 102.pdf
  id: 102
  num_pages: 8
  openreview_id: BMp3P7oulo
  pdf_file: da346ee74f99b670ebb06fde581fbdc65c43616e.pdf
  start_page: 279
  title: 'RoDia: A New Dataset for Romanian Dialect Identification from Speech'
- abstract: Recent work has proposed explicitly inducing language-wise modularity
    in multilingual LMs via sparse fine-tuning (SFT) on per-language subnetworks as
    a means of better guiding cross-lingual sharing. In this paper, we investigate
    (1) the degree to which language-wise modularity *naturally* arises within models
    with no special modularity interventions, and (2) how cross-lingual sharing and
    interference differ between such models and those with explicit SFT-guided subnetwork
    modularity. In order to do so, we use XLM-R as our multilingual LM. Moreover,
    to quantify language specialization and cross-lingual interaction, we use a Training
    Data Attribution method that estimates the degree to which a model's predictions
    are influenced by in-language or cross-language training examples. Our results
    show that language-specialized subnetworks do naturally arise, and that SFT, rather
    than always increasing modularity, can decrease language specialization of subnetworks
    in favor of more cross-lingual sharing.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multilinguality and Language Diversity
  authors:
  - dblp_id: https://dblp.org/pid/238/0597
    emails: '****@gmail.com'
    first_name: Rochelle
    google_scholar_id: https://scholar.google.nl/citations?user=-_WbyoMAAAAJ&hl=nl&oi=ao
    last_name: Choenni
    name: Rochelle Choenni
    username: ~Rochelle_Choenni1
  - dblp_id: https://dblp.org/pid/33/8156
    emails: '****@gmail.com'
    first_name: Ekaterina
    google_scholar_id: https://scholar.google.com/citations?user=jqOFBGoAAAAJ&hl=en
    homepage: https://www.shutova.org/
    institution: University of Amsterdam
    last_name: Shutova
    name: Ekaterina Shutova
    semantic_scholar_id: https://www.semanticscholar.org/author/Ekaterina-Shutova/2362276
    username: ~Ekaterina_Shutova1
  - dblp_id: https://dblp.org/pid/117/4050
    emails: '****@gmail.com'
    first_name: Dan
    google_scholar_id: https://scholar.google.com/citations?user=tT9mhNMAAAAJ
    homepage: http://www.dhgarrette.com/
    institution: Google Research
    last_name: Garrette
    name: Dan Garrette
    semantic_scholar_id: https://www.semanticscholar.org/author/Dan-Garrette/2758616
    username: ~Dan_Garrette1
  decision: toFindings
  end_page: 301
  file: 104.pdf
  id: 104
  num_pages: 15
  openreview_id: mpD599y1Eo
  pdf_file: dd7d6bdc4b05d3c506cce2b63ad3935d5b158416.pdf
  start_page: 287
  title: Examining Modularity in Multilingual LMs via Language-Specialized Subnetworks
- abstract: "While enabling large language models to implement function calling (known\
    \ as APIs) can greatly enhance the performance of Large Language Models (LLMs),\
    \ function calling is still a challenging task due to the complicated relations\
    \ between different APIs, especially in a context-learning setting without fine-tuning.\
    \ This paper introduces \"Reverse Chain'', a controllable, target-driven approach\
    \ designed to empower LLMs with the capability to operate external APIs only via\
    \ prompts. Recognizing that most LLMs have limited tool-use capabilities, Reverse\
    \ Chain limits LLMs to executing simple tasks, e.g., API Selection and Argument\
    \ Completion. Furthermore, to manage a controllable multi-function calling, \n\
    Reverse Chain adopts a generic rule-based on a backward reasoning process. This\
    \ rule determines when to do API selection or Argument completion. To evaluate\
    \ the multi-tool-use capability of LLMs, we have released a compositional multi-tool\
    \ task dataset, available at https://github.com/zhangyingerjelly/reverse-chain.\
    \ Extensive numerical experiments validate the remarkable proficiency of Reverse\
    \ Chain in managing multiple API calls."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - emails: '****@zju.edu.cn'
    first_name: Yinger
    last_name: Zhang
    name: Yinger Zhang
    orcid: https://orcid.org/0000-0001-6453-7462
    username: ~Yinger_Zhang1
  - emails: '****@gmail.com'
    first_name: Hui
    last_name: Cai
    name: Hui Cai
    username: ~Hui_Cai1
  - emails: '****@antgroup.com'
    first_name: Xierui
    last_name: Song
    name: Xierui SONG
    orcid: https://orcid.org/0000-0002-4580-1683
    username: ~Xierui_SONG1
  - emails: '****@gmail.com'
    first_name: Yicheng
    homepage: https://www.linkedin.com/in/eason-chen-a6a419b7/
    last_name: Chen
    name: Yicheng Chen
    username: ~Yicheng_Chen1
  - emails: '****@gmail.com'
    first_name: Rui
    last_name: Sun
    name: Rui Sun
    username: ~Rui_Sun12
  - dblp_id: https://dblp.org/pid/15/2171
    emails: '****@gmail.com'
    first_name: Jing
    institution: Ant Group
    last_name: Zheng
    name: Jing Zheng
    username: ~Jing_Zheng1
  decision: toFindings
  end_page: 325
  file: 110.pdf
  id: 110
  num_pages: 24
  openreview_id: 9T3d60u4Gr
  pdf_file: 78d49661c23cc2ae8e2e7f89b99d646924b8dccd.pdf
  start_page: 302
  title: 'Reverse Chain: A Generic-Rule for LLMs to Master Multi-API Planning'
- abstract: Modeling long-range dependencies in sequential data is a crucial step
    in sequence learning.  A recently developed model, the Structured State Space
    (S4),  demonstrated significant effectiveness in modeling long-range sequences.
    However, It is unclear whether the success of S4 can be attributed to its intricate
    parameterization and HiPPO initialization or simply due to State Space Models
    (SSMs). To further investigate the potential of the deep SSMs, we start with exponential
    smoothing (ETS), a simple SSM, and propose a stacked architecture by directly
    incorporating it into an element-wise MLP. We augment simple ETS with additional
    parameters and complex field to reduce the inductive bias. Despite increasing
    less than 1\% of parameters of element-wise MLP, our models achieve comparable
    results to S4 on the LRA benchmark.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Efficient/Low-Resource Methods for NLP
  authors:
  - emails: '****@pku.edu.cn'
    first_name: JiqunChu
    last_name: JiqunChu
    name: JiqunChu
    orcid: https://orcid.org/0000-0002-8402-6253
    username: ~JiqunChu1
  - emails: '****@pku.edu.cn'
    first_name: Zuoquan
    homepage: https://www.math.pku.edu.cn/teachers/linzq/eindex.html
    institution: Peking University
    last_name: Lin
    name: Zuoquan Lin
    username: ~Zuoquan_Lin3
  decision: toFindings
  end_page: 337
  file: 111.pdf
  id: 111
  num_pages: 12
  openreview_id: TKeFhx5X18
  pdf_file: afe3162c668010295de6d107c7010c01899f7d46.pdf
  start_page: 326
  title: 'Incorporating Exponential Smoothing into MLP: a Simple but Effective Sequence
    Model'
- abstract: 'Object navigation (ObjectNav) requires an agent to navigate through unseen
    environments to find queried objects. Many previous methods attempted to solve
    this task by relying on supervised or reinforcement learning, where they are trained
    on limited household datasets with close-set objects. However, two key challenges
    are unsolved: understanding free-form natural language instructions that demand
    open-set objects, and generalizing to new environments in a zero-shot manner.
    Aiming to solve the two challenges, in this paper, we propose **OpenFMNav**, an
    **Open**-set **F**oundation **M**odel based framework for zero-shot object **Nav**igation.
    We first unleash the reasoning abilities of large language models (LLMs) to extract
    proposed objects from natural language instructions that meet the user''s demand.
    We then leverage the generalizability of large vision language models (VLMs) to
    actively discover and detect candidate objects from the scene, building a *Versatile
    Semantic Score Map (VSSM)*. Then, by conducting common sense reasoning on *VSSM*,
    our method can perform effective language-guided exploration and exploitation
    of the scene and finally reach the goal. By leveraging the reasoning and generalizing
    abilities of foundation models, our method can understand free-form human instructions
    and perform effective open-set zero-shot navigation in diverse environments. Extensive
    experiments on the HM3D ObjectNav benchmark show that our method surpasses all
    the strong baselines on all metrics, proving our method''s effectiveness. Furthermore,
    we perform real robot demonstrations to validate our method''s open-set-ness and
    generalizability to real-world environments.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multimodality and Language Grounding to Vision, Robotics and Beyond
  authors:
  - dblp_id: https://dblp.org/pid/358/9222
    emails: '****@stu.pku.edu.cn'
    first_name: Yuxuan
    last_name: Kuang
    name: Yuxuan Kuang
    semantic_scholar_id: https://www.semanticscholar.org/author/Yuxuan-Kuang/2256991055
    username: ~Yuxuan_Kuang1
  - emails: '****@nd.edu'
    first_name: Hai
    institution: University of Notre Dame and University of Notre Dame
    last_name: Lin
    name: Hai Lin
    username: ~Hai_Lin1
  - dblp_id: https://dblp.uni-trier.de/pid/69/339-1.html
    emails: '****@nd.edu'
    first_name: Meng
    google_scholar_id: https://scholar.google.com/citations?user=LZIPfCkAAAAJ
    homepage: http://www.meng-jiang.com/
    institution: University of Notre Dame
    last_name: Jiang
    name: Meng Jiang
    orcid: https://orcid.org/0000-0002-3009-519X
    username: ~Meng_Jiang3
  decision: toFindings
  end_page: 351
  file: 113.pdf
  id: 113
  num_pages: 14
  openreview_id: OfSuf4idcd
  pdf_file: c6c6c337d2ff1b319f43ad8e2c54bf9fc7778a56.pdf
  start_page: 338
  title: 'OpenFMNav: Towards Open-Set Zero-Shot Object Navigation via Vision-Language
    Foundation Models'
- abstract: Following an interaction with a patient, physicians are responsible for
    the submission of clinical documentation, often organized as a SOAP note. A clinical
    note is not simply a summary of the conversation but requires the use of appropriate
    medical terminology. The relevant information can then be extracted and organized
    according to the structure of the SOAP note. In this paper we analyze two different
    approaches to generate the different sections of a SOAP note based on the audio
    recording of the conversation, and specifically examine them in terms of note
    consistency. The first approach generates the sections independently, while the
    second method generates them all together. In this work we make use of PEGASUS-X
    Transformer models and observe that both methods lead to similar ROUGE values
    (less than 1\% difference) and have no difference in terms of the Factuality metric.
    We perform a human evaluation to measure aspects of consistency and demonstrate
    that LLMs like Llama2 can be used to perform the same tasks with roughly the same
    agreement as the human annotators. Between the Llama2 analysis and the human reviewers
    we observe a Cohen Kappa inter-rater reliability of 0.79, 1.00, and 0.32 for consistency
    of age, gender, and body part injury, respectively. With this we demonstrate the
    usefulness of leveraging an LLM to measure quality indicators that can be identified
    by humans but are not currently captured by automatic metrics. This allows scaling
    evaluation to larger data sets, and we find that clinical note consistency improves
    by generating each new section conditioned on the output of all previously generated
    sections.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Summarization
  authors:
  - emails: '****@gmail.com'
    first_name: Nathan
    homepage: https://natebrake.com
    institution: 3M
    last_name: Brake
    name: Nathan Brake
    username: ~Nathan_Brake1
  - dblp_id: https://dblp.org/pid/75/5600
    emails: '****@reihl-schaaf.net'
    first_name: Thomas
    google_scholar_id: https://scholar.google.com/citations?user=gyDVO1IAAAAJ&hl=en
    last_name: Schaaf
    name: Thomas Schaaf
    orcid: https://orcid.org/0000-0002-9569-4759
    semantic_scholar_id: https://www.semanticscholar.org/author/Thomas-Schaaf/145849024
    username: ~Thomas_Schaaf2
  decision: toFindings
  end_page: 363
  file: 118.pdf
  id: 118
  num_pages: 12
  openreview_id: BY1PhXOqC8
  pdf_file: b8b7034367021b84ea389cc42abb3d89f228cb3e.pdf
  start_page: 352
  title: Comparing Two Model Designs for Clinical Note Generation; Is an LLM a Useful
    Evaluator of Consistency?
- abstract: The natural language generation domain has witnessed great success thanks
    to Transformer models. Although they have achieved state-of-the-art generative
    quality, they often neglect generative diversity. Prior attempts to tackle this
    issue suffer from either low model capacity or over-complicated architectures.
    Some recent methods employ the VAE framework to enhance diversity, but their latent
    variables fully depend on the input context, restricting exploration of the latent
    space. In this paper, we introduce VOLTA, a framework that elevates generative
    diversity by bridging Transformer with VAE via a more effective cross-attention-based
    connection, departing from conventional embedding concatenation or summation.
    Additionally, we propose integrating InfoGAN-style latent codes to enable input-independent
    variability, further diversifying the generation. Moreover, our framework accommodates
    discrete inputs alongside its existing support for continuous inputs. We perform
    comprehensive experiments with two types of Transformers on six datasets from
    three different NLG tasks to show that our approach can significantly improve
    generative diversity while maintaining generative quality.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Generation
  authors:
  - dblp_id: https://dblp.org/pid/240/2755
    emails: '****@gmail.com'
    first_name: Yueen
    google_scholar_id: https://scholar.google.com/citations?user=_H2BEhYAAAAJ&hl=en
    last_name: Ma
    name: Yueen Ma
    semantic_scholar_id: https://www.semanticscholar.org/author/Yueen-Ma/115643437
    username: ~Yueen_Ma1
  - emails: '****@huawei.com'
    first_name: DaFeng
    homepage: https://github.com/homiec
    institution: Huawei Technologies Ltd.
    last_name: Chi
    name: DaFeng Chi
    username: ~DaFeng_Chi1
  - dblp_id: https://dblp.org/pid/65/4699-7
    emails: '****@gmail.com'
    first_name: Jingjing
    google_scholar_id: https://scholar.google.com/citations?view_op=list_works&hl=en&user=op6xj6oAAAAJ
    homepage: https://www.cse.cuhk.edu.hk/irwin.king/people/jingjing_li
    last_name: Li
    name: Jingjing Li
    semantic_scholar_id: https://www.semanticscholar.org/author/2109057340
    username: ~Jingjing_Li6
  - emails: '****@gmail.com'
    first_name: Kai
    google_scholar_id: https://scholar.google.com/citations?user=wZJPaSEAAAAJ&hl=zh-CN
    homepage: https://www.linkedin.com/in/kai-song-7ba99888/
    last_name: Song
    name: Kai Song
    username: ~Kai_Song2
  - emails: '****@huawei.com'
    first_name: Yuzheng
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=ny9KAREAAAAJ
    institution: Huawei Technologies Ltd.
    last_name: Zhuang
    name: Yuzheng Zhuang
    username: ~Yuzheng_Zhuang1
  - dblp_id: https://dblp.uni-trier.de/pers/hd/k/King:Irwin
    emails: '****@cse.cuhk.edu.hk'
    first_name: Irwin
    google_scholar_id: https://scholar.google.com/citations?user=MXvC7tkAAAAJ&hl=en
    homepage: https://www.cse.cuhk.edu.hk/irwin.king/
    institution: The Chinese University of Hong Kong
    last_name: King
    name: Irwin King
    orcid: https://orcid.org/0000-0001-8106-6447
    username: ~Irwin_King1
  decision: toFindings
  end_page: 378
  file: 119.pdf
  id: 119
  num_pages: 15
  openreview_id: CBs2xemrax
  pdf_file: ca92d995f657b912a880e7233b8834a9fcaeff15.pdf
  start_page: 364
  title: 'VOLTA: Improving Generative Diversity by Variational Mutual Information
    Maximizing Autoencoder'
- abstract: Linguistic bias is a critical problem concerning the diversity, equity,
    and inclusiveness of Natural Language Processing tools. The severity of this problem
    intensifies in security systems, such as speaker verification, where fairness
    is paramount. Speaker verification systems are biometric systems that determine
    whether two speech recordings are of the same speaker. Such user-centric systems
    should be inclusive to bilingual speakers. However, Deep neural network models
    are linguistically biased. Linguistic bias can be full or partial. Partially cross-lingual
    bias occurs when one test trial pair recording is in the training set's language,
    and the other is in an unseen target language. Such linguistic mismatch influences
    the speaker verification model's decision, dissuading bilingual speakers from
    using the system. Domain adaptation can mitigate this problem. However, adapting
    to each existing language is expensive. This paper explores cost-efficient bias
    mitigation techniques for partially cross-lingual speaker verification. We study
    the behavior of five baselines in five partially cross-lingual scenarios. Using
    our baseline behavioral insights, we propose EcoSpeak, a low-cost solution to
    partially cross-lingual speaker verification. EcoSpeak incorporates contrastive
    linguistic (CL) attention. CL attention utilizes linguistic differences in trial
    pairs to emphasize relevant speaker verification embedding parts. Experimental
    results demonstrate EcoSpeak's robustness to partially cross-lingual testing.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Speech recognition, text-to-speech and spoken language understanding
  authors:
  - dblp_id: https://dblp.org/pid/03/8325
    emails: '****@iiitd.ac.in'
    first_name: Divya
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=9qcerEUAAAAJ
    homepage: https://vdivyas.github.io/
    institution: Indraprastha Institute of Information Technology, Delhi
    last_name: Sharma
    middle_name: V
    name: Divya V Sharma
    semantic_scholar_id: https://www.semanticscholar.org/author/Divya-V-Sharma/2118804786
    username: ~Divya_V_Sharma1
  decision: toFindings
  end_page: 394
  file: 120.pdf
  id: 120
  num_pages: 16
  openreview_id: Nh8cJZEpCr
  pdf_file: 04f8a215f1f8752134aa6f852fc8518c90762d71.pdf
  start_page: 379
  title: 'EcoSpeak: Cost-Efficient Bias Mitigation for Partially Cross-Lingual Speaker
    Verification'
- abstract: In text documents such as news articles, the content and key events usually
    revolve around a subset of all the entities mentioned in a document. These entities,
    often deemed as salient entities, provide useful cues of the aboutness of a document
    to a reader. Identifying the salience of entities was found helpful in several
    downstream applications such as search, ranking, and entity-centric summarization,
    among others. Prior work on salient entity detection mainly focused on machine
    learning models that require heavy feature engineering. We show that fine-tuning
    medium-sized language models with a cross-encoder style architecture yields substantial
    performance gains over feature engineering approaches. To this end, we conduct
    a comprehensive benchmarking of four publicly available datasets using models
    representative of the medium-sized pre-trained language model family. Additionally,
    we show that zero-shot prompting of instruction-tuned language models yields inferior
    results, indicating the task's uniqueness and complexity.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Extraction
  authors:
  - dblp_id: https://dblp.org/pid/220/4104
    emails: '****@gmail.com'
    first_name: Rajarshi
    google_scholar_id: https://scholar.google.com.hk/citations?user=qTFkFhIAAAAJ&hl=en
    homepage: https://kingsaint.github.io
    institution: Bloomberg L.P.
    last_name: Bhowmik
    name: Rajarshi Bhowmik
    orcid: https://orcid.org/0000-0002-2954-3709
    semantic_scholar_id: https://www.semanticscholar.org/author/Rajarshi-Bhowmik/46180847
    username: ~Rajarshi_Bhowmik1
  - dblp_id: https://dblp.org/pid/201/1463
    emails: '****@gmail.com'
    first_name: Marco
    last_name: Ponza
    name: Marco Ponza
    username: ~Marco_Ponza1
  - dblp_id: https://dblp.org/pid/278/8453
    emails: '****@bloomberg.net'
    first_name: Atharva
    google_scholar_id: https://scholar.google.com/citations?user=hByN-sQAAAAJ&hl=en
    homepage: https://atharva-tendle.github.io/
    last_name: Tendle
    name: Atharva Tendle
    orcid: https://orcid.org/0000-0003-1206-7360
    username: ~Atharva_Tendle1
  - emails: '****@gmail.com'
    first_name: Anant
    homepage: https://linkedin.com/in/anantzoid
    last_name: Gupta
    name: Anant Gupta
    username: ~Anant_Gupta1
  - emails: '****@gmail.com'
    first_name: Rebecca
    last_name: Jiang
    name: Rebecca Jiang
    orcid: https://orcid.org/0000-0002-7203-9882
    username: ~Rebecca_Jiang1
  - emails: '****@bloomberg.net'
    first_name: Xingyu
    homepage: https://github.com/xingyulu0701
    institution: Bloomberg
    last_name: Lu
    name: Xingyu Lu
    username: ~Xingyu_Lu1
  - emails: '****@bloomberg.net'
    first_name: Qian
    homepage: https://will-qianzhao.github.io/
    institution: Bloomberg
    last_name: Zhao
    name: Qian Zhao
    username: ~Qian_Zhao3
  - dblp_id: https://dblp.org/pid/126/8668
    emails: '****@gmail.com'
    first_name: Daniel
    google_scholar_id: https://scholar.google.com/citations?user=7HSgxLEAAAAJ&hl=en
    homepage: https://www.preotiuc.ro/
    institution: Bloomberg
    last_name: Preotiuc-Pietro
    name: Daniel Preotiuc-Pietro
    semantic_scholar_id: https://www.semanticscholar.org/author/Daniel-Preotiuc-Pietro/1398830377
    username: ~Daniel_Preotiuc-Pietro2
  decision: toFindings
  end_page: 408
  file: 124.pdf
  id: 124
  num_pages: 14
  openreview_id: lKNQu46lDX
  pdf_file: 9c5ef292d81362f64a468c5917067e1d88c787d7.pdf
  start_page: 395
  title: Leveraging Contextual Information for Effective Entity Salience Detection
- abstract: With the rapid development and widespread application of Large Language
    Models (LLMs), the use of Machine-Generated Text (MGT) has become increasingly
    common, bringing with it potential risks, especially in terms of quality and integrity
    in fields like news, education, and science. Current research mainly focuses on
    purely MGT detection, without adequately addressing mixed scenarios including
    AI-revised Human-Written Text (HWT) or human-revised MGT. To tackle this challenge,
    we define mixtext, a form of mixed text involving both AI and human-generated
    content. Then we introduce MixSet, the first dataset dedicated to studying these
    mixtext scenarios. Leveraging MixSet, we executed comprehensive experiments to
    assess the efficacy of prevalent MGT detectors in handling mixtext situations,
    evaluating their performance in terms of effectiveness, robustness, and generalization.
    Our findings reveal that existing detectors struggle to identify mixtext, particularly
    in dealing with subtle modifications and style adaptability. This research underscores
    the urgent need for more fine-grain detectors tailored for mixtext, offering valuable
    insights for future research. Code and Models are available at https://github.com/Dongping-Chen/MixSet.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - emails: '****@foxmail.com'
    first_name: Qihui
    homepage: https://github.com/Mask-Hui
    last_name: Zhang
    name: Qihui Zhang
    username: ~Qihui_Zhang1
  - emails: '****@stu.scu.edu.cn'
    first_name: Chujie
    last_name: Gao
    name: Chujie Gao
    orcid: https://orcid.org/0009-0000-3797-7040
    username: ~Chujie_Gao1
  - emails: '****@hust.edu.cn'
    first_name: Dongping
    homepage: https://github.com/Dongping-Chen
    last_name: Chen
    name: Dongping Chen
    username: ~Dongping_Chen1
  - emails: '****@gmail.com'
    first_name: Yue
    homepage: https://howiehwong.github.io/
    last_name: Huang
    name: Yue Huang
    username: ~Yue_Huang9
  - emails: '****@gmail.com'
    first_name: Yixin
    homepage: https://yixiaoer.github.io/
    last_name: Huang
    name: Yixin Huang
    username: ~Yixin_Huang2
  - emails: '****@gmail.com'
    first_name: Zhenyang
    google_scholar_id: https://scholar.google.com/citations?user=5_9BVJ4AAAAJ&hl=zh-CN
    last_name: Sun
    name: Zhenyang Sun
    username: ~Zhenyang_Sun1
  - emails: '****@gmail.com'
    first_name: Shilin
    homepage: https://github.com/sRk8ARtDkQF
    last_name: Zhang
    name: Shilin Zhang
    username: ~Shilin_Zhang2
  - emails: '****@qq.com'
    first_name: Weiye
    last_name: Li
    name: Weiye Li
    orcid: https://orcid.org/0009-0009-8334-4653
    username: ~Weiye_Li1
  - emails: '****@163.com'
    first_name: Zhengyan
    homepage: https://github.com/Peter-Fu1
    last_name: Fu
    name: Zhengyan Fu
    username: ~Zhengyan_Fu1
  - dblp_id: https://dblp.uni-trier.de/pid/167/0275
    emails: '****@hust.edu.cn'
    first_name: Yao
    google_scholar_id: https://scholar.google.com/citations?user=c3MtqtMAAAAJ&hl=en
    homepage: http://wanyao.me
    institution: Huazhong University of Science and Technology
    last_name: Wan
    name: Yao Wan
    username: ~Yao_Wan2
  - dblp_id: https://dblp.org/pid/121/0780-1.html
    emails: '****@gmail.com'
    first_name: Lichao
    google_scholar_id: https://scholar.google.com/citations?user=WhGUE7AAAAAJ&hl=en
    homepage: https://lichao-sun.github.io/
    institution: Lehigh University
    last_name: Sun
    name: Lichao Sun
    username: ~Lichao_Sun1
  decision: toFindings
  end_page: 436
  file: 137.pdf
  id: 137
  num_pages: 28
  openreview_id: bNLHGVn4OH
  pdf_file: 2264817bd143af269697b62955301907127ea35f.pdf
  start_page: 409
  title: 'LLM-as-a-Coauthor: Can Mixed Human-Written and Machine-Generated Text Be
    Detected?'
- abstract: Community models for malicious content detection, which take into account
    the context from a social graph alongside the content itself, have shown remarkable
    performance on benchmark datasets. Yet, misinformation and hate speech continue
    to propagate on social media networks. This mismatch can be partially attributed
    to the limitations of current evaluation setups that neglect the rapid evolution
    of online content and the underlying social graph. In this paper, we propose a
    novel evaluation setup for model generalisation based on our few-shot subgraph
    sampling approach. This setup tests for generalisation through few labelled examples
    in local explorations of a larger graph, emulating more realistic application
    settings. We show this to be a challenging inductive setup, wherein strong performance
    on the training graph is not indicative of performance on unseen tasks, domains,
    or graph structures. Lastly, we show that graph meta-learners trained with our
    proposed few-shot subgraph sampling outperform standard community models in the
    inductive setup.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Learning for NLP
  authors:
  - emails: '****@uva.nl'
    first_name: Ivo
    homepage: https://www.ivoverhoeven.com/
    last_name: Verhoeven
    name: Ivo Verhoeven
    username: ~Ivo_Verhoeven1
  - dblp_id: https://dblp.org/pid/137/8258
    emails: '****@meta.com'
    first_name: Pushkar
    google_scholar_id: https://scholar.google.com/citations?user=bVcZ1qkAAAAJ&hl
    institution: Meta AI
    last_name: Mishra
    name: Pushkar Mishra
    username: ~Pushkar_Mishra1
  - emails: '****@rahelbeloch.de'
    first_name: Rahel
    last_name: Beloch
    name: Rahel Beloch
    username: ~Rahel_Beloch1
  - dblp_id: https://dblp.org/pid/60/9768
    emails: '****@gmail.com'
    first_name: Helen
    google_scholar_id: https://scholar.google.gr/citations?user=B3WGvRsAAAAJ&hl=en
    homepage: https://www.cl.cam.ac.uk/~hy260/
    institution: Computer Laboratory, University of Cambridge and King's College London
    last_name: Yannakoudakis
    name: Helen Yannakoudakis
    username: ~Helen_Yannakoudakis1
  - dblp_id: https://dblp.org/pid/33/8156
    emails: '****@gmail.com'
    first_name: Ekaterina
    google_scholar_id: https://scholar.google.com/citations?user=jqOFBGoAAAAJ&hl=en
    homepage: https://www.shutova.org/
    institution: University of Amsterdam
    last_name: Shutova
    name: Ekaterina Shutova
    semantic_scholar_id: https://www.semanticscholar.org/author/Ekaterina-Shutova/2362276
    username: ~Ekaterina_Shutova1
  decision: toFindings
  end_page: 463
  file: 138.pdf
  id: 138
  num_pages: 27
  openreview_id: NGE0tDoPRS
  pdf_file: fabde8c386d921c253792a8ed22c4ea6a50a85be.pdf
  start_page: 437
  title: A (More) Realistic Evaluation Setup for Generalisation of Community Models
    on Malicious Content Detection
- abstract: "Large Language Models (LLMs) bring transformative benefits alongside\
    \ unique challenges, including intellectual property (IP) and ethical concerns.\
    \ This position paper explores a novel angle to mitigate these risks, drawing\
    \ parallels between LLMs and established web systems. We identify \"citation\"\
    \u2014the acknowledgement or reference to a source or evidence\u2014as a crucial\
    \ yet missing component in LLMs. Incorporating citation could enhance content\
    \ transparency and verifiability, thereby confronting the IP and ethical issues\
    \ in the deployment of LLMs. We further propose that a comprehensive citation\
    \ mechanism for LLMs should account for both non-parametric and parametric content.\
    \ Despite the complexity of implementing such a citation mechanism, along with\
    \ the potential pitfalls, we advocate for its development. Building on this foundation,\
    \ we outline several research problems in this area, aiming to guide future explorations\
    \ towards building more responsible and accountable LLMs."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Ethics, Bias, and Fairness
  authors:
  - dblp_id: https://dblp.org/pid/29/6643-9
    emails: '****@illinois.edu'
    first_name: Jie
    google_scholar_id: https://scholar.google.com/citations?user=GIoPkMoAAAAJ&hl=en
    homepage: https://jeffhj.github.io/
    institution: University of Illinois, Urbana Champaign
    last_name: Huang
    name: Jie Huang
    semantic_scholar_id: https://www.semanticscholar.org/author/Jie-Huang/1490651934
    username: ~Jie_Huang3
  - dblp_id: https://dblp.uni-trier.de/pid/c/KCCChang.html
    emails: '****@illinois.edu'
    first_name: Kevin
    google_scholar_id: https://scholar.google.com.tw/citations?user=sugWZ6MAAAAJ
    homepage: https://cs.illinois.edu/directory/profile/kcchang
    institution: University of Illinois, Urbana Champaign
    last_name: Chang
    name: Kevin Chang
    orcid: https://orcid.org/0000-0003-0997-6803
    username: ~Kevin_Chang1
  decision: toFindings
  end_page: 473
  file: 139.pdf
  id: 139
  num_pages: 10
  openreview_id: vXlMv6d97D
  pdf_file: 913277a05d6457fbecddbdb78c528099f62b8b1b.pdf
  start_page: 464
  title: 'Citation: A Key to Building Responsible and Accountable Large Language Models'
- abstract: The injection of syntactic information in Variational AutoEncoders (VAEs)
    can result in an overall improvement of performances and generalisation. An effective
    strategy to achieve such a goal is to separate the encoding of distributional
    semantic features and syntactic structures into heterogeneous latent spaces via
    multi-task learning or dual encoder architectures. However, existing works employing
    such techniques are limited to LSTM-based VAEs. This work investigates latent
    space separation methods for structural syntactic injection in Transformer-based
    VAE architectures (i.e., Optimus) through the integration of graph-based models.
    Our empirical evaluation reveals that the proposed end-to-end VAE architecture
    can improve theoverall organisation of the latent space, alleviating the information
    loss occurring in standard VAE setups, and resulting in enhanced performances
    on language modelling and downstream generation tasks.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: 'Semantics: Sentence-level Semantics, Textual Inference and Other
      areas'
  authors:
  - emails: '****@postgrad.manchester.ac.uk'
    first_name: Yingji
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=VyfR-JgAAAAJ
    last_name: Zhang
    name: Yingji Zhang
    username: ~Yingji_Zhang1
  - dblp_id: https://dblp.org/pid/212/3533
    emails: '****@idiap.ch'
    first_name: Marco
    google_scholar_id: https://scholar.google.com/citations?user=nnaBYcIAAAAJ
    homepage: https://www.marcovalentino.net/
    last_name: Valentino
    name: Marco Valentino
    semantic_scholar_id: https://www.semanticscholar.org/author/Marco-Valentino/34102057
    username: ~Marco_Valentino1
  - dblp_id: https://dblp.org/pid/134/6887
    emails: '****@manchester.ac.uk'
    first_name: Danilo
    google_scholar_id: https://scholar.google.com/citations?user=e-K_TIYAAAAJ
    homepage: http://www.danilosc.ml
    institution: University of Manchester
    last_name: Carvalho
    name: Danilo Carvalho
    orcid: https://orcid.org/0000-0002-8124-9224
    semantic_scholar_id: https://www.semanticscholar.org/author/D.-S.-Carvalho/38424153
    username: ~Danilo_Carvalho1
  - dblp_id: https://dblp.org/pid/60/4630.html
    emails: '****@manchester.ac.uk'
    first_name: Ian
    google_scholar_id: https://scholar.google.com.tw/citations?user=M1xicaEAAAAJ
    homepage: http://www.cs.man.ac.uk/~ipratt/
    institution: University of Opole and University of Manchester
    last_name: Pratt-Hartmann
    name: Ian Pratt-Hartmann
    orcid: https://orcid.org/0000-0003-0062-043X
    username: ~Ian_Pratt-Hartmann1
  - emails: '****@manchester.ac.uk'
    first_name: Andre
    homepage: http://andrefreitas.org
    institution: University of Manchester
    last_name: Freitas
    name: Andre Freitas
    username: ~Andre_Freitas1
  decision: toFindings
  end_page: 489
  file: 145.pdf
  id: 145
  num_pages: 16
  openreview_id: 1I0ynJzHCn
  pdf_file: 0234b3599ca1c2af24f3d14acbe2cbd4849388a6.pdf
  start_page: 474
  title: Graph-Induced Syntactic-Semantic Spaces in Transformer-Based Variational
    AutoEncoders
- abstract: Large language models trained primarily in a monolingual setting have
    demonstrated their ability to generalize to machine translation using zero- and
    few-shot examples with in-context learning. However, even though zero-shot translations
    are relatively good, there remains a discernible gap comparing their performance
    with the few-shot setting. In this paper, we investigate the factors contributing
    to this gap and find that this gap can largely be closed (for about 70\%) by matching
    the writing styles of the target corpus. Additionally, we explore potential approaches
    to enhance zero-shot baselines without the need for parallel demonstration examples,
    providing valuable insights into how these methods contribute to improving translation
    metrics.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Translation
  authors:
  - dblp_id: https://dblp.org/pid/208/0745
    emails: '****@jhu.edu'
    first_name: Weiting
    google_scholar_id: https://scholar.google.com/citations?user=hD8E4gYAAAAJ&hl=en
    institution: Johns Hopkins University
    last_name: Tan
    name: Weiting Tan
    semantic_scholar_id: https://www.semanticscholar.org/author/Weiting-Tan/28000727
    username: ~Weiting_Tan1
  - dblp_id: https://dblp.org/pid/140/8357
    emails: '****@jhu.edu'
    first_name: Haoran
    homepage: https://www.fe1ixxu.com/
    institution: Johns Hopkins University
    last_name: Xu
    name: Haoran Xu
    username: ~Haoran_Xu3
  - dblp_id: https://dblp.org/pid/240/5490.html
    emails: '****@jhu.edu'
    first_name: Lingfeng
    google_scholar_id: https://scholar.google.com/citations?user=PoSTdLAAAAAJ&hl=en
    last_name: Shen
    name: Lingfeng Shen
    semantic_scholar_id: https://www.semanticscholar.org/author/Lingfeng-Shen/2121272448
    username: ~Lingfeng_Shen1
  - dblp_id: https://dblp.org/pid/312/6501
    emails: '****@cs.washington.edu'
    first_name: Shuyue Stella
    google_scholar_id: https://scholar.google.com/citations?user=CRfOlOEAAAAJ&hl=en&oi=ao
    homepage: http://stellalisy.com/
    institution: Department of Computer Science, University of Washington
    last_name: Li
    name: Shuyue Stella Li
    semantic_scholar_id: https://www.semanticscholar.org/author/Shuyue-Stella-Li/1951196134
    username: ~Shuyue_Stella_Li1
  - dblp_id: https://dblp.org/pid/143/9465
    emails: '****@jhu.edu'
    first_name: Kenton
    google_scholar_id: https://scholar.google.com/citations?user=JuP-xF8AAAAJ&hl=en&oi=ao
    homepage: http://www.kentonmurray.com
    institution: Johns Hopkins University
    last_name: Murray
    name: Kenton Murray
    orcid: https://orcid.org/0000-0002-5628-1003
    semantic_scholar_id: https://www.semanticscholar.org/author/Kenton-Murray/38730896
    username: ~Kenton_Murray1
  - dblp_id: https://dblp.org/pid/84/4538.html
    emails: '****@jhu.edu'
    first_name: Philipp
    google_scholar_id: https://scholar.google.com/citations?user=OsIZgIYAAAAJ&hl=en
    homepage: http://www.cs.jhu.edu/~phi/
    institution: Johns Hopkins University
    last_name: Koehn
    name: Philipp Koehn
    orcid: https://orcid.org/0000-0003-1565-064X
    semantic_scholar_id: https://www.semanticscholar.org/author/Philipp-Koehn/1755162
    username: ~Philipp_Koehn2
  - dblp_id: https://dblp.org/pid/06/4775
    emails: '****@cs.jhu.edu'
    first_name: Benjamin
    google_scholar_id: https://scholar.google.com.tw/citations?user=wIujAJoAAAAJ
    homepage: http://www.cs.jhu.edu/~vandurme/
    institution: Johns Hopkins University, Johns Hopkins University, Johns Hopkins
      University and Microsoft
    last_name: Van Durme
    name: Benjamin Van Durme
    username: ~Benjamin_Van_Durme2
  - dblp_id: https://dblp.org/pid/252/7831
    emails: '****@jhu.edu'
    first_name: Yunmo
    google_scholar_id: https://scholar.google.com/citations?user=V-g2Tx8AAAAJ&hl=en
    homepage: https://omnuy.me
    institution: Johns Hopkins University
    last_name: Chen
    name: Yunmo Chen
    semantic_scholar_id: https://www.semanticscholar.org/author/Yunmo-Chen/104375103
    username: ~Yunmo_Chen1
  decision: toFindings
  end_page: 502
  file: 148.pdf
  id: 148
  num_pages: 13
  openreview_id: cEYobvcoTO
  pdf_file: 2059e636afa1cf4188aff15ca00b1e5de628fec1.pdf
  start_page: 490
  title: Narrowing the Gap between Zero- and Few-shot Machine Translation by Matching
    Styles
- abstract: Our research integrates graph data with Large Language Models (LLMs),
    which, despite their advancements in various fields using large text corpora,
    face limitations in encoding entire graphs due to context size constraints. This
    paper introduces a new approach to encoding a graph with diverse modalities, such
    as text, image, and motif, coupled with prompts to approximate a graph's global
    connectivity, thereby enhancing LLMs' efficiency in processing complex graph structures.
    The study also presents GraphTMI, a novel benchmark for evaluating LLMs in graph
    structure analysis, focusing on homophily, motif presence, and graph difficulty.
    Key findings indicate that the image modality, especially with vision-language
    models like GPT-4V, is superior to text in balancing token limits and preserving
    essential information and comes close to prior graph neural net (GNN) encoders.
    Furthermore, the research assesses how various factors affect the performance
    of each encoding modality and outlines the existing challenges and potential future
    developments for LLMs in graph understanding and reasoning tasks. Our code and
    data are publicly available on our project page - https://minnesotanlp.github.io/GraphLLM/
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multimodality and Language Grounding to Vision, Robotics and Beyond
  authors:
  - dblp_id: https://dblp.org/pid/331/4274
    emails: '****@umn.edu'
    first_name: Debarati
    google_scholar_id: https://scholar.google.com/citations?user=NpoVjbMAAAAJ&hl=en
    last_name: Das
    name: Debarati Das
    semantic_scholar_id: https://www.semanticscholar.org/author/Debarati-Das/2605263/
    username: ~Debarati_Das1
  - emails: '****@gmail.com'
    first_name: Ishaan
    last_name: Gupta
    name: Ishaan Gupta
    username: ~Ishaan_Gupta2
  - dblp_id: https://dblp.org/pid/s/JaideepSrivastava
    emails: '****@umn.edu'
    first_name: Jaideep
    google_scholar_id: https://scholar.google.com.tw/citations?user=Y4J5SOwAAAAJ
    homepage: https://www.cs.umn.edu/people/jaideep-srivastava
    institution: University of Minnesota - Twin Cities
    last_name: Srivastava
    name: Jaideep Srivastava
    orcid: https://orcid.org/0000-0001-9385-7545
    semantic_scholar_id: https://www.semanticscholar.org/author/J.-Srivastava/144235071
    username: ~Jaideep_Srivastava1
  - dblp_id: https://dblp.org/pid/69/9056
    emails: '****@gmail.com'
    first_name: Dongyeop
    google_scholar_id: https://scholar.google.co.kr/citations?user=fMKZOjwAAAAJ&hl=en
    homepage: https://dykang.github.io/
    institution: University of Minnesota
    last_name: Kang
    name: Dongyeop Kang
    semantic_scholar_id: https://www.semanticscholar.org/author/Dongyeop-Kang/48493368
    username: ~Dongyeop_Kang2
  decision: toFindings
  end_page: 519
  file: 149.pdf
  id: 149
  num_pages: 17
  openreview_id: Oa6Ypy8vaM
  pdf_file: 69d956cbfbc1b893fb01b6c825bef4de90421790.pdf
  start_page: 503
  title: 'Which Modality should I use - Text, Motif, or Image? : Understanding Graphs
    with Large Language Models'
- abstract: "We propose on-the-fly ensembling of a neural machine translation (NMT)\
    \ model with a large language model (LLM), prompted on the same task and input.\
    \ \n\nThrough experiments on 4 language directions with varying data amounts,\
    \ we find that a slightly weaker-at-translation LLM can improve translations of\
    \ a NMT model, and such an ensemble can produce better translations than ensembling\
    \ two stronger NMT models.\n\nWe demonstrate that our ensemble method can be combined\
    \  with various techniques from LLM prompting, such as in context learning and\
    \ translation context."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Translation
  authors:
  - dblp_id: https://dblp.org/pid/46/3446
    emails: '****@gmail.com'
    first_name: Hieu
    homepage: http://statmt.org/hieu/
    institution: Microsoft
    last_name: Hoang
    name: Hieu Hoang
    semantic_scholar_id: https://www.semanticscholar.org/author/152378023
    username: ~Hieu_Hoang2
  - dblp_id: https://dblp.org/pid/132/8648
    emails: '****@berkeley.edu'
    first_name: Huda
    google_scholar_id: https://scholar.google.com/citations?user=muJh2XQAAAAJ&hl=en&oi=ao
    homepage: https://khayrallah.github.io/
    institution: Microsoft
    last_name: Khayrallah
    name: Huda Khayrallah
    semantic_scholar_id: https://www.semanticscholar.org/author/Huda-Khayrallah/3115181
    username: ~Huda_Khayrallah1
  - dblp_id: https://dblp.org/pid/79/7927
    emails: '****@microsoft.com'
    first_name: Marcin
    google_scholar_id: https://scholar.google.com/citations?user=Uh_GH14AAAAJ&hl=en
    institution: Microsoft
    last_name: Junczys-Dowmunt
    name: Marcin Junczys-Dowmunt
    semantic_scholar_id: https://www.semanticscholar.org/author/Marcin-Junczys-Dowmunt/1733933
    username: ~Marcin_Junczys-Dowmunt1
  decision: toFindings
  end_page: 532
  file: 154.pdf
  id: 154
  num_pages: 13
  openreview_id: kB2rQ411lt
  pdf_file: 6a48c88979f23218ff6191a03276b1018f532d69.pdf
  start_page: 520
  title: On-the-Fly Fusion of Large Language Models and Machine Translation
- abstract: "Recent works in relation extraction (RE) have achieved promising benchmark\
    \ accuracy; however, our adversarial attack experiments show that these works\
    \ excessively rely on entities, making their generalization capability questionable.\
    \ \nTo address this issue, we propose an adversarial training method specifically\
    \ designed for RE. \nOur approach introduces both sequence- and token-level perturbations\
    \ to the sample and uses a separate perturbation vocabulary to improve the search\
    \ for entity and context perturbations.\nFurthermore, we introduce a probabilistic\
    \ strategy for leaving clean tokens in the context during adversarial training.\
    \ \nThis strategy enables a larger attack budget for entities and coaxes the model\
    \ to leverage relational patterns embedded in the context. \nExtensive experiments\
    \ show that compared to various adversarial training methods, our method significantly\
    \ improves both the accuracy and robustness of the model. \nAdditionally, experiments\
    \ on different data availability settings highlight the effectiveness of our method\
    \ in low-resource scenarios.\nWe also perform in-depth analyses of our proposed\
    \ method and provide further hints.\nWe will release our code at https://github.com/David-Li0406/READ."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Extraction
  authors:
  - dblp_id: https://dblp.org/pid/13/5856-8
    emails: '****@ucsd.edu'
    first_name: Dawei
    google_scholar_id: https://scholar.google.com/citations?user=JaX6HGAAAAAJ&hl=en
    homepage: https://david-li0406.github.io/
    last_name: Li
    name: Dawei Li
    username: ~Dawei_Li5
  - emails: '****@ucsd.edu'
    first_name: William
    homepage: http://example.com
    last_name: Hogan
    middle_name: P
    name: William P Hogan
    username: ~William_P_Hogan1
  - dblp_id: https://dblp.org/pid/151/3145.html
    emails: '****@ucsd.edu'
    first_name: Jingbo
    google_scholar_id: https://scholar.google.com/citations?user=0SkFI4MAAAAJ&hl=en
    homepage: https://shangjingbo1226.github.io/
    institution: University of California, San Diego
    last_name: Shang
    name: Jingbo Shang
    semantic_scholar_id: https://www.semanticscholar.org/author/Jingbo-Shang/2884976
    username: ~Jingbo_Shang2
  decision: toFindings
  end_page: 548
  file: 155.pdf
  id: 155
  num_pages: 16
  openreview_id: dv96IkOEdV
  pdf_file: 3f75e75ad30c11ca3bef5fac527e631cea3195c8.pdf
  start_page: 533
  title: 'READ: Improving Relation Extraction from an ADversarial Perspective'
- abstract: The extensive scope of large language models (LLMs) across various domains
    underscores the critical importance of responsibility in their application, beyond
    natural language processing. In particular, the randomized nature of LLMs, coupled
    with inherent biases and historical stereotypes in data, raises critical concerns
    regarding reliability and equity. Addressing these challenges are necessary before
    using LLMs for applications with societal impact. Towards addressing this gap,
    we introduce REQUAL-LM, a novel method for finding reliable and equitable LLM
    outputs through aggregation. Specifically, we develop a Montecarlo method based
    on repeated sampling to find a reliable output close to the mean of the underlying
    distribution of possible outputs. We formally define the terms such as reliability
    and bias, and design an equity-aware aggregation to minimize harmful bias while
    finding a highly reliable output. REQUAL-LM does not require specialized hardware,
    does not impose a significant computing load, and uses LLMs as a blackbox. This
    design choice enables seamless scalability alongside the rapid advancement of
    LLM technologies. Our system does not require retraining the LLMs, which makes
    it deployment ready and easy to adapt. Our comprehensive experiments using various
    tasks and datasets demonstrate that REQUAL-LM effectively mitigates bias and selects
    a more equitable response, specifically the outputs that properly represents minority
    groups.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Ethics, Bias, and Fairness
  authors:
  - emails: '****@uic.edu'
    first_name: Sana
    google_scholar_id: https://scholar.google.com/citations?user=-RDRcqAAAAAJ&hl=en
    last_name: Ebrahimi
    name: Sana Ebrahimi
    username: ~Sana_Ebrahimi1
  - emails: '****@uic.edu'
    first_name: Nima
    google_scholar_id: https://scholar.google.com/citations?user=Q4P6b34AAAAJ&hl=en
    homepage: https://neemashahbazi.github.io/
    last_name: Shahbazi
    name: Nima Shahbazi
    username: ~Nima_Shahbazi1
  - dblp_id: https://dblp.uni-trier.de/pid/04/7892
    emails: '****@uic.edu'
    first_name: Abolfazl
    google_scholar_id: https://scholar.google.com/citations?user=cJXIgasAAAAJ
    homepage: https://www.cs.uic.edu/~asudeh/
    institution: University of Illinois Chicago
    last_name: Asudeh
    name: Abolfazl Asudeh
    orcid: https://orcid.org/0000-0002-5251-6186
    username: ~Abolfazl_Asudeh1
  decision: toFindings
  end_page: 560
  file: 157.pdf
  id: 157
  num_pages: 12
  openreview_id: BO5UWiPiN9
  pdf_file: 0c4918479be23b1fc451d516dc2491b6e9b5a671.pdf
  start_page: 549
  title: 'REQUAL-LM: Reliability and Equity through Aggregation in Large Language
    Models'
- abstract: Statistical fairness stipulates equivalent outcomes for every protected
    group, whereas causal fairness prescribes that a model makes the same prediction
    for an individual regardless of their protected characteristics. Counterfactual
    data augmentation (CDA) is effective for reducing bias in NLP models, yet models
    trained with CDA are often evaluated only on metrics that are closely tied to
    the causal fairness notion; similarly, sampling-based methods designed to promote
    statistical fairness are rarely evaluated for causal fairness. In this work, we
    evaluate both statistical and causal debiasing methods for gender bias in NLP
    models, and find that while such methods are effective at reducing bias as measured
    by the targeted metric, they do not necessarily improve results on other bias
    metrics. We demonstrate that combinations of statistical and causal debiasing
    techniques are able to reduce bias measured through both types of metrics.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Ethics, Bias, and Fairness
  authors:
  - dblp_id: https://dblp.org/pid/170/4543
    emails: '****@virginia.edu'
    first_name: Hannah
    google_scholar_id: https://scholar.google.com/citations?user=vXAeCu8AAAAJ
    homepage: https://hannahxchen.github.io/
    institution: University of Virginia
    last_name: Chen
    name: Hannah Chen
    orcid: https://orcid.org/0000-0002-0249-9680
    semantic_scholar_id: https://www.semanticscholar.org/author/Hannah-Chen/1876074899
    username: ~Hannah_Chen1
  - dblp_id: https://dblp.org/pid/94/8323
    emails: '****@virginia.edu'
    first_name: Yangfeng
    google_scholar_id: https://scholar.google.com/citations?user=pg02-e8AAAAJ&hl=en
    homepage: http://yangfengji.net
    institution: University of Virginia
    last_name: Ji
    name: Yangfeng Ji
    semantic_scholar_id: https://www.semanticscholar.org/author/Yangfeng-Ji/40608686
    username: ~Yangfeng_Ji1
  - dblp_id: https://dblp.uni-trier.de/pid/e/DavidEvans
    emails: '****@virginia.edu'
    first_name: David
    google_scholar_id: https://scholar.google.com/citations?user=DsR4PucAAAAJ&hl=en
    homepage: https://www.cs.virginia.edu/evans/
    institution: University of Virginia
    last_name: Evans
    name: David Evans
    username: ~David_Evans1
  decision: toFindings
  end_page: 582
  file: 175.pdf
  id: 175
  num_pages: 22
  openreview_id: WLEIC50lmX
  pdf_file: 073d9c6a2a54f3595cdf47d071125be07397a0c0.pdf
  start_page: 561
  title: Addressing Both Statistical and Causal Gender Fairness in NLP Models
- abstract: Text-based recommendation holds a wide range of practical applications
    due to its versatility, as textual descriptions can represent nearly any type
    of item. However, directly employing the original item descriptions may not yield
    optimal recommendation performance due to the lack of comprehensive information
    to align with user preferences. Recent advances in large language models (LLMs)
    have showcased their remarkable ability to harness commonsense knowledge and reasoning.
    In this study, we introduce a novel approach, coined LLM-Rec, which incorporates
    four distinct prompting strategies of text enrichment for improving personalized
    text-based recommendations. Our empirical experiments reveal that using LLM-augmented
    text significantly enhances recommendation quality. Even basic MLP (Multi-Layer
    Perceptron) models achieve comparable or even better results than complex content-based
    methods. Notably, the success of LLM-Rec lies in its prompting strategies, which
    effectively tap into the language model's comprehension of both general and specific
    item characteristics. This highlights the importance of employing diverse prompts
    and input augmentation techniques to boost the recommendation effectiveness of
    LLMs.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - dblp_id: https://dblp.org/pid/256/5541
    emails: '****@ur.rochester.edu'
    first_name: Hanjia
    google_scholar_id: https://scholar.google.com/citations?user=tPhwyYsAAAAJ&hl=en
    homepage: https://brucelyu17.github.io/
    institution: University of Rochester
    last_name: Lyu
    name: Hanjia Lyu
    orcid: https://orcid.org/0000-0002-3876-0094
    semantic_scholar_id: https://www.semanticscholar.org/author/Hanjia-Lyu/1486450921
    username: ~Hanjia_Lyu1
  - dblp_id: https://dblp.org/pid/08/237-2
    emails: '****@cs.ucla.edu'
    first_name: Song
    google_scholar_id: https://scholar.google.com/citations?user=SjbhMQEAAAAJ&hl=en&oi=ao
    homepage: https://songjiang0909.github.io/
    last_name: Jiang
    name: Song Jiang
    semantic_scholar_id: https://www.semanticscholar.org/author/Song-Jiang/2249954878
    username: ~Song_Jiang1
  - dblp_id: https://dblp.org/pid/136/2474
    emails: '****@meta.com'
    first_name: Hanqing
    google_scholar_id: https://scholar.google.com/citations?user=ubUx3R0AAAAJ&hl=en
    homepage: https://hanqingzeng.com
    institution: Meta AI
    last_name: Zeng
    name: Hanqing Zeng
    semantic_scholar_id: https://www.semanticscholar.org/author/Hanqing-Zeng/33352252
    username: ~Hanqing_Zeng1
  - dblp_id: https://dblp.org/pid/61/3251
    emails: '****@ieee.org'
    first_name: Yinglong
    institution: Meta
    last_name: Xia
    name: Yinglong Xia
    orcid: https://orcid.org/0000-0002-8155-5440
    username: ~Yinglong_Xia1
  - dblp_id: https://dblp.org/pid/33/8610
    emails: '****@fb.com'
    first_name: Qifan
    google_scholar_id: https://scholar.google.com/citations?user=LrSyLosAAAAJ&hl=en
    homepage: https://wqfcr.github.io/
    institution: Meta AI
    last_name: Wang
    name: Qifan Wang
    username: ~Qifan_Wang1
  - dblp_id: https://dblp.org/pid/119/2063
    emails: '****@meta.com'
    first_name: Si
    google_scholar_id: https://scholar.google.com/citations?user=JCs-EyYAAAAJ&hl=en
    homepage: https://sizhang2.web.illinois.edu/
    institution: Meta
    last_name: Zhang
    name: Si Zhang
    username: ~Si_Zhang1
  - emails: '****@fb.com'
    first_name: Ren
    last_name: Chen
    name: Ren Chen
    username: ~Ren_Chen1
  - emails: '****@meta.com'
    first_name: Chris
    homepage: http://www.christopherleung.ca/
    institution: Meta AI and College of Computing, Georgia Institute of Technology
    last_name: Leung
    name: Chris Leung
    username: ~Chris_Leung1
  - emails: '****@gmail.com'
    first_name: Jiajie
    last_name: Tang
    name: Jiajie Tang
    username: ~Jiajie_Tang1
  - dblp_id: https://dblp.org/pid/25/5545
    emails: '****@cs.rochester.edu'
    first_name: Jiebo
    google_scholar_id: https://scholar.google.com/citations?user=CcbnBvgAAAAJ&hl=en
    homepage: https://www.cs.rochester.edu/u/jluo/
    institution: University of Rochester, University of Rochester, University of Rochester
      and University of Rochester
    last_name: Luo
    name: Jiebo Luo
    orcid: https://orcid.org/0000-0002-4516-9729
    semantic_scholar_id: https://www.semanticscholar.org/author/Jiebo-Luo/33642939
    username: ~Jiebo_Luo1
  decision: toFindings
  end_page: 612
  file: 177.pdf
  id: 177
  num_pages: 30
  openreview_id: oYXX8FKgcg
  pdf_file: 4be19f5f97a81ee5639ae32685e8f7525c275655.pdf
  start_page: 583
  title: 'LLM-Rec: Personalized Recommendation via Prompting Large Language Models'
- abstract: Large language models (LLMs) have show their remarkable ability in various
    natural language tasks. However, there are concerns that LLMs are possible to
    be used improperly or even illegally. To prevent the malicious usage of LLMs,
    detecting LLM-generated text becomes crucial in the deployment of LLM applications.
    Watermarking is an effective strategy to detect the LLM-generated content by encoding
    a pre-defined secret watermark to facilitate the detection process. However, the
    majority of existing watermark methods leverage the simple hashes of precedent
    tokens to partition vocabulary. Such watermarks can be easily eliminated by paraphrase
    and, correspondingly, the detection effectiveness will be greatly compromised.
    Thus, to enhance the robustness against paraphrase, we propose a semantics-based
    watermark framework, SemaMark. It leverages the semantics as an alternative to
    simple hashes of tokens since the semantic meaning of the sentences will be likely
    preserved under paraphrase and the watermark can remain robust. Comprehensive
    experiments are conducted to demonstrate the effectiveness and robustness of SemaMark
    under different paraphrases.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Ethics, Bias, and Fairness
  authors:
  - emails: '****@msu.edu'
    first_name: Jie
    homepage: https://renjie3.github.io/
    institution: Baidu and Michigan State University
    last_name: Ren
    name: Jie Ren
    username: ~Jie_Ren6
  - emails: '****@msu.edu'
    first_name: Han
    google_scholar_id: https://scholar.google.com/citations?user=mX2rL3IAAAAJ&hl=en
    homepage: https://cse.msu.edu/~xuhan1/
    institution: Michigan State University
    last_name: Xu
    name: Han Xu
    username: ~Han_Xu1
  - dblp_id: https://dblp.org/pid/155/1107
    emails: '****@gmail.com'
    first_name: Yiding
    google_scholar_id: https://scholar.google.com/citations?user=c7oiMdIAAAAJ&hl=en
    homepage: https://liuyiding.net
    institution: Baidu
    last_name: Liu
    name: Yiding Liu
    username: ~Yiding_Liu1
  - emails: '****@msu.edu'
    first_name: Yingqian
    homepage: https://yingqiancui.github.io/
    last_name: Cui
    name: Yingqian Cui
    username: ~Yingqian_Cui1
  - dblp_id: https://dblp.org/pid/16/1524
    emails: '****@gmail.com'
    first_name: Shuaiqiang
    google_scholar_id: https://scholar.google.com.hk/citations?user=8SbYYcIAAAAJ&hl=en
    homepage: http://wangshuaiqiang.net/
    institution: Baidu Inc.
    last_name: Wang
    name: Shuaiqiang Wang
    semantic_scholar_id: https://www.semanticscholar.org/author/Shuaiqiang-Wang/2386396
    username: ~Shuaiqiang_Wang2
  - dblp_id: https://dblp.org/pid/91/4572
    emails: '****@acm.org'
    first_name: Dawei
    google_scholar_id: https://scholar.google.com/citations?user=GuQ9bpAAAAAJ&hl=zh-CN
    institution: Baidu
    last_name: Yin
    name: Dawei Yin
    username: ~Dawei_Yin1
  - dblp_id: https://dblp.org/pid/64/10812
    emails: '****@msu.edu'
    first_name: Jiliang
    google_scholar_id: https://scholar.google.com/citations?user=WtzKMWAAAAAJ&hl=en&oi=ao
    homepage: https://www.cse.msu.edu/~tangjili/
    institution: Michigan State University
    last_name: Tang
    name: Jiliang Tang
    username: ~Jiliang_Tang1
  decision: toFindings
  end_page: 625
  file: 178.pdf
  id: 178
  num_pages: 13
  openreview_id: YZ3RPnx94m
  pdf_file: 370c2cc98a6be70793d86c9d8847f18cdb123978.pdf
  start_page: 613
  title: A Robust Semantics-based Watermark for Large Language Model against Paraphrasing
- abstract: 'Large language models are rapidly replacing help forums like StackOverflow,
    and are especially helpful to non-professional programmers and end users. These
    users are often interested in \emph{data-centric tasks}, like spreadsheet manipulation
    and data wrangling, which are hard to solve if the intent is only communicated
    using a natural-language description, without including data. But how do we decide
    how much data and which data to include in the prompt?


    This paper makes two contributions towards answering this question.  First, we
    create a dataset of real-world NL-to-code tasks manipulating tabular data, mined
    from StackOverflow posts. Second, we introduce a novel \emph{cluster-then-select}
    prompting technique, which adds the most representative rows from the input data
    to the LLM prompt. Our experiments show that LLM performance is indeed sensitive
    to the amount of data passed in the prompt, and that for tasks with a lot of syntactic
    variation in the input table,

    our cluster-then-select technique outperforms a random selection baseline.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - dblp_id: https://dblp.org/pid/228/9106
    emails: '****@eng.ucsd.edu'
    first_name: Shraddha
    google_scholar_id: https://scholar.google.com/citations?user=hcG7ffkAAAAJ&hl=en
    homepage: https://shraddhabarke.github.io/
    last_name: Barke
    name: Shraddha Barke
    username: ~Shraddha_Barke1
  - dblp_id: https://dblp.org/pid/66/4776.html
    emails: '****@gmx.net'
    first_name: Christian
    google_scholar_id: https://scholar.google.de/citations?user=xkzwZeoAAAAJ&hl=de
    institution: Research, Microsoft
    last_name: Poelitz
    name: Christian Poelitz
    username: ~Christian_Poelitz1
  - dblp_id: https://dblp.org/pid/276/1629
    emails: '****@microsoft.com'
    first_name: Carina
    google_scholar_id: https://scholar.google.com/citations?user=63f9xyYAAAAJ&hl=en&oi=ao
    institution: Microsoft
    last_name: Negreanu
    middle_name: Suzana
    name: Carina Suzana Negreanu
    semantic_scholar_id: https://www.semanticscholar.org/author/C.-Negreanu/21777559
    username: ~Carina_Suzana_Negreanu1
  - dblp_id: https://dblp.org/pid/z/BGZorn.html
    emails: '****@microsoft.com'
    first_name: Benjamin
    google_scholar_id: https://scholar.google.com/citations?user=mpar-iAAAAAJ&hl=en&oi=ao
    homepage: https://www.microsoft.com/en-us/research/people/zorn/
    last_name: Zorn
    name: Benjamin Zorn
    orcid: https://orcid.org/0000-0001-9719-9842
    username: ~Benjamin_Zorn1
  - emails: '****@gmail.com'
    first_name: "Jos\xE9"
    homepage: https://www.josecambronero.com
    institution: Microsoft
    last_name: Cambronero
    name: "Jos\xE9 Cambronero"
    username: "~Jos\xE9_Cambronero1"
  - dblp_id: https://dblp.org/pid/g/AndrewDGordon
    emails: '****@microsoft.com'
    first_name: Andrew
    google_scholar_id: https://scholar.google.com/citations?user=mfBjUiIAAAAJ
    homepage: https://www.microsoft.com/en-us/research/people/adg/
    institution: University of Edinburgh and Microsoft Research
    last_name: Gordon
    middle_name: D.
    name: Andrew D. Gordon
    orcid: https://orcid.org/0000-0002-5809-2484
    username: ~Andrew_D._Gordon1
  - emails: '****@microsoft.com'
    first_name: Vu
    google_scholar_id: https://scholar.google.com/citations?user=mijlpU4AAAAJ
    homepage: https://www.vuminhle.com/
    institution: Microsoft
    last_name: Le
    name: Vu Le
    username: ~Vu_Le2
  - dblp_id: https://dblp.org/pid/131/8497
    emails: '****@gmail.com'
    first_name: Elnaz
    google_scholar_id: https://scholar.google.com/citations?user=roaZYUcAAAAJ&hl=en
    homepage: https://www.microsoft.com/en-us/research/people/elnouri/
    last_name: Nouri
    name: Elnaz Nouri
    semantic_scholar_id: https://www.semanticscholar.org/author/E.-Nouri/2074997
    username: ~Elnaz_Nouri1
  - emails: '****@ucsd.edu'
    first_name: Nadia
    google_scholar_id: https://scholar.google.com.tw/citations?user=CxzUX0EAAAAJ
    homepage: https://cseweb.ucsd.edu/~npolikarpova/
    institution: University of California, San Diego
    last_name: Polikarpova
    name: Nadia Polikarpova
    username: ~Nadia_Polikarpova1
  - emails: '****@microsoft.com'
    first_name: Advait
    institution: Microsoft
    last_name: Sarkar
    name: Advait Sarkar
    orcid: https://orcid.org/0000-0002-5401-3478
    username: ~Advait_Sarkar1
  - emails: '****@microsoft.com'
    first_name: Brian
    institution: Research, Microsoft
    last_name: Slininger
    name: Brian Slininger
    username: ~Brian_Slininger1
  - dblp_id: https://dblp.org/pid/16/751
    emails: '****@gmail.com'
    first_name: Neil
    last_name: Toronto
    name: Neil Toronto
    username: ~Neil_Toronto1
  - dblp_id: https://dblp.org/pid/59/4124-1
    emails: '****@microsoft.com'
    first_name: Jack
    homepage: https://jackw.io
    institution: Microsoft
    last_name: Williams
    name: Jack Williams
    semantic_scholar_id: https://www.semanticscholar.org/author/Jack-Williams/2111787940
    username: ~Jack_Williams2
  decision: toFindings
  end_page: 638
  file: 182.pdf
  id: 182
  num_pages: 13
  openreview_id: E7SIfkOzrA
  pdf_file: 56dd6ee87a1fdba54fd28dc051c56c80a8a38a4d.pdf
  start_page: 626
  title: Solving Data-centric Tasks using Large Language Models
- abstract: 'This paper presents a study on strategies to enhance the translation
    capabilities of large language models (LLMs) in the context of machine translation
    (MT) tasks. The paper proposes a novel paradigm consisting of three stages: Secondary
    Pre-training using Extensive Monolingual Data, Continual Pre-training with Interlinear
    Text Format Documents, and Leveraging Source-Language Consistent Instruction for
    Supervised Fine-Tuning. Previous research on LLMs focused on various strategies
    for supervised fine-tuning (SFT), but their effectiveness has been limited. While
    traditional machine translation approaches rely on vast amounts of parallel bilingual
    data, our paradigm highlights the importance of using smaller sets of high-quality
    bilingual data. We argue that the focus should be on augmenting LLMs'' cross-lingual
    alignment abilities during pre-training rather than solely relying on extensive
    bilingual data during SFT. Experimental results conducted using the Llama2\cite{touvron2023llama}

    model, particularly on Chinese-Llama2\cite{Chinese-LLaMA-Alpaca} after monolingual
    augmentation, demonstrate the improved translation capabilities of LLMs. A significant
    contribution of our approach lies in Stage2: Continual Pre-training with Interlinear
    Text Format Documents, which requires less than 1B training data, making our method
    highly efficient. Additionally, in Stage3, we observed that setting instructions
    consistent with the source language benefits the supervised fine-tuning process.
    Experimental results demonstrate that our approach surpasses previous work and
    achieves superior performance compared to models such as NLLB-54B\cite{nllbteam2022language}
    and GPT3.5-text-davinci-003, despite having a significantly smaller parameter
    count of only 7B or 13B. This achievement establishes our method as a pioneering
    strategy in the field of machine translation.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Translation
  authors:
  - dblp_id: https://dblp.org/pid/189/7455
    emails: '****@huawei.com'
    first_name: Jiaxin
    google_scholar_id: https://scholar.google.com/citations?user=RLPmDoUAAAAJ
    last_name: Guo
    name: Jiaxin GUO
    semantic_scholar_id: https://www.semanticscholar.org/author/Jiaxian-Guo/30686662
    username: ~Jiaxin_GUO1
  - dblp_id: https://dblp.uni-trier.de/pid/54/4089.html
    emails: '****@gmail.com'
    first_name: Hao
    google_scholar_id: https://scholar.google.com/citations?user=lOsjM5sAAAAJ&hl=en
    homepage: https://github.com/yanghaocsg
    last_name: Yang
    name: Hao Yang
    username: ~Hao_Yang7
  - emails: '****@126.com'
    first_name: Zongyao
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=4nm_f7sAAAAJ
    last_name: Li
    name: Zongyao Li
    username: ~Zongyao_Li1
  - dblp_id: https://dblp.org/pid/166/0470.html
    emails: '****@163.com'
    first_name: Daimeng
    google_scholar_id: https://scholar.google.com/citations?user=v5eYxNUAAAAJ
    last_name: Wei
    name: Daimeng Wei
    semantic_scholar_id: https://www.semanticscholar.org/author/Daimeng-Wei/8884457
    username: ~Daimeng_Wei1
  - dblp_id: https://dblp.org/pid/268/1964.html
    emails: '****@huawei.com'
    first_name: Hengchao
    google_scholar_id: https://scholar.google.com/citations?user=BkhK0BMAAAAJ&hl=en
    last_name: Shang
    name: Hengchao Shang
    semantic_scholar_id: https://www.semanticscholar.org/author/Hengchao-Shang/1768147214
    username: ~Hengchao_Shang1
  - emails: '****@163.com'
    first_name: Xiaoyu
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=IR6soO8AAAAJ
    institution: Huawei Technologies Ltd.
    last_name: Chen
    name: Xiaoyu Chen
    username: ~Xiaoyu_Chen5
  decision: toFindings
  end_page: 649
  file: 190.pdf
  id: 190
  num_pages: 11
  openreview_id: 3Y1Hmqbwte
  pdf_file: 1ac3cddb9af70fdcd2c5237ea8173f95bf08cf92.pdf
  start_page: 639
  title: A Novel Paradigm Boosting Translation Capabilities of Large Language Models
- abstract: We present a new challenge to examine whether large language models understand
    social norms. In contrast to existing datasets, our dataset requires a fundamental
    understanding of social norms to solve. Our dataset features the largest set of
    social norm skills, consisting of $402$ skills and $12,383$ questions covering
    a wide set of social norms ranging from opinions and arguments to culture and
    laws. We design our dataset according to the K-12 curriculum. This enables the
    direct comparison of the social understanding of large language models to humans,
    more specifically, elementary students. While prior work generates nearly random
    accuracy on our benchmark, recent large language models such as GPT3.5-Turbo and
    LLaMA2-Chat are able to improve the performance significantly, only slightly below
    human performance. We then propose a multi-agent framework based on large language
    models to improve the models' ability to understand social norms. This method
    further improves large language models to be on par with humans. Given the increasing
    adoption of large language models in real-world applications, our finding is particularly
    important and presents a unique direction for future improvements.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - emails: '****@pku.edu.cn'
    first_name: Ye
    google_scholar_id: https://scholar.google.com/citations?user=h8WQaTkAAAAJ
    last_name: Yuan
    name: Ye Yuan
    username: ~Ye_Yuan12
  - emails: '****@stu.pku.edu.cn'
    first_name: Kexin
    homepage: https://github.com/Kexin2000
    institution: Peking University
    last_name: Tang
    name: Kexin Tang
    username: ~Kexin_Tang2
  - dblp_id: https://dblp.org/pid/217/2324
    emails: '****@pku.edu.cn'
    first_name: Jianhao
    google_scholar_id: https://scholar.google.com/citations?user=9fppVAUAAAAJ&hl=en&oi=ao
    last_name: Shen
    name: Jianhao Shen
    username: ~Jianhao_Shen1
  - dblp_id: https://dblp.org/pid/73/1844-4
    emails: '****@pku.edu.cn'
    first_name: Ming
    google_scholar_id: https://scholar.google.com/citations?user=LbzoQBsAAAAJ&hl=zh-CN
    homepage: https://cs.pku.edu.cn/info/1080/1371.htm
    institution: Peking University
    last_name: Zhang
    name: Ming Zhang
    orcid: https://orcid.org/0000-0002-9809-3430
    username: ~Ming_Zhang5
  - dblp_id: https://dblp.org/pid/62/3432
    emails: '****@gmail.com'
    first_name: Chenguang
    google_scholar_id: https://scholar.google.com/citations?user=hsZ2aj0AAAAJ&hl=en
    homepage: https://cgraywang.github.io/
    institution: Washington University, Saint Louis
    last_name: Wang
    name: Chenguang Wang
    username: ~Chenguang_Wang1
  decision: toFindings
  end_page: 699
  file: 195.pdf
  id: 195
  num_pages: 50
  openreview_id: 0cduiqVeAx
  pdf_file: bc3ec5e8376d5f44e004bb1e53fb0d13e12ffc2a.pdf
  start_page: 650
  title: Measuring Social Norms of Large Language Models
- abstract: This work addresses source-free domain adaptation (SFDA) for Question
    Answering (QA), wherein a model trained on a source domain is adapted to unlabeled
    target domains without additional source data. Existing SFDA methods only focus
    on the adaptation phase, overlooking the impact of source domain training on model
    generalizability. In this paper, we argue that source model training itself is
    also critical for improving the adaptation performance and stability. To this
    end, we investigate the role of prompt learning as an effective method to internalize
    domain-agnostic QA knowledge, which can be integrated into source training. After
    source training, an interactive self-learning strategy is proposed to further
    fine tune both model and prompt in the model adaptation phase. This leads to the
    Prompt-Assisted Self-Adaptive Learning (PASAL), an innovative SFDA approach for
    QA. Empirical evaluation on four benchmark datasets shows that PASAL surpasses
    existing methods in managing domain gaps and demonstrates greater stability across
    various target domains, validating the significance of source domain training
    for effective domain adaptation.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Question Answering
  authors:
  - emails: '****@uwo.ca'
    first_name: Maxwell
    google_scholar_id: https://scholar.google.com/citations?user=gI49wecAAAAJ&hl=en
    last_name: Yin
    middle_name: Juncheng
    name: Maxwell Juncheng Yin
    semantic_scholar_id: https://www.semanticscholar.org/author/Maxwell-J.-Yin/2258180851
    username: ~Maxwell_Juncheng_Yin1
  - dblp_id: https://dblp.org/pid/41/6565-4.html
    emails: '****@csd.uwo.ca'
    first_name: Boyu
    google_scholar_id: https://scholar.google.com/citations?user=qAZM5KcAAAAJ&hl=en
    homepage: https://sites.google.com/site/borriewang/
    institution: University of Western Ontario
    last_name: Wang
    name: Boyu Wang
    username: ~Boyu_Wang3
  - emails: '****@uwo.ca'
    first_name: Charles
    google_scholar_id: https://scholar.google.co.uk/citations?hl=en&user=eFJ_tVsAAAAJ&view_op=list_works&sortby=pubdate
    homepage: http://cling.csd.uwo.ca/
    institution: Western University
    last_name: Ling
    name: Charles Ling
    username: ~Charles_Ling1
  decision: toFindings
  end_page: 713
  file: 209.pdf
  id: 209
  num_pages: 14
  openreview_id: awtcw3vz9S
  pdf_file: b13b00eefdba7d5d18356b31cd25c40ad9092a56.pdf
  start_page: 700
  title: Source-Free Unsupervised Domain Adaptation for Question Answering via Prompt-Assisted
    Self-learning
- abstract: Scientific document summarization has been a challenging task due to the
    long structure of the input text. The long input hinders the simultaneous effective
    modeling of both global high-order relations between sentences and local intra-sentence
    relations which is the most critical step in extractive summarization. However,
    existing methods mostly focus on one type of relation, neglecting the simultaneous
    effective modeling of both relations, which can lead to insufficient learning
    of semantic representations. In this paper, we propose HAESum, a novel approach
    utilizing graph neural networks to locally and globally model documents based
    on their hierarchical discourse structure. First, intra-sentence relations are
    learned using a local heterogeneous graph. Subsequently, a novel hypergraph self-attention
    layer is introduced to further enhance the characterization of high-order inter-sentence
    relations. We validate our approach on two benchmark datasets, and the experimental
    results demonstrate the effectiveness of HAESum and the importance of considering
    hierarchical structures in modeling long scientific documents.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Summarization
  authors:
  - emails: '****@bupt.edu.cn'
    first_name: Chenlong
    homepage: https://github.com/MoLICHENXI
    last_name: Zhao
    name: Chenlong Zhao
    username: ~Chenlong_Zhao1
  - emails: '****@bupt.edu.cn'
    first_name: Xiwen
    homepage: https://github.com/Zhou773
    last_name: Zhou
    name: Xiwen Zhou
    username: ~Xiwen_Zhou1
  - emails: '****@bupt.edu.cn'
    first_name: Xiaopeng
    homepage: https://github.com/2000xxp
    last_name: Xie
    name: Xiaopeng Xie
    username: ~Xiaopeng_Xie1
  - emails: '****@bupt.edu.cn'
    first_name: Yong
    google_scholar_id: https://scholar.google.com/citations?user=HPxykGUAAAAJ&hl=zh-CN
    institution: Beijing University of Posts and Telecommunications
    last_name: Zhang
    name: Yong Zhang
    username: ~Yong_Zhang14
  decision: toFindings
  end_page: 726
  file: 212.pdf
  id: 212
  num_pages: 13
  openreview_id: 9UH847aeNK
  pdf_file: 6a0a4d209c77b1212f20c37de92924c57931cc86.pdf
  start_page: 714
  title: Hierarchical Attention Graph for Scientific Document Summarization in Global
    and Local Level
- abstract: Linguistic entrainment, or alignment, represents a phenomenon where linguistic
    patterns employed by conversational participants converge to one another. While
    entrainment has been shown to produce a more natural user experience, most dialogue
    systems do not have any provisions for it. In this work, we introduce methods
    for achieving dialogue entrainment in a GPT-2-based end-to-end task-oriented dialogue
    system through the utilization of shared vocabulary. We experiment with training
    instance weighting, entrainment-specific loss, and additional conditioning to
    generate responses that align with the user. We demonstrate that all three approaches
    produce significantly better entrainment than the base, non-entrainment-optimized
    model, as confirmed by both automated and manual evaluation metrics.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Dialogue and Interactive Systems
  authors:
  - emails: '****@gmail.com'
    first_name: Nalin
    google_scholar_id: https://scholar.google.com/citations?user=zFNkIuMAAAAJ&hl=en
    homepage: https://knalin55.github.io/
    last_name: Kumar
    name: Nalin Kumar
    username: ~Nalin_Kumar2
  - dblp_id: https://dblp.org/pid/126/8739
    emails: '****@ufal.mff.cuni.cz'
    first_name: Ondrej
    google_scholar_id: https://scholar.google.cz/citations?user=PI7rRV0AAAAJ&hl=en
    homepage: https://ufal.mff.cuni.cz/ondrej-dusek
    institution: Charles University, Prague
    last_name: Dusek
    name: Ondrej Dusek
    orcid: https://orcid.org/0000-0002-1415-1702
    semantic_scholar_id: https://www.semanticscholar.org/author/Ondrej-Dusek/2544049
    username: ~Ondrej_Dusek1
  decision: toFindings
  end_page: 735
  file: 213.pdf
  id: 213
  num_pages: 9
  openreview_id: VDqKQnz3UQ
  pdf_file: 9a44ac004100b362bde8a0580c139b09a60b7fde.pdf
  start_page: 727
  title: 'LEEETs-Dial: Linguistic Entrainment in End-to-End Task-oriented Dialogue
    systems'
- abstract: 'In the context of computational models of dependency syntax, most dependency
    treebanks have the restriction that any valid dependency tree must have exactly
    one edge coming out of the root node in addition to respecting the spanning tree
    constraints. Many algorithms for dependency tree sampling were recently proposed,
    both for sampling with and without replacement.


    In this paper we propose a new algorithm called Wilson Reject SWOR for the case
    of sampling without replacement by adapting the Wilson Reject algorithm originally
    created for sampling with replacement and combining it with a Trie data structure.
    Experimental results indicate the efficiency of our approach in the scenario of
    sampling without replacement from dependency graphs with random weights.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Efficient/Low-Resource Methods for NLP
  authors:
  - dblp_id: https://dblp.org/pid/326/2365.html
    emails: '****@s.unibuc.ro'
    first_name: Bogdan
    google_scholar_id: https://scholar.google.com/citations?user=KF9-El0AAAAJ&hl=en
    homepage: https://github.com/dobrebogdan
    institution: University of Bucharest
    last_name: Dobre
    middle_name: Mihai
    name: Bogdan Mihai Dobre
    orcid: https://orcid.org/0009-0002-1418-6005
    semantic_scholar_id: https://www.semanticscholar.org/author/Bogdan-Dobre/2175482534
    username: ~Bogdan_Mihai_Dobre1
  decision: toFindings
  end_page: 741
  file: 222.pdf
  id: 222
  num_pages: 6
  openreview_id: 3c4LFPqU4i
  pdf_file: 31efc294573f2a49e645501f5963f10a24c38e38.pdf
  start_page: 736
  title: Efficient Dependency Tree Sampling Without Replacement
- abstract: 'Open-domain Question Answering (OpenQA) aims at answering factual questions
    with an external large-scale knowledge corpus. However, real-world knowledge is
    not static; it updates and evolves continually. Such a dynamic characteristic
    of knowledge poses a vital challenge for these models, as the trained models need
    to constantly adapt to the latest information to make sure that the answers remain
    accurate. In addition, it is still unclear how well an OpenQA model can transfer
    to completely new knowledge domains. In this paper, we investigate the generalization
    performance of a retrieval-augmented QA model in two specific scenarios: 1) adapting
    to updated versions of the same knowledge corpus;  2) switching to completely
    different knowledge domains. We observe that the generalization challenges of
    OpenQA models stem from the reader''s over-reliance on memorizing the knowledge
    from the external corpus, which hinders the model from generalizing to a new knowledge
    corpus. We introduce Corpus-Invariant Tuning (CIT), a simple but effective training
    strategy, to mitigate the knowledge over-memorization by controlling the likelihood
    of retrieved contexts during training. Extensive experimental results on multiple
    OpenQA benchmarks show that CIT achieves significantly better generalizability
    without compromising the model''s performance in its original corpus and domain.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Question Answering
  authors:
  - emails: '****@illinois.edu'
    first_name: Zixuan
    google_scholar_id: https://scholar.google.com/citations?user=SujuzXsAAAAJ&hl=en
    homepage: http://zhangzx-uiuc.github.io/
    last_name: Zhang
    name: Zixuan Zhang
    username: ~Zixuan_Zhang2
  - emails: '****@illinois.edu'
    first_name: Revanth
    google_scholar_id: https://scholar.google.com/citations?user=SXP5Ej0AAAAJ&hl=en
    homepage: https://gangiswag.github.io
    last_name: Gangi Reddy
    name: Revanth Gangi Reddy
    username: ~Revanth_Gangi_Reddy1
  - dblp_id: https://dblp.org/pid/82/6573.html
    emails: '****@gmail.com'
    first_name: Kevin
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=tFwkk-YAAAAJ
    homepage: http://www.kevinsmall.org/
    institution: Amazon
    last_name: Small
    name: Kevin Small
    semantic_scholar_id: https://www.semanticscholar.org/author/Kevin-Small/50044599
    username: ~Kevin_Small1
  - dblp_id: https://dblp.org/pid/07/4227-1
    emails: '****@tongzhang-ml.org'
    first_name: Tong
    google_scholar_id: https://scholar.google.com/citations?user=LurWtuYAAAAJ&hl=en&oi=ao
    homepage: http://tongzhang-ml.org
    institution: UIUC
    last_name: Zhang
    name: Tong Zhang
    orcid: https://orcid.org/0000-0002-5511-2558
    username: ~Tong_Zhang2
  - emails: '****@illinois.edu'
    first_name: Heng
    google_scholar_id: https://scholar.google.com/citations?user=z7GCqT4AAAAJ&hl=en
    homepage: http://blender.cs.illinois.edu/hengji.html
    institution: University of Illinois, Urbana-Champaign
    last_name: Ji
    name: Heng Ji
    username: ~Heng_Ji3
  decision: toFindings
  end_page: 753
  file: 226.pdf
  id: 226
  num_pages: 12
  openreview_id: aLF8FcnEow
  pdf_file: c900acfdf7013472ad295c2380cd6fe972b6ea60.pdf
  start_page: 742
  title: Towards Better Generalization in Open-Domain Question Answering by Mitigating
    Context Memorization
- abstract: 'Existing grammatical error correction tools do not provide natural language
    explanations of the errors that they correct in user-written text. However, such
    explanations are essential for helping users learn the language by gaining a deeper
    understanding of its grammatical rules (DeKeyser, 2003; Ellis et al., 2006).


    To address this gap, we propose the task of grammar error explanation, where a
    system needs to provide one-sentence explanations for each grammatical error in
    a pair of erroneous and corrected sentences. The task is not easily solved by
    prompting LLMs: we find that, using one-shot prompting, GPT-4 only explains 40.6%
    of the errors and does not even attempt to explain 39.8% of the errors.


    Since LLMs struggle to identify grammar errors, we develop a two-step pipeline
    that leverages fine-tuned and prompted large language models to perform structured
    atomic token edit extraction, followed by prompting GPT-4 to explain each edit.
    We evaluate our pipeline on German, Chinese, and English grammar error correction
    data. Our atomic edit extraction achieves an F1 of 0.93 on German, 0.91 on Chinese,
    and 0.891 on English. Human evaluation of generated explanations reveals that
    93.9% of German errors, 96.4% of Chinese errors, and 92.20% of English errors
    are correctly detected and explained. To encourage further research, we open-source
    our data and code.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - dblp_id: https://dblp.org/pid/331/5829
    emails: '****@umass.edu'
    first_name: Yixiao
    google_scholar_id: https://scholar.google.com/citations?user=4OgciqMAAAAJ&hl=en&oi=ao
    homepage: https://yixiao-song.github.io
    institution: University of Massachusetts at Amherst
    last_name: Song
    name: Yixiao Song
    semantic_scholar_id: https://www.semanticscholar.org/author/Yixiao-Song/8280888
    username: ~Yixiao_Song1
  - dblp_id: https://dblp.org/pid/207/8485
    emails: '****@gmail.com'
    first_name: Kalpesh
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=9g2BsMUAAAAJ
    homepage: http://martiansideofthemoon.github.io/
    institution: Google
    last_name: Krishna
    name: Kalpesh Krishna
    semantic_scholar_id: https://www.semanticscholar.org/author/Kalpesh-Krishna/26161085
    username: ~Kalpesh_Krishna1
  - emails: '****@linguist.umass.edu'
    first_name: Rajesh
    homepage: https://people.umass.edu/bhatt/
    institution: University of Massachusetts at Amherst
    last_name: Bhatt
    name: Rajesh Bhatt
    username: ~Rajesh_Bhatt1
  - dblp_id: https://dblp.org/pid/47/1252
    emails: '****@ttic.edu'
    first_name: Kevin
    google_scholar_id: http://scholar.google.com/citations?user=kDHs7DYAAAAJ&hl=en
    homepage: http://ttic.uchicago.edu/~kgimpel/index.html
    institution: Toyota Technological Institute at Chicago
    last_name: Gimpel
    name: Kevin Gimpel
    username: ~Kevin_Gimpel1
  - dblp_id: https://dblp.org/pid/148/9178
    emails: '****@cs.umass.edu'
    first_name: Mohit
    google_scholar_id: https://scholar.google.com/citations?user=rBVA5tcAAAAJ&hl=en
    homepage: http://cs.umass.edu/~miyyer
    institution: University of Massachusetts Amherst
    last_name: Iyyer
    name: Mohit Iyyer
    username: ~Mohit_Iyyer1
  decision: toFindings
  end_page: 781
  file: 229.pdf
  id: 229
  num_pages: 28
  openreview_id: Zhq4fJyk3p
  pdf_file: 87207fa0bc9dc014b30da1517fbbd4b2e48970fb.pdf
  start_page: 754
  title: GEE! Grammar Error Explanation with Large Language Models
- abstract: Large Language Models (LLMs) have demonstrated significant success across
    various domains. However, their application in complex decision-making tasks frequently
    necessitates intricate prompt engineering or fine-tuning, leading to challenges
    in unseen downstream tasks and heavy demands on computational resources. Meanwhile,
    Reinforcement Learning (RL) has been recognized as effective in decision-making
    problems but struggles in environments with sparse rewards, such as open-world
    games. To overcome these challenges, we introduce AdaRefiner, a novel framework
    designed to enhance the synergy between LLMs and RL feedback. The key component
    of AdaRefiner is a lightweight Adapter Language Model (LM), which automatically
    refines task comprehension based on feedback from RL agents. This method mitigates
    the need for intricate prompt engineering and intensive LLM fine-tuning while
    maintaining the LLMs' generalization abilities and enhancing their decision-making
    capabilities in downstream tasks. Empirical evaluations of AdaRefiner on $22$
    diverse tasks within the open-world game \textit{Crafter} have demonstrated its
    superior effectiveness, especially in guiding agents towards higher-level and
    common-sense skills. Our work makes contributions to the automatic self-refinement
    of LLMs with RL feedback, offering a more adaptable and efficient solution for
    complex decision-making problems. The code is available at https://github.com/PKU-RL/AdaRefiner.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multimodality and Language Grounding to Vision, Robotics and Beyond
  authors:
  - dblp_id: https://dblp.org/pid/73/10693-2
    emails: '****@stu.pku.edu.cn'
    first_name: Wanpeng
    google_scholar_id: https://scholar.google.com/citations?user=_IKNf9EAAAAJ
    homepage: https://www.zhangwp.com
    institution: Peking University
    last_name: Zhang
    name: Wanpeng Zhang
    orcid: https://orcid.org/0000-0001-5351-3449
    username: ~Wanpeng_Zhang1
  - dblp_id: https://dblp.org/pid/99/965
    emails: '****@pku.edu.cn'
    first_name: Zongqing
    google_scholar_id: https://scholar.google.com.tw/citations?user=k3IFtTYAAAAJ
    homepage: https://z0ngqing.github.io/
    institution: Peking University
    last_name: Lu
    name: Zongqing Lu
    username: ~Zongqing_Lu1
  decision: toFindings
  end_page: 799
  file: 246.pdf
  id: 246
  num_pages: 18
  openreview_id: g4fmKFDLqz
  pdf_file: 573e02a21d91cb7279017958a3ac20fe52452036.pdf
  start_page: 782
  title: 'AdaRefiner: Refining Decisions of Language Models with Adaptive Feedback'
- abstract: 'Language models pre-trained on general text have achieved impressive
    results in diverse fields. Yet, the distinct linguistic characteristics of task-oriented
    dialogues (TOD) compared to general text limit the practical utility of existing
    language models. Current task-oriented dialogue pre-training methods overlook
    the one-to-many property of conversations, where multiple responses can be appropriate
    given the same conversation context.

    In this paper, we propose a novel dialogue pre-training model called DivTOD, which
    collaborates with LLMs to learn diverse task-oriented dialogue representations.
    DivTOD guides LLMs in transferring diverse knowledge to smaller models while removing
    domain knowledge that contradicts task-oriented dialogues. Experiments show that
    our model outperforms strong TOD baselines on various downstream dialogue tasks
    and learns the intrinsic diversity of task-oriented dialogues.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - dblp_id: https://dblp.org/pid/174/3836
    emails: '****@bupt.edu.cn'
    first_name: Weihao
    homepage: https://zeng-wh.github.io/
    last_name: Zeng
    name: Weihao Zeng
    semantic_scholar_id: https://www.semanticscholar.org/author/Weihao-Zeng/2069640466
    username: ~Weihao_Zeng2
  - emails: '****@bupt.edu.cn'
    first_name: Dayuan
    last_name: Fu
    name: Dayuan Fu
    orcid: https://orcid.org/0000-0003-3614-6653
    username: ~Dayuan_Fu2
  - dblp_id: https://dblp.org/pid/79/2314
    emails: '****@bupt.cn'
    first_name: Keqing
    google_scholar_id: https://scholar.google.com/citations?user=811USNoAAAAJ&hl=en
    homepage: https://helicqin.github.io/about/index.html
    institution: Meituan Group
    last_name: He
    name: Keqing He
    semantic_scholar_id: https://www.semanticscholar.org/author/Keqing-He/2058349088
    username: ~Keqing_He1
  - emails: '****@bupt.edu.cn'
    first_name: Yejie
    homepage: https://github.com/banksy23
    last_name: Wang
    name: Yejie Wang
    username: ~Yejie_Wang1
  - emails: '****@bupt.edu.cn'
    first_name: Yukai
    google_scholar_id: https://scholar.google.com/citations?view_op=new_profile&hl=zh-CN
    last_name: Xu
    name: Yukai Xu
    username: ~Yukai_Xu1
  - dblp_id: https://dblp.org/pid/41/5448
    emails: '****@bupt.edu.cn'
    first_name: Weiran
    last_name: Xu
    name: Weiran Xu
    semantic_scholar_id: https://www.semanticscholar.org/author/Weiran-Xu/1753096
    username: ~Weiran_Xu1
  decision: toFindings
  end_page: 813
  file: 256.pdf
  id: 256
  num_pages: 14
  openreview_id: h4FZwiZzUv
  pdf_file: 537dad0e33ba65c84edfde5f418312c06d0c92c5.pdf
  start_page: 800
  title: 'DivTOD: Unleashing the Power of LLMs for Diversifying Task-Oriented Dialogue
    Representations'
- abstract: 'Recent advancements in language modeling have led to the emergence

    of Large Language Models (LLMs) capable of

    various natural language processing tasks.

    Despite their success in text-based tasks, applying LLMs to the speech domain

    remains limited and challenging. This paper presents BLOOMZMMS, a novel model

    that integrates a multilingual LLM with a multilingual speech encoder,

    aiming to harness the capabilities of LLMs for speech recognition and beyond.

    Utilizing a multi-instructional training approach, we demonstrate the transferability

    of linguistic knowledge from the text to the speech modality.

    Our experiments, conducted on 1900 hours of transcribed data from 139 languages,

    establish that a multilingual speech representation can be effectively

    learned and aligned with a multilingual LLM. While this learned representation

    initially shows limitations in task generalization, we address this issue by

    generating synthetic targets in a multi-instructional style.

    Our zero-shot evaluation results confirm the robustness of our approach across

    multiple tasks, including speech translation and multilingual spoken language

    understanding, thereby opening new avenues for applying LLMs in the speech domain.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Speech recognition, text-to-speech and spoken language understanding
  authors:
  - dblp_id: https://dblp.org/pid/224/0248
    emails: '****@gmail.com'
    first_name: Pavel
    google_scholar_id: https://scholar.google.de/citations?user=PKoLd2EAAAAJ&hl=en
    institution: University of Stuttgart, University of Stuttgart
    last_name: Denisov
    name: Pavel Denisov
    orcid: https://orcid.org/0009-0003-3486-2696
    semantic_scholar_id: https://www.semanticscholar.org/author/Pavel-Denisov/51151648
    username: ~Pavel_Denisov1
  - dblp_id: https://dblp.org/pid/10/9231
    emails: '****@ims.uni-stuttgart.de'
    first_name: Thang
    google_scholar_id: https://scholar.google.de/citations?user=Nxbi5YgAAAAJ&hl=en
    homepage: https://www.ims.uni-stuttgart.de/institut/mitarbeiter/thangvu
    institution: University of Stuttgart, University of Stuttgart
    last_name: Vu
    name: Thang Vu
    semantic_scholar_id: https://www.semanticscholar.org/author/Ngoc-Thang-Vu/4160376
    username: ~Thang_Vu2
  decision: toFindings
  end_page: 834
  file: 258.pdf
  id: 258
  num_pages: 21
  openreview_id: IZtnqF2p7W
  pdf_file: b315a6508d9b9328d19daf49cbe507ef85c9f678.pdf
  start_page: 814
  title: Teaching a Multilingual Large Language Model to Understand Multilingual Speech
    via Multi-Instructional Training
- abstract: We are currently in an era of fierce competition among various large language
    models (LLMs), continuously pushing the boundaries of benchmark performance. However,
    genuinely assessing the capabilities of these LLMs has become a challenging and
    critical issue due to potential data contamination. In this paper, we propose
    a novel and valuable method, Clean-Eval, which mitigates the issue of data contamination
    and evaluates the LLMs more cleanly. Clean-Eval employs a neural-based model to
    paraphrase and back-translate the contaminated data into a candidate set, generating
    expressions with the same meaning but in different surface forms. A semantic detector
    is then used to filter those generated low-quality samples to narrow down this
    candidate set. Candidates with moderate BLEURT scores against the original samples
    are selected as the final evaluation set. According to human assessment, this
    set is almost semantically equivalent to the original contamination set but expressed
    differently. We conduct experiments on 20 existing benchmarks across diverse tasks,
    and results demonstrate that Clean-Eval substantially restores the actual evaluation
    results on contaminated LLMs under both few-shot learning and fine-tuning scenarios.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - emails: '****@sjtu.edu.cn'
    first_name: Wenhong
    homepage: https://github.com/zwhong714
    last_name: Zhu
    name: Wenhong Zhu
    username: ~Wenhong_Zhu1
  - dblp_id: https://dblp.org/pid/349/2933
    emails: '****@sjtu.edu.cn'
    first_name: Hongkun
    google_scholar_id: https://scholar.google.com/citations?user=a3UulbMAAAAJ&hl=en
    homepage: https://hongkunhao.github.io/
    last_name: Hao
    name: Hongkun Hao
    username: ~Hongkun_Hao1
  - dblp_id: https://dblp.org/pid/52/6077-2
    emails: '****@gmail.com'
    first_name: Zhiwei
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=SL-AdukAAAAJ
    homepage: https://zwhe99.github.io/
    institution: Shanghai Jiao Tong University
    last_name: He
    name: Zhiwei He
    orcid: https://orcid.org/0000-0002-4807-0062
    semantic_scholar_id: https://www.semanticscholar.org/author/Zhiwei-He/2610876
    username: ~Zhiwei_He1
  - emails: '****@gmail.com'
    first_name: Yun-Ze
    google_scholar_id: https://scholar.google.com/citations?user=qOQwD7UAAAAJ&hl
    homepage: https://yunzesong.github.io/
    last_name: Song
    name: Yun-Ze Song
    username: ~Yun-Ze_Song1
  - emails: '****@zju.edu.cn'
    first_name: Jiao
    homepage: https://github.com/zjujyy?tab=repositories
    last_name: Yueyang
    name: Jiao Yueyang
    username: ~Jiao_Yueyang1
  - emails: '****@mails.tsinghua.edu.cn'
    first_name: Yumeng
    homepage: https://perhacept.github.io/
    last_name: Zhang
    name: Yumeng Zhang
    username: ~Yumeng_Zhang3
  - dblp_id: https://dblp.org/pid/327/7889
    emails: '****@ed.ac.uk'
    first_name: Hanxu
    homepage: https://hanxuhu.github.io
    last_name: Hu
    name: Hanxu Hu
    username: ~Hanxu_Hu1
  - dblp_id: https://dblp.org/pid/161/4646-2.html
    emails: '****@cantab.ac.uk'
    first_name: Yiran
    homepage: https://www.neurosurg.cam.ac.uk/research-groups/brain-tumour-imaging-lab/2988-2/mr-yiran-wei/
    last_name: Wei
    name: YIRAN WEI
    orcid: https://orcid.org/0000-0003-1791-6050
    username: ~YIRAN_WEI1
  - dblp_id: https://dblp.org/pid/w/RuiWang15
    emails: '****@sjtu.edu.cn'
    first_name: Rui
    google_scholar_id: https://scholar.google.com/citations?user=oTU0v5IAAAAJ&h
    homepage: https://wangruinlp.github.io/
    institution: Shanghai Jiao Tong University
    last_name: Wang
    name: Rui Wang
    orcid: https://orcid.org/0000-0001-8007-2503
    username: ~Rui_Wang10
  - emails: '****@link.cuhk.edu.hk'
    first_name: Hongyuan
    last_name: Lu
    name: Hongyuan Lu
    username: ~Hongyuan_Lu2
  decision: toFindings
  end_page: 847
  file: 261.pdf
  id: 261
  num_pages: 13
  openreview_id: 4bLh5DsduU
  pdf_file: 3802aa790e9c4fc77470865f0fa965dfae5572df.pdf
  start_page: 835
  title: "CLEAN\u2013EVAL: Clean Evaluation on Contaminated Large Language Models"
- abstract: "End-to-end speech summarization on long recordings is challenging because\
    \ of the high computational cost.  Block-wise Adaptation for Speech Summarization\
    \ (BASS) summarizes arbitrarily long sequences by sequentially processing abutting\
    \ chunks of audio. Despite the benefits of BASS, it has higher compute time due\
    \ to sequential processing of all blocks, regardless of whether they are relevant\
    \ to the final summary. \nIn this paper, we propose R-BASS, a new relevance-aware\
    \ block-wise adaptation method. First, we introduce two approaches to automatically\
    \ estimate block relevance based on lexical and semantic similarity between the\
    \ block-level transcript and the summary. Experiments on the How2 dataset show\
    \ that using ground truth relevance during inference improves efficiency by 63.9\
    \ % by dropping irrelevant blocks. Finally, we incorporate relevance scores into\
    \ training using a novel relevance loss and relevance predictor, and the proposed\
    \ R-BASS model makes it possible to drop 86.3 % of the blocks while retaining\
    \ comparable performance, resulting in a 2.2x speedup over BASS."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Speech recognition, text-to-speech and spoken language understanding
  authors:
  - dblp_id: https://dblp.org/pid/229/4537
    emails: '****@google.com'
    first_name: Roshan
    google_scholar_id: https://scholar.google.com/citations?user=yZ4QLqsAAAAJ&hl=en
    homepage: https://roshansh-cmu.github.io/
    institution: Google
    last_name: Sharma
    name: Roshan Sharma
    semantic_scholar_id: https://www.semanticscholar.org/author/Roshan-Sharma/145521253
    username: ~Roshan_Sharma1
  - emails: '****@umass.edu'
    first_name: Ruchira
    last_name: Sharma
    name: Ruchira Sharma
    username: ~Ruchira_Sharma3
  - emails: '****@andrew.cmu.edu'
    first_name: Hira
    homepage: https://www.linkedin.com/in/hiradhamyal/
    institution: Carnegie Mellon University
    last_name: Dhamyal
    name: Hira Dhamyal
    username: ~Hira_Dhamyal1
  - emails: '****@cs.cmu.edu'
    first_name: Rita
    homepage: http://mlsp.cs.cmu.edu/people/rsingh/index.html
    institution: School of Computer Science, Carnegie Mellon University
    last_name: Singh
    name: Rita Singh
    username: ~Rita_Singh1
  - dblp_id: https://dblp.org/pid/60/3996
    emails: '****@cs.cmu.edu'
    first_name: Bhiksha
    homepage: https://www.cs.cmu.edu/directory/bhikshar/
    institution: Carnegie Mellon University, Carnegie Mellon University and Mohamed
      bin Zayed University of Artificial Intelligence
    last_name: Raj
    name: Bhiksha Raj
    username: ~Bhiksha_Raj1
  decision: toFindings
  end_page: 857
  file: 264.pdf
  id: 264
  num_pages: 10
  openreview_id: NyOI69rlOW
  pdf_file: a40a8aa78e30e6e09c407e8a9b2a014aa4c70130.pdf
  start_page: 848
  title: 'R-BASS : Relevance-aided Block-wise Adaptation for Speech Summarization'
- abstract: 'Large language models (LLMs) often struggle with maintaining accuracy
    throughout multiple multiple reasoning steps, especially in mathematical reasoning
    where an error in earlier steps can propagate to subsequent ones and it ultimately
    leading to an incorrect answer.

    To reduce error propagation, guided decoding is employed to direct the LM decoding
    on a step-by-step basis. We argue that in guided decoding, assessing the potential
    of an incomplete reasoning path can be more advantageous than simply ensuring
    per-step correctness, as the former approach leads towards a correct final answer.
    This transforms the task into a $\textit{value estimation}$ problem in planning.


    Inspired by the findings that $\textit{outcome supervision for guided decoding
    essentially acts as a value model}$, we propose Outcome-supervised Value Model
    (OVM) that employs outcome supervision for training a value model, which prioritizes
    steps that lead to accurate conclusions. Furthermore, the OVM eliminates the need
    for labor-intensive annotations of step-level correctness, thereby significantly
    enhancing its scalability. Our experiments on two multi-step mathematical reasoning
    datasets, GSM8K and Game of 24, demonstrate the superior performance of the OVM
    model. Notably, in GSM8K, our $\textbf{OVM-7B model achieves state-of-the-art
    results among LLMs up to  13B parameters}$; especially it does not utilize GPT-4
    or code execution. These findings offer a novel perspective on the role of outcome
    supervision in training value models for multi-step reasoning tasks and provide
    theoretical justification for its advantage in value estimation for guided decoding.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Learning for NLP
  authors:
  - emails: '****@link.cuhk.edu.cn'
    first_name: Fei
    google_scholar_id: https://scholar.google.com/citations?user=EsCgPkQAAAAJ&hl=en
    last_name: Yu
    name: Fei Yu
    username: ~Fei_Yu3
  - emails: '****@sribd.cn'
    first_name: Anningzhe
    homepage: https://github.com/Ganz1994
    institution: ShenZhen research institute of big data
    last_name: Gao
    name: Anningzhe Gao
    username: ~Anningzhe_Gao1
  - dblp_id: https://dblp.org/pid/169/1793
    emails: '****@gmail.com'
    first_name: Benyou
    google_scholar_id: https://scholar.google.com/citations?user=Jk4vJU8AAAAJ&hl=en
    homepage: https://wabyking.github.io/old.html
    institution: The Chinese University of Hong Kong, Shenzhen
    last_name: Wang
    name: Benyou Wang
    orcid: https://orcid.org/0000-0002-1501-9914
    username: ~Benyou_Wang2
  decision: toFindings
  end_page: 875
  file: 265.pdf
  id: 265
  num_pages: 18
  openreview_id: YEkrIlBbDu
  pdf_file: b9c6af70952cc2686713bcb61b9b2250d211016e.pdf
  start_page: 858
  title: OVM, Outcome-supervised Value Models for Planning in Mathematical Reasoning
- abstract: Large language models (LLMs) have shown excellent performance on various
    NLP tasks. To use LLMs as strong sequential recommenders, we explore the in-context
    learning approach to sequential recommendation.  We investigate the effects of
    instruction format, task consistency, demonstration selection, and number of demonstrations.
    As increasing the number of demonstrations in ICL does not improve accuracy despite
    using a long prompt, we propose a novel method called LLMSRec-Syn that incorporates
    multiple demonstration users into one aggregated demonstration. Our experiments
    on three recommendation datasets show that LLMSRec-Syn outperforms state-of-the-art
    LLM-based sequential recommendation methods. In some cases, LLMSRec-Syn can perform
    on par with or even better than supervised learning methods. Our code is publicly
    available at https://github.com/demoleiwang/LLMSRec_Syn.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - emails: '****@phdcs.smu.edu.sg'
    first_name: Lei
    google_scholar_id: https://scholar.google.com/citations?user=VidA02oAAAAJ&hl=en
    homepage: https://demoleiwang.github.io/HomePage/
    institution: Singapore Management University
    last_name: Wang
    name: Lei Wang
    semantic_scholar_id: https://www.semanticscholar.org/author/Lei-Wang/145131956
    username: ~Lei_Wang28
  - dblp_id: https://dblp.org/pid/l/EePengLim.html
    emails: '****@smu.edu.sg'
    first_name: Ee-Peng
    google_scholar_id: https://scholar.google.com.tw/citations?user=r0wOAikAAAAJ
    homepage: https://sis.smu.edu.sg/faculty/profile/9626
    institution: Singapore Management University
    last_name: Lim
    name: Ee-Peng Lim
    orcid: https://orcid.org/0000-0003-0065-8665
    username: ~Ee-Peng_Lim1
  decision: toFindings
  end_page: 895
  file: 268.pdf
  id: 268
  num_pages: 20
  openreview_id: fLiU2O3GcT
  pdf_file: 4496b835a01d5550493049667771afd6c4407162.pdf
  start_page: 876
  title: 'The Whole is Better than the Sum: Using Aggregated Demonstrations in In-Context
    Learning for Sequential Recommendation'
- abstract: We present BYOKG, a universal question-answering (QA) system that can
    operate on any knowledge graph (KG), requires no human-annotated training data,
    and can be ready to use within a day---attributes that are out-of-scope for current
    KGQA systems. BYOKG draws inspiration from the remarkable ability of humans to
    comprehend information present in an unseen KG through exploration---starting
    at random nodes, inspecting the labels of adjacent nodes and edges, and combining
    them with their prior world knowledge. Exploration in BYOKG leverages an LLM-backed
    symbolic agent that generates a diverse set of query-program exemplars, which
    are then used to ground a retrieval-augmented reasoning procedure to synthesize
    programs for arbitrary questions. BYOKG is effective over both small- and large-scale
    graphs, showing dramatic gains in zero-shot QA accuracy of 27.89 and 59.88 F1
    on GrailQA and MetaQA, respectively. We further find that performance of BYOKG
    reliably improves with continued exploration as well as improvements in the base
    LLM, notably outperforming a state-of-the-art fine-tuned model by 7.08 F1 on a
    sub-sampled zero-shot split of GrailQA. Lastly, we verify our universality claim
    by evaluating BYOKG on a domain-specific materials science KG and show that it
    improves zero-shot performance by 46.33 F1.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Question Answering
  authors:
  - dblp_id: https://dblp.org/pid/301/7894
    emails: '****@cs.umass.edu'
    first_name: Dhruv
    google_scholar_id: https://scholar.google.com/citations?user=7-AxhB4AAAAJ
    homepage: https://people.cs.umass.edu/~dagarwal/
    institution: Department of Computer Science, University of Massachusetts at Amherst
    last_name: Agarwal
    name: Dhruv Agarwal
    username: ~Dhruv_Agarwal2
  - emails: '****@gmail.com'
    first_name: Rajarshi
    google_scholar_id: https://scholar.google.com/citations?user=FKoKAwIAAAAJ&hl=en
    homepage: http://rajarshd.github.io
    institution: AWS AI Labs
    last_name: Das
    name: Rajarshi Das
    username: ~Rajarshi_Das1
  - dblp_id: https://dblp.org/pid/189/9166
    emails: '****@gmail.com'
    first_name: Sopan
    google_scholar_id: https://scholar.google.co.in/citations?user=BbdF4ysAAAAJ
    homepage: https://sopankhosla.github.io/
    institution: Amazon Web Services
    last_name: Khosla
    name: Sopan Khosla
    semantic_scholar_id: https://www.semanticscholar.org/author/Sopan-Khosla/10258164
    username: ~Sopan_Khosla2
  - dblp_id: https://dblp.org/pid/88/5093.html
    emails: '****@amazon.com'
    first_name: Rashmi
    google_scholar_id: https://scholar.google.com/citations?user=vZ89ydMAAAAJ&hl=en
    institution: Amazon
    last_name: Gangadharaiah
    name: Rashmi Gangadharaiah
    username: ~Rashmi_Gangadharaiah2
  decision: toFindings
  end_page: 919
  file: 272.pdf
  id: 272
  num_pages: 24
  openreview_id: Z1IscjaN3g
  pdf_file: 69e7cb7fc0f87b2bf46a7e0e6af7c7e627de79ce.pdf
  start_page: 896
  title: 'Bring Your Own KG: Self-Supervised Program Synthesis for Zero-Shot KGQA'
- abstract: Pretrained Language Models (PLMs) benefit from external knowledge stored
    in graph structures for various downstream tasks. However, bridging the modality
    gap between graph structures and text remains a significant challenge. Traditional
    methods like linearizing graphs for PLMs lose vital graph connectivity, whereas
    Graph Neural Networks (GNNs) require cumbersome processes for integration into
    PLMs. In this work, we propose a novel graph-guided self-attention mechanism,
    GraSAME. GraSAME seamlessly incorporates token-level structural information into
    PLMs without necessitating additional alignment or concatenation efforts. As an
    end-to-end, lightweight multimodal module, GraSAME follows a multi-task learning
    strategy and effectively bridges the gap between graph and textual modalities,
    facilitating dynamic interactions between GNNs and PLMs. Our experiments on the
    graph-to-text generation task demonstrate that GraSAME outperforms baseline models
    and achieves results comparable to state-of-the-art (SOTA) models on WebNLG datasets.
    Furthermore, compared to SOTA models, GraSAME eliminates the need for extra pre-training
    tasks to adjust graph inputs and reduces the number of trainable parameters by
    over 100 million.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Generation
  authors:
  - emails: '****@kit.edu'
    first_name: Shuzhou
    google_scholar_id: https://scholar.google.com/citations?user=ZMQ1C6gAAAAJ&hl=en
    last_name: Yuan
    name: Shuzhou Yuan
    username: ~Shuzhou_Yuan1
  - emails: '****@kit.edu'
    first_name: Michael
    homepage: https://aifb.kit.edu/web/Michael_F%C3%A4rber
    institution: Karlsruhe Institute of Technology
    last_name: "F\xE4rber"
    name: "Michael F\xE4rber"
    username: "~Michael_F\xE4rber1"
  decision: toFindings
  end_page: 933
  file: 277.pdf
  id: 277
  num_pages: 14
  openreview_id: Nzdht6ANkI
  pdf_file: 7957c748113c01da65603eefa8362ef1a2e93d1e.pdf
  start_page: 920
  title: 'GraSAME: Injecting Token-Level Structural Information to Pretrained Language
    Models via Graph-guided Self-Attention Mechanism'
- abstract: "We study (differentially) private federated learning (FL) of language\
    \ models. The language models in cross-device FL are relatively small, which can\
    \ be trained with meaningful formal user-level differential privacy (DP) guarantees\
    \ when massive parallelism in training is enabled by the participation of a moderate\
    \ size of users. Recently, public data has been used to improve privacy-utility\
    \ trade-offs for both large and small language models. In this work, we provide\
    \ a systematic study of using large-scale public data and LLMs to help differentially\
    \ private training of on-device FL models, and further improve the privacy-utility\
    \ tradeoff by techniques of distillation. Moreover, we propose a novel distribution\
    \ matching algorithm with theoretical grounding to sample public data close to\
    \ private data distribution, which signi\uFB01cantly improves the sample ef\uFB01\
    ciency of (pre-)training on public data. The proposed method is ef\uFB01cient\
    \ and effective for training private models by taking advantage of public data,\
    \ especially for customized on-device architectures that do not have ready-touse\
    \ pre-trained models."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Learning for NLP
  authors:
  - dblp_id: https://dblp.org/pid/236/6319
    emails: '****@nvidia.com'
    first_name: Boxin
    google_scholar_id: https://scholar.google.com/citations?user=YOf2ATIAAAAJ&hl=en
    homepage: https://wbx.life
    institution: NVIDIA
    last_name: Wang
    name: Boxin Wang
    username: ~Boxin_Wang1
  - dblp_id: https://dblp.org/pid/251/9129.html
    emails: '****@stanford.edu'
    first_name: Yibo
    homepage: https://yiboz.me/
    institution: Stanford University and University of Illinois, Urbana Champaign
    last_name: Zhang
    middle_name: Jacky
    name: Yibo Jacky Zhang
    username: ~Yibo_Jacky_Zhang1
  - dblp_id: https://dblp.org/pid/52/4472-7.html
    emails: '****@google.com'
    first_name: Yuan
    google_scholar_id: https://scholar.google.com/citations?user=Q82vvqcAAAAJ&hl=en
    institution: Google DeepMind
    last_name: Cao
    name: Yuan Cao
    orcid: https://orcid.org/0000-0002-1267-8930
    semantic_scholar_id: https://www.semanticscholar.org/author/Yuan-Cao/145144022
    username: ~Yuan_Cao2
  - dblp_id: https://dblp.org/pid/50/3402-26
    emails: '****@illinois.edu'
    first_name: Bo
    google_scholar_id: https://scholar.google.com/citations?user=K8vJkTcAAAAJ&hl=en
    homepage: http://boli.cs.illinois.edu/
    institution: University of Illinois, Urbana Champaign and University of California
      Berkeley
    last_name: Li
    name: Bo Li
    username: ~Bo_Li19
  - dblp_id: ''
    emails: '****@gmail.com'
    first_name: Hugh
    google_scholar_id: ''
    homepage: ''
    institution: Google
    last_name: McMahan
    middle_name: Brendan
    name: Hugh Brendan McMahan
    username: ~Hugh_Brendan_McMahan1
  - dblp_id: https://dblp.org/pid/80/4366
    emails: '****@gmail.com'
    first_name: Sewoong
    google_scholar_id: https://scholar.google.com/citations?user=55TAOdgAAAAJ&hl=en&oi=ao
    homepage: https://homes.cs.washington.edu/~sewoong/
    institution: University of Washington, University of Illinois at Urbana-Champaign
      and University of Washington, Seattle
    last_name: Oh
    name: Sewoong Oh
    username: ~Sewoong_Oh3
  - dblp_id: https://dblp.org/pid/83/2535-2
    emails: '****@google.com'
    first_name: Zheng
    google_scholar_id: https://scholar.google.com/citations?user=TfWlMTYAAAAJ&hl=en
    homepage: https://sites.google.com/site/xuzhustc/
    institution: Google
    last_name: Xu
    name: Zheng Xu
    username: ~Zheng_Xu2
  - dblp_id: https://dblp.org/pid/40/10701
    emails: '****@zaheer.ml'
    first_name: Manzil
    google_scholar_id: https://scholar.google.com/citations?user=A33FhJMAAAAJ&hl=en
    homepage: https://www.aclweb.org/anthology/people/m/manzil-zaheer/
    institution: Zaheer and DeepMind
    last_name: Zaheer
    name: Manzil Zaheer
    username: ~Manzil_Zaheer1
  decision: toFindings
  end_page: 949
  file: 287.pdf
  id: 287
  num_pages: 16
  openreview_id: hg8cpCAXdt
  pdf_file: 2ca3265e3ef6cbae93de13fbcde80899e20b730f.pdf
  start_page: 934
  title: Can Public Large Language Models Help Private Cross-device Federated Learning?
- abstract: 'We explore the use of language as a perceptual representation for vision-and-language
    navigation (VLN), with a focus on low-data settings. Our approach uses off-the-shelf
    vision systems  for image captioning and object detection to convert an  agent''s
    egocentric panoramic view at each time step  into natural language descriptions.
    We then finetune a pretrained language model to select an action, based on the
    current view and the  trajectory history, that would best fulfill the navigation
    instructions. In contrast to the standard setup which adapts a pretrained language
    model to work directly with continuous visual features from pretrained vision
    models, our approach instead uses (discrete) language as the perceptual representation.
    We explore several use cases of our language-based navigation (LangNav) approach
    on the R2R VLN benchmark: generating synthetic trajectories from a prompted  language
    model (GPT-4) with which to finetune a smaller language model; domain transfer
    where we transfer a policy learned on one simulated environment (ALFRED) to another  (more
    realistic) environment (R2R); and combining both vision- and language-based representations
    for VLN. Our approach is found to improve upon  baselines that rely on visual
    features in settings where only a few expert trajectories (10-100) are available,
    demonstrating the potential of language as a perceptual representation for navigation.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multimodality and Language Grounding to Vision, Robotics and Beyond
  authors:
  - dblp_id: https://dblp.org/pid/188/1139
    emails: '****@mit.edu'
    first_name: Bowen
    google_scholar_id: https://scholar.google.com/citations?user=x9Tpbq8AAAAJ&hl=en
    homepage: http://people.csail.mit.edu/bpan/
    last_name: Pan
    name: Bowen Pan
    username: ~Bowen_Pan2
  - dblp_id: https://dblp.org/pid/126/0986
    emails: '****@gmail.com'
    first_name: Rameswar
    google_scholar_id: https://scholar.google.com/citations?user=_ySuu6gAAAAJ&hl=en
    homepage: https://rpand002.github.io/
    institution: MIT-IBM Watson AI Lab
    last_name: Panda
    name: Rameswar Panda
    username: ~Rameswar_Panda1
  - dblp_id: https://dblp.org/pid/225/4723
    emails: '****@dartmouth.edu'
    first_name: SouYoung
    google_scholar_id: https://scholar.google.com/citations?user=_B-_CzYAAAAJ&hl=en
    homepage: http://souyoungjin.com
    institution: Dartmouth College
    last_name: Jin
    name: SouYoung Jin
    username: ~SouYoung_Jin2
  - dblp_id: http://dblp.uni-trier.de/pers/hd/f/Feris:Rog=eacute=rio_Schmidt
    emails: '****@us.ibm.com'
    first_name: Rogerio
    google_scholar_id: https://scholar.google.com/citations?user=xt3XLjcAAAAJ&hl=en
    homepage: http://rogerioferis.com
    institution: International Business Machines
    last_name: Feris
    name: Rogerio Feris
    username: ~Rogerio_Feris1
  - dblp_id: https://dblp.org/pid/99/5538
    emails: '****@mit.edu'
    first_name: Aude
    google_scholar_id: https://scholar.google.com/citations?user=FNhl50sAAAAJ
    homepage: http://olivalab.mit.edu/
    institution: Massachusetts Institute of Technology, Massachusetts Institute of
      Technology and Massachusetts Institute of Technology
    last_name: Oliva
    name: Aude Oliva
    username: ~Aude_Oliva1
  - dblp_id: http://dblp.uni-trier.de/pers/hd/i/Isola:Phillip
    emails: '****@mit.edu'
    first_name: Phillip
    google_scholar_id: https://scholar.google.com/citations?user=ROILf3EAAAAJ&hl=en&oi=ao
    homepage: http://web.mit.edu/phillipi/
    institution: Massachusetts Institute of Technology
    last_name: Isola
    name: Phillip Isola
    username: ~Phillip_Isola1
  - dblp_id: https://dblp.org/pid/07/1501
    emails: '****@mit.edu'
    first_name: Yoon
    google_scholar_id: https://scholar.google.com/citations?user=n_ts4eYAAAAJ
    homepage: https://people.csail.mit.edu/yoonkim/
    institution: Massachusetts Institute of Technology
    last_name: Kim
    name: Yoon Kim
    semantic_scholar_id: https://www.semanticscholar.org/author/Yoon-Kim/38367242
    username: ~Yoon_Kim1
  decision: toFindings
  end_page: 974
  file: 290.pdf
  id: 290
  num_pages: 25
  openreview_id: EiPdkv2oYw
  pdf_file: 0e8ce403696c42b03de0b623a644597a1018e614.pdf
  start_page: 950
  title: 'LangNav: Language as a Perceptual Representation for Navigation'
- abstract: "Recent advancements in integrating external tools with Large Language\
    \ Models (LLMs) have opened new frontiers, with applications in mathematical reasoning,\
    \ code generators, and smart assistants. However, existing methods, relying on\
    \ simple one-time retrieval strategies, fall short on effectively and accurately\
    \ shortlisting relevant tools. This paper introduces a novel PLUTO (Planning,\
    \ Learning, and Understanding for TOols) approach, encompassing \u201CPlan-and-Retrieve\
    \ (P&R)\u201D and \u201CEdit-and-Ground (E&G)\u201D paradigms. The P&R paradigm\
    \ consists of a neural retrieval module for shortlisting relevant tools and an\
    \ LLM-based query planner that decomposes complex queries into actionable tasks,\
    \ enhancing the effectiveness of tool utilization. The E&G paradigm utilizes LLMs\
    \ to enrich tool descriptions based on user scenarios, bridging the gap between\
    \ user queries and tool functionalities. Experiment results demonstrate that these\
    \ paradigms significantly improve the recall and NDCG in tool retrieval tasks,\
    \ significantly surpassing current state-of-the-art models."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Retrieval and Text Mining
  authors:
  - emails: '****@usc.edu'
    first_name: Tenghao
    google_scholar_id: https://scholar.google.com/citations?user=cZKQGyQAAAAJ&hl=en
    last_name: Huang
    name: Tenghao Huang
    username: ~Tenghao_Huang1
  - emails: '****@usc.edu'
    first_name: Dongwon
    last_name: Jung
    name: Dongwon Jung
    username: ~Dongwon_Jung1
  - emails: '****@gmail.com'
    first_name: Vaibhav
    institution: School of Computer Science, Carnegie Mellon University
    last_name: Kumar
    name: Vaibhav Kumar
    username: ~Vaibhav_Kumar1
  - dblp_id: https://dblp.org/pid/166/3221
    emails: '****@ucla.edu'
    first_name: Mohammad
    google_scholar_id: https://scholar.google.com/citations?user=GJ1sDlEAAAAJ&hl=en
    homepage: https://sites.google.com/view/mkachuee
    institution: Amazon
    last_name: Kachuee
    name: Mohammad Kachuee
    username: ~Mohammad_Kachuee1
  - emails: '****@amazon.com'
    first_name: Xiang
    google_scholar_id: https://scholar.google.com/citations?user=CwKgYWsAAAAJ&hl=en
    last_name: Li
    name: Xiang Li
    username: ~Xiang_Li43
  - dblp_id: https://dblp.org/pid/19/9239
    emails: '****@amazon.com'
    first_name: Puyang
    google_scholar_id: https://scholar.google.com/citations?user=58Iai4wAAAAJ&hl=en
    institution: Amazon
    last_name: Xu
    name: Puyang Xu
    username: ~Puyang_Xu1
  - dblp_id: https://dblp.org/pid/173/2608
    emails: '****@ucdavis.edu'
    first_name: Muhao
    google_scholar_id: https://scholar.google.com/citations?user=k79yEZkAAAAJ&hl=en
    homepage: https://muhaochen.github.io/
    institution: University of California, Davis and University of Southern California
    last_name: Chen
    name: Muhao Chen
    orcid: https://orcid.org/0000-0003-0118-3147
    semantic_scholar_id: https://www.semanticscholar.org/author/Muhao-Chen/1998918
    username: ~Muhao_Chen1
  decision: toFindings
  end_page: 988
  file: 300.pdf
  id: 300
  num_pages: 14
  openreview_id: S36GSvBKAH
  pdf_file: dd338272e49129dc76f8a5a89f4281a1ca65495a.pdf
  start_page: 975
  title: Planning and Editing What You Retrieve for Enhanced Tool Learning
- abstract: 'Vision-language models (VLMs) are achieving increasingly strong performance
    on multimodal tasks. However, reasoning capabilities remain limited particularly
    for smaller VLMs, while those of large-language models (LLMs) have seen numerous
    improvements. We pro-

    pose a technique to transfer capabilities from LLMs to VLMs. On the recently introduced
    ChartQA, our method obtains state-of-the-art

    performance when applied on the PaLI3-5B VLM by Chen et al. (2023c), while also
    enabling much better performance on PlotQA and FigureQA.


    We first improve the chart representation by continuing the pre-training stage
    using an improved version of the chart-to-table translation task by Liu et al.
    (2023a). We then propose constructing a 20x larger dataset than the original training
    set. To improve general reasoning capabilities and improve numerical operations,
    we synthesize reasoning traces using the table representation of charts. Lastly,
    our model is fine-tuned using the multitask loss introduced by Hsieh et al. (2023).


    Our variant ChartPaLI-5B outperforms even 10x larger models such as PaLIX-55B
    without using an upstream OCR system, while keeping inference time constant compared
    to the PaLI3-5B baseline. When rationales are further refined with a simple program-of-thought
    prompt (Chen et al., 2023a), our model outperforms the recently introduced Gemini
    Ultra and GPT-4V.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Question Answering
  authors:
  - dblp_id: https://dblp.org/pid/199/7020
    emails: '****@gmail.com'
    first_name: Victor
    google_scholar_id: https://scholar.google.ch/citations?user=35djUQYAAAAJ
    homepage: https://ai.google/research/people/104909
    institution: Google
    last_name: Carbune
    name: Victor Carbune
    username: ~Victor_Carbune1
  - emails: '****@gmail.com'
    first_name: Hassan
    homepage: https://www.linkedin.com/in/hassan-mansoor-6938364/
    institution: Google
    last_name: Mansoor
    name: Hassan Mansoor
    username: ~Hassan_Mansoor1
  - dblp_id: https://dblp.org/pid/84/11483-1
    emails: '****@google.com'
    first_name: Fangyu
    google_scholar_id: https://scholar.google.ch/citations?user=d19PiS0AAAAJ&hl=en
    homepage: http://fangyuliu.me/about
    institution: Google DeepMind
    last_name: Liu
    name: Fangyu Liu
    orcid: https://orcid.org/0000-0001-7038-3623
    semantic_scholar_id: https://www.semanticscholar.org/author/Fangyu-Liu/144097210
    username: ~Fangyu_Liu1
  - dblp_id: https://dblp.org/pid/202/8441
    emails: '****@gmail.com'
    first_name: Rahul
    google_scholar_id: https://scholar.google.co.in/citations?user=UbcNXvQAAAAJ
    homepage: http://rahular.com
    institution: Mila, McGill University
    last_name: Aralikatte
    name: Rahul Aralikatte
    orcid: https://orcid.org/0000-0002-5525-9664
    semantic_scholar_id: https://www.semanticscholar.org/author/Rahul-Aralikatte/19509693
    username: ~Rahul_Aralikatte1
  - emails: '****@gmail.com'
    first_name: Gilles
    google_scholar_id: https://scholar.google.ch/citations?user=hjebS4UAAAAJ&hl=fr
    institution: Research, Google
    last_name: Baechler
    name: Gilles Baechler
    username: ~Gilles_Baechler1
  - emails: '****@google.com'
    first_name: Jindong
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=Cdw5B5AAAAAJ&view_op=list_works&sortby=pubdate
    homepage: https://research.google/people/JindongJDChen/
    institution: Google
    last_name: Chen
    name: Jindong Chen
    username: ~Jindong_Chen2
  - dblp_id: https://dblp.org/pid/330/5092
    emails: '****@google.com'
    first_name: Abhanshu
    google_scholar_id: https://scholar.google.com/citations?user=1NYSxOUAAAAJ&hl=en
    institution: Research, Google
    last_name: Sharma
    name: Abhanshu Sharma
    username: ~Abhanshu_Sharma1
  decision: toFindings
  end_page: 1004
  file: 302.pdf
  id: 302
  num_pages: 16
  openreview_id: wjaLxfE3sh
  pdf_file: 5d7c7e4c0234304e716163ac6cd8ebe7b0b81fab.pdf
  start_page: 989
  title: 'Chart-based Reasoning: Transferring Capabilities from LLMs to VLMs'
- abstract: Speculative decoding has emerged as a prominent alternative to autoregressive
    decoding for expediting inference in large language models (LLMs). However, prevailing
    assumptions often focus solely on latency reduction, neglecting the computational
    expenses. In this paper, we present \textbf{S}peculate \textbf{L}ess, val\textbf{i}date
    \textbf{M}ore (SLiM), a speculative decoding enhancement to reduce the speculation
    set while validating more effective tokens. SLiM is designed to mitigate LLMs'
    computation costs associated with the token verification by introducing hypothesis
    reduction based on a fast posterior estimation. It consistently surpasses counterparts
    lacking cost reduction across a spectrum from CPU to GPU. Our evaluation with
    diverse conversational datasets shows that SLiM can achieve a substantial $70\%$
    reduction in FLOPs while generating more effective predictions on top of prior
    arts.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Generation
  authors:
  - emails: '****@gatech.edu'
    first_name: Chi-Heng
    google_scholar_id: https://scholar.google.com/citations?user=OqSt2wMAAAAJ&hl=en
    homepage: https://www.chihenglin.com/
    institution: Samsung Research America
    last_name: Lin
    name: Chi-Heng Lin
    username: ~Chi-Heng_Lin1
  - emails: '****@princeton.edu'
    first_name: Shikhar
    google_scholar_id: https://scholar.google.com/citations?user=qiFEpzIAAAAJ&hl=en
    homepage: https://stuli.me
    last_name: Tuli
    name: Shikhar Tuli
    username: ~Shikhar_Tuli1
  - dblp_id: https://dblp.org/pid/317/5043
    emails: '****@gatech.edu'
    first_name: James
    google_scholar_id: https://scholar.google.com/citations?user=rT52aN8AAAAJ&hl=en
    homepage: https://jamessealesmith.github.io/
    institution: Georgia Institute of Technology
    last_name: Smith
    middle_name: Seale
    name: James Seale Smith
    orcid: https://orcid.org/0000-0001-9210-0161
    semantic_scholar_id: https://www.semanticscholar.org/author/James-Smith/2119124761
    username: ~James_Seale_Smith1
  - dblp_id: https://dblp.org/pid/172/1140
    emails: '****@gmail.com'
    first_name: Yen-Chang
    google_scholar_id: https://scholar.google.com/citations?user=7QWAiigAAAAJ&hl=en
    institution: Samsung Research America
    last_name: Hsu
    name: Yen-Chang Hsu
    username: ~Yen-Chang_Hsu1
  - dblp_id: https://dblp.org/pid/30/383
    emails: '****@samsung.com'
    first_name: Yilin
    google_scholar_id: https://scholar.google.com/citations?user=9PSFMzAAAAAJ
    institution: Samsung Research America
    last_name: Shen
    name: Yilin Shen
    username: ~Yilin_Shen1
  - dblp_id: https://dblp.org/pid/55/2789
    emails: '****@samsung.com'
    first_name: Hongxia
    institution: Samsung Research America AI center
    last_name: Jin
    name: Hongxia Jin
    username: ~Hongxia_Jin1
  decision: toFindings
  end_page: 1017
  file: 307.pdf
  id: 307
  num_pages: 13
  openreview_id: deoEalW3qx
  pdf_file: bb06c7dd2bcd01103ce802cb784effa0b235a7c8.pdf
  start_page: 1005
  title: 'SLiM: Speculative Decoding with Hypothesis Reduction'
- abstract: Knowledge graphs play a pivotal role in various applications, such as
    question-answering and fact-checking. Abstract Meaning Representation (AMR) represents
    text as knowledge graphs. Evaluating the quality of these graphs involves matching
    them structurally to each other and semantically to the source text. Existing
    AMR metrics are inefficient and struggle to capture semantic similarity. We also
    lack a systematic evaluation benchmark for assessing structural similarity between
    AMR graphs. To overcome these limitations, we introduce a novel AMR similarity
    metric, rematch, alongside a new evaluation for structural similarity called RARE.
    Among state-of-the-art metrics, rematch ranks second in structural similarity;
    and first in semantic similarity by 1--5 percentage points on the STS-B and SICK-R
    benchmarks. Rematch is also five times faster than the next most efficient metric.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: 'Semantics: Sentence-level Semantics, Textual Inference and Other
      areas'
  authors:
  - emails: '****@iu.edu'
    first_name: Zoher
    google_scholar_id: https://scholar.google.com/citations?user=yjxLPHoAAAAJ&hl=en
    institution: Indiana University
    last_name: Kachwala
    name: Zoher Kachwala
    username: ~Zoher_Kachwala1
  - dblp_id: https://dblp.org/pid/19/7354
    emails: '****@acm.org'
    first_name: Jisun
    google_scholar_id: https://scholar.google.com/citations?user=FYtw3zkAAAAJ&hl=en&inst=14102473421921925766
    institution: Indiana University
    last_name: An
    name: Jisun An
    semantic_scholar_id: https://www.semanticscholar.org/author/Jisun-An/40660541
    username: ~Jisun_An1
  - dblp_id: https://dblp.org/pid/78/2468
    emails: '****@acm.org'
    first_name: Haewoon
    google_scholar_id: https://scholar.google.com/citations?user=dcjrz5MAAAAJ&hl=en&oi=ao
    homepage: http://haewoon.io
    institution: Indiana University
    last_name: Kwak
    name: Haewoon Kwak
    semantic_scholar_id: https://www.semanticscholar.org/author/Haewoon-Kwak/2592694
    username: ~Haewoon_Kwak1
  - dblp_id: https://dblp.org/pid/79/3056
    emails: '****@iu.edu'
    first_name: Filippo
    google_scholar_id: https://scholar.google.com/citations?user=f_kGJwkAAAAJ
    homepage: https://cnets.indiana.edu/fil/
    institution: Indiana University
    last_name: Menczer
    name: Filippo Menczer
    orcid: https://orcid.org/0000-0003-4384-2876
    username: ~Filippo_Menczer1
  decision: toFindings
  end_page: 1028
  file: 309.pdf
  id: 309
  num_pages: 11
  openreview_id: dBnsZ72qUQ
  pdf_file: 85257213a4250e80d76faea0ea549d49221d4a72.pdf
  start_page: 1018
  title: 'REMATCH: Robust and Efficient Matching of Local Knowledge Graphs to Improve
    Structural and Semantic Similarity'
- abstract: This position paper concerns the use of religious texts in Natural Language
    Processing (NLP), which is of special interest to the Ethics of NLP. Religious
    texts are expressions of culturally important values, and machine learned models
    have a  propensity to reproduce cultural values encoded in their training data.
    Furthermore, translations of religious texts are frequently used by NLP researchers
    when language data is scarce. This repurposes the translations from their original
    uses and motivations, which often involve attracting new followers. This paper
    argues that NLP's use of such texts raises considerations that go beyond model
    biases, including data provenance, cultural contexts, and their use in proselytism.
    We argue for more consideration of researcher positionality, and of the perspectives
    of marginalized linguistic and religious communities.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Ethics, Bias, and Fairness
  authors:
  - dblp_id: https://dblp.org/pid/56/1256
    emails: '****@google.com'
    first_name: Ben
    google_scholar_id: https://scholar.google.com/citations?user=vy7uPgsAAAAJ&hl=en
    institution: Google
    last_name: Hutchinson
    name: Ben Hutchinson
    username: ~Ben_Hutchinson1
  decision: toFindings
  end_page: 1043
  file: 313.pdf
  id: 313
  num_pages: 15
  openreview_id: Ch52LDhblg
  pdf_file: 93eb798f81b30c346c46b59179d67a0e696bd210.pdf
  start_page: 1029
  title: 'Modeling the Sacred: Considerations when Using Religious Texts in Natural
    Language Processing'
- abstract: Large Language Models (LLMs) have demonstrated impressive abilities in
    recent years with regards to code generation and understanding. However, little
    work has investigated how documentation and other code properties affect an LLM's
    ability to understand and generate code or documentation. We present an empirical
    analysis of how underlying properties of code or documentation can affect an LLM's
    capabilities.  We show that providing an LLM with "incorrect" documentation can
    greatly hinder code understanding, while incomplete or missing documentation does
    not seem to significantly affect an LLM's ability to understand code.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Generation
  authors:
  - emails: '****@mitre.org'
    first_name: William
    homepage: http://williammacke.github.io
    institution: MITRE
    last_name: Macke
    name: William Macke
    username: ~William_Macke1
  - emails: '****@mitre.org'
    first_name: Michael
    homepage: https://github.com/doyled-it
    institution: MITRE
    last_name: Doyle
    middle_name: Dean
    name: Michael Dean Doyle
    orcid: https://orcid.org/0009-0002-0229-5693
    username: ~Michael_Dean_Doyle1
  decision: toFindings
  end_page: 1050
  file: 327.pdf
  id: 327
  num_pages: 7
  openreview_id: hOeQnWo9YF
  pdf_file: c620700927a09be96117705e3c86e37a4ce50f03.pdf
  start_page: 1044
  title: Testing the Effect of Code Documentation on Large Language Model Code Understanding
- abstract: Large language models (LLMs) have recently been used as backbones for
    recommender systems. However, their performance often lags behind conventional
    methods in standard tasks like retrieval. We attribute this to a mismatch between
    LLMs' knowledge and the knowledge crucial for effective recommendations. While
    LLMs excel at natural language reasoning, they cannot model complex user-item
    interactions inherent in recommendation tasks. We propose bridging the knowledge
    gap and equipping LLMs with recommendation-specific knowledge to address this.
    Operations such as Masked Item Modeling (MIM) and Bayesian Personalized Ranking
    (BPR) have found success in conventional recommender systems. Inspired by this,
    we simulate these operations through natural language to generate auxiliary-task
    data samples that encode item correlations and user preferences. Fine-tuning LLMs
    on such auxiliary-task data samples and incorporating more informative recommendation-task
    data samples facilitates the injection of recommendation-specific knowledge into
    LLMs. Extensive experiments across retrieval, ranking, and rating prediction tasks
    on LLMs such as FLAN-T5-Base and FLAN-T5-XL show the effectiveness of our technique
    in domains such as Amazon Toys \& Games, Beauty, and Sports \& Outdoors. Notably,
    our method outperforms conventional and LLM-based baselines, including the current
    SOTA, by significant margins in retrieval, showcasing its potential for enhancing
    recommendation quality.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Retrieval and Text Mining
  authors:
  - emails: '****@uic.edu'
    first_name: Yuwei
    google_scholar_id: https://scholar.google.com/citations?user=ZOLP_QUAAAAJ&hl=en
    last_name: Cao
    name: Yuwei Cao
    username: ~Yuwei_Cao1
  - dblp_id: https://dblp.org/pid/89/7487
    emails: '****@google.com'
    first_name: Nikhil
    google_scholar_id: https://scholar.google.com/citations?user=4cgHaJ0AAAAJ&hl=en
    institution: Research, Google
    last_name: Mehta
    name: Nikhil Mehta
    username: ~Nikhil_Mehta1
  - dblp_id: https://dblp.org/pid/139/1029
    emails: '****@google.com'
    first_name: Xinyang
    google_scholar_id: https://scholar.google.com/citations?user=r0c4bz4AAAAJ&hl=en
    institution: Google
    last_name: Yi
    name: Xinyang Yi
    username: ~Xinyang_Yi1
  - emails: '****@google.com'
    first_name: Raghunandan
    homepage: https://scholar.google.com/citations?user=PCtRSvUAAAAJ&hl=en
    last_name: Hulikal Keshavan
    name: Raghunandan Hulikal Keshavan
    username: ~Raghunandan_Hulikal_Keshavan1
  - dblp_id: https://dblp.org/pid/63/5281
    emails: '****@gmail.com'
    first_name: Lukasz
    homepage: https://www.linkedin.com/in/lukasz-heldt-6226691
    last_name: Heldt
    name: Lukasz Heldt
    username: ~Lukasz_Heldt1
  - dblp_id: https://dblp.org/pid/85/4697
    emails: '****@google.com'
    first_name: Lichan
    google_scholar_id: https://scholar.google.com/citations?view_op=list_works&hl=en&hl=en&user=QnAQRaYAAAAJ
    institution: 'Google '
    last_name: Hong
    name: Lichan Hong
    username: ~Lichan_Hong1
  - dblp_id: https://dblp.org/pid/13/310
    emails: '****@google.com'
    first_name: Ed
    google_scholar_id: https://scholar.google.com/citations?user=VuWl-KUAAAAJ&hl=en
    homepage: http://edchi.net
    institution: Google
    last_name: Chi
    middle_name: H.
    name: Ed H. Chi
    username: ~Ed_H._Chi1
  - emails: '****@gmail.com'
    first_name: Maheswaran
    google_scholar_id: https://scholar.google.com/citations?user=HLkvYl0AAAAJ&hl=en
    homepage: http://smahesh.com
    last_name: Sathiamoorthy
    name: Maheswaran Sathiamoorthy
    username: ~Maheswaran_Sathiamoorthy1
  decision: toFindings
  end_page: 1066
  file: 329.pdf
  id: 329
  num_pages: 16
  openreview_id: KEhF1eChyx
  pdf_file: e9d77a304b89785c6a92d673303e38fbee3acecf.pdf
  start_page: 1051
  title: Aligning Large Language Models with Recommendation Knowledge
- abstract: 'Instead of pretraining multilingual language models from scratch, a more
    efficient method is to adapt existing pretrained language models (PLMs) to new
    languages via vocabulary extension and continued pretraining. However, this method
    usually randomly initializes the embeddings of new subwords and introduces substantially
    more embedding parameters to the model, thus weakening the efficiency. To address
    these issues, we propose a novel framework: $\textbf{O}$ne $\textbf{F}$or $\textbf{A}$ll
    ($\textbf{OFA}$), which wisely initializes the embeddings of unseen subwords and
    thus can adapt a PLM to multiple languages efficiently and effectively. OFA takes
    advantage of external well-aligned multilingual static word vectors and injects
    the alignment knowledge into the subword embeddings. In addition, OFA applies
    matrix factorization and replaces the cumbersome embeddings with two lower-dimensional
    matrices, which largely reduces the number of parameters. We show OFA accelerates
    the convergence of continued pretraining, which is environmentally friendly as
    much fewer carbon footprints are generated. Through extensive experiments, we
    demonstrate OFA can achieve competitive or better performance than default continued
    pretraining baselines on a wide range of crosslingual downstream tasks. We make
    our code and models publicly available.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multilinguality and Language Diversity
  authors:
  - dblp_id: https://dblp.org/pid/86/3284
    emails: '****@outlook.com'
    first_name: Yihong
    google_scholar_id: https://scholar.google.com/citations?user=VjJUa5cAAAAJ
    homepage: https://yihongl1u.github.io/
    institution: "Ludwig-Maximilians-Universit\xE4t M\xFCnchen"
    last_name: Liu
    name: Yihong Liu
    orcid: https://orcid.org/0000-0003-1073-0958
    semantic_scholar_id: https://www.semanticscholar.org/author/Yihong-Liu/2107995084
    username: ~Yihong_Liu1
  - dblp_id: https://dblp.org/pid/246/3172
    emails: '****@gmail.com'
    first_name: Peiqin
    google_scholar_id: https://scholar.google.com.hk/citations?user=ZUeyIxMAAAAJ
    homepage: https://lpq29743.github.io
    institution: "Institut f\xFCr Informatik"
    last_name: Lin
    name: Peiqin Lin
    orcid: https://orcid.org/0000-0003-2818-3008
    semantic_scholar_id: https://www.semanticscholar.org/author/Peiqin-Lin/152178958
    username: ~Peiqin_Lin1
  - emails: '****@de.bosch.com'
    first_name: Mingyang
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=d8UFLmsAAAAJ
    homepage: https://mingyang-wang26.github.io/
    last_name: Wang
    name: Mingyang Wang
    username: ~Mingyang_Wang1
  - dblp_id: https://dblp.org/pid/s/HinrichSchutze
    emails: '****@cis.lmu.de'
    first_name: Hinrich
    homepage: https://www.cis.uni-muenchen.de/schuetze/
    last_name: Schuetze
    name: Hinrich Schuetze
    username: ~Hinrich_Schuetze3
  decision: toFindings
  end_page: 1097
  file: 330.pdf
  id: 330
  num_pages: 31
  openreview_id: gMc5KHWmbw
  pdf_file: f51599d4f8f8c97f3e1b196c472eb660552cee52.pdf
  start_page: 1067
  title: 'OFA: A Framework of Initializing Unseen Subword Embeddings for Efficient
    Large-scale Multilingual Continued Pretraining'
- abstract: "The advent of instruction-tuned large language models (LLMs) has significantly\
    \ advanced the field of automatic instruction dataset augmentation. However, the\
    \ method of generating instructions and outputs from inherent knowledge of LLM\
    \ can unintentionally produce hallucinations \u2014 instances of generating factually\
    \ incorrect or misleading information. To overcome this, we propose SELF-EXPERTISE,\
    \ automatically generating instruction dataset in the legal domain from a seed\
    \ dataset. SELF-EXPERTISE extracts knowledge from the outputs of the seed dataset,\
    \ and generates new instructions, inputs, and outputs. In this way, the proposed\
    \ method reduces hallucination in automatic instruction augmentation. We trained\
    \ an SELF-EXPERTISE augmented instruction dataset on the LLaMA-2 7B model to construct\
    \ Korean legal specialized model, called LxPERT. LxPERT has demonstrated performance\
    \ surpassing GPT-3.5-turbo in both in-domain and out-of-domain datasets. The SELF-EXPERTISE\
    \ augmentation pipeline is not only applicable to the legal field but is also\
    \ expected to be extendable to various domains, potentially advancing domain-specialized\
    \ LLMs."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Efficient/Low-Resource Methods for NLP
  authors:
  - emails: '****@gmail.com'
    first_name: Minju
    homepage: https://github.com/minju0307
    last_name: Kim
    name: Minju Kim
    username: ~Minju_Kim6
  - emails: '****@sogang.ac.kr'
    first_name: Haein
    homepage: https://github.com/Haein1226
    last_name: Jung
    name: Haein Jung
    username: ~Haein_Jung1
  - dblp_id: https://dblp.org/pid/13/3448
    emails: '****@sogang.ac.kr'
    first_name: Myoung-Wan
    google_scholar_id: https://scholar.google.com/citations?user=iNco12gAAAAJ&hl=ko
    homepage: https://isds.sogang.ac.kr/
    institution: Sogang University
    last_name: Koo
    name: Myoung-Wan Koo
    username: ~Myoung-Wan_Koo1
  decision: toFindings
  end_page: 1112
  file: 334.pdf
  id: 334
  num_pages: 15
  openreview_id: bGpHr6f82b
  pdf_file: 4ddd684ae9aaa2820a7898ab344f109132b41037.pdf
  start_page: 1098
  title: 'SELF-EXPERTISE: Knowledge-based Instruction Dataset Augmentation for a Legal
    Expert Language Model'
- abstract: Are multimodal inputs necessary for grammar induction? Recent work has
    shown that multimodal training inputs can improve grammar induction. However,
    these improvements are based on comparisons to weak text-only baselines that were
    trained on relatively little textual data. To determine whether multimodal inputs
    are needed in regimes with large amounts of textual training data, we design a
    stronger text-only baseline, which we refer to as LC-PCFG.  LC-PCFG is a C-PFCG
    that incorporates embeddings from text-only large language models (LLMs). We use
    a fixed grammar family to directly compare LC-PCFG to various multimodal grammar
    induction methods. We compare performance on four benchmark datasets. LC-PCFG
    provides an up to 17% relative improvement in Corpus-F1 compared to state-of-the-art
    multimodal grammar induction methods. LC-PCFG is also more computationally efficient,
    providing an up to 85% reduction in parameter count and $8.8\times$ reduction
    in training time compared to multimodal approaches. These results suggest that
    multimodal inputs may not be necessary for grammar induction, and emphasize the
    importance of strong vision-free baselines for evaluating the benefit of multimodal
    approaches.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multimodality and Language Grounding to Vision, Robotics and Beyond
  authors:
  - dblp_id: https://dblp.org/pid/131/1124
    emails: '****@gmail.com'
    first_name: Boyi
    institution: NVIDIA Research and University of California, Berkeley
    last_name: Li
    name: Boyi Li
    username: ~Boyi_Li1
  - dblp_id: https://dblp.org/pid/212/0412
    emails: '****@berkeley.edu'
    first_name: Rodolfo
    google_scholar_id: https://scholar.google.com/citations?user=J2Z-ChoAAAAJ&hl=en
    homepage: https://rcorona.github.io/
    last_name: Corona
    name: Rodolfo Corona
    username: ~Rodolfo_Corona1
  - dblp_id: https://dblp.org/pid/200/8205
    emails: '****@berkeley.edu'
    first_name: Karttikeya
    google_scholar_id: https://scholar.google.com/citations?user=2l1fWEoAAAAJ&hl=en
    homepage: http://karttikeya.github.io/
    last_name: Mangalam
    name: Karttikeya Mangalam
    username: ~Karttikeya_Mangalam1
  - emails: '****@berkeley.edu'
    first_name: Catherine
    google_scholar_id: https://scholar.google.com/citations?user=wYYXcnEAAAAJ&hl=es
    homepage: https://cchen23.github.io/
    institution: University of California Berkeley
    last_name: Chen
    name: Catherine Chen
    semantic_scholar_id: https://www.semanticscholar.org/author/Catherine-Chen/2109063330
    username: ~Catherine_Chen2
  - emails: '****@berkeley.edu'
    first_name: Daniel
    last_name: Flaherty
    name: Daniel Flaherty
    username: ~Daniel_Flaherty1
  - dblp_id: http://dblp.uni-trier.de/pers/hd/b/Belongie:Serge_J=
    emails: '****@di.ku.dk'
    first_name: Serge
    google_scholar_id: https://scholar.google.com/citations?user=ORr4XJYAAAAJ
    homepage: https://di.ku.dk/english/staff/?pure=en%2Fpersons%2Fserge-belongie(0ce65383-3761-4b17-948a-83b461e371e2)%2Fpublications.html
    institution: University of Copenhagen
    last_name: Belongie
    name: Serge Belongie
    orcid: https://orcid.org/0000-0002-0388-5217
    semantic_scholar_id: https://www.semanticscholar.org/author/Serge-J.-Belongie/50172592
    username: ~Serge_Belongie1
  - emails: '****@cornell.edu'
    first_name: Kilian
    google_scholar_id: https://scholar.google.com/citations?user=jsxk8vsAAAAJ&hl=en&oi=ao
    homepage: http://www.cs.cornell.edu/~kilian/
    institution: Cornell University, Cornell University and Cornell University
    last_name: Weinberger
    middle_name: Q
    name: Kilian Q Weinberger
    username: ~Kilian_Q_Weinberger1
  - dblp_id: https://dblp.uni-trier.de/pers/hd/m/Malik:Jitendra
    emails: '****@eecs.berkeley.edu'
    first_name: Jitendra
    google_scholar_id: https://scholar.google.com/citations?user=oY9R5YQAAAAJ&hl=en
    homepage: https://people.eecs.berkeley.edu/~malik/
    institution: Facebook and University of California Berkeley
    last_name: Malik
    name: Jitendra Malik
    username: ~Jitendra_Malik2
  - dblp_id: https://dblp.org/pid/d/TrevorDarrell
    emails: '****@gmail.com'
    first_name: Trevor
    google_scholar_id: https://scholar.google.com.tw/citations?user=bh-uRFMAAAAJ
    homepage: https://people.eecs.berkeley.edu/~trevor/
    institution: Electrical Engineering & Computer Science Department
    last_name: Darrell
    name: Trevor Darrell
    username: ~Trevor_Darrell2
  - dblp_id: https://dblp.org/pid/22/1139
    emails: '****@cs.berkeley.edu'
    first_name: Dan
    homepage: http://people.eecs.berkeley.edu/~klein/
    institution: University of California, Berkeley
    last_name: Klein
    name: Dan Klein
    username: ~Dan_Klein1
  decision: toFindings
  end_page: 1123
  file: 336.pdf
  id: 336
  num_pages: 11
  openreview_id: GUGRcZ8gIn
  pdf_file: 20d35d0f19d3cf8d1db69eac8f451cc23571c03c.pdf
  start_page: 1113
  title: Re-evaluating the Need for Visual Signals in Unsupervised Grammar Induction
- abstract: Few-shot text classification has seen significant advancements, particularly
    with entailment-based methods, which typically use either class labels or intensional
    definitions of class labels in hypotheses for label semantics expression. In this
    paper, we propose EDEntail, a method that employs extensional definition (EDef)
    of class labels in hypotheses, aiming to express the semantics of class labels
    more explicitly. To achieve the above goal, we develop an algorithm to gather
    and select extensional descriptive words of class labels and then order and format
    them into a sequence to form hypotheses. Our method has been evaluated and compared
    with state-of-the-art models on five classification datasets. The results demonstrate
    that our approach surpasses the supervised-learning methods and prompt-based methods
    under the few-shot setting, which underlines the potential of using an extensional
    definition of class labels for entailment-based few-shot text classification.
    Our code is available at https://github.com/MidiyaZhu/EDEntail.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Efficient/Low-Resource Methods for NLP
  authors:
  - emails: '****@e.ntu.edu.sg'
    first_name: Zixiao
    last_name: Zhu
    name: Zixiao Zhu
    orcid: https://orcid.org/0000-0002-8834-868X
    username: ~Zixiao_Zhu2
  - emails: '****@e.ntu.edu.sg'
    first_name: Junlang
    homepage: https://github.com/qianjunlang
    institution: Nanyang Technological University
    last_name: Qian
    name: Junlang Qian
    username: ~Junlang_Qian1
  - emails: '****@e.ntu.edu.sg'
    first_name: Zijian
    last_name: Feng
    name: Zijian Feng
    orcid: https://orcid.org/0000-0003-1311-988X
    semantic_scholar_id: https://www.semanticscholar.org/author/Zijian-Feng/2112599636
    username: ~Zijian_Feng2
  - dblp_id: https://dblp.org/pid/295/8180
    emails: '****@e.ntu.edu.sg'
    first_name: Hanzhang
    last_name: Zhou
    name: Hanzhang Zhou
    orcid: https://orcid.org/0000-0003-3758-636X
    semantic_scholar_id: https://www.semanticscholar.org/author/Hanzhang-Zhou/2111825433
    username: ~Hanzhang_Zhou1
  - dblp_id: https://dblp.uni-trier.de/pers/m/Mao:Kezhi.html
    emails: '****@ntu.edu.sg'
    first_name: Kezhi
    google_scholar_id: https://scholar.google.com/citations?user=jCsRJXUAAAAJ
    homepage: http://research.ntu.edu.sg/expertise/academicprofile/Pages/StaffProfile.aspx?ST_EMAILID=EKZMAO
    institution: Nanyang Technological University
    last_name: Mao
    name: Kezhi Mao
    username: ~Kezhi_Mao1
  decision: toFindings
  end_page: 1137
  file: 344.pdf
  id: 344
  num_pages: 14
  openreview_id: WyHoiMs99M
  pdf_file: f588a4dc8759746aa74f565b9a8ecadc829e7e49.pdf
  start_page: 1124
  title: 'EDEntail: An Entailment-based Few-shot Text Classification with Extensional
    Definition'
- abstract: This paper investigates the question of what makes math word problems
    (MWPs) in English challenging for large language models (LLMs). We conduct an
    in-depth analysis of the key linguistic and mathematical characteristics of MWPs.
    In addition, we train feature-based classifiers to better understand the impact
    of each feature on the overall difficulty of MWPs for prominent LLMs and investigate
    whether this helps predict how well LLMs fare against specific categories of MWPs.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - emails: '****@mbzuai.ac.ae'
    first_name: Kv Aditya
    google_scholar_id: https://scholar.google.com/citations?user=cs5-j9EAAAAJ&hl=en
    institution: Mohamed bin Zayed University of Artificial Intelligence
    last_name: Srivatsa
    name: KV Aditya Srivatsa
    orcid: https://orcid.org/0000-0002-7583-7580
    username: ~KV_Aditya_Srivatsa1
  - dblp_id: https://dblp.org/pid/140/3465.html
    emails: '****@gmail.com'
    first_name: Ekaterina
    google_scholar_id: https://scholar.google.co.uk/citations?user=e2HTYnkAAAAJ&hl=en
    homepage: https://ekochmar.github.io/about/
    institution: Mohamed bin Zayed University of Artificial Intelligence
    last_name: Kochmar
    name: Ekaterina Kochmar
    orcid: https://orcid.org/0000-0003-3328-1374
    semantic_scholar_id: https://www.semanticscholar.org/author/E.-Kochmar/2700917
    username: ~Ekaterina_Kochmar2
  decision: toFindings
  end_page: 1148
  file: 346.pdf
  id: 346
  num_pages: 11
  openreview_id: EJMDzUCvAr
  pdf_file: 7de2554f85e301677e872f0603233df6ffe80053.pdf
  start_page: 1138
  title: What Makes Math Word Problems Challenging for LLMs?
- abstract: 'Despite the recent advances in artificial intelligence, building social
    intelligence remains a challenge.

    Among social signals, laughter is one of the distinctive expressions that occurs
    during social interactions between humans.

    In this work, we tackle a new challenge for machines to understand the rationale
    behind laughter in video, Video Laugh Reasoning.

    We introduce this new task to explain why people laugh in a particular video and
    a dataset for this task.

    Our proposed dataset, SMILE, comprises video clips and language descriptions of
    why people laugh. We propose a baseline by leveraging the reasoning capacity of
    large language models (LLMs) with textual video representation. Experiments show
    that our baseline can generate plausible explanations for laughter. We further
    investigate the scalability of our baseline by probing other video understanding
    tasks and in-the-wild videos. We release our dataset, code, and model checkpoints
    on https://github.com/postech-ami/SMILE-Dataset.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Question Answering
  authors:
  - dblp_id: https://dblp.org/rec/journals/corr/abs-2211-09385
    emails: '****@postech.ac.kr'
    first_name: Lee
    homepage: https://github.com/HyunLee103
    institution: Pohang University of Science and Technology
    last_name: Hyun
    name: Lee Hyun
    semantic_scholar_id: https://www.semanticscholar.org/author/Lee-Hyun/2191077104
    username: ~Lee_Hyun1
  - dblp_id: https://dblp.org/pid/312/3778
    emails: '****@postech.ac.kr'
    first_name: Kim
    google_scholar_id: https://scholar.google.com/citations?user=bWsRk0MAAAAJ&hl=ko&oi=sra
    homepage: https://sites.google.com/view/kimsungbin/
    institution: Pohang University of Science and Technology
    last_name: Sung-Bin
    name: Kim Sung-Bin
    orcid: https://orcid.org/0000-0003-3542-9934
    username: ~Kim_Sung-Bin1
  - dblp_id: https://dblp.org/pid/08/3308
    emails: '****@snu.ac.kr'
    first_name: Seungju
    google_scholar_id: https://scholar.google.com/citations?user=g_anRqAAAAAJ
    homepage: https://seungjuhan.me
    institution: Allen Institute for Artificial Intelligence and Seoul National University
    last_name: Han
    name: Seungju Han
    semantic_scholar_id: https://www.semanticscholar.org/author/Seungju-Han/2423429
    username: ~Seungju_Han2
  - dblp_id: https://dblp.org/pid/188/6210
    emails: '****@yonsei.ac.kr'
    first_name: Youngjae
    google_scholar_id: https://scholar.google.co.kr/citations?user=WDO24ZYAAAAJ&hl=ko
    homepage: https://yj-yu.github.io/home/
    institution: Yonsei University
    last_name: Yu
    name: Youngjae Yu
    username: ~Youngjae_Yu1
  - dblp_id: https://dblp.org/pers/o/Oh:Tae_Hyun
    emails: '****@postech.ac.kr'
    first_name: Tae-Hyun
    google_scholar_id: https://scholar.google.com/citations?user=dMCBjeIAAAAJ&hl=en
    homepage: https://ami.postech.ac.kr
    institution: POSTECH
    last_name: Oh
    name: Tae-Hyun Oh
    orcid: https://orcid.org/0000-0003-0468-1571
    username: ~Tae-Hyun_Oh3
  decision: toFindings
  end_page: 1167
  file: 351.pdf
  id: 351
  num_pages: 19
  openreview_id: gv63IpiTlT
  pdf_file: db47e1448332717ae60d40b7d1ee0433f7700bf3.pdf
  start_page: 1149
  title: 'SMILE: Multimodal Dataset for Understanding Laughter in Video with Language
    Models'
- abstract: Speech-driven 3D motion synthesis seeks to create lifelike animations
    based on human speech, with potential uses in virtual reality, gaming, and the
    film production. Existing approaches reply solely on speech audio for motion generation,
    leading to inaccurate and inflexible synthesis results. To mitigate this problem,
    we introduce a novel text-guided 3D human motion synthesis method, termed T3M.
    Unlike traditional approaches, T3M allows precise control over motion synthesis
    via textual input, enhancing the degree of diversity and user customization. The
    experiment results demonstrate that T3M can greatly outperform the state-of-the-art
    methods in both quantitative metrics and qualitative evaluations. We have publicly
    released our code at https://github.com/Gloria2tt/naacl2024.git
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multimodality and Language Grounding to Vision, Robotics and Beyond
  authors:
  - emails: '****@mails.tsinghua.edu.cn'
    first_name: Gloria
    homepage: https://github.com/Loda2w
    last_name: Gloria
    name: Gloria
    username: ~Gloria2
  - dblp_id: https://dblp.org/pid/179/2126
    emails: '****@foxmail.com'
    first_name: Kaipeng
    google_scholar_id: https://scholar.google.com/citations?user=4OqZBmYAAAAJ&hl=en
    homepage: http://kpzhang93.github.io/
    institution: Shanghai AI Laboratory
    last_name: Zhang
    name: Kaipeng Zhang
    username: ~Kaipeng_Zhang1
  - dblp_id: https://dblp.org/pid/164/7945
    emails: '****@nyu.edu'
    first_name: Sai Qian
    homepage: https://saiqianzhang.com/
    institution: Harvard University, Harvard University, University of Toronto and
      Facebook
    last_name: Zhang
    name: Sai Qian Zhang
    username: ~Sai_Qian_Zhang1
  decision: toFindings
  end_page: 1177
  file: 353.pdf
  id: 353
  num_pages: 10
  openreview_id: MduhclLiVk
  pdf_file: 17a5f0806dc0f2c92b6c4299a3f52759512f17fd.pdf
  start_page: 1168
  title: 'T3M: Text Guided 3D Human Motion Synthesis from Speech'
- abstract: Temporal Knowledge Graph Reasoning (TKGR) is the task of inferring missing
    facts for incomplete TKGs in complex scenarios (e.g., transductive and inductive
    settings), which has been gaining increasing attention. Recently, to mitigate
    dependence on structured connections in TKGs, text-based methods have been developed
    to utilize rich linguistic information from entity descriptions. However, suffering
    from the enormous parameters and inflexibility of pre-trained language models,
    existing text-based methods struggle to balance the textual knowledge and temporal
    information with computationally expensive purpose-built training strategies.
    To tap the potential of text-based models for TKGR in various complex scenarios,
    we propose ChapTER, a Contrastive historical modeling framework with prefix-tuning
    for TEmporal Reasoning. ChapTER feeds history-contextualized text into the pseudo-Siamese
    encoders to strike a textual-temporal balance via contrastive estimation between
    queries and candidates. By introducing virtual time prefix tokens, it applies
    a prefix-based tuning method to facilitate the frozen PLM capable for TKGR tasks
    under different settings. We evaluate ChapTER on four transductive and three few-shot
    inductive TKGR benchmarks, and experimental results demonstrate that ChapTER achieves
    superior performance compared to competitive baselines with only 0.17% tuned parameters.
    We conduct thorough analysis to verify the effectiveness, flexibility and efficiency
    of ChapTER.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - emails: '****@whu.edu.cn'
    first_name: Miao
    google_scholar_id: https://scholar.google.com/citations?user=WVtOGDMAAAAJ&hl=en
    homepage: https://gknl.github.io
    institution: Wuhan University
    last_name: Peng
    name: Miao Peng
    username: ~Miao_Peng1
  - emails: '****@whu.edu.cn'
    first_name: Ben
    last_name: Liu
    name: Ben Liu
    orcid: https://orcid.org/0000-0001-5031-9368
    username: ~Ben_Liu2
  - dblp_id: https://dblp.org/pid/25/1820
    emails: '****@whu.edu.cn'
    first_name: Wenjie
    homepage: https://jaysaligia.site/
    institution: Wuhan University
    last_name: Xu
    name: Wenjie Xu
    username: ~Wenjie_Xu6
  - emails: '****@whu.edu.cn'
    first_name: Zihao
    homepage: https://mail.whu.edu.cn
    institution: Wuhan University
    last_name: Jiang
    name: Zihao Jiang
    username: ~Zihao_Jiang3
  - emails: '****@xiaomi.com'
    first_name: Jiahui
    homepage: https://dblp.uni-trier.de/pid/44/7484.html
    last_name: Zhu
    name: Jiahui Zhu
    username: ~Jiahui_Zhu2
  - emails: '****@whu.edu.cn'
    first_name: Min
    institution: Wuhan University
    last_name: Peng
    name: Min Peng
    orcid: https://orcid.org/0000-0002-8766-1105
    username: ~Min_Peng2
  decision: toFindings
  end_page: 1191
  file: 362.pdf
  id: 362
  num_pages: 14
  openreview_id: z5wPtlXiSq
  pdf_file: cd212e48cafe542631298d93bb0062172247ea00.pdf
  start_page: 1178
  title: 'Deja vu: Contrastive Historical Modeling with Prefix-tuning for Temporal
    Knowledge Graph Reasoning'
- abstract: Hierarchical classification frameworks have been widely used to process
    long sequences, especially in the legal domain for predictions from long legal
    documents. But being black-box models they are unable to explain their predictions
    making them less reliable for practical applications, more so in the legal domain.
    In this work, we develop an extractive explanation algorithm for hierarchical
    frameworks for long sequences based on the sensitivity of the trained model to
    its input perturbations. We perturb using occlusion and develop Ob-HEx; an Occlusion-based
    Hierarchical Explanation-extractor. We adapt Ob-HEx to Hierarchical Transformer
    models trained on long Indian legal texts. And use Ob-HEx to analyze them and
    extract their explanations for the ILDC-Expert dataset, achieving a minimum gain
    of 1 point over the previous benchmark on most of our performance evaluation metrics.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - dblp_id: https://dblp.org/pid/302/7492
    emails: '****@irit.fr'
    first_name: Nishchal
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=geMvjXQAAAAJ
    last_name: Prasad
    name: Nishchal Prasad
    username: ~Nishchal_Prasad1
  - dblp_id: https://dblp.org/pid/05/3211
    emails: '****@irit.fr'
    first_name: Taoufiq
    homepage: https://www.irit.fr/productions-scientifiques/publications/?code=3562&nom=DKAKI
    institution: Institut de Recherche en Informatique de Toulouse
    last_name: Dkaki
    name: Taoufiq Dkaki
    semantic_scholar_id: https://www.semanticscholar.org/author/T.-Dkaki/2654664
    username: ~Taoufiq_Dkaki1
  - dblp_id: https://dblp.org/pid/b/MBoughanem
    emails: '****@irit.fr'
    first_name: Mohand
    google_scholar_id: https://scholar.google.com/citations?user=KKrsEQQAAAAJ&hl=en
    homepage: https://www.irit.fr/~Mohand.Boughanem/index_en.php
    institution: "Universit\xE9 de Toulouse"
    last_name: Boughanem
    name: Mohand Boughanem
    orcid: https://orcid.org/0000-0001-7004-0807
    username: ~Mohand_Boughanem1
  decision: toFindings
  end_page: 1201
  file: 366.pdf
  id: 366
  num_pages: 10
  openreview_id: yfbk1m8cno
  pdf_file: fa96c1f2b2d634d5c85af09f804e002e3bc540e2.pdf
  start_page: 1192
  title: Explanation Extraction from Hierarchical Classification Frameworks for Long
    Legal Documents
- abstract: Although the advancements of pre-trained Large Language Models have significantly
    accelerated recent progress in NLP, their ever-increasing size poses significant
    challenges for conventional fine-tuning, especially in memory-intensive tasks.
    We investigate the potential of Parameter-Efficient Fine-Tuning, focusing on Low-Rank
    Adaptation (LoRA), in the domain of multilingual summarization, a task that is
    both challenging (due to typically long inputs), and relatively unexplored.  We
    conduct an extensive study across different data availability scenarios, including
    high- and low-data settings, and cross-lingual transfer, leveraging models of
    different sizes. Our findings reveal that LoRA is competitive with full fine-tuning
    when trained with high quantities of data, and excels in low-data scenarios and
    cross-lingual transfer. We also study different strategies for few-shot cross-lingual
    transfer, finding that continued LoRA tuning outperforms full fine-tuning and
    the dynamic composition of language-specific LoRA modules.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Efficient/Low-Resource Methods for NLP
  authors:
  - emails: '****@gmail.com'
    first_name: Chenxi
    google_scholar_id: https://scholar.google.com/citations?user=MxJqtPIAAAAJ
    homepage: https://chenxwh.github.io/
    institution: University of Cambridge
    last_name: Whitehouse
    name: Chenxi Whitehouse
    username: ~Chenxi_Whitehouse1
  - emails: '****@google.com'
    first_name: Fantine
    google_scholar_id: https://scholar.google.com/citations?user=79VvQLMAAAAJ&hl=en&authuser=3
    institution: Google
    last_name: Huot
    name: Fantine Huot
    username: ~Fantine_Huot1
  - dblp_id: https://dblp.org/pid/146/3824
    emails: '****@google.com'
    first_name: Jasmijn
    google_scholar_id: https://scholar.google.com/citations?user=VG_wuYkAAAAJ&hl=en
    homepage: https://bastings.github.io
    institution: Google DeepMind
    last_name: Bastings
    name: Jasmijn Bastings
    orcid: https://orcid.org/0000-0002-5445-4417
    semantic_scholar_id: https://www.semanticscholar.org/author/Jasmijn-Bastings/3000862
    username: ~Jasmijn_Bastings1
  - dblp_id: https://dblp.org/pid/125/4062
    emails: '****@gmail.com'
    first_name: Mostafa
    google_scholar_id: https://scholar.google.nl/citations?user=MiHOX3QAAAAJ&hl=en
    homepage: http://mostafadehghani.com/
    institution: Google DeepMind
    last_name: Dehghani
    name: Mostafa Dehghani
    username: ~Mostafa_Dehghani1
  - dblp_id: https://dblp.org/pid/64/7292
    emails: '****@google.com'
    first_name: Chu-Cheng
    institution: Google
    last_name: Lin
    name: Chu-Cheng Lin
    username: ~Chu-Cheng_Lin1
  - dblp_id: https://dblp.org/pid/59/6701
    emails: '****@inf.ed.ac.uk'
    first_name: Mirella
    google_scholar_id: https://scholar.google.com/citations?user=j67B9Q4AAAAJ&hl=en
    homepage: https://homepages.inf.ed.ac.uk/mlap/
    institution: Edinburgh University, University of Edinburgh
    last_name: Lapata
    name: Mirella Lapata
    semantic_scholar_id: https://www.semanticscholar.org/author/Mirella-Lapata/1747893
    username: ~Mirella_Lapata1
  decision: toFindings
  end_page: 1228
  file: 368.pdf
  id: 368
  num_pages: 27
  openreview_id: 3KtpwMaeiP
  pdf_file: 335953571bf4a748fc9a68799d645223113f6d11.pdf
  start_page: 1202
  title: 'Low-Rank Adaptation for Multilingual Summarization: An Empirical Study'
- abstract: "Reasoning methods, best exemplified by the well-known Chain-of-Thought\
    \ (CoT),  empower the reasoning abilities of Large Language Models (LLMs) by eliciting\
    \ them to solve complex tasks in a step-by-step manner. Although they are achieving\
    \ significant success, the ability to deliver multi-step reasoning remains limited\
    \ to English because of the imbalance in the distribution of pre-training data,\
    \ which makes other languages a barrier. \nIn this paper, we propose Cross-lingual\
    \ Tree-of-Thoughts (Cross-ToT), a method for aligning Cross-lingual CoT reasoning\
    \ across languages. The proposed method, through a self-consistent cross-lingual\
    \ prompting mechanism inspired by the Tree-of-Thoughts approach, provides multi-step\
    \ reasoning paths in different languages that, during the steps, lead to the final\
    \ solution. Experimental evaluations show that our method significantly outperforms\
    \ existing prompting methods by reducing the number of interactions and achieving\
    \ state-of-the-art performance."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multilinguality and Language Diversity
  authors:
  - dblp_id: https://dblp.org/pid/278/7831
    emails: '****@gmail.com'
    first_name: Leonardo
    google_scholar_id: https://scholar.google.com/citations?user=QYQZ3UsAAAAJ&hl=it
    institution: Idiap Research Institute
    last_name: Ranaldi
    name: Leonardo Ranaldi
    orcid: https://orcid.org/0000-0001-8488-4146
    semantic_scholar_id: https://www.semanticscholar.org/author/Leonardo-Ranaldi/2008183566
    username: ~Leonardo_Ranaldi1
  - emails: '****@gmail.com'
    first_name: Giulia
    google_scholar_id: https://scholar.google.com/citations?user=FGRnGL8AAAAJ&hl=it
    last_name: Pucci
    name: Giulia Pucci
    semantic_scholar_id: https://www.semanticscholar.org/author/Giulia-Pucci/2199247500
    username: ~Giulia_Pucci1
  - emails: '****@gmail.com'
    first_name: Federico
    google_scholar_id: https://scholar.google.com/citations?user=4hU1e4AAAAAJ&hl=en
    institution: University of Roma "Tor Vergata"
    last_name: Ranaldi
    name: Federico Ranaldi
    username: ~Federico_Ranaldi1
  - dblp_id: https://dblp.org/pid/302/4055
    emails: '****@uniroma2.it'
    first_name: Elena Sofia
    google_scholar_id: https://scholar.google.com/citations?user=XRi2_woAAAAJ
    institution: "Universit\xE0 degli Studi di Roma Tor Vergata"
    last_name: Ruzzetti
    name: Elena Sofia Ruzzetti
    semantic_scholar_id: https://www.semanticscholar.org/author/Elena-Sofia-Ruzzetti/2128108433
    username: ~Elena_Sofia_Ruzzetti1
  - dblp_id: https://dblp.org/pid/32/797
    emails: '****@uniroma2.it'
    first_name: Fabio Massimo
    google_scholar_id: https://scholar.google.it/citations?user=azv7Qr4AAAAJ
    homepage: http://art.uniroma2.it/zanzotto
    institution: University of Rome Tor Vergata
    last_name: Zanzotto
    name: Fabio Massimo Zanzotto
    semantic_scholar_id: https://www.semanticscholar.org/author/Fabio-Massimo-Zanzotto/2337426
    username: ~Fabio_Massimo_Zanzotto1
  decision: toFindings
  end_page: 1241
  file: 371.pdf
  id: 371
  num_pages: 13
  openreview_id: zNV5fnuhJG
  pdf_file: 962370b71c2cdd3d016cd3c143fa9bc4f107ec69.pdf
  start_page: 1229
  title: A Tree-of-Thoughts to Broaden Multi-step Reasoning across Languages
- abstract: 'Large language models can solve new tasks without task-specific fine-tuning.
    This ability, also known as in-context learning (ICL), is considered an emergent
    ability and is primarily seen in large language models with billions of parameters.
    This study investigates if such emergent properties are strictly tied to model
    size or can be demonstrated by smaller models trained on reduced-scale data. To
    explore this, we simplify pre-training data and pre-train 36 causal language models
    with parameters varying from 1 million to 165 million parameters. We show that
    models trained on this simplified pre-training data demonstrate enhanced zero-shot
    capabilities across various tasks in simplified language, achieving performance
    comparable to that of pre-trained models six times larger on unrestricted language.
    This suggests that downscaling the language allows zero-shot learning capabilities
    to emerge in models with limited size.

    Additionally, we find that these smaller models pre-trained on simplified data
    demonstrate a power law relationship between the evaluation loss and the three
    scaling factors: compute, dataset size, and model size.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Efficient/Low-Resource Methods for NLP
  authors:
  - dblp_id: https://dblp.org/pid/138/5752
    emails: '****@student.uml.edu'
    first_name: Sherin
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=jG_G61YAAAAJ
    institution: University of Massachusetts at Lowell
    last_name: Muckatira
    name: Sherin Muckatira
    username: ~Sherin_Muckatira1
  - dblp_id: https://dblp.org/pid/329/4901
    emails: '****@student.uml.edu'
    first_name: Vijeta
    google_scholar_id: https://scholar.google.com/citations?user=CiBtlNQAAAAJ&hl=en&oi=ao
    last_name: Deshpande
    name: Vijeta Deshpande
    orcid: https://orcid.org/0000-0002-7693-8324
    semantic_scholar_id: https://www.semanticscholar.org/author/Vijeta-Deshpande/2212876381
    username: ~Vijeta_Deshpande1
  - dblp_id: https://dblp.uni-trier.de/pid/251/5456
    emails: '****@student.uml.edu'
    first_name: Vladislav
    google_scholar_id: https://scholar.google.com/citations?user=B1Ijov0AAAAJ&hl=en
    homepage: http://vladlialin.com
    institution: University of Massachusetts, Lowell
    last_name: Lialin
    name: Vladislav Lialin
    username: ~Vladislav_Lialin1
  - dblp_id: https://dblp.org/pid/63/873
    emails: '****@gmail.com'
    first_name: Anna
    google_scholar_id: https://scholar.google.com.tw/citations?user=_Q1uzVYAAAAJ
    homepage: http://text-machine.cs.uml.edu
    institution: University of Massachusetts, Lowell, University of Massachusetts
      at Lowell and Amazon
    last_name: Rumshisky
    name: Anna Rumshisky
    semantic_scholar_id: https://www.semanticscholar.org/author/Anna-Rumshisky/1681193
    username: ~Anna_Rumshisky1
  decision: toFindings
  end_page: 1257
  file: 375.pdf
  id: 375
  num_pages: 16
  openreview_id: Ra9NGGLxes
  pdf_file: 91dab08485951069dabbb4ed73b8e096eeb745d6.pdf
  start_page: 1242
  title: Emergent Abilities in Reduced-Scale Generative Language Models
- abstract: "Crowdsourced labels play a crucial role in evaluating task-oriented dialogue\
    \ systems (TDSs). Obtaining high-quality and consistent ground-truth labels from\
    \ annotators presents challenges. When evaluating a TDS, annotators must fully\
    \ comprehend the dialogue before providing judgments. Previous studies suggest\
    \ using only a portion of the dialogue context in the annotation process. However,\
    \ the impact of this limitation on label quality remains unexplored. This study\
    \ investigates the influence of dialogue context on annotation quality, considering\
    \ the truncated context for relevance and usefulness labeling. We further propose\
    \ to use large language models ( LLMs) to summarize the dialogue context to provide\
    \ a rich and short description of the dialogue context and study the impact of\
    \ doing so on the annotator\u2019s performance. Reducing context leads to more\
    \ positive ratings. Conversely, providing the entire dialogue context yields higher-quality\
    \ relevance ratings but introduces ambiguity in usefulness ratings. Using the\
    \ first user utterance as context leads to consistent ratings, akin to those obtained\
    \ using the entire dialogue, with significantly reduced annotation effort. Our\
    \ findings show how task design, particularly the availability of dialogue context,\
    \ affects the quality and consistency of crowdsourced evaluation labels."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Dialogue and Interactive Systems
  authors:
  - dblp_id: https://dblp.org/search?q=clemencia+siro
    emails: '****@uva.nl'
    first_name: Clemencia
    google_scholar_id: https://scholar.google.com/citations?user=VSJ78nMAAAAJ&hl=en&oi=ao
    homepage: https://github.com/Clemenciah
    last_name: Siro
    name: Clemencia Siro
    orcid: https://orcid.org/0000-0001-5301-4244
    username: ~Clemencia_Siro1
  - dblp_id: https://dblp.org/pid/178/6008
    emails: '****@uva.nl'
    first_name: Mohammad
    google_scholar_id: https://scholar.google.com/citations?user=yiZk6coAAAAJ&hl=en
    homepage: https://aliannejadi.com
    institution: University of Amsterdam
    last_name: Aliannejadi
    name: Mohammad Aliannejadi
    orcid: https://orcid.org/0000-0002-9447-4172
    semantic_scholar_id: https://www.semanticscholar.org/author/Mohammad-Aliannejadi/3390352
    username: ~Mohammad_Aliannejadi2
  - dblp_id: http://dblp.uni-trier.de/pers/hd/r/Rijke:Maarten_de
    emails: '****@uva.nl'
    first_name: Maarten
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=AVDkgFIAAAAJ&view_op=list_works&pagesize=100
    institution: University of Amsterdam
    last_name: Rijke
    middle_name: de
    name: Maarten de Rijke
    username: ~Maarten_de_Rijke1
  decision: toFindings
  end_page: 1273
  file: 379.pdf
  id: 379
  num_pages: 16
  openreview_id: 2umbhMlW22
  pdf_file: 5020d0a4e3c1f7a4aeef3abc4a06673ec338e680.pdf
  start_page: 1258
  title: 'Context Does Matter: Implications for Crowdsourced Evaluation Labels in
    Task-Oriented Dialogue Systems'
- abstract: Measuring semantic similarity between texts is a crucial task in natural
    language processing. While existing semantic text matching focuses on pairs of
    similar-length sequences, matching texts with non-comparable lengths has broader
    applications in specific domains, such as comparing professional document summaries
    and content. Current approaches struggle with text pairs of non-comparable lengths
    due to truncation issues. To address this, we split texts into natural sentences
    and decouple sentence representations using supervised contrastive learning (SCL).
    Meanwhile, we adopt the embedded topic model (ETM) for specific domain data. Our
    experiments demonstrate the effectiveness of our model, based on decoupled and
    topic-informed sentence embeddings, in matching texts of significantly different
    lengths across three well-studied datasets.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Retrieval and Text Mining
  authors:
  - dblp_id: https://dblp.uni-trier.de/pid/237/8948
    emails: '****@zju.edu.cn'
    first_name: Xixi
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=-hW21MYAAAAJ
    institution: Zhejiang University
    last_name: Zhou
    name: Xixi Zhou
    orcid: https://orcid.org/0000-0003-0389-3994
    username: ~Xixi_Zhou1
  - emails: '****@zju.edu.cn'
    first_name: Chunbin
    google_scholar_id: https://scholar.google.com.hk/citations?hl=zh-CN&user=BlSFT8UAAAAJ
    last_name: Gu
    name: Chunbin Gu
    username: ~Chunbin_Gu1
  - emails: '****@zju.edu.cn'
    first_name: Xin
    homepage: https://github.com/jx6186
    last_name: Jie
    name: Xin Jie
    username: ~Xin_Jie1
  - dblp_id: https://dblp.org/pid/50/3147
    emails: '****@zju.edu.cn'
    first_name: Jiajun
    google_scholar_id: https://scholar.google.com/citations?user=OgZP2okAAAAJ
    homepage: https://person.zju.edu.cn/bjj
    institution: Zhejiang University
    last_name: Bu
    name: Jiajun Bu
    semantic_scholar_id: https://www.semanticscholar.org/author/Jiajun-Bu/145383433
    username: ~Jiajun_Bu1
  - emails: '****@gmail.com'
    first_name: Haishuai
    institution: Zhejiang University
    last_name: Wang
    name: Haishuai Wang
    orcid: https://orcid.org/0000-0003-1617-0920
    username: ~Haishuai_Wang2
  decision: toFindings
  end_page: 1280
  file: 380.pdf
  id: 380
  num_pages: 7
  openreview_id: g9PjXV6lom
  pdf_file: 46a741d47d98ed2b3741b5eaa09bda40c5f38b47.pdf
  start_page: 1274
  title: Matching Varying-Length Texts via Topic-Informed and Decoupled Sentence Embeddings
- abstract: "In this work, we (1) introduce Curriculum Instruction Tuning, (2) explore\
    \ the potential advantages of employing diverse curriculum strategies, and (3)\
    \ delineate a synthetic instruction-response generation framework that complements\
    \ our theoretical approach. Distinct from the existing instruction tuning dataset,\
    \ our generation pipeline is systematically structured to emulate the sequential\
    \ and orderly characteristic of human learning. Additionally, we describe a methodology\
    \ for generating instruction-response datasets that extensively span the various\
    \ stages of human education, from middle school through the graduate level, utilizing\
    \ educational subject catalogs.\n\nBefore training, we meticulously organize the\
    \ instruction data to ensure that questions escalate in difficulty regarding (A)\
    \ the subject matter and (B) the intricacy of the instructions. The findings of\
    \ our study reveal that substantial improvements in performance can be achieved\
    \ through the mere application of curriculum ordering to instruction data\u2014\
    achieving gains of +4.76 on TruthfulQA, +2.98 on MMLU, +2.8 on OpenbookQA, and\
    \ +1.28 on ARC-hard\u2014compared to random shuffling. This enhancement is achieved\
    \ without incurring additional computational expenses. Through comprehensive experimentation,\
    \ we observe that the advantages of our proposed method are consistently evident\
    \ across nine benchmarks."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Learning for NLP
  authors:
  - emails: '****@gmail.com'
    first_name: Bruce W
    google_scholar_id: https://scholar.google.com/citations?user=a9HZkjMAAAAJ&hl=en
    homepage: https://www.viper.upenn.edu/people/bruce-lee
    last_name: Lee
    name: Bruce W Lee
    orcid: https://orcid.org/0000-0003-0921-7794
    semantic_scholar_id: https://www.semanticscholar.org/author/Bruce-W.-Lee/14445971
    username: ~Bruce_W_Lee1
  - dblp_id: https://dblp.org/pid/86/125
    emails: '****@ewha.ac.kr'
    first_name: Hyunsoo
    google_scholar_id: https://scholar.google.com/citations?hl=ko&user=3RBDJHEAAAAJ&view_op=list_works&sortby=pubdate
    homepage: https://hyunsoocho77.github.io
    institution: Ewha Women's University
    last_name: Cho
    name: Hyunsoo Cho
    semantic_scholar_id: https://www.semanticscholar.org/author/Hyunsoo-Cho/2111237576
    username: ~Hyunsoo_Cho1
  - dblp_id: https://dblp.org/pid/163/5657
    emails: '****@gmail.com'
    first_name: Kang Min
    google_scholar_id: https://scholar.google.com/citations?user=BqaWtH8AAAAJ
    institution: NAVER
    last_name: Yoo
    name: Kang Min Yoo
    semantic_scholar_id: https://www.semanticscholar.org/author/Kang-Min-Yoo/31760501
    username: ~Kang_Min_Yoo2
  decision: toFindings
  end_page: 1309
  file: 384.pdf
  id: 384
  num_pages: 29
  openreview_id: q2WjTkafwG
  pdf_file: ece8c10479057c0b4a3752932dca12c6f12771c5.pdf
  start_page: 1281
  title: Instruction Tuning with Human Curriculum
- abstract: This paper investigates the potential of using natural language descriptions
    as an alternative to direct image-based observations for learning policies in
    reinforcement learning. Due to the inherent challenges in managing image-based
    observations, which include abundant information and irrelevant features, we propose
    a method that compresses images into a natural language form for state representation.
    This approach allows better interpretability and leverages the processing capabilities
    of large-language models. We conducted several experiments involving tasks that
    required image-based observation. The results demonstrated that policies trained
    using natural language descriptions of images yield better generalization than
    those trained directly from images, emphasizing the potential of this approach
    in practical settings.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multimodality and Language Grounding to Vision, Robotics and Beyond
  authors:
  - dblp_id: https://dblp.org/pid/08/2425-1
    emails: '****@purdue.edu'
    first_name: Md Masudur
    google_scholar_id: https://scholar.google.com/citations?user=0nUv7b0AAAAJ&hl=en
    homepage: https://www.cs.purdue.edu/homes/rahman64/
    institution: Purdue University
    last_name: Rahman
    name: Md Masudur Rahman
    orcid: https://orcid.org/0000-0002-3633-0621
    semantic_scholar_id: https://www.semanticscholar.org/author/Md-Masudur-Rahman/50743443
    username: ~Md_Masudur_Rahman2
  - dblp_id: https://dblp.org/pid/117/4903
    emails: '****@purdue.edu'
    first_name: Yexiang
    homepage: https://www.cs.purdue.edu/people/faculty/yexiang/
    institution: Purdue University, Purdue University and Purdue University
    last_name: Xue
    name: Yexiang Xue
    username: ~Yexiang_Xue1
  decision: toFindings
  end_page: 1319
  file: 389.pdf
  id: 389
  num_pages: 10
  openreview_id: xeZyIsQfke
  pdf_file: 4b8151e04878d9373bd514f41d1a05d729f4e8e5.pdf
  start_page: 1310
  title: Natural Language-based State Representation in Deep Reinforcement Learning
- abstract: "Binary code analysis is indispensable for a variety of software security\
    \ tasks. Applying deep learning to binary code analysis has drawn great attention\
    \ because of its notable performance. Today, source code is frequently compiled\
    \ for various Instruction Set Architectures (ISAs). It is thus critical to expand\
    \ binary analysis capabilities to multiple ISAs. Given a binary analysis task,\
    \ the scale of available data on different ISAs varies. As a result, the rich\
    \ datasets (e.g., malware) for certain ISAs, such as x86, lead to a disproportionate\
    \ focus on these ISAs and a negligence of other ISAs, such as PowerPC, which suffer\
    \ from the \u201Cdata scarcity\u201D problem. To address the problem, we propose\
    \ to learn cross-architecture instruction embeddings (CAIE), where semantically-similar\
    \ instructions, regardless of their ISAs, have close embeddings in a shared space.\
    \ Consequently, we can transfer a model trained on a data-rich ISA to another\
    \ ISA with less available data. We consider four ISAs (x86, ARM, MIPS, and PowerPC)\
    \ and conduct both intrinsic and extrinsic evaluations (including malware detection\
    \ and function similarity comparison). The results demonstrate the effectiveness\
    \ of our approach to generate high-quality CAIE with good transferability."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - emails: '****@gmu.edu'
    first_name: Junzhe
    homepage: https://github.com/JWang169
    last_name: Wang
    name: Junzhe Wang
    username: ~Junzhe_Wang3
  - emails: '****@gmu.edu'
    first_name: Qiang
    homepage: https://cs.gmu.edu/~zeng/
    institution: George Mason University
    last_name: Zeng
    name: Qiang Zeng
    username: ~Qiang_Zeng6
  - dblp_id: https://dblp.org/pid/153/5297
    emails: '****@gmu.edu'
    first_name: Lannan
    google_scholar_id: https://scholar.google.com.tw/citations?user=JPXjw04AAAAJ
    homepage: https://lannan.github.io
    institution: George Mason University
    last_name: Luo
    name: Lannan Luo
    username: ~Lannan_Luo3
  decision: toFindings
  end_page: 1332
  file: 392.pdf
  id: 392
  num_pages: 13
  openreview_id: YFsBm7SOpO
  pdf_file: 378e488c1e1634f65b3e06a673b35d7056470ceb.pdf
  start_page: 1320
  title: Learning Cross-Architecture Instruction Embeddings for Binary Code Analysis
    in Low-Resource Architectures
- abstract: "Despite remarkable advancements in mitigating hallucinations in large\
    \ language models (LLMs) by retrieval augmentation, it remains challenging to\
    \ measure the reliability of LLMs using static question-answering (QA) data. Specifically,\
    \ given the potential of data contamination (e.g., leading to memorization), good\
    \ static benchmark performance does not ensure that model can reliably use the\
    \ provided evidence for responding, which is essential to avoid hallucination\
    \ when the required knowledge is new or private. Inspired by adversarial machine\
    \ learning, we investigate the feasibility of automatically perturbing existing\
    \ static one for dynamic evaluation. Specifically, this paper presents ReEval,\
    \ an LLM-based framework using prompt chaining to perturb the original evidence\
    \ for generating new test cases for evaluating the LLMs\u2019 reliability in using\
    \ new evidence for answering.\n\nWe implement ReEval using ChatGPT and evaluate\
    \ the resulting variants of two popular open-domain QA datasets on a collection\
    \ of\nLLMs under various prompting settings. Our generated data is human-readable\
    \ and useful to trigger hallucination in LLM. Accurate models on static data are\
    \ observed to produce unsupported answers from the perturbed evidence, with pronounced\
    \ accuracy drops across LLMs including GPT-4. We find that our adversarial examples\
    \ are transferable across all considered LLMs. The examples generated by a small\
    \ model can be used to evaluate a much larger model, making our approach cost-effective."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - dblp_id: https://dblp.org/pid/97/4403
    emails: '****@seas.upenn.edu'
    first_name: Xiaodong
    google_scholar_id: https://scholar.google.com/citations?user=nmyIoRMAAAAJ
    homepage: https://www.xiaodongyu.me/
    institution: University of Pennsylvania, University of Pennsylvania
    last_name: Yu
    name: Xiaodong Yu
    semantic_scholar_id: https://www.semanticscholar.org/author/Xiaodong-Yu/3099583
    username: ~Xiaodong_Yu2
  - dblp_id: https://dblp.org/pid/09/5158-2
    emails: '****@gmail.com'
    first_name: Hao
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=d9s3sbQAAAAJ&view_op=list_works&sortby=pubdate
    homepage: https://sites.google.com/site/hcheng2site/Home
    institution: Microsoft Research
    last_name: Cheng
    name: Hao Cheng
    orcid: https://orcid.org/0000-0001-7988-3149
    semantic_scholar_id: https://www.semanticscholar.org/author/Hao-Cheng/47413820
    username: ~Hao_Cheng4
  - dblp_id: https://dblp.org/pid/65/622
    emails: '****@gmail.com'
    first_name: Xiaodong
    google_scholar_id: https://scholar.google.com/citations?user=NIewcxMAAAAJ&hl=en
    last_name: Liu
    name: Xiaodong Liu
    username: ~Xiaodong_Liu1
  - dblp_id: https://dblp.org/pid/r/DanRoth
    emails: '****@seas.upenn.edu'
    first_name: Dan
    google_scholar_id: https://scholar.google.com/citations?user=E-bpPWgAAAAJ&hl=en
    homepage: https://www.cis.upenn.edu/~danroth/
    institution: Amazon and University of Pennsylvania
    last_name: Roth
    name: Dan Roth
    semantic_scholar_id: https://www.semanticscholar.org/author/D.-Roth/144590225
    username: ~Dan_Roth3
  - dblp_id: https://dblp.org/pid/92/5339
    emails: '****@microsoft.com'
    first_name: Jianfeng
    homepage: https://www.microsoft.com/en-us/research/people/jfgao/
    institution: Microsoft Research
    last_name: Gao
    name: Jianfeng Gao
    username: ~Jianfeng_Gao1
  decision: toFindings
  end_page: 1351
  file: 398.pdf
  id: 398
  num_pages: 19
  openreview_id: dHa8h2XhQc
  pdf_file: d5122807b21ec21ad4e3114360b0d70f5f5c1d0d.pdf
  start_page: 1333
  title: 'ReEval: Automatic Hallucination Evaluation for Retrieval-Augmented Large
    Language Models via Transferable Adversarial Attacks'
- abstract: 'Automated speaking assessment (ASA) typically involves automatic speech
    recognition (ASR) and hand-crafted feature extraction from the ASR transcript
    of a learner''s speech. Recently, self-supervised learning (SSL) has shown stellar
    performance compared to traditional methods. However, SSL-based ASA systems are
    faced with at least three data-related challenges: limited annotated data, uneven
    distribution of learner proficiency levels and non-uniform score intervals between
    different CEFR proficiency levels. To address these challenges, we explore the
    use of two novel modeling strategies: metric-based classification and loss re-weighting,
    leveraging distinct SSL-based embedding features. Extensive experimental results
    on the ICNALE benchmark dataset suggest that our approach can outperform existing
    strong baselines by a sizable margin, achieving a significant improvement of more
    than 10% in CEFR prediction accuracy.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Speech recognition, text-to-speech and spoken language understanding
  authors:
  - dblp_id: https://dblp.org/pid/212/4799
    emails: '****@ntnu.edu.tw'
    first_name: Tien-Hong
    google_scholar_id: https://scholar.google.com/citations?user=RNgvU2wAAAAJ&hl=zh-TW
    homepage: https://teinhonglo.github.io/
    institution: National Taiwan Normal University
    last_name: Lo
    name: Tien-Hong Lo
    semantic_scholar_id: https://www.semanticscholar.org/author/Tien-Hong-Lo/31773575
    username: ~Tien-Hong_Lo1
  - emails: '****@ntnu.edu.tw'
    first_name: Fu-An
    google_scholar_id: https://scholar.google.com.tw/citations?user=0LqFPZcAAAAJ&hl=zh-TW
    institution: National Taiwan Normal University
    last_name: Chao
    name: Fu-An Chao
    username: ~Fu-An_Chao1
  - emails: '****@ntnu.edu.tw'
    first_name: Tzu-i
    homepage: https://github.com/Zoeee-cool
    last_name: WU
    name: TZU-I WU
    username: ~TZU-I_WU1
  - emails: '****@ntnu.edu.tw'
    first_name: Yao-Ting
    last_name: Sung
    name: Yao-Ting Sung
    username: ~Yao-Ting_Sung1
  - dblp_id: https://dblp.org/pid/c/BerlinChen
    emails: '****@ntnu.edu.tw'
    first_name: Berlin
    google_scholar_id: https://scholar.google.com.tw/citations?user=-2c31OsAAAAJ&hl=en
    homepage: https://sites.google.com/site/berlinchenatntnu/home
    institution: National Taiwan Normal University
    last_name: Chen
    name: Berlin Chen
    orcid: https://orcid.org/0000-0003-0693-8932
    username: ~Berlin_Chen2
  decision: toFindings
  end_page: 1362
  file: 404.pdf
  id: 404
  num_pages: 11
  openreview_id: kqFca5EgdD
  pdf_file: 5937394cd460e41c7a1c134e1b6e262483b7dfb6.pdf
  start_page: 1352
  title: An Effective Automated Speaking Assessment Approach to Mitigating Data Scarcity
    and Imbalanced Distribution
- abstract: With the rapid advancement of large language models (LLMs), there is a
    pressing need for a comprehensive evaluation suite to assess their capabilities
    and limitations. Existing LLM leaderboards often reference scores reported in
    other papers without consistent settings and prompts, which may inadvertently
    encourage cherry-picking favored settings and prompts for better results. In this
    work, we introduce GPT-Fathom, an open-source and reproducible LLM evaluation
    suite built on top of OpenAI Evals. We systematically evaluate 10+ leading LLMs
    as well as OpenAI's legacy models on 20+ curated benchmarks across 7 capability
    categories, all under aligned settings. Our retrospective study on OpenAI's earlier
    models offers valuable insights into the evolutionary path from GPT-3 to GPT-4.
    Currently, the community is eager to know how GPT-3 progressively improves to
    GPT-4, including technical details like whether adding code data improves LLM's
    reasoning capability, which aspects of LLM capability can be improved by SFT and
    RLHF, how much is the alignment tax, etc. Our analysis sheds light on many of
    these questions, aiming to improve the transparency of advanced LLMs.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - emails: '****@illinois.edu'
    first_name: Shen
    google_scholar_id: https://scholar.google.com/citations?user=c3ZV_s7leXQC&hl=en
    homepage: https://shenz2-2000.github.io
    institution: Department of Computer Science
    last_name: Zheng
    name: Shen Zheng
    semantic_scholar_id: https://www.semanticscholar.org/me/research
    username: ~Shen_Zheng2
  - emails: '****@gatech.edu'
    first_name: Yuyu
    google_scholar_id: https://scholar.google.com/citations?user=TIC2ujUAAAAJ&hl=en
    institution: ByteDance
    last_name: Zhang
    name: Yuyu Zhang
    username: ~Yuyu_Zhang1
  - emails: '****@gmail.com'
    first_name: Yijie
    last_name: Zhu
    name: Yijie Zhu
    username: ~Yijie_Zhu1
  - emails: '****@bytedance.com'
    first_name: Chenguang
    google_scholar_id: https://scholar.google.com/citations?user=AAGUBuIAAAAJ&hl=en
    last_name: Xi
    name: Chenguang Xi
    username: ~Chenguang_Xi2
  - emails: '****@bytedance.com'
    first_name: Pengyang
    homepage: https://twitter.com/fuergaosi
    last_name: Gao
    name: Pengyang Gao
    username: ~Pengyang_Gao1
  - emails: '****@bytedance.com'
    first_name: Zhou
    homepage: https://www.linkedin.com/in/zhou-xun-a5aa4561/
    last_name: Xun
    name: zhou Xun
    username: ~zhou_Xun2
  - dblp_id: https://dblp.uni-trier.de/pid/c/KCCChang.html
    emails: '****@illinois.edu'
    first_name: Kevin
    google_scholar_id: https://scholar.google.com.tw/citations?user=sugWZ6MAAAAJ
    homepage: https://cs.illinois.edu/directory/profile/kcchang
    institution: University of Illinois, Urbana Champaign
    last_name: Chang
    name: Kevin Chang
    orcid: https://orcid.org/0000-0003-0997-6803
    username: ~Kevin_Chang1
  decision: toFindings
  end_page: 1382
  file: 406.pdf
  id: 406
  num_pages: 20
  openreview_id: uZeNIJihhD
  pdf_file: 93135a62a9a6948065eb2544310fdfe99ee084ee.pdf
  start_page: 1363
  title: 'GPT-Fathom: Benchmarking Large Language Models to Decipher the Evolutionary
    Path towards GPT-4 and Beyond'
- abstract: Word representations are an important aspect of Natural Language Processing
    (NLP).  Representations are trained using large corpora, either as independent
    static embeddings or as part of a deep contextualized model.  While word embeddings
    are useful, they struggle on rare and unknown words.  As such, a large body of
    work has been done on estimating rare and unknown words.  However, most of the
    methods focus on static embeddings, with few models focused on contextualized
    representations.  In this work, we propose SPRUCE, a rare/unknown embedding architecture
    that focuses on contextualized representations.  This architecture uses subword
    attention and embedding post-processing combined with the contextualized model
    to produce high quality embeddings.  We then demonstrate these techniques lead
    to improved performance in most intrinsic and downstream tasks.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Learning for NLP
  authors:
  - emails: '****@masonlive.gmu.edu'
    first_name: Raj
    institution: George Mason University
    last_name: Patel
    name: Raj Patel
    semantic_scholar_id: https://www.semanticscholar.org/author/R.-Patel/46811038
    username: ~Raj_Patel1
  - dblp_id: https://dblp.org/pid/50/5158
    emails: '****@cs.gmu.edu'
    first_name: Carlotta
    google_scholar_id: https://scholar.google.com/citations?user=aKkIMogAAAAJ&hl=en
    homepage: http://www.cs.gmu.edu/~carlotta
    institution: George Mason University and George Mason University
    last_name: Domeniconi
    name: Carlotta Domeniconi
    username: ~Carlotta_Domeniconi1
  decision: toFindings
  end_page: 1389
  file: 412.pdf
  id: 412
  num_pages: 7
  openreview_id: RrcZTCLYru
  pdf_file: a4c99be888985e872f79217bccd6450cfea48dfd.pdf
  start_page: 1383
  title: Subword Attention and Post-Processing for Rare and Unknown Contextualized
    Embeddings
- abstract: Help documents are supposed to aid smartphone users in resolving queries
    such as "How to block calls from unknown numbers?". However, given a query, identifying
    the right help document, understanding instructions from the document, and using
    them to resolve the issue at hand is challenging. The user experience may be  enhanced
    by converting the instructions in the help document to a step-by-step tutorial
    overlaid on the phone UI. Successful execution of this task  requires overcoming
    research challenges in retrieval, parsing, and grounding in the multilingual-multimodal
    setting. For example, user queries in one language may have to be matched against
    instructions in another language, which in turn needs to be grounded in a multimodal
    UI in yet another language. Moreover, there isn't any relevant dataset for such
    a task. In order to bridge this gap, we introduce UGIF-DataSet, a multi-lingual,
    multi-modal UI grounded dataset for step-by-step task completion on the smartphone,
    containing 4,184 tasks across 8 languages. The instruction steps in UGIF-DataSet  are
    available only in English, so the challenge involves operations in the cross-modal,
    cross-lingual setting. We compare the performance of different large language
    models for this task and find that the end-to-end task completion rate drops from
    48% in  English to 32% for other languages, demonstrating significant overall
    headroom for improvement. We are hopeful that UGIF-DataSet and our analysis will
    aid further research on the important problem of sequential task completion in
    the multilingual and multimodal setting.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - dblp_id: https://dblp.org/pid/156/0334
    emails: '****@google.com'
    first_name: Sagar
    homepage: http://www.sagargv.com
    last_name: Gubbi Venkatesh
    name: Sagar Gubbi Venkatesh
    semantic_scholar_id: https://www.semanticscholar.org/author/S.-Gubbi/2183758
    username: ~Sagar_Gubbi_Venkatesh1
  - dblp_id: https://dblp.org/pid/282/0169.html
    emails: '****@google.com'
    first_name: Partha
    google_scholar_id: https://scholar.google.com.tw/citations?user=CIZwXAcAAAAJ
    homepage: https://parthatalukdar.github.io/
    institution: Google Research and Indian Institute of Science, Bangalore
    last_name: Talukdar
    name: Partha Talukdar
    semantic_scholar_id: https://www.semanticscholar.org/author/P.-Talukdar/2408872
    username: ~Partha_Talukdar1
  - dblp_id: https://dblp.org/pid/72/1429
    emails: '****@icsi.berkeley.edu'
    first_name: Srini
    institution: Google research
    last_name: Narayanan
    name: Srini Narayanan
    username: ~Srini_Narayanan1
  decision: toFindings
  end_page: 1399
  file: 414.pdf
  id: 414
  num_pages: 10
  openreview_id: W2fe4OdGfj
  pdf_file: 8eb3936e5720a780b9e8c7940e260ed4ea0d84e5.pdf
  start_page: 1390
  title: 'UGIF-DataSet: A New Dataset for Cross-lingual, Cross-modal Sequential actions
    on the UI'
- abstract: 'Large code datasets have become increasingly accessible for pre-training
    source code models. However, for the fine-tuning phase, obtaining representative
    training data that fully covers the code distribution for specific downstream
    tasks remains challenging due to the task-specific nature and limited labeling
    resources. These lead to out-of-distribution (OOD) generalization issues with
    unexpected model inference behaviors that have not been systematically studied
    yet.

    In this paper, we contribute the first systematic approach that simulates various
    OOD scenarios along different dimensions of source code data properties and study
    the fine-tuned model behaviors in such scenarios. We investigate the behaviors
    of models under different fine-tuning methodologies, including full fine-tuning
    and Low-Rank Adaptation (LoRA) fine-tuning methods. Our comprehensive analysis,
    conducted on four state-of-the-art pretrained models and applied to two code generation
    tasks, exposes multiple failure modes attributed to OOD generalization issues.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Generation
  authors:
  - dblp_id: https://dblp.org/pid/125/1613
    emails: '****@cispa.saarland'
    first_name: Hossein
    homepage: https://cispa.de/en/people/hossein.hajipour
    institution: CISPA Helmholtz Center for Information Security
    last_name: Hajipour
    name: Hossein Hajipour
    username: ~Hossein_Hajipour1
  - emails: '****@gmail.com'
    first_name: Ning
    google_scholar_id: https://scholar.google.com/citations?user=TaJND9YAAAAJ&hl=en
    homepage: https://ningyu1991.github.io/
    institution: Salesforce Research
    last_name: Yu
    name: Ning Yu
    username: ~Ning_Yu2
  - emails: '****@cispa.de'
    first_name: Cristian-Alexandru
    google_scholar_id: https://scholar.google.com/citations?user=JmpDeRQAAAAJ&hl=en
    homepage: http://www.staicu.org/
    last_name: Staicu
    name: Cristian-Alexandru Staicu
    username: ~Cristian-Alexandru_Staicu2
  - dblp_id: https://dblp.org/pers/hd/f/Fritz:Mario
    emails: '****@cispa.saarland'
    first_name: Mario
    google_scholar_id: https://scholar.google.de/citations?user=4V1nNm4AAAAJ&hl=en&oi=ao
    homepage: https://fritz.cispa.saarland
    institution: CISPA Helmholtz Center for Information Security and Saarland University
    last_name: Fritz
    name: Mario Fritz
    username: ~Mario_Fritz1
  decision: toFindings
  end_page: 1416
  file: 416.pdf
  id: 416
  num_pages: 17
  openreview_id: 3spHAbE9NA
  pdf_file: e3074f8c94c89362cfbb48880a91c2cf1c4ab44c.pdf
  start_page: 1400
  title: 'SimSCOOD: Systematic Analysis of Out-of-Distribution Generalization in Fine-tuned
    Source Code Models'
- abstract: Large Language Models (LLMs) have exhibited remarkable proficiency across
    a wide array of NLP tasks. However, the escalation in model size also engenders
    substantial deployment costs. While few efforts have explored model pruning techniques
    to reduce the size of LLMs, they mainly center on general or task-specific weights.
    This leads to suboptimal performance due to lacking specificity on the target
    domain or generality on different tasks when applied to domain-specific challenges.
    This work introduces an innovative unstructured dual-pruning methodology, D-Pruner,
    for domain-specific compression on LLM. It extracts a compressed, domain-specific,
    and task- agnostic LLM by identifying LLM weights that are pivotal for general
    capabilities, like linguistic capability and multi-task solving, and domain-specific
    knowledge. More specifically, we first assess general weight importance by quantifying
    the error incurred upon their removal with the help of an open-domain calibration
    dataset. Then, we utilize this general weight importance to refine the training
    loss, so that it preserves generality when fitting into a specific domain. Moreover,
    by efficiently approximating weight importance with the refined training loss
    on a domain-specific calibration dataset, we obtain a pruned model emphasizing
    generality and specificity. Our comprehensive experiments across various tasks
    in healthcare and legal domains show the effectiveness of D-Pruner in domain-specific
    compression. Our code is available at https://github.com/psunlpgroup/D-Pruner.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Efficient/Low-Resource Methods for NLP
  authors:
  - emails: '****@psu.edu'
    first_name: Nan
    google_scholar_id: https://scholar.google.com/citations?user=PDuBGKYAAAAJ&hl=en
    homepage: https://zn1010.github.io
    institution: Pennsylvania State University
    last_name: Zhang
    name: Nan Zhang
    semantic_scholar_id: https://www.semanticscholar.org/author/Nan-Zhang/2174299795
    username: ~Nan_Zhang9
  - dblp_id: https://dblp.org/pid/62/8146
    emails: '****@nec-labs.com'
    first_name: Yanchi
    google_scholar_id: https://scholar.google.com/citations?user=faLmr-YAAAAJ&hl=en
    institution: NEC-Labs
    last_name: Liu
    name: Yanchi Liu
    username: ~Yanchi_Liu1
  - dblp_id: https://dblp.org/pid/221/5767
    emails: '****@nec-labs.com'
    first_name: Xujiang
    google_scholar_id: https://scholar.google.com/citations?user=k2-JcFAAAAAJ&hl=en
    homepage: https://zxj32.github.io/
    last_name: Zhao
    name: Xujiang Zhao
    username: ~Xujiang_Zhao1
  - dblp_id: https://dblp.uni-trier.de/pers/hd/c/Cheng:Wei
    emails: '****@nec-labs.com'
    first_name: Wei
    google_scholar_id: https://scholar.google.com/citations?user=PRrGVmoAAAAJ&hl=en
    homepage: https://chengw07.github.io/
    institution: NEC-Labs
    last_name: Cheng
    name: Wei Cheng
    username: ~Wei_Cheng1
  - dblp_id: https://dblp.org/pid/257/5653
    emails: '****@gmail.com'
    first_name: Runxue
    google_scholar_id: https://scholar.google.com/citations?user=jlcUOeoAAAAJ&hl=en
    homepage: https://brx18.github.io/
    institution: NEC Labs America
    last_name: Bao
    name: Runxue Bao
    username: ~Runxue_Bao2
  - dblp_id: https://dblp.org/pid/60/2536-37
    emails: '****@psu.edu'
    first_name: Rui
    google_scholar_id: https://scholar.google.com/citations?user=nhuB5CEAAAAJ&hl=en
    homepage: https://ryanzhumich.github.io/
    institution: Pennsylvania State University
    last_name: Zhang
    name: Rui Zhang
    semantic_scholar_id: https://www.semanticscholar.org/author/Rui-Zhang/144142360
    username: ~Rui_Zhang7
  - dblp_id: https://dblp.org/pid/19/3308
    emails: '****@psu.edu'
    first_name: Prasenjit
    google_scholar_id: https://scholar.google.com/citations?user=8PbgiPkAAAAJ&hl=en
    homepage: http://www.personal.psu.edu/pum10/
    institution: Pennsylvania State University
    last_name: Mitra
    name: Prasenjit Mitra
    semantic_scholar_id: https://www.semanticscholar.org/author/P.-Mitra/143930195
    username: ~Prasenjit_Mitra1
  - dblp_id: https://dblp.org/pid/08/57
    emails: '****@nec-labs.com'
    first_name: Haifeng
    google_scholar_id: https://scholar.google.com/citations?user=QzakB68AAAAJ&hl=en
    homepage: https://haifengchen.gitlab.io/intro/
    institution: NEC-Labs
    last_name: Chen
    name: Haifeng Chen
    username: ~Haifeng_Chen1
  decision: toFindings
  end_page: 1428
  file: 417.pdf
  id: 417
  num_pages: 12
  openreview_id: SouismhE25
  pdf_file: 138e0efe082a7848c620dab525f0eb8b21ad1709.pdf
  start_page: 1417
  title: Pruning as a Domain-specific LLM Extractor
- abstract: "Recent large language models (LLM) are\nleveraging human feedback to\
    \ improve their\ngeneration quality. However, human feedback\nis costly to obtain,\
    \ especially during inference.\nIn this work, we propose LLMRefine, an\ninference\
    \ time optimization method to refine\nLLM\u2019s output. The core idea is to use\n\
    a learned fine-grained feedback model to\npinpoint defects and guide LLM to refine\n\
    them iteratively. Using original LLM as a\nproposal of edits, LLMRefine searches\
    \ for\ndefect-less text via simulated annealing, trading\noff the exploration\
    \ and exploitation. We\nconduct experiments on three text generation\ntasks, including\
    \ machine translation, long-\nform question answering (QA), and topical\nsummarization.\
    \ LLMRefine consistently\noutperforms all baseline approaches, achieving\nimprovements\
    \ up to 1.7 MetricX points on\ntranslation tasks, 8.1 ROUGE-L on ASQA, 2.2\nROUGE-L\
    \ on topical summarization."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Translation
  authors:
  - emails: '****@ucsb.edu'
    first_name: Wenda
    last_name: Xu
    name: Wenda Xu
    username: ~Wenda_Xu1
  - dblp_id: https://dblp.org/pid/222/9395
    emails: '****@google.com'
    first_name: Daniel
    google_scholar_id: https://scholar.google.com/citations?user=TQYzWDEAAAAJ&hl=en
    homepage: https://danieldeutsch.github.io/
    institution: Google
    last_name: Deutsch
    name: Daniel Deutsch
    semantic_scholar_id: https://www.semanticscholar.org/author/Daniel-Deutsch/145346875
    username: ~Daniel_Deutsch1
  - emails: '****@gmail.com'
    first_name: Mara
    institution: Google
    last_name: Finkelstein
    name: Mara Finkelstein
    username: ~Mara_Finkelstein1
  - dblp_id: https://dblp.org/pid/220/2049
    emails: '****@google.com'
    first_name: Juraj
    google_scholar_id: https://scholar.google.com/citations?user=yC7hPnoAAAAJ
    institution: Google
    last_name: Juraska
    name: Juraj Juraska
    semantic_scholar_id: https://www.semanticscholar.org/author/Juraj-Juraska/47080255
    username: ~Juraj_Juraska1
  - dblp_id: https://dblp.uni-trier.de/pers/hd/z/Zhang_0002:Biao
    emails: '****@google.com'
    first_name: Biao
    google_scholar_id: https://scholar.google.com/citations?user=gqPKjaIAAAAJ&hl=zh-CN
    institution: Google Research
    last_name: Zhang
    name: Biao Zhang
    semantic_scholar_id: https://www.semanticscholar.org/author/Biao-Zhang/48335426
    username: ~Biao_Zhang2
  - emails: '****@google.com'
    first_name: Zhongtao
    homepage: https://www.linkedin.com/in/ztliu62
    institution: Google
    last_name: Liu
    name: Zhongtao Liu
    username: ~Zhongtao_Liu1
  - dblp_id: https://dblp.org/pid/08/9282
    emails: '****@cs.ucsb.edu'
    first_name: William Yang
    google_scholar_id: https://scholar.google.com/citations?user=gf8Ms_8AAAAJ&hl=en
    homepage: https://www.cs.ucsb.edu/~william/
    institution: UC Santa Barbara
    last_name: Wang
    name: William Yang Wang
    username: ~William_Yang_Wang2
  - dblp_id: https://dblp.org/pid/13/7007-5.html
    emails: '****@cs.cmu.edu'
    first_name: Lei
    google_scholar_id: https://scholar.google.com/citations?user=BYXqAlwAAAAJ&hl=en
    homepage: https://www.cs.cmu.edu/~leili
    institution: School of Computer Science, Carnegie Mellon University
    last_name: Li
    name: Lei Li
    orcid: https://orcid.org/0000-0003-3095-9776
    semantic_scholar_id: https://www.semanticscholar.org/author/Lei-Li/143900005
    username: ~Lei_Li11
  - dblp_id: https://dblp.org/pid/57/8503
    emails: '****@gmx.de'
    first_name: Markus
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=t-VqdOUAAAAJ
    institution: Google
    last_name: Freitag
    name: Markus Freitag
    semantic_scholar_id: https://www.semanticscholar.org/author/Markus-Freitag/35307070
    username: ~Markus_Freitag2
  decision: toFindings
  end_page: 1445
  file: 427.pdf
  id: 427
  num_pages: 17
  openreview_id: 5VIvEAtfzK
  pdf_file: 8dfc76921386cb0f57a2d12bc0c50ddab1192952.pdf
  start_page: 1429
  title: 'LLMRefine: Pinpointing and Refining Large Language Models via Fine-Grained
    Actionable Feedback'
- abstract: "In noisy label learning, instance selection based on small-loss criteria\
    \ has been proven to be highly effective. However, in the case of noisy multi-label\
    \ text classification (NMLTC), the presence of noise is not limited to the instance-level\
    \ but extends to the (instance-label) pair-level.\nThis gives rise to two main\
    \ challenges.\n(1) The loss information at the pair-level fails to capture the\
    \ variations between instances. \n(2) There are two types of noise at the pair-level:\
    \ false positives and false negatives. Identifying false negatives from a large\
    \ pool of negative pairs presents an exceedingly difficult task. \nTo tackle these\
    \ issues, we propose a novel approach called instance-label pair correction (iLaCo),\
    \ which aims to address the problem of noisy pair selection and correction in\
    \ NMLTC tasks.\nSpecifically, we first introduce a holistic selection metric that\
    \ identifies noisy pairs by simultaneously considering global loss information\
    \ and instance-specific ranking information.\nSecondly, we employ a filter guided\
    \ by label correlation to focus exclusively on negative pairs with label relevance.\
    \ This filter significantly reduces the difficulty of identifying false negatives.\n\
    Experimental analysis indicates that our \\ilaco framework effectively corrects\
    \ noisy pairs in NMLTC datasets, leading to a significant improvement in model\
    \ performance."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Learning for NLP
  authors:
  - dblp_id: https://dblp.org/pid/301/7227
    emails: '****@bjtu.edu.cn'
    first_name: Pengyu
    google_scholar_id: https://scholar.google.com/citations?user= W2RlvvAAAAAJ
    last_name: Xu
    name: Pengyu Xu
    semantic_scholar_id: https://www.semanticscholar.org/author/Pengyu-Xu/2153916677
    username: ~Pengyu_Xu2
  - dblp_id: https://dblp.org/pid/217/8048
    emails: '****@tencent.com'
    first_name: Mingyang
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=3aBZza8AAAAJ
    institution: Tencent MLPD
    last_name: Song
    name: Mingyang Song
    orcid: https://orcid.org/0000-0001-9492-261X
    semantic_scholar_id: https://www.semanticscholar.org/author/Mingyang-Song/2110311849
    username: ~Mingyang_Song1
  - emails: '****@bjtu.edu.cn'
    first_name: Linkaida
    homepage: https://github.com/linkaidaLiu
    last_name: Liu
    name: Linkaida Liu
    username: ~Linkaida_Liu2
  - emails: '****@bjtu.edu.cn'
    first_name: Bing
    homepage: http://liubing.com
    last_name: Liu
    name: Bing Liu
    username: ~Bing_Liu9
  - emails: '****@outlook.com'
    first_name: Hongjian
    homepage: https://outlook.live.com/mail/0/
    last_name: Sun
    name: Hongjian Sun
    username: ~Hongjian_Sun2
  - dblp_id: https://dblp.org/pid/54/2770
    emails: '****@gmail.com'
    first_name: Liping
    google_scholar_id: https://scholar.google.com/citations?user=zStEDu4AAAAJ&hl=zh-CN
    institution: Beijing Jiaotong University
    last_name: Jing
    name: Liping Jing
    username: ~Liping_Jing3
  - dblp_id: https://dblp.org/pid/52/5812
    emails: '****@bjtu.edu.cn'
    first_name: Jian
    institution: Beijing Jiaotong University
    last_name: Yu
    name: Jian Yu
    username: ~Jian_Yu1
  decision: toFindings
  end_page: 1458
  file: 430.pdf
  id: 430
  num_pages: 13
  openreview_id: lBXuipM1LY
  pdf_file: 1308de21572a412865ceca9867bcb177376117cc.pdf
  start_page: 1446
  title: Noisy Multi-Label Text Classification via Instance-Label Pair Correction
- abstract: Large language models (LLMs) have demonstrated superior performance compared
    to previous methods on various tasks, and often serve as the foundation models
    for many researches and services. However, the untrustworthy third-party LLMs
    may covertly introduce vulnerabilities for downstream tasks. In this paper, we
    explore the vulnerability of LLMs through the lens of backdoor attacks. Different
    from existing backdoor attacks against LLMs, ours scatters multiple trigger keys
    in different prompt components. Such a Composite Backdoor Attack (CBA) is shown
    to be stealthier than implanting the same multiple trigger keys in only a single
    component. CBA ensures that the backdoor is activated only when all trigger keys
    appear. Our experiments demonstrate that CBA is effective in both natural language
    processing (NLP) and multimodal tasks. For instance, with 3\% poisoning samples
    against the LLaMA-7B model on the Emotion dataset, our attack achieves a 100\%
    Attack Success Rate (ASR) with a False Triggered Rate (FTR) below 2.06\% and negligible
    model accuracy degradation. Our work highlights the necessity of increased security
    research on the trustworthiness of foundation LLMs.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Ethics, Bias, and Fairness
  authors:
  - dblp_id: https://dblp.org/pid/51/944
    emails: '****@cispa.de'
    first_name: Hai
    google_scholar_id: https://scholar.google.com/citations?user=DECqXdAAAAAJ&hl=en
    homepage: https://miraclehh.github.io
    institution: CISPA Helmholtz Center for Information Security
    last_name: Huang
    name: Hai Huang
    username: ~Hai_Huang4
  - dblp_id: https://dblp.org/pid/58/10770-1
    emails: '****@gmail.com'
    first_name: Zhengyu
    google_scholar_id: https://scholar.google.com/citations?user=pC8KpPMAAAAJ
    homepage: https://zhengyuzhao.github.io/
    institution: Xi'an Jiaotong University
    last_name: Zhao
    name: Zhengyu Zhao
    username: ~Zhengyu_Zhao1
  - dblp_id: https://dblp.org/pid/b/MichaelBackes1
    emails: '****@cispa.de'
    first_name: Michael
    homepage: https://cispa.de/de/people/backes
    institution: CISPA Helmholtz Center for Information Security
    last_name: Backes
    name: Michael Backes
    username: ~Michael_Backes3
  - dblp_id: https://dblp.org/pid/14/4412
    emails: '****@netapp.com'
    first_name: Yun
    google_scholar_id: https://scholar.google.com/citations?user=Gx_JJ6cAAAAJ&hl=en
    institution: NetApp
    last_name: Shen
    name: Yun Shen
    username: ~Yun_Shen3
  - dblp_id: https://dblp.org/pid/06/6785-16
    emails: '****@cispa.de'
    first_name: Yang
    google_scholar_id: https://scholar.google.com/citations?user=Xeb2888AAAAJ&hl=en
    homepage: https://yangzhangalmo.github.io/
    institution: CISPA Helmholtz Center for Information Security
    last_name: Zhang
    name: Yang Zhang
    username: ~Yang_Zhang15
  decision: toFindings
  end_page: 1472
  file: 432.pdf
  id: 432
  num_pages: 14
  openreview_id: vAFozxhYZI
  pdf_file: 248fb0957e72cee38f46739f457976a62b259141.pdf
  start_page: 1459
  title: Composite Backdoor Attacks Against Large Language Models
- abstract: 'In the age of large language models (LLMs) and the widespread adoption
    of AI-driven content creation, the landscape of information dissemination has
    witnessed a paradigm shift. With the proliferation of both human-written and machine-generated
    real and fake news, robustly and effectively discerning the veracity of news articles
    has become an intricate challenge. While substantial research has been dedicated
    to fake news detection, it has either assumed that all news articles are human-written
    or has abruptly assumed that all machine-generated news was fake. Thus, a significant
    gap exists in understanding the interplay between machine-paraphrased real news,
    machine-generated fake news, human-written fake news, and human-written real news.
    In this paper, we study this gap by conducting a comprehensive evaluation of fake
    news detectors trained in various scenarios. Our primary objectives revolve around
    the following pivotal question: How can we adapt fake news detectors to the era
    of LLMs?

    Our experiments reveal an interesting pattern that detectors trained exclusively
    on human-written articles can indeed perform well at detecting machine-generated
    fake news, but not vice versa. Moreover, due to the bias of detectors against
    machine-generated texts \cite{su2023fake}, they should be trained on datasets
    with a lower machine-generated news ratio than the test set. Building on our findings,
    we provide a practical strategy for the development of robust fake news detectors.
    \footnote{The data and the code can be found at https://github.com/mbzuai-nlp/Fakenews-dataset'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Ethics, Bias, and Fairness
  authors:
  - emails: '****@gmail.com'
    first_name: Jinyan
    google_scholar_id: https://scholar.google.com/citations?hl=zh-CN&user=yRNsFuMAAAAJ&view_op=list_works&authuser=1
    homepage: https://jinyansu1.github.io/
    institution: Cornell University
    last_name: Su
    name: Jinyan Su
    username: ~Jinyan_Su1
  - dblp_id: https://dblp.org/pid/c/ClaireCardie
    emails: '****@cs.cornell.edu'
    first_name: Claire
    google_scholar_id: https://scholar.google.com/citations?user=ex9BQiIAAAAJ&hl=en
    homepage: https://www.cs.cornell.edu/home/cardie/
    institution: Cornell University
    last_name: Cardie
    name: Claire Cardie
    semantic_scholar_id: https://www.semanticscholar.org/author/Claire-Cardie/1748501
    username: ~Claire_Cardie1
  - dblp_id: https://dblp.uni-trier.de/pid/19/1947
    emails: '****@gmail.com'
    first_name: Preslav
    google_scholar_id: https://scholar.google.com/citations?user=DfXsKZ4AAAAJ
    homepage: https://www.hbku.edu.qa/en/staff/dr-preslav-nakov
    last_name: Nakov
    name: Preslav Nakov
    orcid: https://orcid.org/0000-0002-3600-1510
    semantic_scholar_id: https://www.semanticscholar.org/author/Preslav-Nakov/1683562
    username: ~Preslav_Nakov2
  decision: toFindings
  end_page: 1490
  file: 433.pdf
  id: 433
  num_pages: 18
  openreview_id: eTleSsHmSx
  pdf_file: b42b2462c54622d7b6deb865ac91bf82b0ca65f2.pdf
  start_page: 1473
  title: Adapting Fake News Detection to the Era of Large Language Models
- abstract: Due to the success of large-scale visual-language pretraining (VLP) models
    and the widespread use of image-text retrieval in industry areas, it is now critically
    necessary to reduce the model size and streamline their mobile-device deployment.
    Single- and dual-stream model structures are commonly used in image-text retrieval
    with the goal of closing the semantic gap between textual and visual modalities.
    While single-stream models use deep feature fusion to achieve more accurate cross-model
    alignment, dual-stream models are better at offline indexing and fast inference.
    We propose a Multi-teacher Cross-modality Alignment Distillation (MCAD) technique
    to integrate the advantages of single- and dual-stream models. By incorporating
    the fused single-stream features into the image and text features of the dual-stream
    model, we formulate new modified teacher similarity distributions and features.
    Then, we conduct both distribution and feature distillation to boost the capability
    of the student dual-stream model, achieving high retrieval performance without
    increasing inference complexity. Extensive experiments demonstrate the remarkable
    performance and high efficiency of MCAD on image-text retrieval tasks. Furthermore,
    we implement a lightweight CLIP model on Snapdragon/Dimensity chips with only
    $\sim$100M running memory and $\sim$8.0ms search latency, achieving the mobile-device
    application of VLP models.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multimodality and Language Grounding to Vision, Robotics and Beyond
  authors:
  - emails: '****@163.com'
    first_name: Youbo
    homepage: https://youblei.github.io/
    last_name: Lei
    name: Youbo Lei
    username: ~Youbo_Lei1
  - emails: '****@qq.com'
    first_name: Feifei
    homepage: https://github.com/EchoMaster
    institution: Researcher at OPPO Research Institute
    last_name: He
    name: Feifei He
    username: ~Feifei_He1
  - dblp_id: https://dblp.uni-trier.de/pid/65/4423-15
    emails: '****@gmail.com'
    first_name: Chen
    google_scholar_id: https://scholar.google.com/citations?user=CANDhfAAAAAJ&hl=zh-CN
    institution: OPPO Research Institute
    last_name: Chen
    name: Chen Chen
    orcid: https://orcid.org/0000-0003-3498-2527
    username: ~Chen_Chen24
  - emails: '****@uni.sydney.edu.au'
    first_name: Yingbin
    homepage: https://github.com/Marco692
    last_name: Mo
    name: Yingbin Mo
    username: ~Yingbin_Mo1
  - emails: '****@oppo.com'
    first_name: Sijia
    homepage: https://github.com/lsjiiia
    institution: OPPO Research Institute
    last_name: Li
    name: Sijia Li
    username: ~Sijia_Li5
  - emails: '****@oppo.com'
    first_name: Defeng
    homepage: https://github.com/DefengXie/
    last_name: Xie
    name: Defeng Xie
    username: ~Defeng_Xie2
  - dblp_id: https://dblp.org/pid/129/0998
    emails: '****@oppo.com'
    first_name: Haonan
    google_scholar_id: https://scholar.google.com/citations?user=EPBgKu0AAAAJ&hl=en
    institution: OPPO Guangdong Mobile Telecommunications Co., Ltd.
    last_name: Lu
    name: Haonan Lu
    orcid: https://orcid.org/0000-0001-6332-2785
    semantic_scholar_id: https://www.semanticscholar.org/author/H.-Lu/2130373
    username: ~Haonan_Lu1
  decision: toFindings
  end_page: 1503
  file: 438.pdf
  id: 438
  num_pages: 13
  openreview_id: i3JaiOyra5
  pdf_file: 66b7205bf31207c025467bad4e9f3271e94a07c2.pdf
  start_page: 1491
  title: 'MCAD: Multi-teacher Cross-modal Alignment Distillation for efficient image-text
    retrieval'
- abstract: "Ranking documents using Large Language Models (LLMs) by directly feeding\
    \ the query and candidate documents into the prompt is an interesting and practical\
    \ problem. However, \nresearchers have found it difficult to outperform fine-tuned\
    \ baseline rankers on benchmark datasets.\nWe analyze pointwise and listwise ranking\
    \ prompts used by existing methods and argue that off-the-shelf LLMs do not fully\
    \ understand these challenging ranking formulations. In this paper, we propose\
    \ to significantly reduce the burden on LLMs by using a new technique called Pairwise\
    \ Ranking Prompting (PRP).\nOur results are the first in the literature to achieve\
    \ state-of-the-art ranking performance on standard benchmarks using moderate-sized\
    \ open-sourced LLMs. On TREC-DL 2019\\&2020, PRP based on the Flan-UL2 model with\
    \ 20B parameters performs favorably with the previous best approach in the literature,\
    \ which is based on the blackbox commercial GPT-4 that has 50x (estimated) model\
    \ size, while outperforming other LLM-based solutions, such as InstructGPT which\
    \ has 175B parameters, by over 10\\% for all ranking metrics. By using the same\
    \ prompt template on seven BEIR tasks, PRP outperforms supervised baselines and\
    \ outperforms the blackbox commercial ChatGPT solution by 4.2\\% and pointwise\
    \ LLM-based solutions by more than 10\\% on average NDCG@10.\nFurthermore, we\
    \ propose several variants of PRP to improve efficiency and show that it is possible\
    \ to achieve competitive results even with linear complexity."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Retrieval and Text Mining
  authors:
  - dblp_id: https://dblp.org/pid/06/864-1
    emails: '****@gmail.com'
    first_name: Zhen
    google_scholar_id: https://scholar.google.com/citations?user=Kv1yk3YAAAAJ&hl=en
    homepage: http://alumni.cs.ucr.edu/~zqin001/
    institution: Google
    last_name: Qin
    name: Zhen Qin
    username: ~Zhen_Qin5
  - dblp_id: https://dblp.org/pid/144/7357
    emails: '****@google.com'
    first_name: Rolf
    google_scholar_id: https://scholar.google.com/citations?user=4yjQ964AAAAJ
    homepage: https://www.jagerman.nl/
    institution: Google
    last_name: Jagerman
    name: Rolf Jagerman
    orcid: https://orcid.org/0000-0002-5169-495X
    username: ~Rolf_Jagerman2
  - dblp_id: https://dblp.org/pid/37/10077
    emails: '****@gmail.com'
    first_name: Kai
    google_scholar_id: https://scholar.google.com/citations?user=VorTj3AAAAAJ&hl=en
    homepage: https://khui.github.io/
    institution: Google
    last_name: Hui
    name: Kai Hui
    orcid: https://orcid.org/0000-0002-3110-7404
    semantic_scholar_id: https://www.semanticscholar.org/author/Kai-Hui/47214884
    username: ~Kai_Hui1
  - dblp_id: https://dblp.org/pid/10/9988
    emails: '****@google.com'
    first_name: Honglei
    google_scholar_id: https://scholar.google.com/citations?user=FxEDj4wAAAAJ
    homepage: https://hongleizhuang.github.io/
    institution: Google Research
    last_name: Zhuang
    name: Honglei Zhuang
    orcid: https://orcid.org/0000-0001-8134-1509
    semantic_scholar_id: https://www.semanticscholar.org/author/Honglei-Zhuang/39371343
    username: ~Honglei_Zhuang1
  - dblp_id: https://dblp.org/pid/140/6764
    emails: '****@google.com'
    first_name: Junru
    google_scholar_id: https://scholar.google.com/citations?user=nBbGvyEAAAAJ&hl=en
    homepage: http://sandbox3aster.github.io/
    institution: Google Research
    last_name: Wu
    name: Junru Wu
    username: ~Junru_Wu2
  - emails: '****@google.com'
    first_name: Le
    google_scholar_id: https://scholar.google.com/citations?user=X_knTr4AAAAJ&hl=en&authuser=1
    institution: Google
    last_name: Yan
    name: Le Yan
    username: ~Le_Yan1
  - dblp_id: https://dblp.org/pid/178/3627
    emails: '****@gmail.com'
    first_name: Jiaming
    google_scholar_id: https://scholar.google.com/citations?user=-ZJ0sCoAAAAJ&hl=en
    homepage: https://mickeysjm.github.io
    institution: Google Research
    last_name: Shen
    name: Jiaming Shen
    semantic_scholar_id: https://www.semanticscholar.org/author/Jiaming-Shen/3363642
    username: ~Jiaming_Shen1
  - dblp_id: https://dblp.org/pid/134/5653
    emails: '****@google.com'
    first_name: Tianqi
    google_scholar_id: https://scholar.google.com/citations?user=pUKhiMIAAAAJ&hl=en
    institution: Google
    last_name: Liu
    name: Tianqi Liu
    username: ~Tianqi_Liu1
  - dblp_id: https://dblp.org/pid/14/8399
    emails: '****@gmail.com'
    first_name: Jialu
    google_scholar_id: https://scholar.google.com/citations?user=BUERw4QAAAAJ&hl=en
    homepage: https://jialu.info/
    institution: Google Research
    last_name: Liu
    name: Jialu Liu
    username: ~Jialu_Liu1
  - dblp_id: https://dblp.org/pid/95/2272
    emails: '****@google.com'
    first_name: Donald
    google_scholar_id: https://scholar.google.com/citations?user=bmXpOd8AAAAJ&hl=en
    homepage: https://research.google/people/DonaldMetzler/
    institution: Google
    last_name: Metzler
    name: Donald Metzler
    orcid: https://orcid.org/0000-0003-4276-6269
    semantic_scholar_id: https://www.semanticscholar.org/author/Donald-Metzler/1680617
    username: ~Donald_Metzler1
  - dblp_id: https://dblp.org/pid/67/2661
    emails: '****@google.com'
    first_name: Xuanhui
    institution: Google
    last_name: Wang
    name: Xuanhui Wang
    semantic_scholar_id: https://www.semanticscholar.org/author/Xuanhui-Wang/1526973500
    username: ~Xuanhui_Wang1
  - dblp_id: https://dblp.org/pid/80/4305
    emails: '****@google.com'
    first_name: Michael
    google_scholar_id: https://scholar.google.com/citations?user=C9mxM5IAAAAJ&hl=en
    homepage: http://bendersky.github.io/
    institution: Google
    last_name: Bendersky
    name: Michael Bendersky
    orcid: https://orcid.org/0000-0002-2941-6240
    username: ~Michael_Bendersky1
  decision: toFindings
  end_page: 1518
  file: 439.pdf
  id: 439
  num_pages: 15
  openreview_id: dtOszTJMB9
  pdf_file: 4e577eae5302a96a1cdf9ed3921df108180083fe.pdf
  start_page: 1504
  title: Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting
- abstract: Federated Multilingual Modeling (FMM) plays a crucial role in the applications
    of natural language processing due to the increasing diversity of languages and
    the growing demand for data privacy. However, FMM faces limitations stemming from
    (1) the substantial communication costs in networking and (2) the conflicts arising
    from parameter interference between different languages. To address these challenges,
    we introduce a communication-efficient federated learning framework with low-rank
    adaptation and language family clustering for Multilingual Modeling (MM). In this
    framework, we maintain the weights of the base model, exclusively updating the
    lightweight Low-rank adaptation (LoRA) parameters to minimize communication costs.
    Additionally, we mitigate parameter conflicts by grouping languages based on their
    language family affiliations, as opposed to aggregating all LoRA parameters. Experiments
    demonstrate that our proposed model not only surpasses the baseline models in
    performance but also reduces the communication overhead. Our code is available
    at https://github.com/zhihan-guo/FedLFC.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - emails: '****@cse.cuhk.edu.hk'
    first_name: Zhihan
    homepage: https://zhihan-guo.github.io/
    last_name: Guo
    name: Zhihan Guo
    orcid: https://orcid.org/0000-0002-1889-583X
    username: ~Zhihan_Guo2
  - dblp_id: https://dblp.org/pid/55/5266
    emails: '****@cse.cuhk.edu.hk'
    first_name: Yifei
    google_scholar_id: https://scholar.google.com/citations?user=DmwXESQAAAAJ&hl=en
    homepage: https://yifeiacc.github.io/
    institution: Department of Computer Science and Engineering, The Chinese University
      of Hong Kong
    last_name: Zhang
    name: Yifei Zhang
    username: ~Yifei_Zhang6
  - dblp_id: https://dblp.org/pid/16/1234
    emails: '****@gmail.com'
    first_name: Zhuo
    institution: Harbin Institute of Technology
    last_name: Zhang
    name: Zhuo Zhang
    orcid: https://orcid.org/0000-0002-1835-5411
    semantic_scholar_id: https://www.semanticscholar.org/author/Zhuo-Zhang/2197470934
    username: ~Zhuo_Zhang5
  - dblp_id: https://dblp.org/pid/68/1538
    emails: '****@gmail.com'
    first_name: Zenglin
    google_scholar_id: https://scholar.google.com/citations?user=gF0H9nEAAAAJ&hl=zh-CN
    homepage: http://faculty.hit.edu.cn/xuzenglin
    institution: Harbin Institute of Technology, Shenzhen
    last_name: Xu
    name: Zenglin Xu
    orcid: https://orcid.org/0000-0001-5550-6461
    semantic_scholar_id: https://www.semanticscholar.org/author/Zenglin-Xu/1683510
    username: ~Zenglin_Xu2
  - dblp_id: https://dblp.uni-trier.de/pers/hd/k/King:Irwin
    emails: '****@cse.cuhk.edu.hk'
    first_name: Irwin
    google_scholar_id: https://scholar.google.com/citations?user=MXvC7tkAAAAJ&hl=en
    homepage: https://www.cse.cuhk.edu.hk/irwin.king/
    institution: The Chinese University of Hong Kong
    last_name: King
    name: Irwin King
    orcid: https://orcid.org/0000-0001-8106-6447
    username: ~Irwin_King1
  decision: toFindings
  end_page: 1528
  file: 441.pdf
  id: 441
  num_pages: 10
  openreview_id: hHmQUXQdwx
  pdf_file: 1857c6993f6c9fcb611d90c96375dc523482de29.pdf
  start_page: 1519
  title: 'FedLFC: Towards Efficient Federated Multilingual Modeling with LoRA-based
    Language Family Clustering'
- abstract: In multi-objective text generation, we aim to optimize over multiple weighted
    aspects (e.g., toxicity, semantic preservation, fluency) of the generated text.
    However, multi-objective weighting schemes may change dynamically in practice
    according to deployment requirements, evolving business needs, personalization
    requirements on edge devices, or the availability of new language models and/or
    objective requirements. Ideally, we need an efficient method to adapt to the dynamic
    requirements of the overall objective. To address these requirements, we propose
    a linear combination of objective-specific language models to \textbf{efficiently}
    adapt the decoding process and optimize for the desired objective \textbf{without}
    the significant computational overhead of retraining one or more language models.  We
    show empirically that we can leverage Gaussian Process black box optimization
    to adapt the language model decoder weights to outperform other fixed weighting
    schemes and standard baselines of the task in only a few iterations of decoding.
    Overall this approach enables highly efficient adaptation of controllable language
    models via multi-objective weighting schemes that may evolve dynamically in practical
    deployment situations.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Learning for NLP
  authors:
  - emails: '****@mail.utoronto.ca'
    first_name: Mohammad Mahdi
    google_scholar_id: https://scholar.google.com/citations?user=P15R_U0AAAAJ&hl=en
    homepage: https://mahdiabdollahpour.github.io/
    last_name: Abdollah Pour
    name: Mohammad Mahdi Abdollah Pour
    username: ~Mohammad_Mahdi_Abdollah_Pour1
  - dblp_id: https://dblp.org/pid/139/4919.html
    emails: '****@gmail.com'
    first_name: Ali
    google_scholar_id: https://scholar.google.com/citations?user=Df_-YLgAAAAJ
    homepage: https://alipsgh.github.io/
    institution: LG Electronics
    last_name: Pesaranghader
    name: Ali Pesaranghader
    orcid: https://orcid.org/0000-0002-5971-6180
    semantic_scholar_id: https://www.semanticscholar.org/author/Ali-Pesaranghader/2699756
    username: ~Ali_Pesaranghader1
  - dblp_id: https://dblp.org/pid/195/8189
    emails: '****@mie.utoronto.ca'
    first_name: Eldan
    google_scholar_id: https://scholar.google.com/citations?user=FUWruCQAAAAJ
    homepage: https://eldanc.github.io/
    institution: University of Toronto
    last_name: Cohen
    name: Eldan Cohen
    semantic_scholar_id: https://www.semanticscholar.org/author/Eldan-Cohen/47761679
    username: ~Eldan_Cohen1
  - dblp_id: https://dblp.org/pid/88/3374
    emails: '****@gmail.com'
    first_name: Scott
    google_scholar_id: https://scholar.google.ca/citations?user=kB8UPNIAAAAJ&hl=en
    homepage: http://d3m.mie.utoronto.ca/
    institution: Department of Mechanical and Industrial Engineering, University of
      Toronto and Department of Computer Science
    last_name: Sanner
    name: Scott Sanner
    username: ~Scott_Sanner1
  decision: toFindings
  end_page: 1536
  file: 444.pdf
  id: 444
  num_pages: 8
  openreview_id: WWp1uPhzTW
  pdf_file: 9974a18d6f4ec0f30a82c81e55eaf35461b64410.pdf
  start_page: 1529
  title: Gaussian Process Optimization for Adaptable Multi-Objective Text Generation
    using Linearly-Weighted Language Models
- abstract: 'We present an empirical study of groundedness in long-form question answering
    (LFQA) by retrieval-augmented  large language models (LLMs).

    In particular, we evaluate whether every generated sentence is grounded in the
    retrieved documents or the model''s pre-training data.

    Across 3 datasets and 4 model families, our findings reveal that a significant
    fraction of generated sentences are consistently ungrounded, even when those sentences
    contain correct ground-truth answers.

    Additionally, we examine the impacts of factors such as model size, decoding strategy,
    and instruction tuning on groundedness. Our results show that while larger models
    tend to ground their outputs more effectively, a significant portion of correct
    answers remains compromised by hallucinations. This study provides novel insights
    into the groundedness challenges in LFQA and underscores the necessity for more
    robust mechanisms in LLMs to mitigate the generation of ungrounded content.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - dblp_id: https://dblp.org/pid/329/3838
    emails: '****@inf.ethz.ch'
    first_name: Alessandro
    google_scholar_id: https://scholar.google.com/citations?user=Fx50TZQAAAAJ
    homepage: https://alestolfo.github.io
    institution: ETHZ - ETH Zurich
    last_name: Stolfo
    name: Alessandro Stolfo
    semantic_scholar_id: https://www.semanticscholar.org/author/Alessandro-Stolfo/2175480389
    username: ~Alessandro_Stolfo1
  decision: toFindings
  end_page: 1552
  file: 449.pdf
  id: 449
  num_pages: 16
  openreview_id: yMQ1hZQIxP
  pdf_file: 06cab338ccc5450cfd608347b6e82651ffd80aff.pdf
  start_page: 1537
  title: 'Groundedness in Retrieval-augmented Long-form Generation: An Empirical Study'
- abstract: "Pre-trained language models (PLMs) play a crucial role in various applications,\
    \ including sensitive domains such as the hiring process. However, extensive research\
    \ has unveiled that these models tend to replicate social biases present in their\
    \ pre-training data, raising ethical concerns.  \nIn this study, we propose the\
    \ TagDebias method, which proposes debiasing a dataset using type tags. It then\
    \ proceeds to fine-tune PLMs on this debiased dataset. Experiments show that our\
    \ proposed TagDebias model, when applied to a ranking task, exhibits significant\
    \ improvements in bias scores."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Ethics, Bias, and Fairness
  authors:
  - emails: '****@gmail.com'
    first_name: Mehrnaz
    last_name: Moslemi
    name: Mehrnaz Moslemi
    username: ~Mehrnaz_Moslemi1
  - dblp_id: https://dblp.org/pid/92/4064
    emails: '****@polymtl.ca'
    first_name: Amal
    google_scholar_id: https://scholar.google.ca/citations?user=lqDGv9YAAAAJ&hl=en
    homepage: http://www.labowest.ca/
    institution: Polytechnique Montreal
    last_name: Zouaq
    name: Amal Zouaq
    semantic_scholar_id: https://www.semanticscholar.org/author/A.-Zouaq/145698212
    username: ~Amal_Zouaq1
  decision: toFindings
  end_page: 1567
  file: 451.pdf
  id: 451
  num_pages: 15
  openreview_id: 6JlUXzYfg0
  pdf_file: 9233e978ff0639f4aa96b87d1ee9c13225e6fb6c.pdf
  start_page: 1553
  title: 'TagDebias: Entity and Concept Tagging for Social Bias Mitigation in Pretrained
    Language Models'
- abstract: Keyphrase Generation (KPG) is the task of automatically generating appropriate
    keyphrases for a given text, with a wide range of real-world applications such
    as document indexing and tagging, information retrieval, and text summarization.
    NLP research makes a distinction between present and absent keyphrases based on
    whether a keyphrase is directly present as a sequence of words in the document
    during evaluation. However, present and absent keyphrases are treated together
    in a text-to-text generation framework during training. We treat present keyphrase
    extraction as a sequence labeling problem and propose a new absent keyphrase generation
    model that uses a modified cross-attention layer with additional heads to capture
    diverse views for the same context encoding in this paper. Our experiments show
    improvements over the state-of-the-art for four datasets for present keyphrase
    extraction and five datasets for absent keyphrase generation among the six English
    datasets we explored, covering long and short documents.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Generation
  authors:
  - emails: '****@gmail.com'
    first_name: Edwin
    google_scholar_id: https://scholar.google.ca/citations?hl=en&user=lBk_AUIAAAAJ
    last_name: Thomas
    name: Edwin Thomas
    username: ~Edwin_Thomas2
  - dblp_id: https://dblp.org/pid/127/0212
    emails: '****@gmail.com'
    first_name: Sowmya
    google_scholar_id: https://scholar.google.com/citations?user=e4UbD1UAAAAJ&hl=en
    institution: National Research Council Canada
    last_name: Vajjala
    name: Sowmya Vajjala
    orcid: https://orcid.org/0000-0002-4033-9936
    semantic_scholar_id: https://www.semanticscholar.org/author/Sowmya-Vajjala/2070714
    username: ~Sowmya_Vajjala2
  decision: toFindings
  end_page: 1584
  file: 453.pdf
  id: 453
  num_pages: 17
  openreview_id: upuSJmprIy
  pdf_file: 664de7fcd02af4d50a33ac64214e0ea64eb23584.pdf
  start_page: 1568
  title: Improving Absent Keyphrase Generation with Diversity Heads
- abstract: 'Many pretrained multilingual models exhibit cross-lingual transfer ability,
    which is often attributed to a learned language-neutral representation during
    pretraining. However, it remains unclear what factors contribute to the learning
    of a language-neutral representation, and whether the learned language-neutral
    representation suffices to facilitate cross-lingual transfer. We propose a synthetic
    task, Multilingual Othello (mOthello), as a testbed to delve into these two questions.
    We find that: (1) models trained with naive multilingual pretraining fail to learn
    a language-neutral representation across all input languages; (2) the introduction
    of "anchor tokens" (i.e., lexical items that are identical across languages) helps
    cross-lingual representation alignment; and (3) the learning of a language-neutral
    representation alone is not sufficient to facilitate cross-lingual transfer. Based
    on our findings, we propose a novel approach -- multilingual pretraining with
    unified output space -- that both induces the learning of language-neutral representation
    and facilitates cross-lingual transfer.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multilinguality and Language Diversity
  authors:
  - emails: '****@brown.edu'
    first_name: Tianze
    homepage: https://ethahtz.github.io/
    institution: Brown University
    last_name: Hua
    name: Tianze Hua
    username: ~Tianze_Hua1
  - dblp_id: https://dblp.org/pid/33/303
    emails: '****@brown.edu'
    first_name: Tian
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=2L4W0cQAAAAJ
    homepage: https://tttyuntian.github.io/
    institution: Brown University
    last_name: Yun
    name: Tian Yun
    orcid: https://orcid.org/0000-0003-1671-5484
    semantic_scholar_id: https://www.semanticscholar.org/author/Tian-Yun/2127600348
    username: ~Tian_Yun2
  - dblp_id: https://dblp.org/pid/141/4059
    emails: '****@brown.edu'
    first_name: Ellie
    google_scholar_id: https://scholar.google.com/citations?user=sFyrSa8AAAAJ&hl=en
    homepage: http://cs.brown.edu/people/epavlick/
    institution: Brown University and Brown University
    last_name: Pavlick
    name: Ellie Pavlick
    username: ~Ellie_Pavlick1
  decision: toFindings
  end_page: 1598
  file: 456.pdf
  id: 456
  num_pages: 14
  openreview_id: aVGFY20meh
  pdf_file: 97b47563f949bb21f776709d74e89529691dbd60.pdf
  start_page: 1585
  title: 'mOthello: When Do Cross-Lingual Representation Alignment and Cross-Lingual
    Transfer Emerge in Multilingual Models?'
- abstract: As the field of Natural Language Processing (NLP) increasingly adopts
    transformer-based models, the issue of bias becomes more pronounced. Such bias,
    manifesting through stereotypes and discriminatory practices, can disadvantage
    certain groups. Our study focuses on direct and indirect bias in the model explanations,
    where the model makes predictions relying heavily on identity tokens or associated
    contexts. We present a novel analysis of bias in model explanation, especially
    the subtle indirect bias, underlining the limitations of traditional fairness
    metrics. We first define direct and indirect bias in model explanations, which
    is complementary to fairness in predictions. We then develop an indirect bias
    discovery algorithm for quantitatively evaluating indirect bias in transformer
    models using their in-built self-attention matrix. We also propose an indirect
    bias mitigation algorithm to ensure fairness in transformer models by leveraging
    attention explanations. Our evaluation shows the significance of indirect bias
    and the effectiveness of our indirect bias discovery and mitigation.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Ethics, Bias, and Fairness
  authors:
  - emails: '****@uncc.edu'
    first_name: Farsheed
    google_scholar_id: https://scholar.google.com/citations?user=Shp2ZpUAAAAJ&hl=en
    homepage: https://webpages.charlotte.edu/fhaque/
    institution: University of North Carolina at Charlotte
    last_name: Haque
    name: Farsheed Haque
    username: ~Farsheed_Haque1
  - emails: '****@uncc.edu'
    first_name: Depeng
    homepage: https://webpages.charlotte.edu/dxu7/
    institution: University of North Carolina at Charlotte
    last_name: Xu
    name: Depeng Xu
    username: ~Depeng_Xu2
  - dblp_id: https://dblp.org/pers/y/Yuan:Shuhan
    emails: '****@usu.edu'
    first_name: Shuhan
    google_scholar_id: https://scholar.google.com/citations?user=14J_wuIAAAAJ&hl=en
    institution: Utah State University
    last_name: Yuan
    name: Shuhan Yuan
    username: ~Shuhan_Yuan2
  decision: toFindings
  end_page: 1614
  file: 462.pdf
  id: 462
  num_pages: 16
  openreview_id: RWe7cIKH0a
  pdf_file: 6622a9b0b1cf5748970e19a92652eb466befed95.pdf
  start_page: 1599
  title: Discovering and Mitigating Indirect Bias in Attention-Based Model Explanations
- abstract: The convergence of text, visual, and audio data is crucial towards human-like
    artificial intelligence, however the current Vision-Language-Speech landscape
    is dominated by encoder-only models that lack generative abilities. We propose
    closing this gap with i-Code V2, one of the first models capable of generating
    natural language from any combination of Vision, Language, and Speech data. i-Code
    V2 leverages state-of-the-art single-modality encoders, combining their outputs
    with a new modality-fusing encoder to project combinations of modalities into
    a shared representational space. Language tokens are generated from these representations
    via an autoregressive decoder. i-Code V2 is pretrained end-to-end on a large collection
    of dual- and single-modality datasets with a novel text completion objective that
    can be generalized across arbitrary combinations of modalities. i-Code V2 matches
    or outperforms state-of-the-art single- and dual-modality baselines on 7 multimodal
    tasks, demonstrating the power of generative multimodal pretraining across a diversity
    of tasks and signals.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multimodality and Language Grounding to Vision, Robotics and Beyond
  authors:
  - dblp_id: https://dblp.org/pid/217/8277
    emails: '****@gmail.com'
    first_name: Ziyi
    google_scholar_id: https://scholar.google.com/citations?user=JkyLIM0AAAAJ&hl=en
    institution: Microsoft
    last_name: Yang
    name: Ziyi Yang
    semantic_scholar_id: https://www.semanticscholar.org/author/121937604
    username: ~Ziyi_Yang1
  - emails: '****@microsoft.com'
    first_name: Mahmoud
    google_scholar_id: https://scholar.google.com/citations?user=x7Ddt3oAAAAJ
    homepage: https://www.microsoft.com/en-us/research/people/mkhademi/
    institution: Microsoft and University of British Columbia
    last_name: Khademi
    name: MAHMOUD KHADEMI
    username: ~MAHMOUD_KHADEMI2
  - dblp_id: https://dblp.org/pid/154/6421
    emails: '****@gmail.com'
    first_name: Yichong
    google_scholar_id: https://scholar.google.com/citations?user=sYza2XwAAAAJ&hl=en
    homepage: http://xycking.wixsite.com/yichongxu
    institution: Microsoft
    last_name: Xu
    name: Yichong Xu
    username: ~Yichong_Xu1
  - dblp_id: https://dblp.org/pid/205/3986
    emails: '****@stanford.edu'
    first_name: Reid
    google_scholar_id: https://scholar.google.com/citations?user=FkufKDgAAAAJ&hl=en&oi=ao
    institution: Research, Microsoft
    last_name: Pryzant
    name: Reid Pryzant
    username: ~Reid_Pryzant1
  - dblp_id: https://dblp.org/pid/227/2871.html
    emails: '****@gmail.com'
    first_name: Yuwei
    google_scholar_id: https://scholar.google.com/citations?user=Om_-hHsAAAAJ&hl=en
    homepage: https://yuwfan.github.io/
    institution: Snap Inc.
    last_name: Fang
    name: Yuwei Fang
    username: ~Yuwei_Fang1
  - dblp_id: https://dblp.org/pid/48/7536
    emails: '****@gmail.com'
    first_name: Chenguang
    google_scholar_id: https://scholar.google.com/citations?user=1b2kKWoAAAAJ&hl=en
    institution: Zoom
    last_name: Zhu
    name: Chenguang Zhu
    username: ~Chenguang_Zhu1
  - dblp_id: https://dblp.org/pid/92/1489-1
    emails: '****@gmail.com'
    first_name: Dongdong
    google_scholar_id: https://scholar.google.com.sg/citations?user=sYKpKqEAAAAJ&hl=zh-CN
    homepage: http://www.dongdongchen.bid/
    institution: Microsoft Research
    last_name: Chen
    name: Dongdong Chen
    semantic_scholar_id: https://www.semanticscholar.org/author/Dongdong-Chen/49025801
    username: ~Dongdong_Chen1
  - dblp_id: https://dblp.org/pid/67/2224
    emails: '****@microsoft.com'
    first_name: Yao
    google_scholar_id: https://scholar.google.com/citations?user=o7OfErXuEJIC&hl=en
    homepage: https://www.microsoft.com/en-us/research/people/yaoqian/
    last_name: Qian
    name: Yao Qian
    username: ~Yao_Qian2
  - emails: '****@microsoft.com'
    first_name: Xuemei
    institution: Microsoft
    last_name: Gao
    name: Xuemei Gao
    username: ~Xuemei_Gao1
  - dblp_id: https://dblp.org/pid/72/3762-4
    emails: '****@microsoft.com'
    first_name: Yi-Ling
    google_scholar_id: https://scholar.google.com/citations?user=jI5oDhYAAAAJ&hl=zh-TW&authuser=2
    institution: Microsoft
    last_name: Chen
    name: Yi-Ling Chen
    username: ~Yi-Ling_Chen3
  - emails: '****@gmyr.net'
    first_name: Robert
    google_scholar_id: https://scholar.google.com/citations?user=Ds27ENUAAAAJ&hl=en
    institution: Paderborn University and Microsoft
    last_name: Gmyr
    name: Robert Gmyr
    username: ~Robert_Gmyr1
  - emails: '****@microsoft.com'
    first_name: Naoyuki
    google_scholar_id: https://scholar.google.com/citations?user=j7q6qbAAAAAJ
    homepage: https://www.microsoft.com/en-us/research/people/nakanda/
    institution: Microsoft
    last_name: Kanda
    name: Naoyuki Kanda
    username: ~Naoyuki_Kanda1
  - emails: '****@gmail.com'
    first_name: Noel
    google_scholar_id: https://scholar.google.com/citations?user=8BnjC-4AAAAJ&hl=en&oi=ao
    homepage: http://www.noelcodella.com/
    institution: Microsoft
    last_name: Codella
    middle_name: C
    name: Noel C Codella
    username: ~Noel_C_Codella1
  - dblp_id: https://dblp.org/pid/43/5134-1
    emails: '****@gmail.com'
    first_name: Bin
    google_scholar_id: https://scholar.google.com/citations?authuser=1&user=t5HZdzoAAAAJ
    institution: Microsoft
    last_name: Xiao
    name: Bin Xiao
    orcid: https://orcid.org/0000-0001-6477-5911
    username: ~Bin_Xiao2
  - emails: '****@microsoft.com'
    first_name: Yu
    homepage: https://www.microsoft.com/en-us/research/people/yushi
    institution: Microsoft
    last_name: Shi
    name: Yu Shi
    username: ~Yu_Shi3
  - dblp_id: https://dblp.org/pid/66/6740
    emails: '****@microsoft.com'
    first_name: Lu
    google_scholar_id: https://scholar.google.com/citations?user=k9TsUVsAAAAJ&hl=en
    homepage: https://www.microsoft.com/en-us/research/people/luyuan/
    institution: Microsoft
    last_name: Yuan
    name: Lu Yuan
    username: ~Lu_Yuan1
  - emails: '****@gmail.com'
    first_name: Takuya
    google_scholar_id: https://scholar.google.com/citations?user=xKMJDZQAAAAJ&hl=en
    institution: Microsoft
    last_name: Yoshioka
    name: Takuya Yoshioka
    username: ~Takuya_Yoshioka1
  - dblp_id: https://dblp.org/pid/232/1866-1.html
    emails: '****@microsoft.com'
    first_name: Michael
    homepage: https://www.microsoft.com/en-us/research/people/nzeng/
    institution: Microsoft
    last_name: Zeng
    name: Michael Zeng
    semantic_scholar_id: https://www.semanticscholar.org/author/Michael-Zeng/48262024
    username: ~Michael_Zeng1
  - dblp_id: https://dblp.org/pid/41/4753
    emails: '****@live.com'
    first_name: Xuedong
    last_name: Huang
    name: Xuedong Huang
    username: ~Xuedong_Huang1
  decision: toFindings
  end_page: 1627
  file: 465.pdf
  id: 465
  num_pages: 13
  openreview_id: XoibnXxFJ9
  pdf_file: a767932e559804cebc913842240bd68342dfa48e.pdf
  start_page: 1615
  title: 'i-Code V2: An Autoregressive Generation Framework over Vision, Language,
    and Speech Data'
- abstract: 'Knowledge-to-text generators often struggle to faithfully generate descriptions
    for the input facts: they may produce hallucinations that contradict the input,
    or describe facts not present in the input. To reduce hallucinations, we propose
    a decoding-only method, TWEAK (Think While Effectively Articulating Knowledge),
    which can be integrated with any generator without retraining. TWEAK treats the
    generated sequences at each decoding step and its future sequences as hypotheses,
    and ranks each generation candidate based on the extent to which their hypotheses
    are supported by the input facts using a Hypothesis Verification Model (HVM).
    We first demonstrate the effectiveness of TWEAK by using a Natural Language Inference
    (NLI) model as the HVM and report improved faithfulness with a minimal impact
    on the quality. We then replace the NLI model with a task-specific HVM trained
    with a first-of-a-kind dataset, FATE (Fact-Aligned Textual Entailment), which
    pairs input facts with their original and perturbed descriptions. We test TWEAK
    with two generators, and the best TWEAK variants improve on average for the two
    models by 2.24/7.17 points in faithfulness (FactKB) in in/out-of-distribution
    evaluations, respectively, and with only a 0.14/0.32-point decline in quality
    (BERTScore).'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Generation
  authors:
  - emails: '****@sms.ed.ac.uk'
    first_name: Yifu
    google_scholar_id: https://scholar.google.com/citations?user=OA6GaMwAAAAJ&hl=en
    homepage: https://yfqiu.netlify.app/
    last_name: Qiu
    name: Yifu QIU
    semantic_scholar_id: https://www.semanticscholar.org/author/Yifu-Qiu/2159539050
    username: ~Yifu_QIU1
  - dblp_id: https://dblp.org/pid/97/10826
    emails: '****@gmail.com'
    first_name: Varun
    homepage: https://users.soe.ucsc.edu/~varunembar
    institution: University of California, Santa Cruz
    last_name: Embar
    middle_name: R.
    name: Varun R. Embar
    username: ~Varun_R._Embar1
  - dblp_id: https://dblp.org/pid/04/5629
    emails: '****@inf.ed.ac.uk'
    first_name: Shay
    homepage: http://homepages.inf.ed.ac.uk/scohen
    institution: University of Edinburgh
    last_name: Cohen
    middle_name: B
    name: Shay B Cohen
    semantic_scholar_id: https://www.semanticscholar.org/author/Shay-B.-Cohen/40146204
    username: ~Shay_B_Cohen6
  - dblp_id: https://dblp.org/pid/23/5290
    emails: '****@gmail.com'
    first_name: Benjamin
    homepage: https://www.linkedin.com/in/benjaminhan/
    institution: Apple
    last_name: Han
    name: Benjamin Han
    username: ~Benjamin_Han1
  decision: toFindings
  end_page: 1644
  file: 466.pdf
  id: 466
  num_pages: 17
  openreview_id: yXNxCWOwND
  pdf_file: 8942940306eb15c820d32ca52d2ce4bce69a221d.pdf
  start_page: 1628
  title: 'Think While You Write: Hypothesis Verification Promotes Faithful Knowledge-to-Text
    Generation'
- abstract: "Large language models (LLMs) have shown promising ability to generate\
    \ synthetic query-document pairs by prompting with as few as 8 demonstrations.\
    \ This has enabled building better IR models, especially for tasks with no training\
    \ data. Typically, such synthetic query generation (QGen) approaches condition\
    \ on an input context (e.g. a text document) and generate a query relevant to\
    \ that context, or condition the QGen additionally on the relevance label (e.g.\
    \ relevant vs irrelevant) to generate queries across relevance buckets. However,\
    \ we find that such QGen approaches are sub-optimal as they require the model\
    \ to reason about the desired label and the input from  a handful of examples.\
    \ In this work, we propose to reduce this burden of LLMs by generating queries\
    \ simultaneously for different labels.  \nWe hypothesize that instead of asking\
    \ the model to generate, say, an irrelevant query given an input context, asking\
    \ the model to generate an irrelevant query relative to a relevant query is a\
    \ much simpler task. Extensive experimentation across nine IR datasets shows that\
    \ synthetic queries generated in such a fashion translates to better downstream\
    \ performance."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Retrieval and Text Mining
  authors:
  - dblp_id: https://dblp.org/pid/225/7684
    emails: '****@google.com'
    first_name: Aditi
    google_scholar_id: https://scholar.google.com/citations?user=iNuUxiwAAAAJ&hl=en
    institution: Google
    last_name: Chaudhary
    name: Aditi Chaudhary
    semantic_scholar_id: https://www.semanticscholar.org/author/Aditi-Chaudhary/51250894
    username: ~Aditi_Chaudhary1
  - dblp_id: https://dblp.org/pid/01/7071-1
    emails: '****@google.com'
    first_name: Karthik
    google_scholar_id: https://scholar.google.com/citations?user=x1zTxLoAAAAJ
    institution: Google
    last_name: Raman
    name: Karthik Raman
    username: ~Karthik_Raman1
  - dblp_id: https://dblp.org/pid/80/4305
    emails: '****@google.com'
    first_name: Michael
    google_scholar_id: https://scholar.google.com/citations?user=C9mxM5IAAAAJ&hl=en
    homepage: http://bendersky.github.io/
    institution: Google
    last_name: Bendersky
    name: Michael Bendersky
    orcid: https://orcid.org/0000-0002-2941-6240
    username: ~Michael_Bendersky1
  decision: toFindings
  end_page: 1664
  file: 467.pdf
  id: 467
  num_pages: 20
  openreview_id: 9QBDkKTsWo
  pdf_file: 40d5a895d2795a2e238ec11207a0e62355d3c40a.pdf
  start_page: 1645
  title: It's All Relative! -- A Synthetic Query Generation Approach for Improving
    Zero-Shot Relevance Prediction
- abstract: Reinforcement learning from human feedback (RLHF) has been extensively
    employed to align large language models with user intent. However, proximal policy
    optimization (PPO) based RLHF is occasionally unstable requiring significant hyperparameter
    finetuning, and computationally expensive to maximize the estimated reward during
    alignment. Recently, direct preference optimization (DPO) is proposed to address
    those challenges. However, DPO often relies on contrastive responses generated
    from human annotator and alternative LLM, instead of the policy model, limiting
    the effectiveness of the RLHF. In this paper, we addresses both challenges by
    systematically combining rejection sampling (RS) and DPO. Our proposed method,
    RS-DPO, initiates with the development of a supervised fine-tuned policy model
    (SFT). A varied set of k responses per prompt are sampled directly from the SFT
    model. RS-DPO identifies pairs of contrastive samples based on their reward distribution.
    Finally, we apply DPO with the contrastive samples to align the model to human
    preference. Our experiments indicate that our proposed method effectively fine-tunes
    LLMs with limited resource environments, leading to improved alignment with user
    intent. Furthermore, it outperforms existing methods, including RS, PPO, and DPO.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Efficient/Low-Resource Methods for NLP
  authors:
  - emails: '****@amazon.com'
    first_name: Saeed
    institution: Amazon
    last_name: Khaki
    name: Saeed Khaki
    username: ~Saeed_Khaki2
  - emails: '****@amazon.com'
    first_name: JinJin
    institution: Amazon
    last_name: Li
    name: JinJin Li
    username: ~JinJin_Li2
  - emails: '****@amazon.com'
    first_name: Lan
    homepage: https://www.linkedin.com/in/lan-ma-18b52746
    institution: Amazon
    last_name: Ma
    name: Lan Ma
    username: ~Lan_Ma4
  - emails: '****@amazon.com'
    first_name: Liu
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=HKbIOqAAAAAJ&view_op=list_works
    last_name: Yang
    name: Liu Yang
    username: ~Liu_Yang16
  - emails: '****@amazon.com'
    first_name: Prathap
    homepage: https://www.amazon.science/author/prathap-ramachandra
    institution: Amazon
    last_name: Ramachandra
    name: Prathap Ramachandra
    username: ~Prathap_Ramachandra1
  decision: toFindings
  end_page: 1680
  file: 468.pdf
  id: 468
  num_pages: 16
  openreview_id: CR8Qt2kT6K
  pdf_file: cc6ec4727ad2b0f39662fac209229c22d0541cc7.pdf
  start_page: 1665
  title: 'RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization Method
    for Alignment of Large Language Models'
- abstract: Domain adaptation from labeled source domains to the target domain is
    important in practical summarization scenarios. However, the key challenge is
    domain knowledge disentanglement. In this work, we explore how to disentangle
    domain-invariant knowledge from source domains while learning specific knowledge
    of the target domain. Specifically, we propose a hypernetwork-assisted encoder-decoder
    architecture with parameter-efficient fine-tuning. It leverages a hypernetwork
    instruction learning module to generate domain-specific parameters from the encoded
    inputs accompanied by task-related instruction. Further, to better disentangle
    and transfer knowledge from source domains to the target domain, we introduce
    a meta-knowledge distillation strategy to build a meta-teacher model that captures
    domain-invariant knowledge across multiple domains and use it to transfer knowledge
    to students. Experiments on three dialogue summarization datasets show the effectiveness
    of the proposed model. Human evaluations also show the superiority of our model
    with regard to the summary generation quality.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Summarization
  authors:
  - emails: '****@stu.ecnu.edu.cn'
    first_name: Changqun
    google_scholar_id: https://scholar.google.com.hk/citations?hl=zh-CN&user=2_7PXIIAAAAJ&view_op
    homepage: https://github.com/lcqlalala/
    last_name: Li
    name: Changqun Li
    orcid: https://orcid.org/0009-0006-0259-5169
    semantic_scholar_id: https://www.semanticscholar.org/author/Changqun-Li/5059790
    username: ~Changqun_Li1
  - dblp_id: https://dblp.org/pid/94/6774
    emails: '****@cs.ecnu.edu.cn'
    first_name: Linlin
    last_name: Wang
    name: Linlin Wang
    username: ~Linlin_Wang1
  - emails: '****@cs.ecnu.edu.cn'
    first_name: Xin
    homepage: https://faculty.ecnu.edu.cn/_s16/lx2_6212/main.psp
    last_name: Lin
    middle_name: Alex
    name: Xin Alex Lin
    username: ~Xin_Alex_Lin1
  - emails: '****@ica.stc.sh.cn'
    first_name: Shizhou
    homepage: https://github.com/JinFish
    last_name: Huang
    name: Shizhou Huang
    username: ~Shizhou_Huang2
  - dblp_id: https://dblp.org/pid/42/963-1.html
    emails: '****@cs.ecnu.edu.cn'
    first_name: Liang
    google_scholar_id: https://scholar.google.com/citations?view_op=list_works&hl=en&user=aW_cCQwAAAAJ
    last_name: He
    name: Liang He
    username: ~Liang_He2
  decision: toFindings
  end_page: 1695
  file: 472.pdf
  id: 472
  num_pages: 15
  openreview_id: WXhvpvN1Lh
  pdf_file: 661b0c8c61bec8ff6c9155307235778d5732c96d.pdf
  start_page: 1681
  title: Hypernetwork-Assisted Parameter-Efficient Fine-Tuning with Meta-Knowledge
    Distillation for Domain Knowledge Disentanglement
- abstract: Large Language Models (LLMs) are powerful tools which have been both dominant
    and commonplace in the field of Artificial Intelligence. Yet, LLMs have a tendency
    to devolve into toxic degeneration, wherein otherwise safe and unproblematic models
    begin generating toxic content. For the sake of social responsibility and inspired
    by the biological mechanisms of inhibition control, we introduce the paradigm
    of Education for Societal Norms (ESN). By collecting and labeling examples as
    acceptable and unacceptable (in this case toxic and non-toxic), and including
    a corresponding acceptable rewrite with every unacceptable example, we introduce
    a new mechanism for LLM detoxification.  We annotate a dataset of 2,850 entries
    and use it to fine-tune a model, which we call a Model with Inhibition Control
    (MICo). Evaluating this model on toxicity detection capability, rewrite detoxification,
    meaning preservation, and overall toxicity reduction, we discover significant
    improvements over the baseline model. In our experiments we show that overall
    toxicity of this model is more than 60\% reduced, with over 75\% reduction in
    severe toxicity.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Computational Social Science and Cultural Analytics
  authors:
  - emails: '****@jh.edu'
    first_name: Roy
    homepage: https://engineering.jhu.edu/ams/people/graduate-students/
    institution: Whiting School of Engineering
    last_name: Siegelmann
    name: Roy Siegelmann
    username: ~Roy_Siegelmann1
  - dblp_id: https://dblp.org/pid/230/8151
    emails: '****@amazon.com'
    first_name: Ninareh
    google_scholar_id: https://scholar.google.com/citations?user=1R3XgHQAAAAJ&hl=en
    homepage: https://scf.usc.edu/~ninarehm/
    institution: Amazon
    last_name: Mehrabi
    name: Ninareh Mehrabi
    username: ~Ninareh_Mehrabi2
  - dblp_id: https://dblp.org/pid/183/3699
    emails: '****@amazon.com'
    first_name: Palash
    google_scholar_id: https://scholar.google.com/citations?user=kNeah3kAAAAJ&hl=en&oi=ao
    institution: Amazon
    last_name: Goyal
    name: Palash Goyal
    username: ~Palash_Goyal1
  - dblp_id: https://dblp.org/pid/137/3282
    emails: '****@amazon.com'
    first_name: Prasoon
    homepage: https://www.cs.utexas.edu/~pgoyal/
    institution: Amazon
    last_name: Goyal
    name: Prasoon Goyal
    username: ~Prasoon_Goyal2
  - dblp_id: https://dblp.org/pid/176/9576
    emails: '****@amazon.com'
    first_name: Lisa
    institution: Amazon
    last_name: Bauer
    name: Lisa Bauer
    semantic_scholar_id: https://www.semanticscholar.org/author/Lisa-Bauer/12620674
    username: ~Lisa_Bauer1
  - dblp_id: https://dblp.org/pid/187/5905
    emails: '****@gmail.com'
    first_name: Jwala
    google_scholar_id: https://scholar.google.com/citations?user=1bUxjvoAAAAJ&hl=en
    homepage: https://jwaladhamala.com/
    institution: Amazon Alexa AI
    last_name: Dhamala
    name: Jwala Dhamala
    orcid: https://orcid.org/0000-0002-5396-9187
    semantic_scholar_id: https://www.semanticscholar.org/author/J.-Dhamala/3475586
    username: ~Jwala_Dhamala1
  - dblp_id: https://dblp.org/pid/16/3411
    emails: '****@isi.edu'
    first_name: Aram
    google_scholar_id: https://scholar.google.com/citations?user=rJTwW0MAAAAJ&hl=en
    homepage: http://www.isi.edu/~galstyan
    institution: Information Sciences Institute, University of Southern California
      and Amazon Alexa
    last_name: Galstyan
    name: Aram Galstyan
    username: ~Aram_Galstyan1
  - emails: '****@amazon.com'
    first_name: Rahul
    google_scholar_id: https://scholar.google.com/citations?user=1CFrm2YAAAAJ&hl=en
    last_name: Gupta
    name: Rahul Gupta
    username: ~Rahul_Gupta3
  - dblp_id: https://dblp.org/pid/50/5680
    emails: '****@ieee.org'
    first_name: Reza
    google_scholar_id: https://scholar.google.com/citations?user=00ncu3cAAAAJ&hl=en
    institution: Amazon
    last_name: Ghanadan
    name: Reza Ghanadan
    username: ~Reza_Ghanadan1
  decision: toFindings
  end_page: 1703
  file: 474.pdf
  id: 474
  num_pages: 8
  openreview_id: FCOvhj2AMG
  pdf_file: 35bb9629112523b78d3c9335fc93b4ee8fd51c45.pdf
  start_page: 1696
  title: 'MICo: Preventative Detoxification of Large Language Models through Inhibition
    Control'
- abstract: "To meet the requirements of real-world applications, it is essential\
    \ to control generations of large language models (LLMs). \nPrior research has\
    \ tried to introduce reinforcement learning (RL) into controllable text generation\
    \ while most existing methods suffer from overfitting issues (finetuning-based\
    \ methods) or semantic collapse (post-processing methods). However, current RL\
    \ methods are generally guided by coarse-grained (sentence/paragraph-level) feedback,\
    \ which may lead to suboptimal performance owing to semantic twists or progressions\
    \ within sentences. To tackle that, we propose a novel reinforcement learning\
    \ algorithm named TOLE which formulates TOken-LEvel rewards for controllable text\
    \ generation, and employs a \"first-quantize-then-noise\" paradigm to enhance\
    \ the robustness of the RL algorithm. Furthermore, TOLE can be flexibly extended\
    \ to multiple constraints with little computational expense. Experimental results\
    \ show that our algorithm can achieve superior performance on both single-attribute\
    \ and multi-attribute control tasks. We have released our codes at https://github.com/WindyLee0822/CTG."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Generation
  authors:
  - emails: '****@hust.edu.cn'
    first_name: Wendi
    google_scholar_id: https://scholar.google.com/citations?user=hK19TbcAAAAJ
    homepage: http://www.none.cn
    last_name: Li
    name: Wendi Li
    username: ~Wendi_Li1
  - dblp_id: https://dblp.org/pid/24/4105-2
    emails: '****@hust.edu.cn'
    first_name: Wei
    google_scholar_id: https://scholar.google.com.sg/citations?hl=en&user=_JaPEsAAAAAJ&view_op=list_works
    homepage: https://www.eric-weiwei.com
    institution: Huazhong University of Science and Technology
    last_name: Wei
    name: Wei Wei
    orcid: https://orcid.org/0000-0003-4488-0102
    semantic_scholar_id: https://www.semanticscholar.org/author/Wei-Wei/47748186
    username: ~Wei_Wei14
  - emails: '****@gmail.com'
    first_name: Kaihe
    google_scholar_id: https://scholar.google.com/citations?user=G-Bv7GYAAAAJ&hl=en
    last_name: xu
    name: Kaihe xu
    username: ~Kaihe_xu1
  - emails: '****@163.com'
    first_name: Wenfeng
    homepage: http://Adimission.pku.edu.cn
    last_name: Xie
    name: Wenfeng xie
    username: ~Wenfeng_xie1
  - dblp_id: https://dblp.org/pid/327/3353
    emails: '****@163.com'
    first_name: Dangyang
    institution: Pingan Technology
    last_name: Chen
    name: Dangyang Chen
    username: ~Dangyang_Chen1
  - dblp_id: https://dblp.org/pid/96/3060-1.html
    emails: '****@gmail.com'
    first_name: Yu
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=ORPxbV4AAAAJ
    homepage: https://ych133.github.io
    institution: Microsoft Research
    last_name: Cheng
    name: Yu Cheng
    semantic_scholar_id: https://www.semanticscholar.org/author/Yu-Cheng/145215470
    username: ~Yu_Cheng1
  decision: toFindings
  end_page: 1719
  file: 475.pdf
  id: 475
  num_pages: 16
  openreview_id: SBOom8fLDj
  pdf_file: 4d8aacba31e346ef31280bb40296970a468e642b.pdf
  start_page: 1704
  title: Reinforcement Learning with Token-level Feedback for Controllable Text Generation
- abstract: Large Language Models (LLMs) have shown great ability in solving traditional
    natural language tasks and elementary reasoning tasks with appropriate prompting
    techniques. However, their ability is still limited in solving complicated science
    problems. In this work, we aim to push the upper bound of the reasoning capability
    of LLMs by proposing a collaborative multi-agent, multi-reasoning-path (CoMM)
    prompting framework. Specifically, we prompt LLMs to play different roles in a
    problem-solving team, and encourage different role-play agents to collaboratively
    solve the target task. In particular, we discover that applying different reasoning
    paths for different roles is an effective strategy to implement few-shot prompting
    approaches in the multi-agent scenarios. Empirical results demonstrate the effectiveness
    of the proposed methods on two college-level science problems over competitive
    baselines. Our further analysis shows the necessity of prompting LLMs to play
    different roles or experts independently.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - dblp_id: https://dblp.org/pid/98/4148
    emails: '****@tamu.edu'
    first_name: Pei
    google_scholar_id: https://scholar.google.com/citations?user=9sOFHvcAAAAJ&hl=en
    homepage: https://brickee.github.io/
    institution: Texas A&M University - College Station
    last_name: Chen
    name: Pei Chen
    semantic_scholar_id: https://www.semanticscholar.org/author/Pei-Chen/2901524
    username: ~Pei_Chen2
  - dblp_id: https://dblp.org/pid/71/208-7
    emails: '****@outlook.com'
    first_name: Shuai
    google_scholar_id: https://scholar.google.com.au/citations?user=PPjdxlcAAAAJ&hl=en
    homepage: http://shuaizhang.tech/
    institution: Amazon
    last_name: Zhang
    name: Shuai Zhang
    orcid: https://orcid.org/0000-0002-7866-4611
    semantic_scholar_id: https://www.semanticscholar.org/author/Shuai-Zhang/1583098764
    username: ~Shuai_Zhang7
  - dblp_id: https://dblp.org/pid/81/10356
    emails: '****@amazon.com'
    first_name: Boran
    google_scholar_id: https://scholar.google.com/citations?user=Prwxh24AAAAJ
    last_name: Han
    name: Boran Han
    orcid: https://orcid.org/0000-0001-9567-7062
    username: ~Boran_Han1
  decision: toFindings
  end_page: 1738
  file: 476.pdf
  id: 476
  num_pages: 19
  openreview_id: CxJSQWgeK7
  pdf_file: 0f7d2ba0b3bb4f82232ded4e9ff0911c5dfc1318.pdf
  start_page: 1720
  title: 'CoMM: Collaborative Multi-Agent, Multi-Reasoning-Path Prompting for Complex
    Problem Solving'
- abstract: 'Gender-inclusive NLP research has documented the harmful limitations
    of gender binary-centric large language models (LLM), such as the inability to
    correctly use gender-diverse English neopronouns (e.g., xe, zir, fae). While data
    scarcity is a known culprit, the precise mechanisms through which scarcity affects
    this behavior remain underexplored. We discover LLM misgendering is significantly
    influenced by Byte-Pair Encoding (BPE) tokenization, the tokenizer powering many
    popular LLMs. Unlike binary pronouns, BPE overfragments neopronouns, a direct
    consequence of data scarcity during tokenizer training. This disparate tokenization
    mirrors tokenizer limitations observed in multilingual and low-resource NLP, unlocking
    new misgendering mitigation strategies. We propose two techniques: (1) pronoun
    tokenization parity, a method to enforce consistent tokenization across gendered
    pronouns, and (2) utilizing pre-existing LLM pronoun knowledge to improve neopronoun
    proficiency. Our proposed methods outperform finetuning with standard BPE, improving
    neopronoun accuracy from 14.1% to 58.4%. Our paper is the first to link LLM misgendering
    to tokenization and deficient neopronoun grammar, indicating that LLMs unable
    to correctly treat neopronouns as pronouns are more prone to misgender.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Ethics, Bias, and Fairness
  authors:
  - emails: '****@gmail.com'
    first_name: Anaelia
    homepage: https://anaeliaovalle.github.io/
    institution: University of California, Los Angeles
    last_name: Ovalle
    name: Anaelia Ovalle
    orcid: https://orcid.org/0000-0002-0531-7520
    username: ~Anaelia_Ovalle1
  - dblp_id: https://dblp.org/pid/230/8151
    emails: '****@amazon.com'
    first_name: Ninareh
    google_scholar_id: https://scholar.google.com/citations?user=1R3XgHQAAAAJ&hl=en
    homepage: https://scf.usc.edu/~ninarehm/
    institution: Amazon
    last_name: Mehrabi
    name: Ninareh Mehrabi
    username: ~Ninareh_Mehrabi2
  - dblp_id: https://dblp.org/pid/183/3699
    emails: '****@amazon.com'
    first_name: Palash
    google_scholar_id: https://scholar.google.com/citations?user=kNeah3kAAAAJ&hl=en&oi=ao
    institution: Amazon
    last_name: Goyal
    name: Palash Goyal
    username: ~Palash_Goyal1
  - dblp_id: https://dblp.org/pid/187/5905
    emails: '****@gmail.com'
    first_name: Jwala
    google_scholar_id: https://scholar.google.com/citations?user=1bUxjvoAAAAJ&hl=en
    homepage: https://jwaladhamala.com/
    institution: Amazon Alexa AI
    last_name: Dhamala
    name: Jwala Dhamala
    orcid: https://orcid.org/0000-0002-5396-9187
    semantic_scholar_id: https://www.semanticscholar.org/author/J.-Dhamala/3475586
    username: ~Jwala_Dhamala1
  - dblp_id: https://dblp.org/pid/18/2428
    emails: '****@kwchang.net'
    first_name: Kai-Wei
    google_scholar_id: https://scholar.google.com/citations?user=fqDBtzYAAAAJ&hl=en
    homepage: http://kwchang.net
    institution: University of California, Los Angeles
    last_name: Chang
    name: Kai-Wei Chang
    semantic_scholar_id: https://www.semanticscholar.org/author/Kai-Wei-Chang/2782886
    username: ~Kai-Wei_Chang1
  - dblp_id: https://dblp.org/pid/16/6366
    emails: '****@cs.toronto.edu'
    first_name: Richard
    google_scholar_id: https://scholar.google.ca/citations?user=iBeDoRAAAAAJ&hl=en
    homepage: http://www.cs.columbia.edu/~zemel
    institution: Department of Computer Science, Columbia University and Department
      of Computer Science, University of Toronto
    last_name: Zemel
    name: Richard Zemel
    username: ~Richard_Zemel1
  - dblp_id: https://dblp.org/pid/16/3411
    emails: '****@isi.edu'
    first_name: Aram
    google_scholar_id: https://scholar.google.com/citations?user=rJTwW0MAAAAJ&hl=en
    homepage: http://www.isi.edu/~galstyan
    institution: Information Sciences Institute, University of Southern California
      and Amazon Alexa
    last_name: Galstyan
    name: Aram Galstyan
    username: ~Aram_Galstyan1
  - dblp_id: https://dblp.org/pid/153/5384
    emails: '****@gmail.com'
    first_name: Yuval
    google_scholar_id: https://scholar.google.com/citations?user=aYAcXccAAAAJ&hl=en
    homepage: http://www.yuvalpinter.com
    institution: Ben-Gurion University of the Negev
    last_name: Pinter
    name: Yuval Pinter
    orcid: https://orcid.org/0000-0003-3174-1621
    semantic_scholar_id: https://www.semanticscholar.org/author/Yuval-Pinter/1826312
    username: ~Yuval_Pinter1
  - emails: '****@amazon.com'
    first_name: Rahul
    google_scholar_id: https://scholar.google.com/citations?user=1CFrm2YAAAAJ&hl=en
    last_name: Gupta
    name: Rahul Gupta
    username: ~Rahul_Gupta3
  decision: toFindings
  end_page: 1756
  file: 477.pdf
  id: 477
  num_pages: 18
  openreview_id: 9dM8hCNDHH
  pdf_file: 65ebc0c2072b165dc68f7172a05794534e9ba401.pdf
  start_page: 1739
  title: 'Tokenization Matters: Navigating Data-Scarce Tokenization for Gender Inclusive
    Language Technologies'
- abstract: "The Euclidean space is the familiar space for training neural models\
    \ and performing arithmetic operations.\nHowever, many data types inherently possess\
    \ complex geometries, and model training methods involve operating over their\
    \ latent representations, which cannot be effectively captured in the Euclidean\
    \ space.\nThe hyperbolic space provides a more generalized representative geometry\
    \ to model the hierarchical complexities of the tree-like structure of natural\
    \ language.\nWe propose AdaPT a set of guidelines for initialization, parametrization,\
    \ and training of neural networks, which adapts to the dataset and can be used\
    \ with different manifolds. \nAdaPT can be generalized over any existing neural\
    \ network training methodology and leads to more stable training without a substantial\
    \ increase in training time.\nWe apply AdaPT guidelines over two state-of-the-art\
    \ deep learning approaches and empirically demonstrate its effectiveness through\
    \ experiments on three tasks over 12 languages across speech and text.\nThrough\
    \ extensive qualitative analysis, we put forward the applicability of AdaPT as\
    \ a set of guidelines optimally utilizing the manifold geometry, which can be\
    \ extended to various downstream tasks across languages and modalities."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multilinguality and Language Diversity
  authors:
  - dblp_id: https://dblp.org/pid/222/5147
    emails: '****@gmail.com'
    first_name: Ramit
    homepage: https://sites.google.com/iiitd.ac.in/ramitsawhney/
    institution: Georgia Institute of Technology
    last_name: Sawhney
    name: Ramit Sawhney
    semantic_scholar_id: https://www.semanticscholar.org/author/Ramit-Sawhney/51042088
    username: ~Ramit_Sawhney1
  - emails: '****@utexas.edu'
    first_name: Shrey
    google_scholar_id: https://scholar.google.com/citations?user=2ef4p28AAAAJ&hl=en
    homepage: https://sites.google.com/view/shrey-pandit/home
    last_name: Pandit
    name: Shrey Pandit
    semantic_scholar_id: https://www.semanticscholar.org/author/Shrey-Pandit/1824294087
    username: ~Shrey_Pandit1
  - emails: '****@andrew.cmu.edu'
    first_name: Vishwa
    homepage: https://sites.google.com/view/vishwavshah/
    last_name: Shah
    name: Vishwa Shah
    username: ~Vishwa_Shah1
  - dblp_id: https://dblp.org/pid/92/6840
    emails: '****@gmail.com'
    first_name: Megh
    homepage: http://megh-thakkar.github.io
    last_name: Thakkar
    name: Megh Thakkar
    username: ~Megh_Thakkar1
  - dblp_id: https://dblp.org/pid/62/2078
    emails: '****@salesforce.com'
    first_name: Shafiq
    google_scholar_id: https://scholar.google.com/citations?user=hR249csAAAAJ&hl=en
    homepage: https://raihanjoty.github.io/
    institution: SalesForce.com and Nanyang Technological University
    last_name: Joty
    name: Shafiq Joty
    username: ~Shafiq_Joty1
  decision: toFindings
  end_page: 1771
  file: 478.pdf
  id: 478
  num_pages: 15
  openreview_id: MsWENtfKq4
  pdf_file: 7b8ffbcfb3267c08c3d0bfb2d0a59495afb87a35.pdf
  start_page: 1757
  title: 'AdaPT: A Set of Guidelines for Hyperbolic Multimodal Multilingual NLP'
- abstract: While most existing works on LLM prompting techniques focus only on how
    to select a better set of data samples inside one single prompt input (In-Context
    Learning or ICL), why can not we design and leverage multiple prompts together
    to further improve the LLM's performance? In this work, we propose In-Context
    Sampling (ICS), a low-resource LLM prompting technique to produce confident predictions
    by optimizing the construction of multiple ICL prompt inputs. Extensive experiments
    with three open-source LLMs (FlanT5-XL,  Mistral-7B, and Mixtral-8x7B) on four
    NLI datasets (e-SNLI, Multi-NLI, ANLI, and Contract-NLI) and one QA dataset (CommonsenseQA)
    illustrate that ICS can consistently enhance LLMs'  performance. An in-depth evaluation
    with three data similarity-based ICS strategies suggests that these strategies
    can further elevate LLM's performance, which sheds light on a new yet promising
    future research direction.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Efficient/Low-Resource Methods for NLP
  authors:
  - dblp_id: https://dblp.org/pid/256/9562
    emails: '****@gmail.com'
    first_name: Bingsheng
    google_scholar_id: https://scholar.google.com/citations?user=hJlsDfAAAAAJ&hl=en&oi=ao
    institution: Northeastern University
    last_name: Yao
    name: Bingsheng Yao
    orcid: https://orcid.org/0009-0004-8329-4610
    semantic_scholar_id: https://www.semanticscholar.org/author/Bingsheng-Yao/1490485182
    username: ~Bingsheng_Yao1
  - emails: '****@link.cuhk.edu.cn'
    first_name: Guiming
    google_scholar_id: https://scholar.google.com/citations?user=Gype-NsAAAAJ&hl=en
    last_name: Chen
    middle_name: Hardy
    name: Guiming Hardy Chen
    username: ~Guiming_Hardy_Chen1
  - emails: '****@outlook.com'
    first_name: Ruishi
    google_scholar_id: https://scholar.google.com/citations?user=RPQrY5AAAAAJ
    homepage: https://motion115.github.io/
    last_name: Zou
    name: Ruishi Zou
    orcid: https://orcid.org/0009-0001-3798-6833
    username: ~Ruishi_Zou1
  - dblp_id: https://dblp.org/pid/245/9896-3
    emails: '****@gmail.com'
    first_name: Yuxuan
    google_scholar_id: https://scholar.google.com/citations?user=t_KJvIYAAAAJ&hl=zh-CN
    homepage: https://yuxuan.lu
    last_name: Lu
    name: Yuxuan Lu
    orcid: https://orcid.org/0000-0002-8520-0540
    semantic_scholar_id: https://www.semanticscholar.org/author/Yuxuan-Lu/2155710822
    username: ~Yuxuan_Lu2
  - emails: '****@northeastern.edu'
    first_name: Jiachen
    homepage: http://www.jiachen-li.com
    last_name: Li
    name: Jiachen Li
    username: ~Jiachen_Li11
  - dblp_id: https://dblp.org/pid/57/1330
    emails: '****@sjtu.edu.cn'
    first_name: Shao
    google_scholar_id: https://scholar.google.com/citations?user=UG36L2YAAAAJ&hl=en
    homepage: https://shaozhang.info
    institution: Shanghai Jiao Tong University
    last_name: Zhang
    name: Shao Zhang
    orcid: https://orcid.org/0000-0002-0111-0776
    semantic_scholar_id: https://www.semanticscholar.org/author/Shao-Zhang/2116577679
    username: ~Shao_Zhang1
  - emails: '****@northeastern.edu'
    first_name: Yisi
    google_scholar_id: https://scholar.google.com/citations?user=OPAGntEAAAAJ&hl=en
    institution: Apple
    last_name: Sang
    name: Yisi Sang
    username: ~Yisi_Sang1
  - dblp_id: https://dblp.org/pid/128/6972-1
    emails: '****@msu.edu'
    first_name: Sijia
    google_scholar_id: https://scholar.google.com/citations?user=C7dO_UgAAAAJ
    homepage: https://lsjxjtu.github.io/
    institution: Michigan State University
    last_name: Liu
    name: Sijia Liu
    semantic_scholar_id: https://www.semanticscholar.org/author/Sijia-Liu/143743061
    username: ~Sijia_Liu1
  - dblp_id: https://dblp.org/pid/h/JamesAHendler
    emails: '****@cs.rpi.edu'
    first_name: James
    google_scholar_id: https://scholar.google.com.tw/citations?user=JNPbTdIAAAAJ
    homepage: http://www.cs.rpi.edu/~hendler/
    institution: Rensselaer Polytechnic Institute
    last_name: Hendler
    name: James Hendler
    username: ~James_Hendler1
  - dblp_id: https://dblp.org/pid/161/3389
    emails: '****@northeastern.edu'
    first_name: Dakuo
    google_scholar_id: https://scholar.google.com/citations?user=Uno8dugAAAAJ&hl=en
    homepage: https://www.dakuowang.com
    institution: Northeastern University
    last_name: Wang
    name: Dakuo Wang
    orcid: https://orcid.org/0000-0001-9371-9441
    username: ~Dakuo_Wang1
  decision: toFindings
  end_page: 1790
  file: 479.pdf
  id: 479
  num_pages: 19
  openreview_id: rZc03D8RgG
  pdf_file: 7316de7a31808880e9ee914a8ed3a7f9982efdb2.pdf
  start_page: 1772
  title: More Samples or More Prompts? Exploring Effective Few-Shot In-Context Learning
    for LLMs with In-Context Sampling
- abstract: Automated synthesis of zeolite, one of the most important catalysts in
    chemical industries, holds great significance for attaining economic and environmental
    benefits. Structural synthesis data extracted through NLP technologies from zeolite
    experimental procedures can significantly expedite automated synthesis owing to
    its machine readability. However, the utilization of NLP technologies in information
    extraction of zeolite synthesis remains restricted due to the lack of annotated
    datasets. In this paper, we formulate an event extraction task to mine structural
    synthesis actions from experimental narratives for modular automated synthesis.
    Furthermore, we introduce ZSEE, a novel dataset containing fine-grained event
    annotations of zeolite synthesis actions. Our dataset features 16 event types
    and 13 argument roles which cover all the experimental operational steps of zeolite
    synthesis. We explore current state-of-the-art event extraction methods on ZSEE,
    perform error analysis based on the experimental results, and summarize the challenges
    and corresponding research directions to further facilitate the automated synthesis
    of zeolites. The code is publicly available at https://github.com/Hi-0317/ZSEE.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Extraction
  authors:
  - emails: '****@mail.ecust.edu.cn'
    first_name: Song
    last_name: He
    name: Song He
    orcid: https://orcid.org/0000-0003-2411-2317
    username: ~Song_He1
  - dblp_id: https://dblp.org/pid/14/6370-3
    emails: '****@ecust.edu.cn'
    first_name: Xin
    google_scholar_id: https://scholar.google.ca/citations?user=FczuYdgAAAAJ&hl=en
    institution: East China University of Science and Technology
    last_name: Peng
    name: Xin Peng
    orcid: https://orcid.org/0000-0001-9277-8415
    username: ~Xin_Peng5
  - emails: '****@163.com'
    first_name: Yihan
    homepage: https://github.com/YihanMie
    institution: Shanghai Research Institute of Petrochemical Technology, Sinopec
      Corporation
    last_name: Cai
    name: Yihan Cai
    orcid: https://orcid.org/0000-0001-5426-259X
    username: ~Yihan_Cai1
  - emails: '****@sinopec.com'
    first_name: Xin
    last_name: Li
    name: Xin Li
    semantic_scholar_id: https://www.semanticscholar.org/author/Xin-Li/50079670
    username: ~Xin_Li61
  - emails: '****@sinopec.com'
    first_name: Zhiqing
    last_name: Yuan
    name: Zhiqing Yuan
    orcid: https://orcid.org/0000-0002-9789-7082
    username: ~Zhiqing_Yuan1
  - dblp_id: https://dblp.org/pid/82/3485
    emails: '****@qq.com'
    first_name: WenLi
    google_scholar_id: https://scholar.google.com/citations?hl=zh-CN&user=3o-0DvYAAAAJ
    institution: East China University of Science and Technology
    last_name: Du
    name: WenLi Du
    username: ~WenLi_Du2
  - emails: '****@sinopec.com'
    first_name: Weimin
    homepage: https://www.cae.cn/cae/html/main/colys/81679859.html
    institution: East China University of Science and Technology
    last_name: Yang
    name: Weimin Yang
    username: ~Weimin_Yang1
  decision: toFindings
  end_page: 1808
  file: 487.pdf
  id: 487
  num_pages: 18
  openreview_id: 4edjmFcnqY
  pdf_file: 5f73182179698e4adf424eb43a47ab59d85a779c.pdf
  start_page: 1791
  title: 'ZSEE: A Dataset based on Zeolite Synthesis Event Extraction for Automated
    Synthesis Platform'
- abstract: A primary challenge in abstractive summarization is hallucination---the
    phenomenon where a model generates plausible text that is absent in the source
    text. We hypothesize that the domain (or topic) of the source text triggers the
    model to generate text that is highly probable in the domain, neglecting the details
    of the source text. To alleviate this model bias, we introduce a decoding strategy
    based on domain-conditional pointwise mutual information.  This strategy adjusts
    the generation probability of each token by comparing it with the token's marginal
    probability within the domain of the source text. According to evaluation on the
    XSUM dataset, our method demonstrates improvement in terms of faithfulness and
    source relevance.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Summarization
  authors:
  - emails: '****@snu.ac.kr'
    first_name: Kyubyung
    last_name: Chae
    name: Kyubyung Chae
    username: ~Kyubyung_Chae1
  - emails: '****@gmail.com'
    first_name: Jaepill
    homepage: https://github.com/cjaep
    last_name: Choi
    name: Jaepill choi
    username: ~Jaepill_choi1
  - dblp_id: https://dblp.org/pid/40/8877
    emails: '****@gmail.com'
    first_name: Yohan
    google_scholar_id: https://scholar.google.com/citations?user=xp3LGRQAAAAJ&hl=en
    homepage: https://yohanjo.github.io/
    institution: Seoul National University
    last_name: Jo
    name: Yohan Jo
    semantic_scholar_id: https://www.semanticscholar.org/author/Yohan-Jo/39947629
    username: ~Yohan_Jo1
  - emails: '****@snu.ac.kr'
    first_name: Taesup
    google_scholar_id: https://scholar.google.com/citations?user=7V7yNeoAAAAJ&hl=en
    institution: Seoul National University
    last_name: Kim
    name: Taesup Kim
    username: ~Taesup_Kim1
  decision: toFindings
  end_page: 1820
  file: 488.pdf
  id: 488
  num_pages: 12
  openreview_id: N5gW9kxJ7Z
  pdf_file: c106c739648e354e49f71228018cbc78fa215980.pdf
  start_page: 1809
  title: Mitigating Hallucination in Abstractive Summarization with Domain-Conditional
    Mutual Information
- abstract: Recent advancements in open-domain dialogue systems have been propelled
    by the emergence of high-quality large language models (LLMs) and various effective
    training methodologies. Nevertheless, the presence of toxicity within these models
    presents a significant challenge that can potentially diminish the user experience.
    In this study, we introduce an innovative training algorithm, an improvement upon
    direct preference optimization (DPO), called adversarial DPO (ADPO). The ADPO
    algorithm is designed to train models to assign higher probability distributions
    to preferred responses and lower distributions to unsafe responses, which are
    self-generated using the toxic control token. We demonstrate that ADPO enhances
    the model's resilience against harmful conversations while minimizing performance
    degradation. Furthermore, we illustrate that ADPO offers a more stable training
    procedure compared to the traditional DPO. To the best of our knowledge, this
    is the first adaptation of the DPO algorithm that directly incorporates harmful
    data into the generative model, thereby reducing the need to artificially create
    safe dialogue data.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Dialogue and Interactive Systems
  authors:
  - emails: '****@postech.ac.kr'
    first_name: San
    institution: POSTECH
    last_name: Kim
    name: San Kim
    username: ~San_Kim3
  - dblp_id: https://dblp.org/pid/l/GGLee
    emails: '****@postech.ac.kr'
    first_name: Gary
    google_scholar_id: https://scholar.google.com.tw/citations?user=t30saScAAAAJ
    homepage: http://nlp.postech.ac.kr/~gblee/
    last_name: Lee
    name: Gary Lee
    username: ~Gary_Lee1
  decision: toFindings
  end_page: 1835
  file: 489.pdf
  id: 489
  num_pages: 15
  openreview_id: RaAMIH9l96
  pdf_file: c5648ffbfba1e042a0691cc9d1b6ce97cbda02a3.pdf
  start_page: 1821
  title: 'Adversarial DPO: Harnessing Harmful Data for Reducing Toxicity with Minimal
    Impact on Coherence and Evasiveness in Dialogue Agents'
- abstract: Prompt engineering is an essential technique for enhancing the abilities
    of large language models (LLMs) by providing explicit and specific instructions.
    It enables LLMs to excel in various tasks, such as arithmetic reasoning, question
    answering, summarization, relation extraction, machine translation, and sentiment
    analysis. Researchers have been actively exploring different prompt engineering
    strategies, such as Chain of Thought (CoT), Zero-CoT, and In-context learning.
    However, an unresolved problem arises from the fact that current approaches lack
    a solid mathematical solution for determining optimal prompts. To address this
    issue in prompt engineering, we propose a new and effective approach called Prompt
    Space. Our methodology utilizes text embeddings to obtain basis vectors by matrix
    decomposition, and then constructs a space for representing all prompts. Prompt
    Space significantly outperforms state-of-the-art prompt paradigms on ten public
    reasoning benchmarks. Notably, without the help of the CoT method and the prompt
    "Let's think step by step", Prompt Space shows superior performance over the few-shot
    method. Overall, our approach provides a robust and effective mathematical framework
    for selecting simple and effective prompts. This advancement marks a significant
    step towards improving prompt engineering for a wide variety of applications in
    LLMs. Our code is publicly available at https://github.com/YouBLEI/Prompt-Space
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - emails: '****@gmail.com'
    first_name: Fobo
    last_name: Shi
    name: Fobo Shi
    orcid: https://orcid.org/0000-0002-9357-4745
    username: ~Fobo_Shi1
  - emails: '****@dartmouth.edu'
    first_name: Peijun
    google_scholar_id: https://scholar.google.com/citations?user=8iYd-1UAAAAJ&hl=en
    homepage: https://scholar.google.com/citations?user=8iYd-1UAAAAJ&hl=en
    institution: Dartmouth College
    last_name: Qing
    name: Peijun Qing
    username: ~Peijun_Qing2
  - emails: '****@my.cityu.edu.hk'
    first_name: Dong
    google_scholar_id: https://scholar.google.com/citations?user=eWqjeiEAAAAJ&hl=en
    last_name: Yang
    name: Dong Yang
    orcid: https://orcid.org/0000-0003-4530-1934
    username: ~Dong_Yang2
  - dblp_id: https://dblp.org/pid/84/864
    emails: '****@gmail.com'
    first_name: Nan
    last_name: Wang
    name: Nan Wang
    username: ~Nan_Wang5
  - emails: '****@163.com'
    first_name: Youbo
    homepage: https://youblei.github.io/
    last_name: Lei
    name: Youbo Lei
    username: ~Youbo_Lei1
  - dblp_id: https://dblp.org/pid/129/0998
    emails: '****@oppo.com'
    first_name: Haonan
    google_scholar_id: https://scholar.google.com/citations?user=EPBgKu0AAAAJ&hl=en
    institution: OPPO Guangdong Mobile Telecommunications Co., Ltd.
    last_name: Lu
    name: Haonan Lu
    orcid: https://orcid.org/0000-0001-6332-2785
    semantic_scholar_id: https://www.semanticscholar.org/author/H.-Lu/2130373
    username: ~Haonan_Lu1
  - dblp_id: https://dblp.org/pid/59/554
    emails: '****@business.rutgers.edu'
    first_name: Xiaodong
    institution: Rutgers University
    last_name: Lin
    name: Xiaodong Lin
    username: ~Xiaodong_Lin2
  - dblp_id: https://dblp.org/pid/292/1938.html
    emails: '****@gmail.com'
    first_name: Duantengchuan
    google_scholar_id: https://scholar.google.com/citations?user=VDzqb5UAAAAJ&hl=en&oi=sra
    last_name: Li
    name: Duantengchuan Li
    semantic_scholar_id: https://www.semanticscholar.org/author/Duantengchuan-Li/2108506492
    username: ~Duantengchuan_Li1
  decision: toFindings
  end_page: 1862
  file: 492.pdf
  id: 492
  num_pages: 27
  openreview_id: 3JLHtE1U3I
  pdf_file: 9be479d18b08aac44160643b7fe3db4f14f2fb1b.pdf
  start_page: 1836
  title: Prompt Space Optimizing Few-shot Reasoning Success with Large Language Models
- abstract: Aspect-based sentiment analysis (ABSA) is a task that aims to determine
    the sentiment polarity of aspects by identifying opinion words. Recent advancements
    have predominantly been rooted either in semantic or syntactic methods. However,
    both of them tend to interference from local factors such as irrelevant words
    and edges, hindering the precise identification of opinion words. In this paper,
    we present Distance-based and Aspect-oriented Graph Convolutional Network (DAGCN)
    to address the aforementioned issue. Firstly, we introduce the Distance-based
    Syntactic Weight (DSW). It focuses on the local scope of aspects in the pruned
    dependency trees, thereby reducing the candidate pool of opinion words. Additionally,
    we propose Aspect-Fusion Attention (AF) to further filter opinion words within
    the local context and consider cases where opinion words are distant from the
    aspect. With the combination of DSW and AF, we achieve precise identification
    of corresponding opinion words. Extensive experiments on three public datasets
    demonstrate that the proposed model outperforms state-of-the-art models and verify
    the effectiveness of the proposed architecture.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Sentiment Analysis, Stylistic Analysis, and Argument Mining
  authors:
  - emails: '****@gmail.com'
    first_name: Zhihao
    last_name: Wang
    name: Zhihao Wang
    orcid: https://orcid.org/0009-0007-8232-025X
    username: ~Zhihao_Wang8
  - emails: '****@shnu.edu.cn'
    first_name: Bo
    homepage: http://teacher.shnu.edu.cn/xxjdgcxy/zb/list.htm
    institution: Shanghai Normal University
    last_name: Zhang
    name: Bo Zhang
    username: ~Bo_Zhang31
  - emails: '****@shnu.edu.cn'
    first_name: Ru
    homepage: http://teacher.shnu.edu.cn/xxjdgcxy/yr/list.htm
    institution: Shanghai Normal University
    last_name: Yang
    name: Ru Yang
    username: ~Ru_Yang1
  - emails: '****@shnu.edu.cn'
    first_name: Chang
    institution: Shanghai Normal University
    last_name: Guo
    name: Chang Guo
    orcid: https://orcid.org/0000-0001-9047-4303
    username: ~Chang_Guo1
  - dblp_id: https://dblp.org/pid/l/MaozhenLi.html
    emails: '****@brunel.ac.uk'
    first_name: Maozhen
    institution: Brunel University Uxbridge
    last_name: Li
    name: Maozhen Li
    username: ~Maozhen_Li1
  decision: toFindings
  end_page: 1876
  file: 495.pdf
  id: 495
  num_pages: 14
  openreview_id: LhLtQKPHvg
  pdf_file: e066572f2037dece5483cf3c89bec9f72f9722a1.pdf
  start_page: 1863
  title: 'DAGCN: Distance-based and Aspect-oriented Graph Convolutional Network for
    Aspect-based Sentiment Analysis'
- abstract: We study the patent phrase similarity inference task, which measures the
    semantic similarity between two patent phrases. As patent documents employ legal
    and highly technical language, existing semantic textual similarity methods that
    use localized contextual information do not perform satisfactorily in inferring
    patent phrase similarity. To address this, we introduce a graph-augmented approach
    to amplify the global contextual information of the patent phrases. For each patent
    phrase, we construct a phrase graph that links to its focal patents and a list
    of patents that are either cited by or cite these focal patents. The augmented
    phrase embedding is then derived from combining its localized contextual embedding
    with its global embedding within the phrase graph. We further propose a self-supervised
    learning objective that capitalizes on the retrieved topology to refine both the
    contextualized embedding and the graph parameters in an end-to-end manner. Experimental
    results from a unique patent phrase similarity dataset demonstrate that our approach
    significantly enhances the representation of patent phrases, resulting in marked
    improvements in similarity inference in a self-supervised fashion. Substantial
    improvements are also observed in the supervised setting, underscoring the potential
    benefits of leveraging retrieved phrase graph augmentation.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: 'Semantics: Sentence-level Semantics, Textual Inference and Other
      areas'
  authors:
  - emails: '****@connect.ust.hk'
    first_name: Zhuoyi
    homepage: https://github.com/CheukNgai
    last_name: Peng
    name: Zhuoyi PENG
    username: ~Zhuoyi_PENG1
  - emails: '****@ust.hk'
    first_name: Yi
    google_scholar_id: https://scholar.google.com.hk/citations?user=Prh_dHkAAAAJ&hl=en&authuser=2
    homepage: http://yya518.github.io/
    institution: Hong Kong University of Science and Technology
    last_name: Yang
    name: Yi Yang
    username: ~Yi_Yang7
  decision: toFindings
  end_page: 1890
  file: 499.pdf
  id: 499
  num_pages: 14
  openreview_id: 5DwAnsNwi8
  pdf_file: 39ee56314bf69ea87bb2d291bbb63a62a219b9d7.pdf
  start_page: 1877
  title: 'Connecting the Dots: Inferring Patent Phrase Similarity with Retrieved Phrase
    Graphs'
- abstract: "Sample diversity depends on the task; within mathematics, precision and\
    \ determinism are paramount, while storytelling thrives on creativity and surprise.\
    \ This paper presents a simple self-regulating approach where we adjust sample\
    \ diversity inference parameters dynamically based on the input prompt\u2014in\
    \ contrast to existing methods that require expensive and inflexible setups, or\
    \ maintain static values during inference. Capturing a broad spectrum of sample\
    \ diversities can be formulated as a straightforward self-supervised inference\
    \ task, which we find significantly improves the quality of responses generically\
    \ without model retraining or fine-tuning. In particular, our method demonstrates\
    \ significant improvement in all supercategories of the MMLU multitask benchmark\
    \ (GPT-3.5: $+4.4\\%$, GPT-4: $+1.5\\%$), which captures a large variety of difficult\
    \ tasks covering STEM, the humanities and social sciences."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Generation
  authors:
  - emails: '****@durham.ac.uk'
    first_name: Mingyue
    homepage: https://www.durham.ac.uk/staff/mingyue-liu/
    last_name: Liu
    name: Mingyue Liu
    username: ~Mingyue_Liu1
  - emails: '****@9lines.org'
    first_name: Jonathan
    google_scholar_id: https://scholar.google.com/citations?user=XOjzg10AAAAJ&hl=en&oi=ao
    homepage: https://jonathanfrawley.com
    institution: Durham University
    last_name: Frawley
    name: Jonathan Frawley
    orcid: https://orcid.org/0000-0002-9437-7399
    username: ~Jonathan_Frawley1
  - emails: '****@durham.ac.uk'
    first_name: Sarah
    homepage: https://www.durham.ac.uk/staff/sarah-wyer/
    last_name: Wyer
    name: Sarah Wyer
    username: ~Sarah_Wyer1
  - dblp_id: http://dblp.uni-trier.de/pers/hd/s/Shum:Hubert_P=_H=
    emails: '****@durham.ac.uk'
    first_name: Hubert P. H.
    google_scholar_id: https://scholar.google.com/citations?user=pkPLCEYAAAAJ
    homepage: http://hubertshum.com
    institution: Durham University
    last_name: Shum
    name: Hubert P. H. Shum
    orcid: https://orcid.org/0000-0001-5651-6039
    semantic_scholar_id: https://www.semanticscholar.org/author/Hubert-P.-H.-Shum/2840036
    username: ~Hubert_P._H._Shum1
  - emails: '****@durham.ac.uk'
    first_name: Sara
    google_scholar_id: https://scholar.google.com/citations?user=cMT6QLkAAAAJ&hl=en
    homepage: https://sluckelman.webspace.durham.ac.uk/
    institution: Durham University
    last_name: Uckelman
    middle_name: L.
    name: Sara L. Uckelman
    orcid: https://orcid.org/0000-0002-0719-371X
    username: ~Sara_L._Uckelman1
  - emails: '****@durham.ac.uk'
    first_name: Sue
    google_scholar_id: https://scholar.google.com/citations?user=wpkjKkgAAAAJ&hl=en
    homepage: http://www.sueblack.co.uk
    institution: Durham University
    last_name: Black
    name: Sue Black
    orcid: https://orcid.org/0000-0002-9315-9517
    username: ~Sue_Black2
  - dblp_id: https://dblp.org/pid/28/11416
    emails: '****@durham.ac.uk'
    first_name: Chris
    google_scholar_id: https://scholar.google.com/citations?user=F5QdeesAAAAJ
    homepage: http://cwkx.com
    institution: Durham University and Durham University
    last_name: Willcocks
    middle_name: G.
    name: Chris G. Willcocks
    orcid: https://orcid.org/0000-0001-6821-3924
    semantic_scholar_id: https://www.semanticscholar.org/author/Chris-G.-Willcocks/2240006
    username: ~Chris_G._Willcocks1
  decision: toFindings
  end_page: 1899
  file: 500.pdf
  id: 500
  num_pages: 9
  openreview_id: R1VJvlaSU8
  pdf_file: e77b5d43a67b4d5580c0b128ecb84490a93eef56.pdf
  start_page: 1891
  title: Self-Regulated Sample Diversity in Large Language Models
- abstract: Learning-to-rank (LTR) algorithms aim to order a set of items according
    to some criteria. They are at the core of applications such as web search and
    social media recommendations, and are an area of rapidly increasing interest,
    with the rise of large language models (LLMs) and the widespread impact of these
    technologies on society. In this paper, we survey the diverse use cases of LTR
    methods in natural language processing (NLP) research, looking at previously under-studied
    aspects such as multilingualism in LTR applications and statistical significance
    testing for LTR problems. We also consider how large language models are changing
    the LTR landscape. This survey is aimed at NLP researchers and practitioners interested
    in understanding the formalisms and best practices regarding the application of
    LTR approaches in their research.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - emails: '****@mail.utoronto.ca'
    first_name: Justin
    google_scholar_id: https://scholar.google.ca/citations?user=4i_u8mUAAAAJ&hl=en
    last_name: Lee
    name: Justin Lee
    orcid: https://orcid.org/0000-0003-1367-7478
    username: ~Justin_Lee1
  - dblp_id: https://dblp.org/pid/124/3161
    emails: '****@gmail.com'
    first_name: Gabriel
    institution: National Research Council Canada
    last_name: Bernier-Colborne
    name: Gabriel Bernier-Colborne
    semantic_scholar_id: https://www.semanticscholar.org/author/Gabriel-Bernier-Colborne/1405461933
    username: ~Gabriel_Bernier-Colborne1
  - emails: '****@gmail.com'
    first_name: Tegan
    google_scholar_id: https://scholar.google.ca/citations?user=XpscC-EAAAAJ&hl=en
    homepage: http://teganmaharaj.com
    institution: Toronto University and Ecole Polytechnique de Montreal
    last_name: Maharaj
    name: Tegan Maharaj
    username: ~Tegan_Maharaj1
  - dblp_id: https://dblp.org/pid/127/0212
    emails: '****@gmail.com'
    first_name: Sowmya
    google_scholar_id: https://scholar.google.com/citations?user=e4UbD1UAAAAJ&hl=en
    institution: National Research Council Canada
    last_name: Vajjala
    name: Sowmya Vajjala
    orcid: https://orcid.org/0000-0002-4033-9936
    semantic_scholar_id: https://www.semanticscholar.org/author/Sowmya-Vajjala/2070714
    username: ~Sowmya_Vajjala2
  decision: toFindings
  end_page: 1917
  file: 503.pdf
  id: 503
  num_pages: 18
  openreview_id: UiSyUKkthq
  pdf_file: db40ac31819b0bfcfadb775bbc87afafaab2a4fc.pdf
  start_page: 1900
  title: Methods, Applications, and Directions of Learning-to-Rank in NLP Research
- abstract: "Recent studies introduced effective compression techniques for Large\
    \ Language Models (LLMs) via post-training quantization or low-bit weight representation.\
    \ \nAlthough quantized weights offer storage efficiency and allow for faster inference,\
    \ existing works have indicated that quantization might compromise performance\
    \ and exacerbate biases in LLMs.\nThis study investigates the confidence and calibration\
    \ of quantized models, considering factors such as language model type and scale\
    \ as contributors to quantization loss.\nFirstly, we reveal that quantization\
    \ with GPTQ to 4-bit results in a decrease in confidence regarding true labels,\
    \ with varying impacts observed among different language models. \nSecondly, we\
    \ observe fluctuations in the impact on confidence across different scales. \n\
    Finally, we propose an explanation for quantization loss based on confidence levels,\
    \ indicating that quantization disproportionately affects samples where the full\
    \ model exhibited low confidence levels in the first place.\nWe make our code\
    \ and quantized models publicly available."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - dblp_id: https://dblp.org/pid/320/8136
    emails: '****@univ-lyon2.fr'
    first_name: Irina
    google_scholar_id: https://scholar.google.com/citations?user=YPYPhTAAAAAJ&hl=en
    last_name: Proskurina
    name: Irina Proskurina
    semantic_scholar_id: https://www.semanticscholar.org/author/Irina-Proskurina/2165662096
    username: ~Irina_Proskurina1
  - emails: '****@gmail.com'
    first_name: Luc
    institution: "Laboratoire ERIC, Universit\xE9 Lumi\xE9re (Lyon II)"
    last_name: Brun
    name: Luc Brun
    username: ~Luc_Brun3
  - dblp_id: https://dblp.org/pid/226/4189
    emails: '****@univ-lyon2.fr'
    first_name: Guillaume
    google_scholar_id: https://scholar.google.com/citations?user=MgjT0IUAAAAJ&hl=fr
    homepage: https://guillaumemetzler.github.io
    institution: "Universit\xE9 Lumi\xE9re (Lyon II)"
    last_name: Metzler
    name: Guillaume Metzler
    username: ~Guillaume_Metzler1
  - dblp_id: https://dblp.uni-trier.de/pers/hd/v/Velcin:Julien.html
    emails: '****@univ-lyon2.fr'
    first_name: Julien
    google_scholar_id: https://scholar.google.com/citations?user=_ZreLBMAAAAJ&hl=en
    homepage: http://eric.univ-lyon2.fr/jvelcin/
    institution: ERIC
    last_name: Velcin
    name: Julien Velcin
    orcid: https://orcid.org/0000-0002-2262-045X
    username: ~Julien_Velcin1
  decision: toFindings
  end_page: 1928
  file: 505.pdf
  id: 505
  num_pages: 11
  openreview_id: uCv3NwVKCT
  pdf_file: 5c023deffe89c77c21262ee57ce1d44bae0b5a2a.pdf
  start_page: 1918
  title: When Quantization Affects Confidence of Large Language Models?
- abstract: Generating medical reports for X-ray images presents a significant challenge,
    particularly in unpaired scenarios where access to paired image-report data for
    training is unavailable. Previous works have typically learned a joint embedding
    space for images and reports, necessitating a specific labeling schema for both.  We
    introduce an innovative approach that eliminates the need for consistent labeling
    schemas, thereby enhancing data accessibility and enabling the use of incompatible
    datasets. This approach is based on cycle-consistent mapping functions that transform
    image embeddings into report embeddings, coupled with report auto encoding for
    medical report generation. Our model and objectives consider intricate local details
    and the overarching semantic context within images and reports.  This approach
    facilitates the learning of effective mapping functions, resulting in the generation
    of coherent reports. It outperforms state-of-the-art results in unpaired chest
    X-ray report generation, demonstrating improvements in both language and clinical
    metrics.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multimodality and Language Grounding to Vision, Robotics and Beyond
  authors:
  - dblp_id: https://dblp.org/pid/265/6032
    emails: '****@campus.technion.ac.il'
    first_name: Elad
    institution: Technion, Technion
    last_name: Hirsch
    name: Elad Hirsch
    username: ~Elad_Hirsch1
  - emails: '****@campus.technion.ac.il'
    first_name: Gefen
    last_name: Dawidowicz
    name: Gefen Dawidowicz
    username: ~Gefen_Dawidowicz1
  - dblp_id: http://dblp.uni-trier.de/pers/hd/t/Tal:Ayellet
    emails: '****@ee.technion.ac.il'
    first_name: Ayellet
    google_scholar_id: https://scholar.google.com.tw/citations?user=eFGgX-QAAAAJ
    homepage: http://webee.technion.ac.il/people/ayellet/
    institution: Technion and Technion
    last_name: Tal
    name: Ayellet Tal
    username: ~Ayellet_Tal1
  decision: toFindings
  end_page: 1944
  file: 509.pdf
  id: 509
  num_pages: 16
  openreview_id: 068atOuCTD
  pdf_file: ad1d753ba0e94868734c6705dbb74e87806f73a9.pdf
  start_page: 1929
  title: 'MedCycle: Unpaired Medical Report Generation via Cycle-Consistency'
- abstract: "The logical information contained in text is\nof significant importance\
    \ for logical reasoning.\nPrevious approaches have relied on embedding\ntext into\
    \ a low-dimensional vector to capture\nlogical information and perform reasoning\
    \ in\nEuclidean space. These methods involve constructing special graph architectures\
    \ that match\nlogical relations or designing data augmentation frameworks by extending\
    \ texts based on\nsymbolic logic. However, it presents two obvious problems. 1)\
    \ The logical information\nreflected in the text exhibits uncertainty that is\n\
    difficult to represent using a vector. 2) Integrat\x02ing logical information\
    \ requires modeling logical operations (such as \u222A, \u2229, and \xAC), while\
    \ only\nsimple arithmetic operations can be performed\nin Euclidean space. To\
    \ address both the problems, we propose Beta-LR, a probabilistic embedding method\
    \ to capture logical information.\nSpecifically, we embed texts into beta distribution\
    \ on each dimension to eliminate logical uncertainty. We also define neural operators\
    \ that\nenable interpretability and perform logical operations based on the characteristics\
    \ of the beta\ndistribution. We conduct experiments on two\ndatasets, ReClor and\
    \ LogiQA, and our Beta-LR\nachieves competitive results. The experiments\ndemonstrate\
    \ that our method effectively cap\x02tures the logical information in text for\
    \ reasoning purposes. The source code is available at\nhttps://github.com/myz12138/Beta-LR."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: 'Semantics: Sentence-level Semantics, Textual Inference and Other
      areas'
  authors:
  - emails: '****@std.uestc.edu.cn'
    first_name: Yizhuo
    homepage: https://github.com/myz12138/
    last_name: Ma
    name: Yizhuo Ma
    username: ~Yizhuo_Ma1
  - dblp_id: https://dblp.org/pid/86/3966
    emails: '****@uestc.edu.cn'
    first_name: Ke
    institution: University of Electronic Science and Technology of China
    last_name: Qin
    name: Ke Qin
    semantic_scholar_id: https://www.semanticscholar.org/author/Ke-Qin/2150417482
    username: ~Ke_Qin1
  - emails: '****@uestc.edu.cn'
    first_name: Shuang
    institution: University of Electronic Science and Technology of China
    last_name: Liang
    name: Shuang Liang
    orcid: https://orcid.org/0000-0001-7387-2801
    username: ~Shuang_Liang4
  decision: toFindings
  end_page: 1955
  file: 512.pdf
  id: 512
  num_pages: 11
  openreview_id: l4qgiHpLav
  pdf_file: 72f21cc9dc68a79b31f77792079d339ebdeb0714.pdf
  start_page: 1945
  title: 'Beta-LR: Interpretable Logical Reasoning based on Beta Distribution'
- abstract: This study explores the potential of automating clinical coding in Icelandic,
    a language with limited digital resources, by leveraging over 25 years of electronic
    health records (EHR) from the Landspitali University Hospital. Traditionally a
    manual and error-prone task, clinical coding is essential for patient care, billing,
    and research. Our research delves into the effectiveness of Transformer-based
    models in automating this process. We investigate various model training strategies,
    including continued pretraining and model adaptation, under a constrained computational
    budget. Our findings reveal that the best-performing model achieves competitive
    results in both micro and macro F1 scores, with label attention contributing significantly
    to its success. The study also explores the possibility of training on unlabeled
    data. Our research provides valuable insights into the possibilities of using
    NLP for clinical coding in low-resource languages, demonstrating that small countries
    with unique languages and well-segmented healthcare records can achieve results
    comparable to those in higher-resourced languages.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - emails: '****@gmail.com'
    first_name: Haraldur
    institution: ETHZ - ETH Zurich
    last_name: Hauksson
    middle_name: Orri
    name: Haraldur Orri Hauksson
    username: ~Haraldur_Orri_Hauksson1
  - dblp_id: https://dblp.org/pid/155/6896.html
    emails: '****@hi.is'
    first_name: Hafsteinn
    google_scholar_id: https://scholar.google.com/citations?user=QH69Qk8AAAAJ&hl=ja
    institution: deCODE genetics and University of Iceland
    last_name: Einarsson
    name: Hafsteinn Einarsson
    orcid: https://orcid.org/0000-0001-5072-3678
    username: ~Hafsteinn_Einarsson1
  decision: toFindings
  end_page: 1967
  file: 519.pdf
  id: 519
  num_pages: 12
  openreview_id: bNlcRU7HuU
  pdf_file: 46ea2209abb22da2ba21c3ffccf95650f3b691c5.pdf
  start_page: 1956
  title: Applications of BERT Models Towards Automation of Clinical Coding in Icelandic
- abstract: 'Argument mining has focused so far mainly on the identification, extraction,
    and formalization of arguments. An important yet unaddressed

    task consists in the prediction of the argumentative behavior of stakeholders
    in a debate. Predicting the argumentative behavior in advance can support foreseeing
    issues in public policy making or help recognize potential disagreements early
    on and help to resolve them. In this paper, we consider the novel task of predicting
    the argumentative behavior of individual stakeholders. We present ARGENST, a framework
    that relies on a recommender-based architecture to predict the stance and the
    argumentative main point on a specific controversial topic for a given stakeholder,
    which is described in terms of a profile including properties related to demographic
    attributes, religious and political orientation, socio-economic background, etc.
    We evaluate our approach on the well-known debate.org dataset in terms of accuracy
    for predicting stance as well as in terms of similarity of the generated arguments
    to the ground truth arguments using BERTScore. As part of a case study, we show
    how juries of members representing different stakeholder groups and perspectives
    can be assembled to simulate the public opinion on a given topic.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Computational Social Science and Cultural Analytics
  authors:
  - dblp_id: https://dblp.org/pid/294/1423
    emails: '****@techfak.uni-bielefeld.de'
    first_name: Philipp
    google_scholar_id: https://scholar.google.de/citations?user=ooL1D6gAAAAJ&hl=de
    homepage: https://philippheinisch.de/
    institution: "Universit\xE4t Bielefeld"
    last_name: Heinisch
    name: Philipp Heinisch
    orcid: https://orcid.org/0009-0002-8079-5570
    username: ~Philipp_Heinisch1
  - dblp_id: https://dblp.uni-trier.de/pid/217/2512.html
    emails: '****@uni-trier.de'
    first_name: Lorik
    google_scholar_id: https://scholar.google.de/citations?user=nvsYSuMAAAAJ&hl=de&oi=ao
    homepage: https://www.uni-trier.de/index.php?id=66708
    institution: Trier University
    last_name: Dumani
    name: Lorik Dumani
    orcid: https://orcid.org/0000-0001-9567-1699
    username: ~Lorik_Dumani1
  - dblp_id: https://dblp.org/pid/12/1983
    emails: '****@cit-ec.uni-bielefeld.de'
    first_name: Philipp
    google_scholar_id: https://scholar.google.com.tw/citations?user=ZyR3798AAAAJ
    homepage: http://ekvv.uni-bielefeld.de/pers_publ/publ/PersonDetail.jsp?personId=15020699
    institution: Bielefeld University
    last_name: Cimiano
    name: Philipp Cimiano
    username: ~Philipp_Cimiano1
  - dblp_id: https://dblp.uni-trier.de/pid/s/RalfSchenkel.html
    emails: '****@uni-trier.de'
    first_name: Ralf
    google_scholar_id: https://scholar.google.com/citations?user=4gaNzroAAAAJ
    homepage: https://www.uni-trier.de/index.php?id=17320&L=2
    institution: Trier University
    last_name: Schenkel
    name: Ralf Schenkel
    orcid: https://orcid.org/0000-0001-5379-5191
    username: ~Ralf_Schenkel2
  decision: toFindings
  end_page: 1982
  file: 520.pdf
  id: 520
  num_pages: 15
  openreview_id: C64W7XAZKU
  pdf_file: b9976f85ea50c4b48b32a1ce7adf7f940981e2a2.pdf
  start_page: 1968
  title: '"Tell me who you are and I tell you how you argue": Predicting Stances and
    Arguments for Stakeholder Groups'
- abstract: "Instruction tuning aligns the response of large language models (LLMs)\
    \ with human preferences.\nDespite such efforts in human--LLM alignment, we find\
    \ that instruction tuning does not always make LLMs human-like from a cognitive\
    \ modeling perspective. More specifically, next-word probabilities estimated by\
    \ instruction-tuned LLMs are often worse at simulating human reading behavior\
    \ than those estimated by base LLMs.\nIn addition, we explore prompting methodologies\
    \ for simulating human reading behavior with LLMs. \nOur results show that prompts\
    \ reflecting a particular linguistic hypothesis improve psychometric predictive\
    \ power, but are still inferior to small base models.\nThese findings highlight\
    \ that recent advancements in LLMs, i.e., instruction tuning and prompting, do\
    \ not offer better estimates than direct probability measurements from base LLMs\
    \ in cognitive modeling. In other words, pure next-word probability remains a\
    \ strong predictor for human reading behavior, even in the age of LLMs."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Linguistic theories, Cognitive Modeling and Psycholinguistics
  authors:
  - dblp_id: https://dblp.org/pid/228/5787
    emails: '****@mbzuai.ac.ae'
    first_name: Tatsuki
    google_scholar_id: https://scholar.google.co.jp/citations?user=-bqmkaAAAAAJ
    homepage: https://kuribayashi4.github.io/
    institution: Mohamed bin Zayed University of Artificial Intelligence
    last_name: Kuribayashi
    name: Tatsuki Kuribayashi
    semantic_scholar_id: https://www.semanticscholar.org/author/Tatsuki-Kuribayashi/83446147
    username: ~Tatsuki_Kuribayashi1
  - dblp_id: https://dblp.org/pid/249/6882
    emails: '****@g.ecc.u-tokyo.ac.jp'
    first_name: Yohei
    google_scholar_id: https://scholar.google.com/citations?user=GshpLs8AAAAJ&hl=en&oi=ao
    homepage: https://researchmap.jp/oseki/?lang=english
    institution: University of Tokyo
    last_name: Oseki
    name: Yohei Oseki
    orcid: https://orcid.org/0000-0002-1189-1588
    semantic_scholar_id: https://www.semanticscholar.org/author/Yohei-Oseki/50856622
    username: ~Yohei_Oseki1
  - dblp_id: https://dblp.org/pid/65/4863
    emails: '****@ldwin.net'
    first_name: Timothy
    google_scholar_id: https://scholar.google.com/citations?user=wjBD1dkAAAAJ&hl=en
    institution: Mohamed bin Zayed University of Artificial Intelligence and The University
      of Melbourne
    last_name: Baldwin
    name: Timothy Baldwin
    username: ~Timothy_Baldwin1
  decision: toFindings
  end_page: 2005
  file: 523.pdf
  id: 523
  num_pages: 23
  openreview_id: rXCntvoOyX
  pdf_file: 18b7fda5be5a8d6acf13a76dbcde4d7511005788.pdf
  start_page: 1983
  title: Psychometric Predictive Power of Large Language Models
- abstract: "Large Language Models (LLMs) have demonstrated remarkable capabilities\
    \ in various NLP tasks. \nHowever, previous works have shown these models are\
    \ sensitive towards prompt wording, and few-shot demonstrations and their order,\
    \ posing challenges to fair assessment of these models. As these models become\
    \ more powerful, it becomes imperative to understand and address these limitations.\
    \  In this paper, we focus on LLMs robustness on the task of multiple-choice questions---commonly\
    \ adopted task to study reasoning and fact-retrieving capability of LLMs. Investigating\
    \ the sensitivity of LLMs towards the order of options in multiple-choice questions,\
    \ we demonstrate a considerable performance gap of approximately 13% to 85% in\
    \ LLMs on different benchmarks, when answer options are reordered, even when using\
    \ demonstrations in a few-shot setting. Through a detailed analysis, we conjecture\
    \ that this sensitivity arises when LLMs are uncertain about the prediction between\
    \ the top-2/3 choices, and specific options placements may favor certain prediction\
    \ between those top choices depending on the question caused by positional bias.\
    \ We also identify patterns in top-2 choices that amplify or mitigate the model's\
    \ bias toward option placement. We found that for amplifying bias, the optimal\
    \ strategy involves positioning the top two choices as the first and last options.\
    \ Conversely, to mitigate bias, we recommend placing these choices among the adjacent\
    \ options. To validate our conjecture, we conduct various experiments and adopt\
    \ two approaches to calibrate LLMs' predictions, leading to up to 8 percentage\
    \ points improvement across different models and benchmarks."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - dblp_id: https://dblp.org/pid/159/1696
    emails: '****@uci.edu'
    first_name: Pouya
    google_scholar_id: https://scholar.google.com/citations?user=sjY8zjUAAAAJ&hl=en&oi=ao
    homepage: https://pouyapez.github.io
    institution: Megagon Labs
    last_name: Pezeshkpour
    name: Pouya Pezeshkpour
    semantic_scholar_id: https://www.semanticscholar.org/author/Pouya-Pezeshkpour/1713436
    username: ~Pouya_Pezeshkpour2
  - dblp_id: https://dblp.org/pid/294/1598
    emails: '****@gmail.com'
    first_name: Estevam
    google_scholar_id: https://scholar.google.com/citations?user=19J-aOkAAAAJ&hl=en
    institution: Megagon Labs and Carnegie Mellon University
    last_name: Hruschka
    name: Estevam Hruschka
    username: ~Estevam_Hruschka1
  decision: toFindings
  end_page: 2017
  file: 532.pdf
  id: 532
  num_pages: 12
  openreview_id: 1B4KIrvBGY
  pdf_file: 43cf9cc558f38cc96c544de23ba4732bb1b7c62c.pdf
  start_page: 2006
  title: Large Language Models Sensitivity to The Order of Options in Multiple-Choice
    Questions
- abstract: "CLIP-based classifiers rely on the prompt containing a {class name} that\
    \ is known to the text encoder. Therefore, they perform poorly on new classes\
    \ or the classes whose names rarely appear on the Internet (e.g., scientific names\
    \ of birds). For fine-grained classification, we propose PEEB \u2013 an explainable\
    \ and editable classifier to (1) express the class name into a set of text descriptors\
    \ that describe the visual parts of that class; and (2) match the embeddings of\
    \ the detected parts to their textual descriptors in each class to compute a logit\
    \ score for classification. In a zero-shot setting where the class names are unknown,\
    \ PEEB outperforms CLIP by a huge margin (\u223C10\xD7 in top-1 accuracy). Compared\
    \ to part-based classifiers, PEEB is not only the state-of-the-art (SOTA) on the\
    \ supervised-learning setting (88.80% and 92.20% accuracy on CUB-200 and Stanford\
    \ Dogs-120, respectively) but also the first to enable users to edit the text\
    \ descriptors to form a new classifier without any re-training. Compared to concept\
    \ bottleneck models, PEEB is also the SOTA in both zero-shot and supervised-learning\
    \ settings."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multimodality and Language Grounding to Vision, Robotics and Beyond
  authors:
  - dblp_id: https://dblp.org/pid/282/4513
    emails: '****@gmail.com'
    first_name: Thang
    google_scholar_id: https://scholar.google.com/citations?user=eNrX3mYAAAAJ&hl=en
    last_name: Pham
    middle_name: M.
    name: Thang M. Pham
    semantic_scholar_id: https://www.semanticscholar.org/author/2042922511
    username: ~Thang_M._Pham1
  - dblp_id: https://dblp.org/pid/267/9646
    emails: '****@gmail.com'
    first_name: Peijie
    homepage: https://github.com/Chanfeechen
    institution: Auburn University
    last_name: Chen
    name: Peijie Chen
    username: ~Peijie_Chen2
  - emails: '****@auburn.edu'
    first_name: Tin
    google_scholar_id: https://scholar.google.com/citations?user=zSAfD80AAAAJ&hl=vi
    homepage: https://ngthanhtin.github.io/
    last_name: Nguyen
    name: Tin Nguyen
    orcid: https://orcid.org/0000-0002-6798-9808
    username: ~Tin_Nguyen1
  - dblp_id: https://dblp.org/pid/68/3020-2
    emails: '****@gmail.com'
    first_name: Seunghyun
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=UpymOMwAAAAJ&view_op=list_works
    homepage: https://david-yoon.github.io/
    institution: Adobe Research
    last_name: Yoon
    name: Seunghyun Yoon
    orcid: https://orcid.org/0000-0002-7262-3579
    semantic_scholar_id: https://www.semanticscholar.org/author/Seunghyun-Yoon/144517919
    username: ~Seunghyun_Yoon1
  - dblp_id: https://dblp.org/pid/180/0632
    emails: '****@adobe.com'
    first_name: Trung
    google_scholar_id: https://scholar.google.com/citations?user=FpFTduYAAAAJ&hl=en
    homepage: https://sites.google.com/site/trungbuistanford/
    institution: Adobe Research
    last_name: Bui
    name: Trung Bui
    orcid: https://orcid.org/0000-0002-0871-349X
    semantic_scholar_id: https://www.semanticscholar.org/author/Trung-Bui/145262461
    username: ~Trung_Bui1
  - dblp_id: https://dblp.org/pid/52/5285
    emails: '****@gmail.com'
    first_name: Anh
    google_scholar_id: https://scholar.google.com/citations?user=EQw8d9AAAAAJ&hl=en&oi=ao
    homepage: http://anhnguyen.me
    institution: Auburn University
    last_name: Nguyen
    name: Anh Nguyen
    username: ~Anh_Nguyen1
  decision: toFindings
  end_page: 2053
  file: 535.pdf
  id: 535
  num_pages: 36
  openreview_id: XwESJqmoG7
  pdf_file: 83153e4614d648115521f5127cf26909f2ad01b6.pdf
  start_page: 2018
  title: 'PEEB: Part-based Image Classifiers with an Explainable and Editable Language
    Bottleneck'
- abstract: 'Language models (LMs) have greatly propelled the research on natural
    language processing. However, LMs also raise concerns regarding the generation
    of biased or toxic content and the potential disclosure of private information
    from the training dataset. In this work, we present a new efficient approach,
    Ethos, that rectifies LMs to mitigate toxicity and bias in outputs and avoid privacy
    leakage. Ethos is built on task arithmetic. However, unlike current task arithmetic
    algorithms, Ethos distinguishes general beneficial and undesired knowledge when
    reconstructing task vectors. Specifically, Ethos first obtains a set of principal
    components from the pre-trained models using singular value decomposition. Then,
    by projecting the task vector onto principal components, Ethos separates the principal
    components that encode general from those associated with undesired knowledge.
    Ethos performs forgetting or unlearning by only negating the task vector with
    undesired knowledge, thereby minimizing collateral damage on general model utility.
    We demonstrate the efficacy of our approach on three different tasks: bias, toxicity,
    and memorization unlearning. Evaluations show Ethos is more effective in removing
    undesired knowledge while maintaining the overall model performance compared to
    current task arithmetic methods.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Ethics, Bias, and Fairness
  authors:
  - emails: '****@usc.edu'
    first_name: Lei
    google_scholar_id: https://scholar.google.com/citations?user=TxzNHuIAAAAJ&hl=en&oi=ao
    last_name: Gao
    name: Lei Gao
    username: ~Lei_Gao3
  - dblp_id: https://dblp.org/pid/23/6942-1
    emails: '****@usc.edu'
    first_name: Yue
    google_scholar_id: https://scholar.google.com/citations?user=J7vQ-QEAAAAJ&hl=en
    homepage: https://julienniu.wordpress.com/
    institution: University of Southern California
    last_name: Niu
    name: Yue Niu
    username: ~Yue_Niu1
  - emails: '****@usc.edu'
    first_name: Tingting
    institution: University of Southern California
    last_name: Tang
    name: Tingting Tang
    username: ~Tingting_Tang2
  - dblp_id: https://dblp.org/pid/63/1946
    emails: '****@usc.edu'
    first_name: Salman
    google_scholar_id: https://scholar.google.com.tw/citations?user=Qhe5ua0AAAAJ
    homepage: https://www.avestimehr.com
    institution: University of Southern California, University of Southern California
      and University of Southern California
    last_name: Avestimehr
    name: Salman Avestimehr
    username: ~Salman_Avestimehr1
  - emails: '****@usc.edu'
    first_name: Murali
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=NAntFXIAAAAJ
    homepage: http://annavar.am
    institution: University of Southern California
    last_name: Annavaram
    name: Murali Annavaram
    username: ~Murali_Annavaram1
  decision: toFindings
  end_page: 2068
  file: 549.pdf
  id: 549
  num_pages: 15
  openreview_id: mIjL0QvLMd
  pdf_file: 8777e15e71228cbd234df58b2b1d6b446ac09401.pdf
  start_page: 2054
  title: 'Ethos: Rectifying Language Models in Orthogonal Parameter Space'
- abstract: "In-context learning can improve the performances of knowledge-rich tasks\
    \ such as question answering. In such scenarios, in-context examples trigger a\
    \ language model (LM) to surface information stored in its parametric knowledge.\
    \ We study how to better construct in-context example sets, based on whether the\
    \ model is aware of the in-context examples. We identify \u2018known\u2019 examples,\
    \ where models can correctly answer from their parametric knowledge, and \u2018\
    unknown\u2019 ones. Our experiments show that prompting with \u2018unknown\u2019\
    \ examples decreases the performance, potentially as it encourages hallucination\
    \ rather than searching for its parametric knowledge. Constructing an in-context\
    \ example set that presents both known and unknown information performs the best\
    \ across diverse settings. We perform analysis on three multi-answer question\
    \ answering datasets, which allows us to further study answer set ordering strategies\
    \ based on the LM's knowledge of each answer. Together, our study sheds light\
    \ on how to best construct in-context example sets for knowledge-rich tasks."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - emails: '****@gmail.com'
    first_name: Yoonsang
    homepage: https://lilys012.github.io/
    institution: Seoul National University
    last_name: Lee
    name: Yoonsang Lee
    username: ~Yoonsang_Lee1
  - emails: '****@berkeley.edu'
    first_name: Pranav
    google_scholar_id: https://scholar.google.com/citations?user=bQowYEYAAAAJ&hl=en
    homepage: https://pranavatreya.github.io
    institution: University of California, Berkeley
    last_name: Atreya
    name: Pranav Atreya
    username: ~Pranav_Atreya1
  - dblp_id: https://dblp.org/pid/56/8549
    emails: '****@cs.utexas.edu'
    first_name: Xi
    google_scholar_id: https://scholar.google.com/citations?user=qH83GlAAAAAJ
    homepage: https://www.cs.utexas.edu/~xiye/
    last_name: Ye
    name: Xi Ye
    semantic_scholar_id: https://www.semanticscholar.org/author/Xi-Ye/50183897
    username: ~Xi_Ye2
  - dblp_id: https://dblp.org/pid/116/2765
    emails: '****@utexas.edu'
    first_name: Eunsol
    google_scholar_id: https://scholar.google.com/citations?user=dCEcahMAAAAJ&hl=en
    homepage: https://eunsol.github.io/
    institution: University of Texas, Austin
    last_name: Choi
    name: Eunsol Choi
    semantic_scholar_id: https://www.semanticscholar.org/author/Eunsol-Choi/2890423
    username: ~Eunsol_Choi1
  decision: toFindings
  end_page: 2085
  file: 550.pdf
  id: 550
  num_pages: 17
  openreview_id: YRC8q9qvWt
  pdf_file: d0ea3f247c49fb35aa27c01c7ade066f7db16f35.pdf
  start_page: 2069
  title: Crafting In-context Examples according to LMs' Parametric Knowledge
- abstract: This paper focuses on the task of Extreme Multi-Label Classification (XMC)
    whose goal is to predict multiple labels for each instance from an extremely large
    label space. While existing research has primarily focused on fully supervised
    XMC, real-world scenarios often lack supervision signals, highlighting the importance
    of zero-shot settings. Given the large label space, utilizing in-context learning
    approaches is not trivial. We address this issue by introducing In-Context Extreme
    Multi-label Learning (ICXML), a two-stage framework that cuts down the search
    space by generating a set of candidate labels through in-context learning and
    then reranks them. Extensive experiments suggest that ICXML advances the state
    of the art on two diverse public benchmarks.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Retrieval and Text Mining
  authors:
  - emails: '****@cs.umass.edu'
    first_name: Yaxin
    homepage: https://yaxinzhuars.github.io
    last_name: Zhu
    name: Yaxin Zhu
    username: ~Yaxin_Zhu1
  - dblp_id: https://dblp.org/pid/150/5324
    emails: '****@cs.umass.edu'
    first_name: Hamed
    google_scholar_id: https://scholar.google.com/citations?user=d2uzDIAAAAAJ&hl=en&oi=ao
    homepage: https://groups.cs.umass.edu/zamani/
    institution: Google and University of Massachusetts, Amherst
    last_name: Zamani
    name: Hamed Zamani
    username: ~Hamed_Zamani1
  decision: toFindings
  end_page: 2098
  file: 555.pdf
  id: 555
  num_pages: 13
  openreview_id: BzWA4lkKIi
  pdf_file: 64ffab1bab7da09de0373badc11bf7c7120dfdd4.pdf
  start_page: 2086
  title: 'ICXML: An In-Context Learning Framework for Zero-Shot Extreme Multi-Label
    Classification'
- abstract: Recently, contrastive learning has begun to gain popularity in multimodal
    sentiment analysis (MSA). However, most of existing MSA methods based on contrastive
    learning lacks more detailed learning of the distribution of sample pairs with
    different sentiment intensity differences in the contrastive learning representation
    space. In addition, limited research has been conducted on the fusion of each
    modality representation obtained by contrastive learning training.In this paper,
    we propose a novel framework for multimodal sentiment analysis based on Contrastive
    Learning Guided by Sentiment Intensity (CLGSI). Firstly, the proposed contrastive
    learning guided by sentiment intensity selects positive and negative sample pairs
    based on the difference in sentiment intensity and assigns corresponding weights
    accordingly.Subsequently, we propose a new multimodal representation fusion mechanism,
    called Global-Local-Fine-Knowledge (GLFK), which extracts common features between
    different modalities' representations. At the same time, each unimodal encoder
    output is separately processed by a Multilayer Perceptron (MLP) to extract specific
    features of each modality. Finally, joint learning of the common and specific
    features is used to predict sentiment intensity. The effectiveness of CLGSI is
    assessed on two English datasets, MOSI and MOSEI, as well as one Chinese dataset,
    SIMS. We achieve competitive experimental results, which attest to the strong
    generalization performance of our approach. The code for our approach will be
    released in https://github.com/AZYoung233/CLGSI
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Sentiment Analysis, Stylistic Analysis, and Argument Mining
  authors:
  - emails: '****@mail.scut.edu.cn'
    first_name: Yang
    homepage: https://github.com/AZYoung233
    last_name: Yang
    name: Yang Yang
    username: ~Yang_Yang67
  - emails: '****@scut.edu.cn'
    first_name: Xunde
    homepage: https://yanzhao.scut.edu.cn/open/ExpertInfo.aspx?zjbh=ngYfGNMTwMEvAqET7FVqsQ==
    institution: South China University of Technology
    last_name: Dong
    name: Xunde Dong
    username: ~Xunde_Dong1
  - emails: '****@mail.scut.edu.cn'
    first_name: Yupeng
    last_name: Qiang
    name: Yupeng Qiang
    orcid: https://orcid.org/0009-0003-3908-4328
    username: ~Yupeng_Qiang1
  decision: toFindings
  end_page: 2110
  file: 558.pdf
  id: 558
  num_pages: 12
  openreview_id: Uobwqw4QNw
  pdf_file: 0754e8ce8308f8558120eb9fbf2a440404a43f03.pdf
  start_page: 2099
  title: 'CLGSI: A Multimodal Sentiment Analysis Framework based on Contrastive Learning
    Guided by Sentiment Intensity'
- abstract: 'People often answer yes-no questions without explicitly saying yes, no,
    or similar polar key-words. Figuring out the meaning of indirect

    answers is challenging, even for large language models. In this paper, we investigate
    this problem working with dialogues from multiple domains. We present new benchmarks
    in three diverse domains: movie scripts, tennis interviews, and airline customer
    service. We present an approach grounded on distant supervision and blended training
    to quickly adapt to a new dialogue domain. Experimental results show that our
    approach is never detrimental and yields F1 improvements as high as 11-34%.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Question Answering
  authors:
  - emails: '****@arizona.edu'
    first_name: Zijie
    google_scholar_id: https://scholar.google.com/citations?user=g6wwJ6YAAAAJ&hl=en&oi=ao
    homepage: https://wang-zijie.github.io/
    last_name: Wang
    name: Zijie Wang
    orcid: https://orcid.org/0000-0002-4448-7151
    semantic_scholar_id: https://www.semanticscholar.org/author/Zijie-Wang/2117423776
    username: ~Zijie_Wang3
  - dblp_id: https://dblp.org/pid/205/9036
    emails: '****@unca.edu'
    first_name: Farzana
    google_scholar_id: https://scholar.google.com/citations?user=j6dP4yEAAAAJ&hl=en
    homepage: https://www.cs.unca.edu/~frashid
    last_name: Rashid
    name: Farzana Rashid
    username: ~Farzana_Rashid1
  - dblp_id: https://dblp.org/pid/32/369-2
    emails: '****@arizona.edu'
    first_name: Eduardo
    google_scholar_id: https://scholar.google.com/citations?user=AqGa3-MAAAAJ&hl=en
    homepage: https://eduardoblanco.github.io/
    institution: University of Arizona
    last_name: Blanco
    name: Eduardo Blanco
    semantic_scholar_id: https://www.semanticscholar.org/author/Eduardo-Blanco/145186180
    username: ~Eduardo_Blanco1
  decision: toFindings
  end_page: 2128
  file: 561.pdf
  id: 561
  num_pages: 18
  openreview_id: ZQhWq8JZLk
  pdf_file: 0b85280ebf2b7f8c67378c51f7db0f82e2538be0.pdf
  start_page: 2111
  title: Interpreting Answers to Yes-No Questions in Dialogues from Multiple Domains
- abstract: "We introduce \\textsc{Enhancing Perception}, a framework for Large Language\
    \ Models (LLMs) designed to streamline the time-intensive task typically undertaken\
    \ by professional fact-checkers of crafting explanations for fake news. This study\
    \ investigates the effectiveness of enhancing LLM explanations through conversational\
    \ refinement. We compare various questioner agents, including state-of-the-art\
    \ LLMs like GPT-4, Claude 2, PaLM 2, and 193 American participants acting as human\
    \ questioners. Based on the histories of these refinement conversations, we further\
    \ generate comprehensive summary explanations. We evaluated the effectiveness\
    \ of these initial, refined, and summary explanations across 40 news claims by\
    \ involving 2,797 American participants, measuring their self-reported belief\
    \ change regarding both real and fake claims after receiving the explanations.\
    \ Our findings reveal that, in the context of fake news, explanations that have\
    \ undergone conversational refinement\u2014whether by GPT-4 or human questioners,\
    \ who ask more diverse and detail-oriented questions\u2014were significantly more\
    \ effective than both the initial unrefined explanations and the summary explanations.\
    \ Moreover, these refined explanations achieved a level of effectiveness comparable\
    \ to that of expert-written explanations. The results highlight the potential\
    \ of automatic explanation refinement by LLMs in debunking fake news claims."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - emails: '****@gmail.com'
    first_name: Yi-Li
    homepage: https://www.linkedin.com/in/%E4%BA%A6%E7%AB%8B-%E5%BE%90-626a9b88
    institution: National Tsinghua University and Academia Sinica
    last_name: Hsu
    name: Yi-Li Hsu
    username: ~Yi-Li_Hsu1
  - emails: '****@gmail.com'
    first_name: Jui-Ning
    institution: National Taiwan University and , Academia Sinica
    last_name: Chen
    name: Jui-Ning Chen
    username: ~Jui-Ning_Chen1
  - emails: '****@gs.ncku.edu.tw'
    first_name: Yang
    institution: Academia Sinica
    last_name: Fan Chiang
    name: Yang Fan Chiang
    username: ~Yang_Fan_Chiang1
  - emails: '****@gmail.com'
    first_name: Shang-Chien
    last_name: Liu
    name: Shang-Chien Liu
    semantic_scholar_id: https://www.semanticscholar.org/author/Shang-Chien-Liu/2257645855
    username: ~Shang-Chien_Liu1
  - emails: '****@psu.edu'
    first_name: Aiping
    homepage: https://faculty.ist.psu.edu/axx29/
    institution: Pennsylvania State University
    last_name: Xiong
    name: Aiping Xiong
    username: ~Aiping_Xiong1
  - dblp_id: https://dblp.org/pid/82/2054
    emails: '****@iis.sinica.edu.tw'
    first_name: Lun-Wei
    google_scholar_id: https://scholar.google.com/citations?user=SzcLXlkAAAAJ&hl=en
    homepage: http://www.lunweiku.com/
    institution: Academia Sinica
    last_name: Ku
    name: Lun-Wei Ku
    orcid: https://orcid.org/0000-0003-2691-5404
    semantic_scholar_id: https://www.semanticscholar.org/author/Lun-Wei-Ku/1746959
    username: ~Lun-Wei_Ku1
  decision: toFindings
  end_page: 2147
  file: 565.pdf
  id: 565
  num_pages: 19
  openreview_id: IxgSf1tPaW
  pdf_file: 732b8bb718ff96cd8b0fe98011510f1025557eb5.pdf
  start_page: 2129
  title: 'Enhancing Perception: Refining Explanations of News Claims with LLM Conversations'
- abstract: Prompt Engineering has garnered significant attention for enhancing the
    performance of large language models across a multitude of tasks. Techniques such
    as the Chain-of-Thought not only bolster task performance but also delineate a
    clear trajectory of reasoning steps, offering a tangible form of explanation for
    the audience. Prior works on interpretability assess the reasoning chains yielded
    by Chain-of-Thought solely along a singular axis, namely faithfulness. We present
    a comprehensive and multifaceted evaluation of interpretability, examining not
    only faithfulness but also robustness and utility across multiple commonsense
    reasoning benchmarks. Likewise, our investigation is not confined to a single
    prompting technique; it expansively covers a multitude of prevalent prompting
    techniques employed in large language models, thereby ensuring a wide-ranging
    and exhaustive evaluation. In addition, we introduce a simple interpretability
    alignment technique, termed Self-Entailment-Alignment Chain-of-thought, that yields
    more than 70\% improvements across multiple dimensions of interpretability. Code
    is available at https://github.com/SenticNet/CoT_interpretability
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - emails: '****@e.ntu.edu.sg'
    first_name: Yeo
    google_scholar_id: https://scholar.google.com/citations?user=DcUMc_IAAAAJ&hl=en&inst=10972715779114120479
    institution: School of Computer Science and  Engineering, Nanyang Technological
      University
    last_name: Wei Jie
    name: Yeo Wei Jie
    semantic_scholar_id: https://www.semanticscholar.org/author/Wei-Jie-Yeo/2243375066
    username: ~Yeo_Wei_Jie1
  - dblp_id: https://dblp.org/pid/184/2181
    emails: '****@ihpc.a-star.edu.sg'
    first_name: Ranjan
    google_scholar_id: https://scholar.google.com.sg/citations?user=-eQVwhEAAAAJ&hl=en
    last_name: Satapathy
    name: Ranjan Satapathy
    username: ~Ranjan_Satapathy1
  - dblp_id: https://dblp.uni-trier.de/pers/g/Goh:Rick_Siow_Mong
    emails: '****@ihpc.a-star.edu.sg'
    first_name: Rick
    google_scholar_id: https://scholar.google.com.sg/citations?user=fBsBJjoAAAAJ&hl=en
    homepage: https://sites.google.com/view/rickgoh/home
    institution: Institute of High Performance Computing, Singapore, A*STAR
    last_name: Goh
    middle_name: Siow Mong
    name: Rick Siow Mong Goh
    orcid: https://orcid.org/0000-0001-9116-1595
    username: ~Rick_Siow_Mong_Goh1
  - dblp_id: https://dblp.org/pid/80/7421
    emails: '****@sentic.net'
    first_name: Erik
    google_scholar_id: https://scholar.google.com/citations?user=ilSYpW0AAAAJ
    homepage: https://sentic.net/erikcambria/
    institution: Nanyang Technological University
    last_name: Cambria
    name: Erik Cambria
    orcid: https://orcid.org/0000-0002-3030-1280
    semantic_scholar_id: https://www.semanticscholar.org/author/E.-Cambria/49943757
    username: ~Erik_Cambria1
  decision: toFindings
  end_page: 2164
  file: 566.pdf
  id: 566
  num_pages: 17
  openreview_id: D44kz1ZCFH
  pdf_file: 18a8a53e80a2113cd8d8611e1318837dff066b2a.pdf
  start_page: 2148
  title: How Interpretable are Reasoning Explanations from Prompting Large Language
    Models?
- abstract: Large-scale pre-trained language models have displayed unrivaled capacity
    in generating text that closely resembles human-written text. Nevertheless, generating
    texts adhering to specific conditions without fine-tuning or adding new parameters
    can be challenging. Contemporary approaches commonly rely on either prompts or
    auxiliary models to avoid modifying the language models. These auxiliary models
    are designed to assess whether a generated token contributes to meeting the desired
    requirements. These approaches adjust the distribution of the next token during
    the inference phase by leveraging the prediction score of the desired attribute
    to calculate gradients. However, these auxiliary models typically require the
    language model's latent states. This prerequisite challenges integrating various
    existing black box attribute models or tools. We present the Plug-in Language
    Model (PiLM) as a solution to address the limitations. PiLM leverages reinforcement
    learning to utilize black box tools directly, adjusting the latent state to control
    text generation. However, performing backpropagation during the inference phase
    is time-consuming for PiLM. By replacing backpropagation with a simple regression
    model, PiLM can achieve an inference time comparable to that of the original LLM.
    Experiment results show that our approaches in this paper outperform existing
    state-of-the-art methods that rely on gradient-based, weighted decoding, or prompt-based
    methodologies.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Generation
  authors:
  - emails: '****@iis.sinica.edu.tw'
    first_name: Nai-Chi
    last_name: Yang
    name: Nai-Chi Yang
    username: ~Nai-Chi_Yang1
  - dblp_id: https://dblp.org/pid/72/4128
    emails: '****@iis.sinica.edu.tw'
    first_name: Wei-Yun
    google_scholar_id: https://scholar.google.com.tw/citations?user=AHG3DncAAAAJ&hl=zh-TW
    homepage: https://homepage.iis.sinica.edu.tw/pages/ma/index_en.html
    institution: Academia Sinica
    last_name: Ma
    name: Wei-Yun Ma
    username: ~Wei-Yun_Ma1
  - dblp_id: https://dblp.org/pid/45/160.html
    emails: '****@csie.ntu.edu.tw'
    first_name: Pu-Jen
    google_scholar_id: https://scholar.google.com.tw/citations?user=uYdM_rwAAAAJ
    homepage: https://www.csie.ntu.edu.tw/~pjcheng/
    institution: National Taiwan University
    last_name: Cheng
    name: Pu-Jen Cheng
    semantic_scholar_id: https://www.semanticscholar.org/author/Pu-Jen-Cheng/1712392
    username: ~Pu-Jen_Cheng1
  decision: toFindings
  end_page: 2181
  file: 569.pdf
  id: 569
  num_pages: 17
  openreview_id: 7dA20ZRo4P
  pdf_file: b23e2344aa9b8799713fc05b9707fcf38b9faa37.pdf
  start_page: 2165
  title: 'Plug-in Language Model: Controlling Text Generation with a Simple Regression
    Model'
- abstract: "The primary objective of sign language translation (SLT) is to transform\
    \ sign language videos into natural sentences.\nA crucial challenge in this field\
    \ is developing signer-independent SLT systems which requires models to generalize\
    \ effectively to signers not encountered during training.\nThis challenge is exacerbated\
    \ by the limited diversity of signers in existing SLT datasets, which often results\
    \ in suboptimal generalization capabilities of current models.\nAchieving robustness\
    \ to unseen signers is essential for signer-independent SLT.\nHowever, most existing\
    \ method relies on signer identity labels, which is often impractical and costly\
    \ in real-world applications.\nTo address this issue, we propose the Signer Diversity-driven\
    \ Data Augmentation (SDDA) method that can achieve good generalization without\
    \ relying on signer identity labels. SDDA comprises two data augmentation schemes.\
    \ The first is data augmentation based on adversarial training, which aims to\
    \ utilize the gradients of the model to generate adversarial examples. The second\
    \ is data augmentation based on diffusion model, which focuses on using the advanced\
    \ diffusion-based text guided image editing method to modify the appearances of\
    \ the signer in images. \nThe combination of the two strategies significantly\
    \ enriches the diversity of signers in the training process.\nMoreover, we introduce\
    \ a consistency loss and a discrimination loss to enhance the learning of signer-independent\
    \ features.\nOur experimental results demonstrate our model significantly enhances\
    \ the performance of SLT in the signer-independent setting, achieving state-of-the-art\
    \ results without relying on signer identity labels."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multimodality and Language Grounding to Vision, Robotics and Beyond
  authors:
  - emails: '****@stu.xmu.edu.cn'
    first_name: Honghaofu
    homepage: https://github.com/intereter
    last_name: Honghaofu
    name: honghaofu
    username: ~honghaofu1
  - emails: '****@stu.xmu.edu.cn'
    first_name: Liang
    homepage: https://blog.csdn.net/ACM_hades
    last_name: Zhang
    name: Liang Zhang
    username: ~Liang_Zhang9
  - dblp_id: https://dblp.org/pid/144/8117
    emails: '****@stu.xmu.edu.cn'
    first_name: Biao
    last_name: Fu
    name: Biao Fu
    semantic_scholar_id: https://www.semanticscholar.org/author/Biao-Fu/2135548142
    username: ~Biao_Fu1
  - emails: '****@stu.xmu.edu.cn'
    first_name: Rui
    homepage: https://github.com/rzhao-zhsq
    last_name: Zhao
    name: Rui Zhao
    username: ~Rui_Zhao17
  - dblp_id: https://dblp.org/pid/05/9013
    emails: '****@xmu.edu.cn'
    first_name: Jinsong
    homepage: https://cdmc.xmu.edu.cn/info/1010/1054.htm
    institution: Xiamen University
    last_name: Su
    name: Jinsong Su
    username: ~Jinsong_Su1
  - dblp_id: https://dblp.org/pid/73/5055
    emails: '****@xmu.edu.cn'
    first_name: Xiaodong
    institution: Xiamen University, Tsinghua University
    last_name: Shi
    name: Xiaodong Shi
    username: ~Xiaodong_Shi2
  - dblp_id: https://dblp.org/pid/11/1492
    emails: '****@xmu.edu.cn'
    first_name: Yidong
    homepage: http://nlp.xmu.edu.cn/teachers/ydchen/index_en.html
    last_name: Chen
    name: Yidong Chen
    semantic_scholar_id: https://www.semanticscholar.org/author/Yidong-Chen/47558200
    username: ~Yidong_Chen2
  decision: toFindings
  end_page: 2193
  file: 572.pdf
  id: 572
  num_pages: 12
  openreview_id: BFBE1mQFk4
  pdf_file: 0cedbcbc7f1d79d79c85a1ef803899b227719c95.pdf
  start_page: 2182
  title: Signer Diversity-driven Data Augmentation for Signer-Independent Sign Language
    Translation
- abstract: Multilingual modelling can improve machine translation for low-resource
    languages, partly through shared subword representations. This paper studies the
    role of subword segmentation in cross-lingual transfer. We systematically compare
    the efficacy of several subword methods in promoting synergy and preventing interference
    across different linguistic typologies. Our findings show that subword regularisation
    boosts synergy in multilingual modelling, whereas BPE more effectively facilitates
    transfer during cross-lingual fine-tuning. Notably, our results suggest that differences
    in orthographic word boundary conventions (the morphological granularity of written
    words) may impede cross-lingual transfer more significantly than linguistic unrelatedness.
    Our study confirms that decisions around subword modelling can be key to optimising
    the benefits of multilingual modelling.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Phonology, Morphology and Word Segmentation
  authors:
  - dblp_id: https://dblp.uni-trier.de/pid/231/5100
    emails: '****@gmail.com'
    first_name: Francois
    google_scholar_id: https://scholar.google.com/citations?user=fIipSg0AAAAJ&hl=en
    homepage: https://francois-meyer.github.io/
    institution: University of Cape Town
    last_name: Meyer
    name: Francois Meyer
    username: ~Francois_Meyer2
  - dblp_id: https://dblp.org/pid/157/2104
    emails: '****@cs.uct.ac.za'
    first_name: Jan
    google_scholar_id: https://scholar.google.co.uk/citations?user=TlqDbGYAAAAJ
    homepage: http://www.janmbuys.com
    institution: University of Cape Town
    last_name: Buys
    name: Jan Buys
    orcid: https://orcid.org/0000-0003-1994-5832
    semantic_scholar_id: https://www.semanticscholar.org/author/Jan-Buys/144685020
    username: ~Jan_Buys1
  decision: toFindings
  end_page: 2200
  file: 573.pdf
  id: 573
  num_pages: 7
  openreview_id: 4KPqVCQKjP
  pdf_file: e82011b9e93b5d0d09b2d43394bc773c552f423b.pdf
  start_page: 2194
  title: A Systematic Analysis of Subwords and Cross-Lingual Transfer in Multilingual
    Translation
- abstract: In Open-domain Question Answering (ODQA), it is essential to discern relevant
    contexts as evidence and avoid spurious ones among retrieved results. The model
    architecture that uses concatenated multiple contexts in the decoding phase, *i.e.*,
    Fusion-in-Decoder, demonstrates promising performance but generates incorrect
    outputs from seemingly plausible contexts. To address this problem, we propose
    the ***M**ulti-**G**ranularity guided **F**usion-**i**n-**D**ecoder (**MGFiD**)*,
    discerning evidence across multiple levels of granularity. Based on multi-task
    learning, MGFiD harmonizes passage re-ranking with sentence classification. It
    aggregates evident sentences into an *anchor vector* that instructs the decoder.
    Additionally, it improves decoding efficiency by reusing the results of passage
    re-ranking for *passage pruning*. Through our experiments, MGFiD outperforms existing
    models on the Natural Questions (NQ) and TriviaQA (TQA) datasets, highlighting
    the benefits of its multi-granularity solution.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Question Answering
  authors:
  - emails: '****@skku.edu'
    first_name: Eunseong
    homepage: https://github.com/eunseongc
    last_name: Choi
    name: Eunseong Choi
    orcid: https://orcid.org/0000-0003-1400-5227
    username: ~Eunseong_Choi1
  - emails: '****@g.skku.edu'
    first_name: Hyeri
    homepage: https://github.com/Hr0803
    last_name: Lee
    name: Hyeri Lee
    username: ~Hyeri_Lee1
  - dblp_id: https://dblp.org/pid/04/3445
    emails: '****@skku.edu'
    first_name: Jongwuk
    institution: Sungkyunkwan University
    last_name: Lee
    name: Jongwuk Lee
    username: ~Jongwuk_Lee1
  decision: toFindings
  end_page: 2212
  file: 574.pdf
  id: 574
  num_pages: 12
  openreview_id: 3Du0HSe8P6
  pdf_file: 6212fc551e75ba640741039e1c0717de24d4657b.pdf
  start_page: 2201
  title: Multi-Granularity Guided Fusion-in-Decoder
- abstract: We evaluate the performance disparity of the Whisper and MMS families
    of ASR models across the VoxPopuli and Common Voice multilingual datasets, with
    an eye toward intersectionality. Our two most important findings are that model
    size, surprisingly, correlates logarithmically with worst-case performance disparities,
    meaning that larger (and better) models are less fair. We also observe the importance
    of intersectionality. In particular, models often exhibit significant performance
    disparity across binary gender for adolescents.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Ethics, Bias, and Fairness
  authors:
  - dblp_id: https://dblp.org/pid/178/3552
    emails: '****@di.ku.dk'
    first_name: Anna
    google_scholar_id: https://scholar.google.dk/citations?user=scpEAKwAAAAJ&hl=en&oi=ao
    last_name: Zee
    middle_name: Katrine Van
    name: Anna Katrine van Zee
    orcid: https://orcid.org/0009-0004-9027-9304
    semantic_scholar_id: "https://www.semanticscholar.org/author/Anna-J\xF8rgensen/2064903618"
    username: ~Anna_Katrine_van_Zee1
  - dblp_id: https://dblp.org/pid/133/1757
    emails: '****@google.com'
    first_name: Marc
    google_scholar_id: https://scholar.google.lu/citations?user=OPZa8z4AAAAJ&hl=en
    homepage: http://www.marcvanzee.nl
    institution: Research, Google
    last_name: Zee
    middle_name: Van
    name: Marc van Zee
    username: ~Marc_van_Zee1
  - dblp_id: https://dblp.org/pid/30/2756
    emails: '****@di.ku.dk'
    first_name: Anders
    google_scholar_id: https://scholar.google.com.tw/citations?user=x3I4CrYAAAAJ
    homepage: https://anderssoegaard.github.io/
    institution: Copenhagen University
    last_name: "S\xF8gaard"
    name: "Anders S\xF8gaard"
    username: "~Anders_S\xF8gaard1"
  decision: toFindings
  end_page: 2226
  file: 575.pdf
  id: 575
  num_pages: 14
  openreview_id: VaaxMPQXQQ
  pdf_file: a1b415b3b2e4c54fbb529b6bcee8ab4ecbe23bb3.pdf
  start_page: 2213
  title: Group Fairness in Multilingual Speech Recognition Models
- abstract: Making moral judgments is an essential step toward developing ethical
    AI systems. Prevalent approaches are mostly implemented in a bottom-up manner,
    which uses a large set of annotated data to train models based on crowd-sourced
    opinions about morality. These approaches have been criticized for potentially
    overgeneralizing a limited group of annotators' moral stances and lacking explainability.  This
    work proposes a flexible top-down framework to steer (Large) Language Models to
    perform moral reasoning with well-established moral theories from interdisciplinary
    research. The theory-guided top-down framework can incorporate various moral theories.
    Our experiments demonstrate the effectiveness of the proposed framework on datasets
    derived from moral theories.  Furthermore, we show the alignment between different
    moral theories and existing morality datasets. Our analysis exhibits the potential
    and flaws in existing resources (models and datasets) in developing explainable
    moral judgment-making systems.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Ethics, Bias, and Fairness
  authors:
  - emails: '****@se.cuhk.edu.hk'
    first_name: Jingyan
    google_scholar_id: https://scholar.google.com/citations?user=5U2DBhUAAAAJ&hl=en&oi=ao
    last_name: Zhou
    name: Jingyan Zhou
    username: ~Jingyan_Zhou1
  - dblp_id: https://dblp.org/pid/260/5462.html
    emails: '****@link.cuhk.edu.hk'
    first_name: Minda
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=uQlkNn8AAAAJ
    last_name: Hu
    name: Minda Hu
    orcid: https://orcid.org/0000-0003-1048-1998
    username: ~Minda_Hu1
  - emails: '****@se.cuhk.edu.hk'
    first_name: Junan
    homepage: https://tommylja.wordpress.com/
    last_name: Li
    name: Junan Li
    username: ~Junan_Li1
  - emails: '****@se.cuhk.edu.hk'
    first_name: Xiaoying
    institution: The Chinese University of Hong Kong
    last_name: Zhang
    name: Xiaoying Zhang
    semantic_scholar_id: https://www.semanticscholar.org/author/Xiaoying-Zhang/2109108262
    username: ~Xiaoying_Zhang2
  - emails: '****@se.cuhk.edu.hk'
    first_name: Xixin
    homepage: https://www1.se.cuhk.edu.hk/~wuxx/
    institution: The Chinese University of Hong Kong
    last_name: Wu
    name: Xixin Wu
    username: ~Xixin_Wu1
  - dblp_id: https://dblp.uni-trier.de/pers/hd/k/King:Irwin
    emails: '****@cse.cuhk.edu.hk'
    first_name: Irwin
    google_scholar_id: https://scholar.google.com/citations?user=MXvC7tkAAAAJ&hl=en
    homepage: https://www.cse.cuhk.edu.hk/irwin.king/
    institution: The Chinese University of Hong Kong
    last_name: King
    name: Irwin King
    orcid: https://orcid.org/0000-0001-8106-6447
    username: ~Irwin_King1
  - dblp_id: https://dblp.org/pid/92/3270
    emails: '****@se.cuhk.edu.hk'
    first_name: Helen
    homepage: http://www.se.cuhk.edu.hk/people/academic-staff/prof-meng-mei-ling-helen/
    institution: The Chinese University of Hong Kong
    last_name: Meng
    middle_name: M.
    name: Helen M. Meng
    username: ~Helen_M._Meng1
  decision: toFindings
  end_page: 2242
  file: 576.pdf
  id: 576
  num_pages: 16
  openreview_id: mbtz7y4JMx
  pdf_file: 859e3d8ee0f97bf8be5c1e5ebb3094e5a9f0a8f5.pdf
  start_page: 2227
  title: Rethinking Machine Ethics -- Can LLMs Perform Moral Reasoning through the
    Lens of Moral Theories?
- abstract: 'The growing interest in Large Language Models (LLMs) for specialized
    applications has revealed a significant challenge: when tailored to specific domains,
    LLMs tend to experience catastrophic forgetting, compromising their general capabilities
    and leading to a suboptimal user experience. Additionally, crafting a versatile
    model for multiple domains simultaneously often results in a decline in overall
    performance due to confusion between domains. In response to these issues, we
    present the RolE Prompting Guided Multi-Domain Adaptation (REGA) strategy. This
    novel approach effectively manages multi-domain LLM adaptation through three key
    components: 1) Self-Distillation constructs and replays general-domain exemplars
    to alleviate catastrophic forgetting. 2) Role Prompting assigns a central prompt
    to the general domain and a unique role prompt to each specific domain to minimize
    inter-domain confusion during training. 3) Role Integration reuses and integrates
    a small portion of domain-specific data to the general-domain data, which are
    trained under the guidance of the central prompt. The central prompt is used for
    a streamlined inference process, removing the necessity to switch prompts for
    different domains.

    Empirical results demonstrate that REGA effectively alleviates catastrophic forgetting
    and inter-domain confusion. This leads to improved domain-specific performance
    compared to standard fine-tuned models, while still preserving robust general
    capabilities.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - dblp_id: https://dblp.org/pid/06/2293-92
    emails: '****@outlook.com'
    first_name: Rui
    google_scholar_id: https://scholar.google.com/citations?view_op=list_works&hl=zh-CN&user=nTPdz2sAAAAJ
    last_name: Wang
    name: Rui Wang
    semantic_scholar_id: https://www.semanticscholar.org/author/Rui-Wang/2151036536
    username: ~Rui_Wang30
  - dblp_id: https://dblp.org/pid/161/0068
    emails: '****@huawei.com'
    first_name: Fei
    google_scholar_id: https://scholar.google.com/citations?user=gX3493QAAAAJ&hl
    homepage: https://mifei.github.io/
    last_name: Mi
    name: Fei Mi
    semantic_scholar_id: https://www.semanticscholar.org/author/Fei-Mi/33727421
    username: ~Fei_Mi1
  - dblp_id: https://dblp.org/pid/49/6574-19
    emails: '****@gmail.com'
    first_name: Yi
    last_name: Chen
    name: Yi Chen
    semantic_scholar_id: https://www.semanticscholar.org/author/Yi-Chen/2165302640
    username: ~Yi_Chen9
  - emails: '****@se.cuhk.edu.hk'
    first_name: Boyang
    google_scholar_id: https://scholar.google.com/citations?user=S0BbF6wAAAAJ&hl=zh-CN&oi=ao
    homepage: https://amourwaltz.github.io/
    last_name: Xue
    name: Boyang XUE
    username: ~Boyang_XUE1
  - dblp_id: https://dblp.org/pid/72/1462-3.html
    emails: '****@se.cuhk.edu.hk'
    first_name: Hongru
    google_scholar_id: https://scholar.google.com/citations?user=s6UtVYUAAAAJ&hl=en
    homepage: https://rulegreen.github.io/
    last_name: Wang
    name: Hongru WANG
    orcid: https://orcid.org/0000-0001-5027-0138
    semantic_scholar_id: https://www.semanticscholar.org/author/Hongru-Wang/22642319
    username: ~Hongru_WANG1
  - dblp_id: https://dblp.org/pid/66/5923-7
    emails: '****@gmail.com'
    first_name: Qi
    google_scholar_id: https://scholar.google.com/citations?user=sNpTNo8AAAAJ
    last_name: Zhu
    name: Qi Zhu
    semantic_scholar_id: https://www.semanticscholar.org/author/Qi-Zhu/144400493
    username: ~Qi_Zhu8
  - dblp_id: https://dblp.org/pid/w/KamFaiWong
    emails: '****@se.cuhk.edu.hk'
    first_name: Kam-Fai
    homepage: http://www.se.cuhk.edu.hk/~kfwong
    institution: The Chinese University of Hong Kong
    last_name: Wong
    name: Kam-Fai Wong
    orcid: https://orcid.org/0000-0002-9427-5659
    username: ~Kam-Fai_Wong2
  - dblp_id: https://dblp.org/pid/93/5407
    emails: '****@hit.edu.cn'
    first_name: Ruifeng
    homepage: http://faculty.hitsz.edu.cn/xuruifeng
    institution: Harbin Institute of Technology
    last_name: Xu
    name: Ruifeng Xu
    semantic_scholar_id: https://www.semanticscholar.org/author/Ruifeng-Xu/1753529
    username: ~Ruifeng_Xu1
  decision: toFindings
  end_page: 2255
  file: 579.pdf
  id: 579
  num_pages: 13
  openreview_id: wBK5FbR58G
  pdf_file: a965057c4c0efb92d3a76684fbec75efabc69f78.pdf
  start_page: 2243
  title: Role Prompting Guided Domain Adaptation with General Capability Preserve
    for Large Language Models
- abstract: 'Argument mining, dealing with the classification of text based on inference
    and information, denotes a challenging analytical task in the rich context of
    Twitter (now $\mathbb{X}$), a key platform for online discourse and exchange.
    Thereby, Twitter offers a diverse repository of short messages bearing on both
    of these elements. For text classification, transformer approaches, particularly
    BERT, offer state-of-the-art solutions. Our study delves into optimizing the embeddings
    of the understudied BERTweet transformer for argument mining on Twitter and broader
    generalization across topics.

    We explore the impact of pre-classification fine-tuning by aligning similar manifestations
    of inference and information while contrasting dissimilar instances. Using the
    TACO dataset, our approach augments tweets for optimizing BERTweet in a Siamese
    network, strongly improving classification and cross-topic generalization compared
    to standard methods.

    Overall, we contribute the transformer WRAPresentations and classifier WRAP, scoring
    86.62\% F1 for inference detection, 86.30\% for information recognition, and 75.29\%
    across four combinations of these elements, to enhance inference and information-driven
    argument mining on Twitter.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Sentiment Analysis, Stylistic Analysis, and Argument Mining
  authors:
  - emails: '****@uni-duesseldorf.de'
    first_name: Marc
    last_name: Feger
    name: Marc Feger
    orcid: https://orcid.org/0009-0008-6385-1722
    username: ~Marc_Feger1
  - dblp_id: https://dblp.org/pid/25/5167
    emails: '****@l3s.de'
    first_name: Stefan
    google_scholar_id: https://scholar.google.de/citations?user=WR3U5SkAAAAJ&hl=en
    homepage: http://stefandietze.net
    institution: "GESIS  and Heinrich-Heine-University D\xFCsseldorf"
    last_name: Dietze
    name: Stefan Dietze
    username: ~Stefan_Dietze1
  decision: toFindings
  end_page: 2266
  file: 582.pdf
  id: 582
  num_pages: 11
  openreview_id: N4v8cqs9L4
  pdf_file: 36e1c448170e37a04847a9e2c6c6ac7a66d50dde.pdf
  start_page: 2256
  title: 'BERTweet''s TACO Fiesta: Contrasting Flavors On The Path Of Inference And
    Information-Driven Argument Mining On Twitter'
- abstract: We study the ability of neural and hybrid models to generalize logical
    reasoning patterns. We created a series of tests for analyzing various aspects
    of generalization in the context of language and reasoning, focusing on compositionality
    and recursiveness. We used them to study the syllogistic logic in hybrid models,
    where the network assists in premise selection. We analyzed feed-forward, recurrent,
    convolutional, and transformer architectures. Our experiments demonstrate that
    even though the models can capture elementary aspects of the meaning of logical
    terms, they learn to generalize logical reasoning only to a limited degree.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - emails: '****@uw.edu.pl'
    first_name: Manuel
    homepage: https://orcid.org/0000-0002-8195-6201
    last_name: Guzman
    middle_name: Vargas
    name: Manuel Vargas Guzman
    username: ~Manuel_Vargas_Guzman1
  - dblp_id: https://dblp.org/pid/40/4738
    emails: '****@unitn.it'
    first_name: Jakub
    homepage: https://jakubszymanik.com/
    institution: University of Trento
    last_name: Szymanik
    name: Jakub Szymanik
    semantic_scholar_id: https://www.semanticscholar.org/author/Jakub-Szymanik/2325493
    username: ~Jakub_Szymanik1
  - emails: '****@gmail.com'
    first_name: Maciej
    google_scholar_id: https://scholar.google.com/citations?user=iJVwj7oAAAAJ&hl=pl
    homepage: https://www.impan.pl/~malicki/
    last_name: Malicki
    name: Maciej Malicki
    username: ~Maciej_Malicki2
  decision: toFindings
  end_page: 2279
  file: 583.pdf
  id: 583
  num_pages: 13
  openreview_id: CSBO1LzI4g
  pdf_file: a4c9b03a351f2514823ad72deac36e0270f4b3e9.pdf
  start_page: 2267
  title: Testing the limits of logical reasoning in neural and hybrid models
- abstract: With the rising human-like precision of Large Language Models (LLMs) in
    numerous tasks, their utilization in a variety of real-world applications is becoming
    more prevalent. Several studies have shown that LLMs excel on many standard NLP
    benchmarks. However, it is challenging to evaluate LLMs due to test dataset  contamination
    and the limitations of traditional metrics. Since human evaluations are difficult
    to collect, there is a growing interest in the community to use LLMs themselves
    as reference-free evaluators for subjective metrics. However, past work has shown
    that LLM-based evaluators can exhibit bias and have poor alignment with human
    judgments. In this study, we propose a framework for an end-to-end assessment
    of LLMs as evaluators in multilingual scenarios. We create a carefully curated
    dataset, covering 10 languages containing native speaker judgments for the task
    of summarization. This dataset is created specifically to evaluate LLM-based evaluators,
    which we refer to as meta-evaluation (METAL). We compare the performance of LLM-based
    evaluators created using GPT-3.5-Turbo, GPT-4, and PaLM2. Our results indicate
    that LLM-based evaluators based on GPT-4 perform the best across languages, while
    GPT-3.5-Turbo performs poorly. Additionally, we perform an analysis of the reasoning
    provided by LLM-based evaluators and find that it often does not match the reasoning
    provided by human judges.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - emails: '****@gmail.com'
    first_name: Rishav
    google_scholar_id: https://scholar.google.com/citations?user=ctKGG_YAAAAJ&hl=en&oi=ao
    homepage: https://sites.google.com/view/rishavhada
    institution: Microsoft Research India
    last_name: Hada
    name: Rishav Hada
    username: ~Rishav_Hada1
  - dblp_id: https://dblp.org/pid/306/1527
    emails: '****@gmail.com'
    first_name: Varun
    google_scholar_id: https://scholar.google.com/citations?user=tqDhGbwAAAAJ&hl=en
    homepage: https://varungumma.github.io
    institution: Microsoft
    last_name: Gumma
    name: Varun Gumma
    orcid: https://orcid.org/0009-0002-5746-3017
    semantic_scholar_id: https://www.semanticscholar.org/author/Varun-Gumma/2140408530
    username: ~Varun_Gumma1
  - dblp_id: https://dblp.org/pid/49/4653-1
    emails: '****@microsoft.com'
    first_name: Mohamed
    google_scholar_id: https://scholar.google.com/citations?user=_any3jgAAAAJ&hl=en
    institution: Research, Microsoft
    last_name: Ahmed
    name: Mohamed Ahmed
    username: ~Mohamed_Ahmed1
  - dblp_id: https://dblp.org/pid/19/5717
    emails: '****@microsoft.com'
    first_name: Kalika
    google_scholar_id: https://scholar.google.com/citations?user=HSIGxEgAAAAJ&hl=en
    homepage: https://www.microsoft.com/en-us/research/people/kalikab/
    institution: Microsoft Research Labs
    last_name: Bali
    name: Kalika Bali
    semantic_scholar_id: https://www.semanticscholar.org/author/Kalika-Bali/3086996
    username: ~Kalika_Bali1
  - dblp_id: https://dblp.org/pid/27/7642
    emails: '****@microsoft.com'
    first_name: Sunayana
    google_scholar_id: https://scholar.google.com/citations?user=PUxwYrkAAAAJ&hl=en
    homepage: https://www.microsoft.com/en-us/research/people/susitara/
    institution: Microsoft
    last_name: Sitaram
    name: Sunayana Sitaram
    username: ~Sunayana_Sitaram1
  decision: toFindings
  end_page: 2298
  file: 585.pdf
  id: 585
  num_pages: 19
  openreview_id: lt3vTZTphc
  pdf_file: b94d41aad09cd72ea73c0ab04415ec041f432b03.pdf
  start_page: 2280
  title: 'METAL: Towards Multilingual Meta-Evaluation'
- abstract: 'Assessing foundation models'' abilities for human-level tasks is crucial
    for Artificial General Intelligence (AGI) development.

    Traditional benchmarks, which rely on artificial datasets, may not accurately
    represent these capabilities. In this paper, we introduce AGIEval, a novel bilingual
    benchmark designed to assess foundation models in the context of human-centric
    standardized exams, such as college entrance exams, law school admission tests,
    math competitions, and lawyer qualification tests.  We evaluate several state-of-the-art
    foundation models on our benchmark.  Impressively, we show that GPT-4 exceeds
    the average human performance in SAT, LSAT, and math contests, with 95% accuracy
    on SAT Math and 92.5\% on the Chinese college entrance English exam. This demonstrates
    the exceptional performance of contemporary foundation models.  In contrast, we
    also find that GPT-4 is less proficient in tasks requiring complex reasoning or
    specific domain knowledge. Our comprehensive analyses of model capabilities (understanding,
    knowledge, reasoning, and calculation) reveal their strengths and limitations,
    providing valuable insights into future directions for enhancing general capabilities.
    By concentrating on tasks pertinent to human cognition and decision-making, our
    benchmark delivers a meaningful and robust evaluation of foundation models'' performance
    in real-world scenarios.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - dblp_id: https://dblp.org/pid/227/2128.html
    emails: '****@mail2.sysu.edu.cn'
    first_name: Wanjun
    google_scholar_id: https://scholar.google.com/citations?hl=zh-CN&user=FGIZfyQAAAAJ
    homepage: http://zhongwanjun.github.io/
    last_name: Zhong
    name: Wanjun Zhong
    semantic_scholar_id: https://www.semanticscholar.org/author/Wanjun-Zhong/81970097
    username: ~Wanjun_Zhong1
  - dblp_id: https://dblp.org/pid/266/1443
    emails: '****@di.ku.dk'
    first_name: Ruixiang
    google_scholar_id: https://scholar.google.com/citations?user=3M-1U2wAAAAJ&hl=en
    homepage: https://ruixiangcui.github.io/
    last_name: Cui
    name: Ruixiang Cui
    orcid: https://orcid.org/0000-0002-7060-5940
    semantic_scholar_id: https://www.semanticscholar.org/author/Ruixiang-Cui/1717462692
    username: ~Ruixiang_Cui1
  - dblp_id: https://dblp.org/pid/196/5954.html
    emails: '****@stu.pku.edu.cn'
    first_name: Yiduo
    google_scholar_id: https://scholar.google.com/citations?hl=zh-CN&user=ov-Cb2kAAAAJ
    homepage: https://github.com/gydpku
    last_name: Guo
    name: Yiduo Guo
    semantic_scholar_id: https://www.semanticscholar.org/author/Yiduo-Guo/2214448244
    username: ~Yiduo_Guo2
  - dblp_id: https://dblp.org/pid/245/8600.html
    emails: '****@gmail.com'
    first_name: Yaobo
    google_scholar_id: https://scholar.google.com/citations?user=z92gIuEAAAAJ&hl=en
    homepage: https://www.microsoft.com/en-us/research/people/yalia/
    last_name: Liang
    name: Yaobo Liang
    username: ~Yaobo_Liang1
  - emails: '****@microsoft.com'
    first_name: Shuai
    google_scholar_id: https://scholar.google.com/citations?user=GAokfukAAAAJ&hl=zh-CN
    institution: Microsoft
    last_name: Lu
    name: Shuai Lu
    username: ~Shuai_Lu1
  - dblp_id: https://dblp.org/pid/41/3458-1.html
    emails: '****@outlook.com'
    first_name: Yanlin
    google_scholar_id: https://scholar.google.com/citations?hl=zh-CN&user=xSCoImAAAAAJ
    homepage: https://yanlin.info/
    institution: Sun Yat-Sen University
    last_name: Wang
    name: Yanlin Wang
    username: ~Yanlin_Wang1
  - emails: '****@gmail.com'
    first_name: Amin
    last_name: Saied
    name: Amin Saied
    username: ~Amin_Saied1
  - dblp_id: https://dblp.org/pid/79/2536
    emails: '****@microsoft.com'
    first_name: Weizhu
    institution: Microsoft GenAI
    last_name: Chen
    name: Weizhu Chen
    username: ~Weizhu_Chen1
  - dblp_id: https://dblp.org/pid/30/8160
    emails: '****@microsoft.com'
    first_name: Nan
    google_scholar_id: https://scholar.google.com/citations?user=Qaa6OxIAAAAJ&hl=en
    homepage: https://nanduan.github.io/
    institution: Microsoft Research Asia
    last_name: Duan
    name: Nan Duan
    username: ~Nan_Duan1
  decision: toFindings
  end_page: 2314
  file: 586.pdf
  id: 586
  num_pages: 16
  openreview_id: iuszCa0OQw
  pdf_file: 93ae583f22bd47d2c1327f4bef4054ade9f306e6.pdf
  start_page: 2299
  title: 'AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models'
- abstract: "In e-commerce, opinion summarization is the process of summarizing the\
    \ consensus opinions found in product reviews. However, the potential of additional\
    \ sources such as product description and question-answers (QA) has been considered\
    \ less often. Moreover, the absence of any supervised training data makes this\
    \ task challenging. To address this, we propose a novel synthetic dataset creation\
    \ (SDC) strategy that leverages information from reviews as well as additional\
    \ sources for selecting one of the reviews as a pseudo-summary to enable supervised\
    \ training. Our Multi-Encoder Decoder framework for Opinion Summarization (MEDOS)\
    \ employs a separate encoder for each source, enabling effective selection of\
    \ information while generating the summary. For evaluation, due to the unavailability\
    \ of test sets with additional sources, we extend the Amazon, Oposum+, and Flipkart\
    \ test sets and leverage ChatGPT to annotate summaries. Experiments across nine\
    \ test sets demonstrate that the combination of our SDC approach and MEDOS model\
    \ achieves on average a 14.5% improvement in ROUGE-1 F1 over the SOTA. Moreover,\
    \ comparative analysis underlines the significance of incorporating additional\
    \ sources for generating more informative summaries. Human evaluations further\
    \ indicate that MEDOS scores relatively higher in coherence and fluency with 0.41\
    \ and 0.5 (\u22121 to 1) respectively, compared to existing models. To the best\
    \ of our knowledge, we are the first to generate opinion summaries leveraging\
    \ additional sources in a self-supervised setting."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Summarization
  authors:
  - emails: '****@cse.iitb.ac.in'
    first_name: Tejpalsingh
    google_scholar_id: https://scholar.google.com/citations?user=-vOma0QAAAAJ&hl=en
    last_name: Siledar
    name: Tejpalsingh Siledar
    orcid: https://orcid.org/0000-0003-4474-6851
    semantic_scholar_id: https://www.semanticscholar.org/author/Tejpalsingh-Siledar/2199253133
    username: ~Tejpalsingh_Siledar2
  - emails: '****@cse.iitb.ac.in'
    first_name: Rupasai
    homepage: https://www.linkedin.com/in/rupasai-rangaraju/
    last_name: Rangaraju
    name: Rupasai Rangaraju
    username: ~Rupasai_Rangaraju2
  - emails: '****@cse.iitb.ac.in'
    first_name: Sankara
    homepage: https://www.linkedin.com/in/sri-raghava-muddu-20aa98171/
    last_name: Muddu
    middle_name: Sri Raghava Ravindra
    name: Sankara Sri Raghava Ravindra Muddu
    username: ~Sankara_Sri_Raghava_Ravindra_Muddu1
  - dblp_id: https://dblp.org/pid/345/5455-3
    emails: '****@flipkart.com'
    first_name: Suman
    institution: Flipkart
    last_name: Banerjee
    name: Suman Banerjee
    username: ~Suman_Banerjee4
  - emails: '****@flipkart.com'
    first_name: Amey
    last_name: Patil
    name: Amey Patil
    username: ~Amey_Patil1
  - emails: '****@flipkart.com'
    first_name: Sudhanshu
    last_name: Singh
    middle_name: Shekhar
    name: Sudhanshu Shekhar Singh
    username: ~Sudhanshu_Shekhar_Singh1
  - dblp_id: https://dblp.org/pid/70/3304
    emails: '****@gmail.com'
    first_name: Muthusamy
    institution: Flipkart
    last_name: Chelliah
    name: Muthusamy Chelliah
    username: ~Muthusamy_Chelliah1
  - dblp_id: https://dblp.org/pid/76/322
    emails: '****@gmail.com'
    first_name: Nikesh
    last_name: Garera
    name: Nikesh Garera
    username: ~Nikesh_Garera1
  - dblp_id: https://dblp.org/pid/70/9376
    emails: '****@cse.iitb.ac.in'
    first_name: Swaprava
    google_scholar_id: https://scholar.google.com/citations?user=TlpsH9cAAAAJ&hl=en
    homepage: https://www.cse.iitb.ac.in/~swaprava/
    institution: IIT Kanpur and Computer Science and Engineering, Indian Institute
      of Technology Bombay
    last_name: Nath
    name: Swaprava Nath
    orcid: https://orcid.org/0000-0001-8309-5006
    semantic_scholar_id: https://www.semanticscholar.org/author/Swaprava-Nath/40147298
    username: ~Swaprava_Nath1
  - dblp_id: https://dblp.org/pid/p/PushpakBhattacharyya
    emails: '****@cse.iitb.ac.in'
    first_name: Pushpak
    google_scholar_id: https://scholar.google.com.tw/citations?user=vvg-pAkAAAAJ
    homepage: https://www.cse.iitb.ac.in/~pb/
    institution: Indian Institute of Technology, Bombay, Dhirubhai Ambani Institute
      Of Information and Communication Technology
    last_name: Bhattacharyya
    name: Pushpak Bhattacharyya
    semantic_scholar_id: https://www.semanticscholar.org/author/P.-Bhattacharyya/145532184
    username: ~Pushpak_Bhattacharyya1
  decision: toFindings
  end_page: 2332
  file: 589.pdf
  id: 589
  num_pages: 18
  openreview_id: vmmxpFpb25
  pdf_file: f830cdf879672a6d0a79a72f703882b7f03391b8.pdf
  start_page: 2315
  title: Product Description and QA Assisted Self-Supervised Opinion Summarization
- abstract: 'Noting that world knowledge continuously evolves over time, large language
    models (LLMs) need to be properly adjusted by performing the "knowledge editing",
    which involves updating outdated information or correcting false information.
    To achieve  reliable and "massive" editing capabilities in terms of $\textit{generalization}$
    and $\textit{specificity}$, this paper proposes a unified knowledge editing method
    called in-$\textbf{CO}$ntext retrieval-augmented $\textbf{M}$ass-$\textbf{E}$diting
    $\textbf{M}$emory (COMEM), which combines two types of editing approaches: parameter
    updating and in-context knowledge editing (IKE). In particular, COMEM incorporates
    $\textit{retrieval-augmented IKE}$, a novel extension of IKE designed for  massive
    editing tasks, based on an $\textit{updating}$-aware demonstration construction.

    Experimental results on the zsRE and CounterFact datasets demonstrate that COMEM
    outperforms all existing methods, achieving state-of-the-art performance. Our
    code is available at https://github.com/JoveReCode/COMEM.git.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - emails: '****@jbnu.ac.kr'
    first_name: Shanbao
    last_name: Qiao
    name: Shanbao Qiao
    orcid: https://orcid.org/0000-0002-9580-2641
    username: ~Shanbao_Qiao1
  - emails: '****@jbnu.ac.kr'
    first_name: Xuebing
    last_name: Liu
    name: Xuebing Liu
    orcid: https://orcid.org/0000-0002-9705-5678
    username: ~Xuebing_Liu1
  - dblp_id: https://dblp.org/pid/56/3784
    emails: '****@jbnu.ac.kr'
    first_name: Seung-Hoon
    google_scholar_id: https://scholar.google.com/citations?user=vZB0BiQAAAAJ&hl=ko&oi=ao
    homepage: https://nlp.jbnu.ac.kr/
    institution: Chonbuk National University
    last_name: Na
    name: Seung-Hoon Na
    orcid: https://orcid.org/0000-0002-4372-7125
    semantic_scholar_id: https://www.semanticscholar.org/author/Seung-Hoon-Na/1723468
    username: ~Seung-Hoon_Na1
  decision: toFindings
  end_page: 2347
  file: 590.pdf
  id: 590
  num_pages: 15
  openreview_id: vsAdM3RoAU
  pdf_file: 1b0fd13cf5d627462b11d89ace0f9c23d026c6ad.pdf
  start_page: 2333
  title: 'COMEM: In-Context Retrieval-Augmented Mass-Editing Memory in Large Language
    Models'
- abstract: Although automated image captioning methods have benefited considerably
    from the development of large language models (LLMs), generating humorous captions
    is still a challenging task. Humorous captions generated by humans are unique
    to the image and reflect the content of the image. However, captions generated
    using previous captioning models tend to be generic. Therefore, we propose incongruity-resolution
    chain-of-thought (IRCoT) as a novel prompting framework that creates content-specific
    resolutions from fine details extracted from an image. Furthermore, we integrate
    logit bias and negative sampling to suppress the output of generic resolutions.
    The results of experiments with GPT4-V demonstrate that our proposed framework
    effectively generated humorous captions tailored to the content of specific input
    images.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multimodality and Language Grounding to Vision, Robotics and Beyond
  authors:
  - emails: '****@mi.t.u-tokyo.ac.jp'
    first_name: Kohtaro
    homepage: https://kohtaro246.github.io/
    institution: The University of Tokyo
    last_name: Tanaka
    name: Kohtaro Tanaka
    username: ~Kohtaro_Tanaka1
  - dblp_id: https://dblp.org/pid/225/4814
    emails: '****@mi.t.u-tokyo.ac.jp'
    first_name: Kohei
    google_scholar_id: https://scholar.google.com/citations?user=yZFVY5cAAAAJ
    homepage: https://uehara-mech.github.io/
    institution: The University of Tokyo
    last_name: Uehara
    name: Kohei Uehara
    username: ~Kohei_Uehara1
  - dblp_id: https://dblp.org/pid/70/3413
    emails: '****@riken.jp'
    first_name: Lin
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=gIEZe5IAAAAJ
    institution: RIKEN
    last_name: Gu
    name: Lin Gu
    username: ~Lin_Gu4
  - dblp_id: https://dblp.uni-trier.de/pid/153/5464
    emails: '****@mi.t.u-tokyo.ac.jp'
    first_name: Yusuke
    google_scholar_id: https://scholar.google.co.jp/citations?user=emo91rIAAAAJ
    homepage: https://www.mi.t.u-tokyo.ac.jp/mukuta/
    institution: The University of Tokyo
    last_name: Mukuta
    name: YUSUKE Mukuta
    username: ~YUSUKE_Mukuta1
  - dblp_id: http://dblp.uni-trier.de/pers/hd/h/Harada:Tatsuya
    emails: '****@mi.t.u-tokyo.ac.jp'
    first_name: Tatsuya
    google_scholar_id: https://scholar.google.com/citations?hl=ja&user=k8rlJ8AAAAAJ
    homepage: https://www.mi.t.u-tokyo.ac.jp/harada/
    institution: RIKEN and The University of Tokyo
    last_name: Harada
    name: Tatsuya Harada
    username: ~Tatsuya_Harada1
  decision: toFindings
  end_page: 2367
  file: 598.pdf
  id: 598
  num_pages: 20
  openreview_id: IIwHkPJtbu
  pdf_file: 5d783035ba53ba36d30d1ad0be7fd8457e28dd08.pdf
  start_page: 2348
  title: Content-Specific Humorous Image Captioning Using Incongruity Resolution Chain-of-Thought
- abstract: 'Personalization of search results has gained increasing attention in
    the past few years, also thanks to the development of Neural Networks-based approaches
    for Information Retrieval. Recent works have proposed to build user models at
    query time by leveraging the Attention mechanism, which allows weighing the contribution
    of the user-related information w.r.t. the current query.

    This approach allows giving more importance to the user''s interests related to
    the current search performed by the user.


    In this paper, we discuss some shortcomings of the Attention mechanism when employed
    for personalization and introduce a novel Attention variant, the Denoising Attention,
    to solve them.

    Denoising Attention adopts a robust normalization scheme and introduces a filtering
    mechanism to better discern among the user-related data those helpful for personalization.

    Experimental evaluation shows improvements in MAP, MRR, and NDCG above 15% w.r.t.
    other Attention variants at the state-of-the-art.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Retrieval and Text Mining
  authors:
  - dblp_id: https://dblp.org/pid/232/2345.html
    emails: '****@gmail.com'
    first_name: Elias
    google_scholar_id: https://scholar.google.com/citations?user=hjPZlPwAAAAJ
    homepage: https://amenra.github.io/eliasbassani/
    last_name: Bassani
    name: Elias Bassani
    orcid: https://orcid.org/0000-0001-7922-2578
    semantic_scholar_id: https://www.semanticscholar.org/author/52211445
    username: ~Elias_Bassani1
  - emails: '****@unimib.it'
    first_name: Pranav
    institution: University of Milan - Bicocca
    last_name: Kasela
    name: Pranav Kasela
    orcid: https://orcid.org/0000-0003-0972-2424
    username: ~Pranav_Kasela2
  - dblp_id: https://dblp.org/pid/26/4672
    emails: '****@unimib.it'
    first_name: Gabriella
    homepage: https://ikr3.disco.unimib.it/people/gabriella-pasi/
    institution: University of Milan - Bicocca
    last_name: Pasi
    name: Gabriella Pasi
    orcid: https://orcid.org/0000-0002-6080-8170
    username: ~Gabriella_Pasi1
  decision: toFindings
  end_page: 2380
  file: 603.pdf
  id: 603
  num_pages: 13
  openreview_id: kbuL4ApSDh
  pdf_file: ff1dd384ab895505664e36e958f6d14d9a544ae4.pdf
  start_page: 2368
  title: Denoising Attention for Query-aware User Modeling
- abstract: 'Dealing with language heterogeneity has always been one of the challenges
    in neural machine translation (NMT).

    The idea of using mixture-of-experts (MoE) naturally excels in addressing this
    issue by employing different experts to take responsibility for different problems.

    However, the parameter-inefficiency problem in MoE results in less performance
    improvement when boosting the number of parameters.

    Moreover, most of the MoE models are suffering from the training instability problem.

    This paper proposes MoA (Mixture-of-Adapters), a lightweight MoE-based NMT model
    that is trained via an elaborately designed stage-wise training strategy.

    With the standard Transformer as the backbone model, we introduce lightweight
    adapters as experts for easy expansion.

    To improve the parameter efficiency, we explicitly model and distill the language
    heterogeneity into the gating network with clustering.

    After freezing the gating network, we adopt the Gumbel-Max sampling as the routing
    scheme when training experts to balance the knowledge of generalization and specialization
    while preventing expert over-fitting.

    Empirical results show that MoA achieves stable improvements in different translation
    tasks by introducing much fewer extra parameters compared to other MoE baselines.

    Additionally, the performance evaluations on a multi-domain translation task illustrate
    the effectiveness of our training strategy.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Translation
  authors:
  - emails: '****@qq.com'
    first_name: Fan
    google_scholar_id: https://scholar.google.com/citations?user=rQwRyuoAAAJ&hl=zh-CN&user=rQwRyuoAAAAJ
    institution: Communication University of China and Samsung
    last_name: Zhang
    name: Fan Zhang
    username: ~Fan_Zhang33
  - dblp_id: https://dblp.org/pid/136/8671.html
    emails: '****@samsung.com'
    first_name: Mei
    last_name: Tu
    name: Mei Tu
    semantic_scholar_id: https://www.semanticscholar.org/author/Mei-Tu/145942217
    username: ~Mei_Tu2
  - emails: '****@samsung.com'
    first_name: Song
    homepage: https://softconf.com/acl2023/papers/user/scmd.cgi?scmd=updateProfile
    last_name: Liu
    name: Song Liu
    username: ~Song_Liu9
  - dblp_id: https://dblp.org/pid/12/977
    emails: '****@cuc.edu.cn'
    first_name: Jinyao
    institution: Communication University of China
    last_name: Yan
    name: Jinyao Yan
    username: ~Jinyao_Yan2
  decision: toFindings
  end_page: 2392
  file: 604.pdf
  id: 604
  num_pages: 12
  openreview_id: GxQf4YuJp0
  pdf_file: 408e3c8e1726dacbebe31d724e57549e9f866607.pdf
  start_page: 2381
  title: A Lightweight Mixture-of-Experts Neural Machine Translation Model with Stage-wise
    Training Strategy
- abstract: Knowledge probing assesses to which degree a language model (LM) has successfully
    learned relational knowledge during pre-training. Probing is an inexpensive way
    to compare LMs of different sizes and training configurations. However, previous
    approaches rely on the objective function used in pre-training LMs and are thus
    applicable only to masked or causal LMs. As a result, comparing different types
    of LMs becomes impossible. To address this, we propose an approach that uses an
    LM's inherent ability to estimate the log-likelihood of any given textual statement.
    We carefully design an evaluation dataset of 7,731 instances (40,916 in a larger
    variant) from which we produce alternative statements for each relational fact,
    one of which is correct. We then evaluate whether an LM correctly assigns the
    highest log-likelihood to the correct statement. Our experimental evaluation of
    22 common LMs shows that our proposed framework, BEAR, can effectively probe for
    knowledge across different LM types. We release the BEAR datasets and an open-source
    framework that implements the probing approach to the research community to facilitate
    the evaluation and development of LMs.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - emails: '****@hu-berlin.de'
    first_name: Jacek
    homepage: https://www.scienceofintelligence.de/people/jacek-wiland/
    institution: "Department of Computer Science, Humboldt University Berlin, Humboldt\
      \ Universit\xE4t Berlin"
    last_name: Wiland
    name: Jacek Wiland
    username: ~Jacek_Wiland1
  - emails: '****@hu-berlin.de'
    first_name: Max
    homepage: https://maxploner.de
    institution: "Humboldt Universit\xE4t Berlin"
    last_name: Ploner
    name: Max Ploner
    orcid: https://orcid.org/0009-0007-4593-2353
    username: ~Max_Ploner1
  - dblp_id: https://dblp.org/pid/127/0198
    emails: '****@gmail.com'
    first_name: Alan
    google_scholar_id: https://scholar.google.com/citations?user=adKmg3IAAAAJ&hl=en
    homepage: https://alanakbik.github.io/
    institution: "Humboldt Universit\xE4t Berlin"
    last_name: Akbik
    name: Alan Akbik
    semantic_scholar_id: https://www.semanticscholar.org/author/A.-Akbik/2403712
    username: ~Alan_Akbik2
  decision: toFindings
  end_page: 2411
  file: 605.pdf
  id: 605
  num_pages: 19
  openreview_id: BXIf7IZuf2
  pdf_file: 3a5462b5806bd07afa8d8cba7967f7511f76838a.pdf
  start_page: 2393
  title: 'BEAR: A Unified Framework for Evaluating Relational Knowledge in Causal
    and Masked Language Models'
- abstract: 'We present Conformal Intent Classification and Clarification (CICC),
    a framework for fast and accurate intent classification for task-oriented dialogue
    systems. The framework turns heuristic uncertainty scores of any intent classifier
    into a clarification question that is guaranteed to contain the true intent at
    a pre-defined confidence level.

    By disambiguating between a small number of likely intents, the user query can
    be resolved quickly and accurately. Additionally, we propose to augment the framework
    for out-of-scope detection.

    In a comparative evaluation using seven intent recognition datasets we find that
    CICC generates small clarification questions and is capable of out-of-scope detection.

    CICC can help practitioners and researchers substantially in improving the user
    experience of dialogue agents with specific clarification questions.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Dialogue and Interactive Systems
  authors:
  - dblp_id: https://dblp.org/pid/246/4928
    emails: '****@vu.nl'
    first_name: Floris
    google_scholar_id: https://scholar.google.nl/citations?user=8I8iSHkAAAAJ
    homepage: https://florisdh.nl
    last_name: Hengst
    middle_name: Den
    name: Floris den Hengst
    orcid: https://orcid.org/0000-0002-2092-9904
    username: ~Floris_den_Hengst1
  - dblp_id: https://dblp.org/pid/07/746.html
    emails: '****@ing.com'
    first_name: Ralf
    institution: ING Bank
    last_name: Wolter
    name: Ralf Wolter
    username: ~Ralf_Wolter1
  - emails: '****@tudelft.nl'
    first_name: Patrick
    google_scholar_id: https://scholar.google.com/citations?user=e7KRRa8AAAAJ&hl=en
    homepage: https://www.paltmeyer.com/
    last_name: Altmeyer
    name: Patrick Altmeyer
    orcid: https://orcid.org/0000-0003-4726-8613
    semantic_scholar_id: https://www.semanticscholar.org/author/Patrick-Altmeyer/2072038627
    username: ~Patrick_Altmeyer1
  - emails: '****@ing.com'
    first_name: Arda
    last_name: Kaygan
    name: Arda Kaygan
    username: ~Arda_Kaygan1
  decision: toFindings
  end_page: 2432
  file: 606.pdf
  id: 606
  num_pages: 21
  openreview_id: U1FXek8wfm
  pdf_file: 0ba1f7eb409a4e02ef3aa0711291d3e7159a8960.pdf
  start_page: 2412
  title: Conformal Intent Classification and Clarification for Fast and Accurate Intent
    Recognition
- abstract: Anonymity in court rulings is a critical aspect of privacy protection
    in the European Union and Switzerland but with the advent of LLMs, concerns about
    large-scale re-identification of anonymized persons are growing. In accordance
    with the Federal Supreme Court of Switzerland (FSCS), we study re-identification
    risks using actual legal data. Following the initial experiment, we constructed
    an anonymized Wikipedia dataset as a more rigorous testing ground to further investigate
    the findings. In addition to the datasets, we also introduce new metrics to measure
    performance. We systematically analyze the factors that influence successful re-identifications,
    identifying model size, input length, and instruction tuning among the most critical
    determinants. Despite high re-identification rates on Wikipedia, even the best
    LLMs struggled with court decisions. We demonstrate that for now, the risk of
    re-identifications using LLMs is minimal in the vast majority of cases. We hope
    that our system can help enhance the confidence in the security of anonymized
    decisions, thus leading the courts to publish more decisions.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Computational Social Science and Cultural Analytics
  authors:
  - emails: '****@unifr.ch'
    first_name: Alex
    last_name: Nyffenegger
    name: Alex Nyffenegger
    username: ~Alex_Nyffenegger1
  - emails: '****@stuermer.ch'
    first_name: Matthias
    google_scholar_id: https://scholar.google.com/citations?user=QtfXdRoAAAAJ&hl=de
    homepage: https://www.bfh.ch/de/ueber-die-bfh/personen/z5v7sdisx3ru/
    institution: "BFH - Bern University of Applied Sciences and Universit\xE4t Bern"
    last_name: "St\xFCrmer"
    name: "Matthias St\xFCrmer"
    orcid: https://orcid.org/0000-0001-9038-4041
    username: "~Matthias_St\xFCrmer1"
  - dblp_id: https://dblp.org/pid/232/4545
    emails: '****@niklaus.ai'
    first_name: Joel
    google_scholar_id: https://scholar.google.com/citations?user=qJ8iricAAAAJ&hl=en
    homepage: https://niklaus.ai
    institution: "University of Bern, Universit\xE4t Bern"
    last_name: Niklaus
    name: Joel Niklaus
    orcid: https://orcid.org/0000-0002-2779-1653
    semantic_scholar_id: https://www.semanticscholar.org/author/Joel-Niklaus/67042743
    username: ~Joel_Niklaus1
  decision: toFindings
  end_page: 2462
  file: 609.pdf
  id: 609
  num_pages: 30
  openreview_id: gdO9zJforQ
  pdf_file: c461426343396089d1ff69ca3a8df7af15d040e6.pdf
  start_page: 2433
  title: Anonymity at Risk? Assessing Re-Identification Capabilities of Large Language
    Models in Court Decisions
- abstract: 'The impressive development of large language models (LLMs) is expanding
    into the realm of large multimodal models (LMMs), which incorporate multiple types
    of data beyond text.  However, the nature of multimodal models leads to significant
    expenses in the creation of training data. Furthermore, constructing multilingual
    data for LMMs presents its own set of challenges due to language diversity and
    complexity. Therefore, in this study, we propose two cost-effective methods to
    solve this problem: (1) vocabulary expansion and pretraining of multilingual LLM
    for specific languages, and (2) automatic and elaborate construction of multimodal
    datasets using GPT4-V. Based on these methods, we constructed a 91K English-Korean-Chinese
    multilingual, multimodal training dataset. Additionally, we developed a bilingual
    multimodal model that exhibits excellent performance in both Korean and English,
    surpassing existing approaches.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multilinguality and Language Diversity
  authors:
  - emails: '****@gmail.com'
    first_name: DongJae
    google_scholar_id: https://scholar.google.com/citations?user=cJodcu8AAAAJ&hl=ko
    homepage: https://sites.google.com/view/aailab
    institution: Seoul National University of Science and Technology
    last_name: Shin
    name: DongJae Shin
    username: ~DongJae_Shin1
  - emails: '****@gmail.com'
    first_name: HyeonSeok
    google_scholar_id: https://scholar.google.co.kr/citations?user=xfhFleUAAAAJ&hl=ko
    homepage: https://sites.google.com/view/aailab
    institution: Hanbat National University
    last_name: Lim
    name: HyeonSeok Lim
    username: ~HyeonSeok_Lim1
  - emails: '****@gmail.com'
    first_name: Inho
    google_scholar_id: https://scholar.google.com/citations?user=OtFiuPMAAAAJ&hl=ko
    homepage: https://github.com/kotmul
    institution: Seoul National University of Science and Technology
    last_name: Won
    name: Inho Won
    username: ~Inho_Won1
  - emails: '****@gmail.com'
    first_name: ChangSu
    google_scholar_id: https://scholar.google.com/citations?user=LkS0Fa4AAAAJ&hl=ko
    last_name: Choi
    name: ChangSu Choi
    username: ~ChangSu_Choi1
  - emails: '****@gmail.com'
    first_name: Minjun
    google_scholar_id: https://scholar.google.com/citations?user=P33rUn0AAAAJ&hl
    homepage: https://github.com/mjkmain
    last_name: Kim
    name: Minjun Kim
    orcid: https://orcid.org/0009-0008-3240-8971
    username: ~Minjun_Kim4
  - emails: '****@gmail.com'
    first_name: SeungWoo
    homepage: https://github.com/swsong
    institution: Hanbat National University
    last_name: Song
    name: SeungWoo Song
    username: ~SeungWoo_Song1
  - emails: '****@seoultech.ac.kr'
    first_name: HanGyeol
    homepage: https://sites.google.com/view/aailab
    last_name: Yoo
    name: HanGyeol Yoo
    username: ~HanGyeol_Yoo1
  - emails: '****@naver.com'
    first_name: SangMin
    homepage: https://sites.google.com/view/aailab
    institution: Seoul National University of Science and Technology
    last_name: Kim
    name: SangMin Kim
    username: ~SangMin_Kim1
  - emails: '****@seoultech.ac.kr'
    first_name: KyungTae
    google_scholar_id: https://scholar.google.com/citations?user=XRwsGZAAAAAJ&hl
    homepage: https://sites.google.com/view/aailab
    institution: Seoul National University of Science and Technology
    last_name: Lim
    name: KyungTae Lim
    orcid: https://orcid.org/0000-0002-5818-1161
    username: ~KyungTae_Lim1
  decision: toFindings
  end_page: 2473
  file: 610.pdf
  id: 610
  num_pages: 11
  openreview_id: k3w23XsrOd
  pdf_file: 184c6a81a4fcd48b2d723bd80e485f3ffe5f7df0.pdf
  start_page: 2463
  title: 'X-LLaVA: Optimizing Bilingual Large Vision-Language Alignment'
- abstract: 'Most existing retrieval-augmented language models (LMs) assume a naive
    dichotomy within a retrieved document set: query-relevance and irrelevance. Our
    work investigates a more challenging scenario in which even the "relevant" documents
    may contain misleading or incorrect information, causing conflict among the retrieved
    documents and thereby negatively influencing model decisions as noise. We observe
    that existing LMs are highly brittle to the presence of conflicting information
    in both the fine-tuning and in-context few-shot learning scenarios. We propose
    approaches for handling knowledge conflicts among retrieved documents by explicitly
    fine-tuning a discriminator or prompting GPT-3.5 to elicit its discriminative
    capability. Our empirical results on open-domain QA show that these approaches
    significantly enhance model robustness. We also provide our findings on incorporating
    the fine-tuned discriminator''s decision into the in-context learning process,
    proposing a way to exploit the benefits of two disparate learning schemes. Alongside
    our findings, we provide MacNoise, a machine-generated, conflict-induced dataset
    to further encourage research in this direction.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Question Answering
  authors:
  - emails: '****@ed.ac.uk'
    first_name: Giwon
    google_scholar_id: https://scholar.google.com/citations?user=uBu5iKIAAAAJ&hl=ko
    homepage: https://honggiwon.github.io/
    institution: University of Edinburgh, University of Edinburgh
    last_name: Hong
    name: Giwon Hong
    semantic_scholar_id: https://www.semanticscholar.org/author/49886158
    username: ~Giwon_Hong1
  - emails: '****@gmail.com'
    first_name: Jeonghwan
    google_scholar_id: https://scholar.google.com/citations?user=CcnGNN8AAAAJ&hl=ko
    last_name: Kim
    name: Jeonghwan Kim
    username: ~Jeonghwan_Kim2
  - dblp_id: https://dblp.org/pid/251/8697
    emails: '****@gatech.edu'
    first_name: Junmo
    google_scholar_id: https://scholar.google.com/citations?user=BGiZE6MAAAAJ&hl=en
    homepage: https://jm-kang.github.io/
    institution: Georgia Institute of Technology
    last_name: Kang
    name: Junmo Kang
    semantic_scholar_id: https://www.semanticscholar.org/author/Junmo-Kang/153041329
    username: ~Junmo_Kang1
  - emails: '****@kaist.ac.kr'
    first_name: Sung-Hyon
    google_scholar_id: https://scholar.google.com/citations?user=6pdKebMAAAAJ&hl=ko
    homepage: http://ir.kaist.ac.kr/member/professor/
    last_name: Myaeng
    name: Sung-Hyon Myaeng
    username: ~Sung-Hyon_Myaeng1
  - dblp_id: https://dblp.org/pid/121/4230
    emails: '****@kaist.ac.kr'
    first_name: Joyce
    google_scholar_id: https://scholar.google.com/citations?user=TLrKglQAAAAJ&hl=en&gmla=AJsN-F550mi43sT67Q7bX8vwQAGGw314pQuotE9LcFAcCIH3t4sz8vhZfMiGLr0CVPrANVMpQvGkOvurqIdsuH52-jNOKFaGgLcNYEMQKV7ZcDIgqApHjgA&sciund=10341128752263664280
    homepage: http://bdi-lab.kaist.ac.kr/
    institution: KAIST
    last_name: Whang
    middle_name: Jiyoung
    name: Joyce Jiyoung Whang
    orcid: https://orcid.org/0000-0002-4773-3194
    semantic_scholar_id: https://www.semanticscholar.org/author/J.-Whang/34826931
    username: ~Joyce_Jiyoung_Whang2
  decision: toFindings
  end_page: 2495
  file: 611.pdf
  id: 611
  num_pages: 22
  openreview_id: esHgmp1glB
  pdf_file: 09d025c60ba53bdf498ca16d106c48830e0f2096.pdf
  start_page: 2474
  title: Why So Gullible? Enhancing the Robustness of Retrieval-Augmented Models against
    Counterfactual Noise
- abstract: 'In this work, we investigate multilingual speech Pre-Trained models (PTMs)
    for Audio deepfake detection (ADD). We hypothesize that

    multilingual PTMs trained on large-scale diverse multilingual data gain knowledge
    about diverse pitches, accents, and tones, during their

    pre-training phase and making them more robust to variations. As a result, they
    will be more effective for detecting audio deepfakes. To validate our hypothesis,
    we extract representations from state-of-the-art (SOTA) PTMs including monolingual,
    multilingual as well as PTMs trained for speaker and emotion recognition, and
    evaluated them on ASVSpoof 2019 (ASV), In-the-Wild (ITW), and DECRO benchmark
    databases. We show that representations from multilingual PTMs, with simple downstream
    networks, attain the best performance for ADD compared to other PTM representations,
    which validates our hypothesis. We also explore the possibility of fusion of selected
    PTM representations for further improvements in ADD, and we propose a framework,
    MiO (Merge into One) for this purpose. With MiO, we achieve SOTA performance on
    ASV and ITW and comparable performance on DECRO with current SOTA works.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Speech recognition, text-to-speech and spoken language understanding
  authors:
  - emails: '****@iiitd.ac.in'
    first_name: Orchid
    google_scholar_id: https://scholar.google.com/citations?user=EV6cXVIAAAAJ&hl=en
    homepage: https://orchidchetiaphukan.github.io/
    institution: Indraprastha Institute of Information Technology, Delhi
    last_name: Chetia Phukan
    name: Orchid Chetia Phukan
    username: ~Orchid_Chetia_Phukan2
  - emails: '****@gmail.com'
    first_name: Gautam
    last_name: Kashyap
    middle_name: Siddharth
    name: Gautam Siddharth Kashyap
    orcid: https://orcid.org/0000-0003-2140-9617
    username: ~Gautam_Siddharth_Kashyap1
  - dblp_id: https://dblp.org/pid/128/4273
    emails: '****@gmail.com'
    first_name: Arun Balaji
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=TmGAyPYAAAAJ
    homepage: http://faculty.iiitd.ac.in/~arunb/
    institution: Indraprastha Institute of Information Technology, Delhi
    last_name: Buduru
    name: Arun Balaji Buduru
    username: ~Arun_Balaji_Buduru1
  - dblp_id: https://dblp.org/pid/16/7691
    emails: '****@ut.ee'
    first_name: Rajesh
    google_scholar_id: https://scholar.google.com/citations?user=GKegbo0AAAAJ&hl=en
    homepage: https://rajeshsharma.cs.ut.ee/
    institution: institute of computer science, University of Tartu
    last_name: Sharma
    name: Rajesh Sharma
    username: ~Rajesh_Sharma1
  decision: toFindings
  end_page: 2506
  file: 612.pdf
  id: 612
  num_pages: 11
  openreview_id: ms5zeBc4r4
  pdf_file: a989cde7f05c2f410b01cf265495d3b0035b50a5.pdf
  start_page: 2496
  title: 'Heterogeneity over Homogeneity: Investigating Multilingual Speech Pre-Trained
    Models for Detecting Audio Deepfake'
- abstract: 'In the last decade, the United States has lost more than 500,000 people
    from an overdose involving prescription and illicit opioids making it a national
    public health emergency (USDHHS, 2017). Medical practitioners require robust and
    timely tools that can effectively identify at-risk patients. Community-based social
    media platforms such as Reddit allow self-disclosure for users to discuss otherwise
    sensitive drug-related behaviors. We present a moderate size corpus of 2500 opioid-related
    posts from various subreddits labeled with six different phases of opioid use:
    Medical Use, Misuse, Addiction, Recovery, Relapse, Not Using. For every post,
    we annotate span-level extractive explanations and crucially study their role
    both in annotation quality and model development. We evaluate several state-of-the-art
    models in a supervised, few-shot, or zero-shot setting. Experimental results and
    error analysis show that identifying the phases of opioid use disorder is highly
    contextual and challenging. However, we find that using explanations during modeling
    leads to a significant boost in classification accuracy demonstrating their beneficial
    role in a high-stakes domain such as studying the opioid use disorder continuum.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Computational Social Science and Cultural Analytics
  authors:
  - dblp_id: https://dblp.org/pid/229/4179
    emails: '****@gmail.com'
    first_name: Chenghao
    google_scholar_id: https://scholar.google.com/citations?user=B28fiOAAAAAJ&hl=zh-CN
    homepage: https://yangalan123.github.io/
    institution: University of Chicago
    last_name: Yang
    name: Chenghao Yang
    username: ~Chenghao_Yang1
  - dblp_id: https://dblp.org/pid/227/2812
    emails: '****@cs.columbia.edu'
    first_name: Tuhin
    google_scholar_id: https://scholar.google.com/citations?user=HCmFuo8AAAAJ&hl=en
    homepage: https://tuhinjubcse.github.io/
    last_name: Chakrabarty
    name: Tuhin Chakrabarty
    semantic_scholar_id: https://www.semanticscholar.org/author/Tuhin-Chakrabarty/51448832
    username: ~Tuhin_Chakrabarty2
  - emails: '****@friendsresearch.org'
    first_name: Karli
    homepage: https://friendsresearch.org/people/karli-hochstatter-ph-d-m-p-h/
    last_name: Hochstatter
    middle_name: R
    name: Karli R Hochstatter
    username: ~Karli_R_Hochstatter1
  - emails: '****@columbia.edu'
    first_name: Melissa
    homepage: https://sig.columbia.edu/people/melissa-slavin
    institution: Columbia University
    last_name: Slavin
    middle_name: N
    name: Melissa N Slavin
    username: ~Melissa_N_Slavin1
  - emails: '****@columbia.edu'
    first_name: Nabila
    homepage: https://socialwork.columbia.edu/faculty-research/faculty/full-time/nabila-el-bassel/?_vsrefdom=p.10431&utm_id=go_cmp-19590059195_adg-142665234662_ad-645448708224_dsa-19959388920_dev-c_ext-_prd-_mca-_sig-Cj0KCQjw7aqkBhDPARIsAKGa0oI1vRaj3Go5EkJZNNw1WX6Vjg4Cx4ORQdpVA4HLmhEBjoYNV0TM6YMaAsdREALw_wcB&utm_source=google&gclid=Cj0KCQjw7aqkBhDPARIsAKGa0oI1vRaj3Go5EkJZNNw1WX6Vjg4Cx4ORQdpVA4HLmhEBjoYNV0TM6YMaAsdREALw_wcB
    institution: Columbia University and Columbia University
    last_name: El-Bassel
    name: Nabila El-Bassel
    orcid: https://orcid.org/0000-0002-0049-5686
    username: ~Nabila_El-Bassel1
  - dblp_id: https://dblp.org/pid/44/70
    emails: '****@columbia.edu'
    first_name: Smaranda
    google_scholar_id: https://scholar.google.com/citations?user=Esbx2VcAAAAJ&hl=en
    homepage: http://www.cs.columbia.edu/~smara/
    institution: Amazon and Columbia University
    last_name: Muresan
    name: Smaranda Muresan
    semantic_scholar_id: https://www.semanticscholar.org/author/S.-Muresan/2295928
    username: ~Smaranda_Muresan3
  decision: toFindings
  end_page: 2521
  file: 613.pdf
  id: 613
  num_pages: 15
  openreview_id: TQhDdrvOBv
  pdf_file: 80d71e9dc7661d8671b194face9b23e9bd257976.pdf
  start_page: 2507
  title: Identifying Self-Disclosures of Use, Misuse and Addiction in Community-based
    Social Media Posts
- abstract: "Image--text models (ITMs) is the prevalent architecture to solve video\
    \ question--answering tasks, which requires only a few input frames to save huge\
    \ computational cost compared to video--language models.\nHowever, we find existent\
    \ ITM video question--answering solutions either 1) adopt simplistic and unintentional\
    \ sampling strategies, which may miss key frames to offer the answer clues; or\
    \ 2) sample a large number of frames into divided groups, which the computational\
    \ sources can not accommodate. \nIn this work, we aim at an efficient sampling\
    \ method towards the few-frame situations.\nWe first summarize a family of prior\
    \ sampling methods based on question--frame correlation into a unified one, dubbed\
    \ *Most Implied Frames* (MIF). Through some primary results and analysis, Through\
    \ analysis, we form a hypothesis that question-aware sampling is not necessary,\
    \ from which we further propose the other method *Most Dominant Frames* (MDF).\n\
    Experimental results on four public datasets and three advanced ITMs demonstrate\
    \ that our proposed strategies can boost the performance for image--text pretrained\
    \ models, and have a wide application scenario in terms of model architectures\
    \ and dataset types. Our code is available at https://github.com/declare-lab/Sealing\\\
    url{https://github.com/declare-lab/Sealing}."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multimodality and Language Grounding to Vision, Robotics and Beyond
  authors:
  - emails: '****@gmail.com'
    first_name: Wei
    google_scholar_id: https://scholar.google.com/citations?user=rWtF3g8AAAAJ&hl=zh-CN
    homepage: https://clement25.github.io/
    institution: Singapore University of Technology and Design
    last_name: Han
    name: Wei Han
    username: ~Wei_Han4
  - dblp_id: https://dblp.org/pid/12/417-23
    emails: '****@ntu.edu.sg'
    first_name: Hui
    google_scholar_id: https://scholar.google.com.sg/citations?user=XObq9OQAAAAJ&hl=zh-CN
    homepage: https://chchenhui.github.io
    institution: Nanyang Technological University
    last_name: Chen
    name: Hui Chen
    username: ~Hui_Chen4
  - dblp_id: https://dblp.org/pid/k/MinYenKan
    emails: '****@comp.nus.edu.sg'
    first_name: Min-Yen
    google_scholar_id: https://scholar.google.com.tw/citations?user=aNVcd3EAAAAJ
    homepage: https://www.comp.nus.edu.sg/~kanmy/
    institution: National University of Singapore
    last_name: Kan
    name: Min-Yen Kan
    semantic_scholar_id: https://www.semanticscholar.org/author/Min-Yen-Kan/37596605
    username: ~Min-Yen_Kan1
  - dblp_id: https://dblp.org/pid/116/4904
    emails: '****@sutd.edu.sg'
    first_name: Soujanya
    google_scholar_id: https://scholar.google.co.in/citations?user=oS6gRc4AAAAJ&hl=en
    homepage: https://sporia.info
    institution: Singapore University of Technology and Design
    last_name: Poria
    name: Soujanya Poria
    username: ~Soujanya_Poria1
  decision: toFindings
  end_page: 2534
  file: 614.pdf
  id: 614
  num_pages: 13
  openreview_id: xSiVmEwmnx
  pdf_file: 65d1bec5af01c55a40e6e0e0ecb3436268dc51b5.pdf
  start_page: 2522
  title: Self-Adaptive Sampling for Accurate Video Question Answering on Image Text
    Models
- abstract: "Large Language Models (LLMs) have demonstrated impressive capabilities\
    \ for text rewriting.   However  creating  a  smaller  yet  potent language  model\
    \  for  text  rewriting  presents two  formidable  challenges:   costly  data\
    \  collection  and  absence  of  emergent  capabilities.In this paper we present\
    \ solutions to address the  above  challenges.We  propose  an  new instruction\
    \  tuning  method  to  develop  a  mo-bile text rewriting model that leverages\
    \ LLM-generated  data  and  heuristic  reinforcement learning, eliminating the\
    \ need for human data collection. Moreover,  to  bridge  the  performance  gap\
    \  from  the  constraint  size,  we  pro-pose a cascading approach based on the\
    \ confidence levels which are distilled from the large server model\u2019s critiques.\
    \  To evaluate the text rewriting  tasks  for  mobile  scenarios,  we  introduce\
    \ MessageRewriteEval, a human-labeled benchmark that focuses on text rewriting\
    \ of messages through natural language instructions. Through  empirical  experiments,\
    \ we demonstrate that our on-device model surpasses  the  current  state-of-the-art\
    \  LLMs  in text rewriting while maintaining a significantly reduced  model  size\
    \  using  public  benchmark EditEval and our new benchmark.  We also demonstrate\
    \  that  our  proposed  cascading  approach improves model performance further."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - emails: '****@gmail.com'
    first_name: Yun
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=gBGxuWMAAAAJ&view_op=list_works&sortby=pubdate
    homepage: https://scholar.google.com/citations?hl=en&user=gBGxuWMAAAAJ&view_op=list_works&sortby=pubdate
    last_name: Zhu
    name: Yun Zhu
    username: ~Yun_Zhu5
  - emails: '****@google.com'
    first_name: Yinxiao
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=c7HKsEsAAAAJ
    last_name: Liu
    name: Yinxiao Liu
    username: ~Yinxiao_Liu2
  - dblp_id: https://dblp.org/pid/125/7456
    emails: '****@gmail.com'
    first_name: Felix
    google_scholar_id: https://scholar.google.com/citations?user=NA8fvj8AAAAJ
    institution: Google
    last_name: Stahlberg
    name: Felix Stahlberg
    orcid: https://orcid.org/0000-0002-0430-5704
    semantic_scholar_id: https://www.semanticscholar.org/author/Felix-Stahlberg/48404632
    username: ~Felix_Stahlberg1
  - dblp_id: https://dblp.org/pid/37/3396
    emails: '****@google.com'
    first_name: Shankar
    homepage: https://research.google/people/author3286/
    last_name: Kumar
    name: Shankar Kumar
    semantic_scholar_id: https://www.semanticscholar.org/author/Shankar-Kumar/9567965
    username: ~Shankar_Kumar1
  - emails: '****@gmail.com'
    first_name: Yu-Hui
    last_name: Chen
    name: Yu-Hui Chen
    username: ~Yu-Hui_Chen1
  - dblp_id: https://dblp.org/pid/225/6429
    emails: '****@gmail.com'
    first_name: Liangchen
    google_scholar_id: https://scholar.google.com/citations?user=8ei4_E4AAAAJ&hl=en-US
    homepage: https://www.luolc.com
    institution: Google
    last_name: Luo
    name: Liangchen Luo
    username: ~Liangchen_Luo1
  - dblp_id: https://dblp.org/pid/19/2932-4
    emails: '****@gmail.com'
    first_name: Lei
    google_scholar_id: https://scholar.google.com/citations?user=Q0zkC-kAAAAJ&hl=en
    homepage: https://leishu02.github.io/
    institution: Google
    last_name: Shu
    name: Lei Shu
    semantic_scholar_id: https://www.semanticscholar.org/author/Lei-Shu/145142456
    username: ~Lei_Shu1
  - emails: '****@google.com'
    first_name: Renjie
    last_name: Liu
    name: Renjie Liu
    username: ~Renjie_Liu1
  - emails: '****@google.com'
    first_name: Jindong
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=Cdw5B5AAAAAJ&view_op=list_works&sortby=pubdate
    homepage: https://research.google/people/JindongJDChen/
    institution: Google
    last_name: Chen
    name: Jindong Chen
    username: ~Jindong_Chen2
  - emails: '****@gmail.com'
    first_name: Lei
    homepage: https://scholar.google.com/citations?user=7XxgNUsAAAAJ&hl=en&oi=ao
    last_name: Meng
    name: Lei Meng
    username: ~Lei_Meng2
  decision: toFindings
  end_page: 2552
  file: 619.pdf
  id: 619
  num_pages: 18
  openreview_id: ozma7ovTMe
  pdf_file: de5f7cc8c34676fb2bf377165962cc3d83d31bc2.pdf
  start_page: 2535
  title: Towards an On-device Agent for Text Rewriting
- abstract: 'One way to personalize chatbot interactions is by establishing common
    ground with the intended reader. A domain where establishing mutual understanding
    could be particularly impactful is vaccine concerns and misinformation. Vaccine
    interventions are forms of messaging which aim to answer concerns expressed about
    vaccination. Tailoring responses in this domain is difficult, since opinions often
    have seemingly little ideological overlap. We define the task of tailoring vaccine
    interventions to a Common-Ground Opinion (CGO). Tailoring responses to a CGO involves
    meaningfully improving the answer by relating it to an opinion or belief the reader
    holds. In this paper we introduce Tailor-CGO, a dataset for evaluating how well
    responses are tailored to provided CGOs. We benchmark several major LLMs on this
    task; finding GPT-4-Turbo performs significantly better than others. We also build
    automatic evaluation metrics, including an efficient and accurate BERT model that
    outperforms finetuned LLMs, investigate how to successfully tailor vaccine messaging
    to CGOs, and provide actionable recommendations from this investigation.


    Tailor-CGO dataset and code available at: https://github.com/rickardstureborg/tailor-cgo'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - emails: '****@duke.edu'
    first_name: Rickard
    google_scholar_id: https://scholar.google.com/citations?user=mRuxtvIAAAAJ&hl=en
    homepage: http://www.rickard.stureborg.com
    institution: Duke University
    last_name: Stureborg
    name: Rickard Stureborg
    orcid: https://orcid.org/0000-0002-5337-9291
    username: ~Rickard_Stureborg1
  - dblp_id: https://dblp.org/pid/274/1027
    emails: '****@virginia.edu'
    first_name: Sanxing
    google_scholar_id: https://scholar.google.com/citations?user=YtxKsUMAAAAJ
    homepage: https://sanxing.ai/
    institution: Duke University
    last_name: Chen
    name: Sanxing Chen
    semantic_scholar_id: https://www.semanticscholar.org/author/Sanxing-Chen/116881002
    username: ~Sanxing_Chen1
  - emails: '****@duke.edu'
    first_name: Roy
    homepage: https://royxie.com/
    last_name: Xie
    name: Roy Xie
    username: ~Roy_Xie1
  - emails: '****@duke.edu'
    first_name: Aayushi
    last_name: Patel
    middle_name: Kunjal
    name: Aayushi Kunjal Patel
    username: ~Aayushi_Kunjal_Patel1
  - emails: '****@duke.edu'
    first_name: Christopher
    last_name: Li
    name: Christopher Li
    username: ~Christopher_Li2
  - emails: '****@duke.edu'
    first_name: Chloe
    google_scholar_id: https://scholar.google.com/citations?user=GAsrj-EAAAAJ&hl=en
    last_name: Zhu
    name: Chloe Zhu
    username: ~Chloe_Zhu1
  - emails: '****@duke.edu'
    first_name: Tingnan
    last_name: Hu
    name: Tingnan Hu
    username: ~Tingnan_Hu2
  - dblp_id: https://dblp.org/pid/y/JunYang1.html
    emails: '****@cs.duke.edu'
    first_name: Jun
    homepage: https://www.cs.duke.edu/~junyang
    institution: Department of Computer Science, Duke University
    last_name: Yang
    name: Jun Yang
    username: ~Jun_Yang2
  - dblp_id: https://dblp.org/pid/180/5692
    emails: '****@google.com'
    first_name: Bhuwan
    google_scholar_id: https://scholar.google.com/citations?user=2W2ttrQAAAAJ&hl=en&authuser=1&oi=ao
    homepage: https://users.cs.duke.edu/~bdhingra/
    institution: Duke University
    last_name: Dhingra
    name: Bhuwan Dhingra
    semantic_scholar_id: https://www.semanticscholar.org/author/Bhuwan-Dhingra/34994191
    username: ~Bhuwan_Dhingra1
  decision: toFindings
  end_page: 2575
  file: 620.pdf
  id: 620
  num_pages: 23
  openreview_id: PhTrzokS7F
  pdf_file: 763df49d0081e40f51ea4fed9681846b889391ce.pdf
  start_page: 2553
  title: Tailoring Vaccine Messaging with Common-Ground Opinions
- abstract: "This paper introduces a novel neuro-symbolic architecture for relation\
    \ classification (RC) that combines rule-based methods with contemporary deep\
    \ learning techniques. \nThis approach capitalizes on the strengths of both paradigms:\
    \ the adaptability of rule-based systems and the generalization power of neural\
    \ networks. Our architecture consists of two components: a declarative rule-based\
    \ model for transparent classification and a neural component to enhance rule\
    \ generalizability through semantic text matching.\nNotably, our semantic matcher\
    \ is trained in an unsupervised domain-agnostic way, solely with synthetic data.\n\
    Further, these components are loosely coupled, allowing for rule modifications\
    \ without retraining the semantic matcher.\nIn our evaluation, we focused on two\
    \ few-shot relation classification datasets: Few-Shot TACRED and a Few-Shot version\
    \ of NYT29. \nWe show that our proposed method outperforms previous state-of-the-art\
    \ models in three out of four settings, despite not seeing any human-annotated\
    \ training data.\nFurther, we show that our approach remains modular and pliable,\
    \ i.e., the corresponding rules can be locally modified to improve the overall\
    \ model. Human interventions to the rules for the TACRED relation org:parents\
    \ boost the performance on that relation by as much as 26\\% relative improvement,\
    \ without negatively impacting the other relations, and without retraining the\
    \ semantic matching component."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Extraction
  authors:
  - emails: '****@email.arizona.edu'
    first_name: Robert
    homepage: https://www.cs.arizona.edu/person/robert-vacareanu
    institution: University of Arizona
    last_name: Vacareanu
    name: Robert Vacareanu
    username: ~Robert_Vacareanu1
  - emails: '****@arizona.edu'
    first_name: Fahmida
    google_scholar_id: https://scholar.google.com/citations?user=nG0nlG8AAAAJ&hl=en
    homepage: https://www.cs.arizona.edu/person/fahmida-alam
    institution: University of Arizona
    last_name: Alam
    name: Fahmida Alam
    orcid: https://orcid.org/0000-0001-6762-617X
    username: ~Fahmida_Alam1
  - dblp_id: https://dblp.org/pid/272/7670
    emails: '****@arizona.edu'
    first_name: Md Asiful
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=rUDBn2sAAAAJ
    homepage: https://www.cs.arizona.edu/person/md-asiful-islam
    institution: University of Arizona
    last_name: Islam
    name: Md Asiful Islam
    orcid: https://orcid.org/0000-0002-7173-383X
    username: ~Md_Asiful_Islam1
  - emails: '****@arizona.edu'
    first_name: Haris
    homepage: https://hriaz17.github.io/
    last_name: Riaz
    name: Haris Riaz
    username: ~Haris_Riaz1
  - dblp_id: https://dblp.org/pid/18/3479
    emails: '****@email.arizona.edu'
    first_name: Mihai
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=a3133-8AAAAJ&oi=sra
    homepage: http://surdeanu.info/mihai/
    institution: University of Arizona
    last_name: Surdeanu
    name: Mihai Surdeanu
    semantic_scholar_id: https://www.semanticscholar.org/author/M.-Surdeanu/1760868
    username: ~Mihai_Surdeanu1
  decision: toFindings
  end_page: 2594
  file: 626.pdf
  id: 626
  num_pages: 19
  openreview_id: NQkaJ0HqZl
  pdf_file: c14bb38001c6b6e61e8f8b49e3da3e9acaec07a8.pdf
  start_page: 2576
  title: 'Best of Both Worlds: A Pliable and Generalizable Neuro-Symbolic Approach
    for Relation Classification'
- abstract: This paper introduces Q-tuning, a novel approach for continual prompt
    tuning that enables the lifelong learning of a pre-trained language model. When
    learning a new task, Q-tuning trains a task-specific prompt by adding it to a
    prompt queue consisting of the prompts from older tasks. To better transfer the
    knowledge of old tasks, we design an adaptive knowledge aggregation technique
    that reweighs previous prompts in the queue with a learnable low-rank matrix.
    Once the prompt queue reaches its maximum capacity, we leverage a PCA-based eviction
    rule to reduce the queue's size, allowing the newly trained prompt to be added
    while preserving the primary knowledge of old tasks. In order to mitigate the
    accumulation of information loss caused by the eviction, we additionally propose
    a globally shared prefix prompt and a memory retention regularization based on
    information theory. Extensive experiments demonstrate that our approach outperforms
    the state-of-the-art methods substantially on continual prompt tuning benchmarks.
    Moreover, our approach enables lifelong learning on linearly growing task sequences
    while requiring constant complexity for training and inference.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Learning for NLP
  authors:
  - dblp_id: https://dblp.org/pid/84/7055
    emails: '****@gmail.com'
    first_name: Yanhui
    last_name: Guo
    name: Yanhui Guo
    orcid: https://orcid.org/0000-0002-9908-3795
    semantic_scholar_id: https://www.semanticscholar.org/author/Yanhui-Guo/2155598555
    username: ~Yanhui_Guo1
  - emails: '****@amazon.com'
    first_name: Shaoyuan
    google_scholar_id: https://scholar.google.com/citations?user=cTzILkQAAAAJ&hl=en
    institution: Amazon
    last_name: Xu
    name: Shaoyuan Xu
    username: ~Shaoyuan_Xu1
  - emails: '****@gmail.com'
    first_name: Jinmiao
    last_name: Fu
    name: Jinmiao Fu
    username: ~Jinmiao_Fu1
  - dblp_id: https://dblp.org/pid/49/1245
    emails: '****@ece.osu.edu'
    first_name: Jia
    google_scholar_id: https://scholar.google.com/citations?user=Ofx3dScAAAAJ&hl=en
    homepage: https://kevinliu-osu.github.io/index.html
    institution: The Ohio State University
    last_name: Liu
    name: Jia Liu
    username: ~Jia_Liu1
  - dblp_id: https://dblp.org/pid/225/6556
    emails: '****@amazon.com'
    first_name: Chaosheng
    google_scholar_id: https://scholar.google.com/citations?user=nPratvEAAAAJ&hl=en
    homepage: https://chaoshengdong.github.io/
    last_name: Dong
    name: Chaosheng Dong
    orcid: https://orcid.org/0000-0003-4491-0594
    username: ~Chaosheng_Dong1
  - emails: '****@amazon.com'
    first_name: Bryan
    homepage: https://www.linkedin.com/in/bryan-w-79ab1334/
    institution: Amazon
    last_name: Wang
    name: Bryan Wang
    username: ~Bryan_Wang2
  decision: toFindings
  end_page: 2622
  file: 628.pdf
  id: 628
  num_pages: 28
  openreview_id: yUzLKLdo4S
  pdf_file: e4642e7083fda6af67413375925cbaebe6760116.pdf
  start_page: 2595
  title: 'Q-Tuning: Queue-based Prompt Tuning for Lifelong Few-shot Language Learning'
- abstract: By allowing models to predict without task-specific training, in-context
    learning (ICL) with pretrained LLMs has enormous potential in NLP. However, a
    number of problems persist in ICL. In particular, its performance is sensitive
    to the choice and order of in-context examples. Given the same set of in-context
    examples with different orderings, model performance may vary from near random
    to near state-of-the-art. In this work, we formulate in-context example ordering
    as an optimization problem. We examine three problem settings that differ in the
    assumptions they make about what is known about the task. Inspired by the idea
    of learning from label proportions, we propose two principles for in-context example
    ordering guided by model's probability predictions. We apply our proposed principles
    to thirteen text classification datasets and nine different autoregressive LLMs
    with 700M to 13B parameters. We demonstrate that our approach outperforms the
    baselines by improving the classification accuracy, reducing model miscalibration,  and
    also by selecting better in-context examples.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Learning for NLP
  authors:
  - emails: '****@rutgers.edu'
    first_name: Zhichao
    google_scholar_id: https://scholar.google.com/citations?user=LmLtV_kAAAAJ&hl=en&authuser=1
    homepage: https://zhichaoxu-utah.github.io/
    institution: University of Utah
    last_name: Xu
    name: Zhichao Xu
    orcid: https://orcid.org/0000-0002-2370-4487
    username: ~Zhichao_Xu1
  - dblp_id: https://dblp.org/pid/85/2738
    emails: '****@brown.edu'
    first_name: Daniel
    google_scholar_id: https://scholar.google.com/citations?user=BGSFslUAAAAJ&hl=en
    institution: Brown University
    last_name: Cohen
    name: Daniel Cohen
    semantic_scholar_id: https://www.semanticscholar.org/author/144011619
    username: ~Daniel_Cohen2
  - emails: '****@gmail.com'
    first_name: Bei
    homepage: http://www.sci.utah.edu/~beiwang/
    institution: University of Utah
    last_name: Wang
    name: Bei Wang
    username: ~Bei_Wang3
  - dblp_id: https://dblp.org/pid/37/44
    emails: '****@cs.utah.edu'
    first_name: Vivek
    google_scholar_id: https://scholar.google.com/citations?user=TsTUfOIAAAAJ
    homepage: https://svivek.com
    institution: University of Utah
    last_name: Srikumar
    name: Vivek Srikumar
    semantic_scholar_id: https://www.semanticscholar.org/author/Vivek-Srikumar/3052879
    username: ~Vivek_Srikumar1
  decision: toFindings
  end_page: 2640
  file: 634.pdf
  id: 634
  num_pages: 18
  openreview_id: jYSzEP231V
  pdf_file: c4f2f2b367d41097bfe22fbdb3bbfa89e85de8b3.pdf
  start_page: 2623
  title: In-Context Example Ordering Guided by Label Distributions
- abstract: In this paper, we introduce the Financial-STS task, a financial domain-specific
    NLP task designed to measure the nuanced semantic similarity between pairs of
    financial narratives. These narratives originate from the financial statements
    of the same company but correspond to different periods, such as year-over-year
    comparisons. Measuring the subtle semantic differences between these paired narratives
    enables market stakeholders to gauge changes over time in the company's financial
    and operational situations, which is critical for financial decision-making. We
    find that existing pretrained embedding models and LLM embeddings fall short in
    discerning these subtle financial narrative shifts. To address this gap, we propose
    an LLM-augmented pipeline specifically designed for the Financial-STS task. Evaluation
    on a human-annotated dataset demonstrates that our proposed method outperforms
    existing methods trained on classic STS tasks and generic LLM embeddings.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - emails: '****@connect.ust.hk'
    first_name: Jiaxin
    homepage: https://www.linkedin.com/in/jiaxin-jessie-liu-430181184/
    last_name: Liu
    name: Jiaxin Liu
    username: ~Jiaxin_Liu5
  - emails: '****@ust.hk'
    first_name: Yi
    google_scholar_id: https://scholar.google.com.hk/citations?user=Prh_dHkAAAAJ&hl=en&authuser=2
    homepage: http://yya518.github.io/
    institution: Hong Kong University of Science and Technology
    last_name: Yang
    name: Yi Yang
    username: ~Yi_Yang7
  - emails: '****@ust.hk'
    first_name: Kar Yan
    google_scholar_id: https://scholar.google.com/citations?user=prT6OvEAAAAJ
    homepage: https://facultyprofiles.hkust.edu.hk/profiles.php?profile=kar-yan-tam-kytam
    last_name: Tam
    name: KAR YAN TAM
    orcid: https://orcid.org/0000-0003-3242-0184
    username: ~KAR_YAN_TAM1
  decision: toFindings
  end_page: 2652
  file: 636.pdf
  id: 636
  num_pages: 12
  openreview_id: CLStFkhu8M
  pdf_file: 66d046cf083e20796f259990bd5c98bb1c8b4ec6.pdf
  start_page: 2641
  title: 'Beyond Surface Similarity: Detecting Subtle Semantic Shifts in Financial
    Narratives'
- abstract: Off-the-shelf pre-trained language models have become the de facto standard
    in NLP pipelines for a multitude of downstream tasks. However, the inability of
    these models to properly encode numerals limits their performance on tasks requiring
    numeric comprehension. We introduce strategies to semantically prime numerals
    in any corpus by generating anchors governed by the distribution of numerals in
    said corpus, thereby enabling mathematically grounded representations of these
    numeral tokens. We establish the superiority of our proposed techniques through
    evaluation on a range of numeracy tasks for both in-domain (seen) and out-domain
    (unseen) numerals. Further, we expand our empirical evaluations to numerals ranging
    from 1 to 10 billion, a significantly broader range compared to previous studies
    of the same nature, and we demonstrate significant improvements in the mathematical
    grounding of our learned embeddings.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Linguistic theories, Cognitive Modeling and Psycholinguistics
  authors:
  - emails: '****@vt.edu'
    first_name: Mandar
    google_scholar_id: https://scholar.google.com/citations?user=b5M7mAoAAAAJ&hl=en
    homepage: https://people.cs.vt.edu/~mandarsharma/
    institution: Virginia Polytechnic Institute and State University
    last_name: Sharma
    name: Mandar Sharma
    semantic_scholar_id: https://www.semanticscholar.org/author/Mandar-Sharma/1928141171
    username: ~Mandar_Sharma1
  - emails: '****@vt.edu'
    first_name: Rutuja
    last_name: Taware
    name: Rutuja Taware
    username: ~Rutuja_Taware1
  - emails: '****@vanderbilt.edu'
    first_name: Pravesh
    homepage: https://praveshkoirala.com
    institution: Vanderbilt University
    last_name: Koirala
    name: Pravesh Koirala
    username: ~Pravesh_Koirala1
  - dblp_id: https://dblp.org/pid/174/2159
    emails: '****@stevens.edu'
    first_name: Nikhil
    google_scholar_id: https://scholar.google.com/citations?user=lQnOmqEAAAAJ&hl=en
    homepage: https://sites.google.com/view/nikhil-muralidhar
    institution: Stevens Institute of Technology
    last_name: Muralidhar
    name: Nikhil Muralidhar
    username: ~Nikhil_Muralidhar1
  - dblp_id: https://dblp.org/pid/r/NarenRamakrishnan.html
    emails: '****@cs.vt.edu'
    first_name: Naren
    google_scholar_id: https://scholar.google.com/citations?user=fMJwG7MAAAAJ&hl=en
    homepage: https://www.cs.vt.edu/~naren
    institution: Virginia Tech
    last_name: Ramakrishnan
    name: Naren Ramakrishnan
    username: ~Naren_Ramakrishnan1
  decision: toFindings
  end_page: 2660
  file: 640.pdf
  id: 640
  num_pages: 8
  openreview_id: HdVryuFPDG
  pdf_file: d0d066af9c7fc59345c4547e2ad56090330ea0c5.pdf
  start_page: 2653
  title: 'Laying Anchors: Semantically Priming Numerals in Language Modeling'
- abstract: 'Pre-training and fine-tuning framework has become the standard training
    paradigm for NLP tasks and is also widely used in industrial-level applications.
    However, there are still a limitation with this paradigm: simply fine-tuning with
    task-specific objectives tends to converge to local minima, resulting in a sub-optimal
    performance. In this paper, we first propose a new paradigm: knowledge rekindle,
    which aims to re-incorporate the fine-tuned expert model into the training cycle
    and break through the performance upper bounds of experts without introducing
    additional annotated data. Then we further propose a unified expert-guided pre-training
    (UEGP) framework for knowledge rekindle. Specifically, we reuse fine-tuned expert
    models for various downstream tasks as knowledge sources and inject task-specific
    prior knowledge to pre-trained language models (PLMs) by means of knowledge distillation.
    In this process, we perform multi-task learning with knowledge distillation and
    masked language modeling (MLM) objectives. We also further explored whether mixture-of-expert
    guided pre-training (MoEGP) can further enhance the effect of knowledge rekindle.
    Experiments and analysis on eight datasets in GLUE benchmark and a industrial-level
    search re-ranking dataset show the effectiveness of our method.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Learning for NLP
  authors:
  - emails: '****@bupt.edu.cn'
    first_name: Yutao
    homepage: https://murraytom.github.io/
    last_name: Mou
    name: Yutao Mou
    semantic_scholar_id: https://www.semanticscholar.org/author/Yutao-Mou/2048017347
    username: ~Yutao_Mou1
  - dblp_id: https://dblp.org/pid/53/5161
    emails: '****@vip.163.com'
    first_name: Kexiang
    institution: Alibaba Group
    last_name: Wang
    name: Kexiang Wang
    semantic_scholar_id: https://www.semanticscholar.org/author/Kexiang-Wang/94053409
    username: ~Kexiang_Wang1
  - emails: '****@hotmail.com'
    first_name: Jianhe
    google_scholar_id: https://scholar.google.com/citations?user=mmXztpoAAAAJ&hl=en
    last_name: Lin
    name: Jianhe Lin
    username: ~Jianhe_Lin1
  - dblp_id: https://dblp.org/pid/32/2706
    emails: '****@gmail.com'
    first_name: Dehong
    google_scholar_id: https://scholar.google.com/citations?user=mOlY7gUAAAAJ&hl=zh-CN
    homepage: https://madehong.github.io/
    last_name: Ma
    name: Dehong Ma
    username: ~Dehong_Ma1
  - emails: '****@baidu.com'
    first_name: Jun
    homepage: http://fanjun.info/about
    last_name: Fan
    name: Jun Fan
    username: ~Jun_Fan4
  - emails: '****@baidu.com'
    first_name: Daiting
    homepage: https://www.researchgate.net/profile/Daiting-Shi
    last_name: Shi
    name: Daiting Shi
    username: ~Daiting_Shi2
  - dblp_id: https://dblp.org/pid/88/8024
    emails: '****@baidu.com'
    first_name: Zhicong
    last_name: Cheng
    name: Zhicong Cheng
    username: ~Zhicong_Cheng1
  - emails: '****@baidu.com'
    first_name: Gu
    homepage: https://baike.baidu.com/item/%E8%BE%9C%E6%96%AF%E7%BC%AA/60897047
    last_name: Simiu
    name: Gu Simiu
    username: ~Gu_Simiu1
  - dblp_id: https://dblp.org/pid/91/4572
    emails: '****@acm.org'
    first_name: Dawei
    google_scholar_id: https://scholar.google.com/citations?user=GuQ9bpAAAAAJ&hl=zh-CN
    institution: Baidu
    last_name: Yin
    name: Dawei Yin
    username: ~Dawei_Yin1
  - dblp_id: https://dblp.org/pid/41/5448
    emails: '****@bupt.edu.cn'
    first_name: Weiran
    last_name: Xu
    name: Weiran Xu
    semantic_scholar_id: https://www.semanticscholar.org/author/Weiran-Xu/1753096
    username: ~Weiran_Xu1
  decision: toFindings
  end_page: 2673
  file: 641.pdf
  id: 641
  num_pages: 13
  openreview_id: BaH6EhkUrf
  pdf_file: e33d793a3596f007407acd9d8deb3e90921a4976.pdf
  start_page: 2661
  title: 'UEGP: Unified Expert-Guided Pre-training for Knowledge Rekindle'
- abstract: In the current user-server interaction paradigm of prompted generation
    with large language models (LLMs) on cloud, the server fully controls the generation
    process, which leaves zero options for users who want to keep the generated text
    private to themselves. For privacy-aware text generation on cloud, we propose
    LatticeGen, a cooperative protocol in which the server still handles most of the
    computation while the client controls the sampling operation. The key idea is
    that the true generated sequence is mixed with noise tokens by the client and
    hidden in a noised lattice. Only the client knows which tokens are the true ones.
    Considering potential attacks from a hypothetically malicious server and how the
    client can defend against it, we propose the repeated beam-search attack and the
    mixing noise scheme. In our experiments we apply LatticeGen to protect both prompt
    and generation. It is shown that while the noised lattice degrades generation
    quality, LatticeGen successfully protects the true generation to a remarkable
    degree under strong attacks (more than 50% of the semantic remains hidden as measured
    by BERTScore).
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Generation
  authors:
  - emails: '****@ucsd.edu'
    first_name: Mengke
    homepage: https://www.linkedin.com/in/mkzhang2001/
    last_name: Zhang
    name: Mengke Zhang
    username: ~Mengke_Zhang1
  - dblp_id: https://dblp.uni-trier.de/pers/hd/h/He:Tianxing
    emails: '****@gmail.com'
    first_name: Tianxing
    google_scholar_id: https://scholar.google.com/citations?user=egmfjjwAAAAJ&hl=zh-CN
    homepage: https://cloudygoose.github.io/
    last_name: He
    name: Tianxing He
    username: ~Tianxing_He1
  - dblp_id: https://dblp.org/pid/237/9664-3.html
    emails: '****@ucsd.edu'
    first_name: Tianle
    google_scholar_id: https://scholar.google.com/citations?hl=zh-CN&user=aWE2E5IAAAAJ
    homepage: https://wtl666wtl.github.io/
    last_name: Wang
    name: Tianle Wang
    username: ~Tianle_Wang1
  - dblp_id: https://dblp.org/pid/185/3258
    emails: '****@uw.edu'
    first_name: Lu
    google_scholar_id: https://scholar.google.com/citations?user=vokCG-MAAAAJ&hl=en&oi=ao
    homepage: https://lumimim.github.io
    institution: University of Washington and Allen Institute
    last_name: Mi
    name: Lu Mi
    semantic_scholar_id: https://www.semanticscholar.org/author/Lu-Mi/2057207935
    username: ~Lu_Mi1
  - dblp_id: https://dblp.uni-trier.de/pers/m/Mireshghallah:Fatemehsadat.html
    emails: '****@cs.washington.edu'
    first_name: Niloofar
    google_scholar_id: https://scholar.google.com/citations?user=WUCu45YAAAAJ&hl=en&authuser=2
    homepage: http://cseweb.ucsd.edu/~fmireshg/
    institution: University of Washington
    last_name: Mireshghallah
    name: Niloofar Mireshghallah
    username: ~Niloofar_Mireshghallah1
  - dblp_id: https://dblp.org/pid/133/2004.html
    emails: '****@gmail.com'
    first_name: Binyi
    google_scholar_id: https://scholar.google.com/citations?user=a89ft7EAAAAJ&hl=en
    last_name: Chen
    name: Binyi Chen
    orcid: https://orcid.org/0000-0003-0835-9678
    username: ~Binyi_Chen2
  - dblp_id: https://dblp.org/pid/w/HaoWang-14
    emails: '****@gmail.com'
    first_name: Hao
    google_scholar_id: https://scholar.google.com/citations?user=NrOA9QoAAAAJ&hl=en
    homepage: http://www.wanghao.in
    institution: Rutgers University
    last_name: Wang
    name: Hao Wang
    semantic_scholar_id: https://www.semanticscholar.org/author/Hao-Wang/49528584
    username: ~Hao_Wang3
  - dblp_id: https://dblp.org/pid/75/8157
    emails: '****@cs.washington.edu'
    first_name: Yulia
    google_scholar_id: https://scholar.google.com/citations?user=SEDPkrsAAAAJ&hl=en
    homepage: https://homes.cs.washington.edu/~yuliats/
    institution: Department of Computer Science, University of Washington
    last_name: Tsvetkov
    name: Yulia Tsvetkov
    username: ~Yulia_Tsvetkov1
  decision: toFindings
  end_page: 2690
  file: 643.pdf
  id: 643
  num_pages: 17
  openreview_id: FatKZ06YCx
  pdf_file: c617fa06fc03099e183edcbb64e94a9570acad42.pdf
  start_page: 2674
  title: 'LatticeGen: Hiding Generated Text in a Lattice for Privacy-Aware Large Language
    Model Generation on Cloud'
- abstract: "To protect users from massive hateful content, existing works studied\
    \ automated hate speech detection. Despite the existing efforts, one question\
    \ remains: Do automated hate speech detectors conform to social media content\
    \ policies? A platform's content policies are a checklist of content moderated\
    \ by the social media platform. Because content moderation rules are often uniquely\
    \ defined, existing hate speech datasets cannot directly answer this question.\
    \ \n\nThis work seeks to answer this question by creating HateModerate, a dataset\
    \ for testing the behaviors of automated content moderators against content policies.\
    \ First, we engage 28 annotators and GPT in a six-step annotation process, resulting\
    \ in a list of hateful and non-hateful test suites matching each of Facebook's\
    \ 41 hate speech policies. Second, we test the performance of state-of-the-art\
    \ hate speech detectors against HateModerate, revealing substantial failures these\
    \ models have in their conformity to the policies. Third, using HateModerate,\
    \ we augment the training data of a top-downloaded hate detector on HuggingFace.\
    \ We observe significant improvement in the models' conformity to content policies\
    \ while having comparable scores on the original test data. Our dataset and code\
    \ can be found on https://github.com/stevens-textmining/HateModerate."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Ethics, Bias, and Fairness
  authors:
  - emails: '****@stevens.edu'
    first_name: Jiangrui
    institution: Stevens Institute of Technology
    last_name: Zheng
    name: Jiangrui Zheng
    username: ~Jiangrui_Zheng2
  - dblp_id: https://dblp.org/pid/92/1399-1.html
    emails: '****@stevens.edu'
    first_name: Xueqing
    google_scholar_id: https://scholar.google.com/citations?user=g5W4khgAAAAJ&hl=en
    institution: Stevens Institute of Technology
    last_name: Liu
    name: Xueqing Liu
    semantic_scholar_id: https://www.semanticscholar.org/author/Xueqing-Liu/1908282
    username: ~Xueqing_Liu3
  - dblp_id: https://dblp.org/pid/272/0796
    emails: '****@jpmchase.com'
    first_name: Mirazul
    google_scholar_id: https://scholar.google.com/citations?user=1YLCVDgAAAAJ&hl=en
    homepage: https://www.linkedin.com/in/mirazul-haque-b4b331a6/
    institution: J.P. Morgan Chase
    last_name: Haque
    name: Mirazul Haque
    username: ~Mirazul_Haque1
  - emails: '****@stevens.edu'
    first_name: Xing
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=ITsH40UAAAAJ
    institution: Stevens Institute of Technology
    last_name: Qian
    name: Xing Qian
    username: ~Xing_Qian1
  - emails: '****@stevens.edu'
    first_name: Guanqun
    google_scholar_id: https://scholar.google.com/citations?user=4XUIHW0AAAAJ&hl=en&authuser=1
    homepage: https://guanqun-yang.github.io
    last_name: Yang
    name: Guanqun Yang
    orcid: https://orcid.org/0000-0002-3354-1132
    semantic_scholar_id: https://www.semanticscholar.org/author/Guanqun-Yang/2714361
    username: ~Guanqun_Yang1
  - dblp_id: https://dblp.org/pid/03/1094-13
    emails: '****@utdallas.edu'
    first_name: Wei
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=CEVHpQUAAAAJ
    homepage: http://youngwei.com/
    institution: University of Texas, Dallas
    last_name: Yang
    name: Wei Yang
    username: ~Wei_Yang7
  decision: toFindings
  end_page: 2710
  file: 644.pdf
  id: 644
  num_pages: 20
  openreview_id: Ymr3xeQeUM
  pdf_file: 063ac7c79b511c40705615616357127faf49dd17.pdf
  start_page: 2691
  title: 'HateModerate: Testing Hate Speech Detectors against Content Moderation Policies'
- abstract: Emergent Large Language Models (LLMs) use their extraordinary performance
    and powerful deduction capacity to discern from traditional language models. However,
    the expenses of computational resources and storage for these LLMs are stunning,
    quantization then arises as a trending conversation. To address accuracy decay
    caused by quantization, two streams of works in post-training quantization methods
    stand out. One uses other weights to compensate existing quantization error, while
    the other transfers the quantization difficulty to other parts in the model. Combining
    both merits, we introduce Learnable Singular value Increment (LSI) as an advanced
    solution. LSI uses Singular Value Decomposition to extract singular values of
    the weights and make them learnable to help weights compensate each other conditioned
    on activation. Incorporating LSI with existing techniques, we achieve state-of-the-art
    performance in diverse quantization settings, no matter in weight-only, weight-activation
    or extremely low bit scenarios. By unleashing the potential of LSI, efficient
    finetuning on quantized model is no longer a prohibitive problem.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Efficient/Low-Resource Methods for NLP
  authors:
  - emails: '****@qq.com'
    first_name: Yifei
    homepage: https://github.com/yileijin
    last_name: Gao
    name: Yifei Gao
    username: ~Yifei_Gao3
  - emails: '****@gmail.com'
    first_name: Jie
    homepage: https://blog.csdn.net/github_36923418
    last_name: Ou
    name: Jie Ou
    orcid: https://orcid.org/0000-0002-1159-2043
    username: ~Jie_Ou1
  - emails: '****@siat.ac.cn'
    first_name: Lei
    homepage: https://people.ucas.ac.cn/~wanglei1006?language=en
    institution: Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences,
      Chinese Academy of Sciences
    last_name: Wang
    name: Lei Wang
    orcid: https://orcid.org/0000-0001-5990-896X
    username: ~Lei_Wang18
  - emails: '****@mail.bnu.edu.cn'
    first_name: Yuting
    homepage: https://orcid.org/0000-0003-3085-4405
    last_name: Xiao
    name: Yuting Xiao
    orcid: https://orcid.org/0000-0003-3085-4405
    username: ~Yuting_Xiao2
  - emails: '****@tju.edu.cn'
    first_name: Xiangzhiyuan
    homepage: https://github.com/yydzy
    last_name: Xiangzhiyuan
    name: xiangzhiyuan
    username: ~xiangzhiyuan1
  - emails: '****@uestc.edu.cn'
    first_name: Ruiting
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=tPozGQwAAAAJ&view_op=list_works&gmla=AH70aAUUIihD7hRbRtSqNA6DDbX13UG3BFH12pVfuh0loUkCQ0P-vP0n0xPM_BJgvJr3jY32QXH1EvVB84sIKIbJ
    last_name: Dai
    name: Ruiting Dai
    username: ~Ruiting_Dai1
  - dblp_id: https://dblp.uni-trier.de/pers/hd/c/Cheng_0002:Jun
    emails: '****@siat.ac.cn'
    first_name: Jun
    google_scholar_id: https://scholar.google.com/citations?view_op=list_works&hl=zh-CN&hl=zh-CN&user=41p5vIYAAAAJ
    institution: Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences,
      Chinese Academy of Sciences
    last_name: Cheng
    name: Jun Cheng
    username: ~Jun_Cheng6
  decision: toFindings
  end_page: 2722
  file: 654.pdf
  id: 654
  num_pages: 12
  openreview_id: 3NtDYiGyWQ
  pdf_file: 8efd5674ce476285af8b56fbc418bec7fc5fb1b3.pdf
  start_page: 2711
  title: 'Compensate Quantization Errors: Make Weights Hierarchical to Compensate
    Each Other'
- abstract: 'There exists a discrepancy between the token-level objective during training
    and the overall sequence-level quality that is expected from the model. This discrepancy
    leads to issues like exposure bias.

    To align the model with human expectations, sequence-level objectives are often
    used to fine-tune pre-trained models.

    In this paper, we introduce a contrastive preference model that enhances the traditional
    Plackett-Luce model by incorporating an indicator function. Building upon this
    novel preference model, we propose Contrastive Preference Learning (CPL), which
    uses offline samples with list-wise preferences to fine-tune a pre-trained model
    in Neural Machine Translation. Our experiments, conducted on three language pairs,
    demonstrate that CPL outperforms not only the vanilla Transformer model but also
    other token-level and sequence-level baselines. Furthermore, the ablation study
    highlights the essential role of the proposed indicator function in achieving
    this improvement.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Translation
  authors:
  - emails: '****@my.cityu.edu.hk'
    first_name: Jianfei
    homepage: https://scholars.cityu.edu.hk/en/persons/jianfei-he(a2c18133-82d1-40b7-999c-195791800882).html
    institution: City University of Hong Kong
    last_name: He
    name: Jianfei He
    username: ~Jianfei_He1
  - emails: '****@connect.polyu.hk'
    first_name: Shichao
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=M7g3H9YAAAAJ
    homepage: https://shichaosun.github.io
    institution: The Hong Kong Polytechnic University
    last_name: Sun
    name: Shichao Sun
    username: ~Shichao_Sun1
  - dblp_id: https://dblp.org/pid/72/7845
    emails: '****@my.cityu.edu.hk'
    first_name: Sen
    google_scholar_id: https://scholar.google.com/citations?user=7jRjatEAAAAJ&hl=zh-CN
    last_name: Peng
    name: Sen Peng
    orcid: https://orcid.org/0000-0002-3204-4409
    username: ~Sen_Peng2
  - emails: '****@my.cityu.edu.hk'
    first_name: Jie
    homepage: https://xujie.ink/
    last_name: Xu
    name: Jie Xu
    username: ~Jie_Xu21
  - dblp_id: https://dblp.org/pid/j/XiaohuaJia.html
    emails: '****@cityu.edu.hk'
    first_name: Xiaohua
    homepage: http://www.cs.cityu.edu.hk/~jia
    last_name: Jia
    name: Xiaohua Jia
    orcid: https://orcid.org/0000-0001-8702-8302
    username: ~Xiaohua_Jia1
  - dblp_id: https://dblp.org/pid/33/3999
    emails: '****@comp.polyu.edu.hk'
    first_name: Wenjie
    google_scholar_id: https://scholar.google.com/citations?user=Rx5swD4AAAAJ&hl=en
    homepage: https://web.comp.polyu.edu.hk/cswjli/
    institution: The Hong Kong Polytechnic University, The Hong Kong Polytechnic University
    last_name: Li
    name: Wenjie Li
    orcid: https://orcid.org/0000-0002-7360-8864
    semantic_scholar_id: https://www.semanticscholar.org/author/Wenjie-Li/50135338
    username: ~Wenjie_Li1
  decision: toFindings
  end_page: 2734
  file: 660.pdf
  id: 660
  num_pages: 12
  openreview_id: yXCDnAZvKz
  pdf_file: 4cb518e035ff5f4c9ab996738d7b1114aff42363.pdf
  start_page: 2723
  title: Contrastive Preference Learning for Neural Machine Translation
- abstract: To comprehensively gauge the capacity of current models for complex reasoning,
    it is crucial to assess their step-by-step reasoning in a scalable manner. Established
    reference-based evaluation metrics rely on human-annotated reasoning chains as
    references to assess the model-derived chains. However, such ``gold-standard''
    human-written reasoning chains may not be unique and their acquisition is often
    labor-intensive. Existing reference-free reasoning evaluation metrics, while eliminating
    the need for human-crafted reasoning chains as references, often require fine-tuning
    with human-derived chains before evaluation, complicating the process and questioning
    their adaptability to other datasets. To address these challenges, we harness
    GPT-4 to automatically evaluate reasoning chain quality, thereby removing the
    dependency on human-written reasoning chains for both model fine-tuning and evaluative
    purposes.  Leveraging the Socratic method, we develop SocREval (**Soc**ratic Method-Inspired
    **R**easoning **Eval**uation), a novel approach for prompt design in reference-free
    reasoning evaluation. Empirical results from four human annotated datasets reveal
    that SocREval significantly improves GPT-4's performance, surpassing existing
    reference-free and reference-based reasoning evaluation metrics. Beyond its demonstrated
    efficacy, SocREval, proves to be both cost-efficient and robust to prompt writing
    and example selection, as substantiated by our in-depth analysis.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - dblp_id: https://dblp.org/pid/190/7762-1.html
    emails: '****@rochester.edu'
    first_name: Hangfeng
    google_scholar_id: https://scholar.google.com/citations?user=BbpI6QoAAAAJ&hl=en
    homepage: https://hornhehhf.github.io
    institution: University of Rochester
    last_name: He
    name: Hangfeng He
    orcid: https://orcid.org/0000-0001-5136-1218
    username: ~Hangfeng_He3
  - dblp_id: https://dblp.org/pid/48/859.html
    emails: '****@cse.ust.hk'
    first_name: Hongming
    google_scholar_id: https://scholar.google.com/citations?user=i5ETuuQAAAAJ&hl=en
    homepage: http://www.cse.ust.hk/~hzhangal/
    last_name: Zhang
    name: Hongming Zhang
    username: ~Hongming_Zhang2
  - dblp_id: https://dblp.org/pid/r/DanRoth
    emails: '****@seas.upenn.edu'
    first_name: Dan
    google_scholar_id: https://scholar.google.com/citations?user=E-bpPWgAAAAJ&hl=en
    homepage: https://www.cis.upenn.edu/~danroth/
    institution: Amazon and University of Pennsylvania
    last_name: Roth
    name: Dan Roth
    semantic_scholar_id: https://www.semanticscholar.org/author/D.-Roth/144590225
    username: ~Dan_Roth3
  decision: toFindings
  end_page: 2763
  file: 667.pdf
  id: 667
  num_pages: 29
  openreview_id: g9ulVkvz0d
  pdf_file: 9a701bb1650cf37722c194536806719194e61245.pdf
  start_page: 2735
  title: 'SocREval: Large Language Models with the Socratic Method for Reference-free
    Reasoning Evaluation'
- abstract: 'Large language models (LLMs) have demonstrated remarkable potential in
    handling multilingual machine translation (MMT). In this paper, we systematically
    investigate the advantages and challenges of LLMs for MMT by answering two questions:
    1) How well do LLMs perform in translating massive languages? 2) Which factors
    affect LLMs'' performance in translation? We thoroughly evaluate eight popular
    LLMs, including ChatGPT and GPT-4. Our empirical results show that translation
    capabilities of LLMs are continually involving. GPT-4 has beat the strong supervised
    baseline NLLB in 40.91\% of translation directions but still faces a large gap
    towards the commercial translation system like Google Translate, especially on
    low-resource languages. Through further analysis, we discover that LLMs exhibit
    new working patterns when used for MMT. First, LLM can acquire translation ability
    in a resource-efficient way and generate moderate translation even on zero-resource
    languages. Second, instruction semantics can surprisingly be ignored when given
    in-context exemplars. Third, cross-lingual exemplars can provide better task guidance
    for low-resource translation than exemplars in the same language pairs. Code will
    be released at: https://github.com/NJUNLP/MMT-LLM.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Translation
  authors:
  - dblp_id: https://dblp.org/pid/90/7062
    emails: '****@smail.nju.edu.cn'
    first_name: Wenhao
    google_scholar_id: https://scholar.google.com/citations?hl=zh-CN&user=0kXlmmAAAAAJ
    homepage: https://owennju.github.io/
    institution: Nanjing University
    last_name: Zhu
    name: Wenhao Zhu
    semantic_scholar_id: https://www.semanticscholar.org/author/Wenhao-Zhu/2131383723
    username: ~Wenhao_Zhu1
  - dblp_id: https://dblp.org/pid/45/4076-8
    emails: '****@sjtu.edu.cn'
    first_name: Hongyi
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=rn18Dc4AAAAJ&view_op=list_works&gmla=AHoSzlXMvU15QFKF_6Bmu5gliexiPLU6l4BSwdvCqLTg6uHl2s6gFQUmNio0Lxezu5XFawt6tsRcYDGUxoZRYTUTcdmgOw21kehPKpp384O6HUUfMYEb7xSqiX2AJDJQIHiUi0s
    last_name: Liu
    name: Hongyi Liu
    orcid: https://orcid.org/0009-0001-0810-474X
    semantic_scholar_id: https://www.semanticscholar.org/author/Hongyi-Liu/2115669628
    username: ~Hongyi_Liu4
  - dblp_id: https://dblp.org/pid/284/0673
    emails: '****@stu.pku.edu.cn'
    first_name: Qingxiu
    google_scholar_id: https://scholar.google.com/citations?user=ibcR7VkAAAAJ&hl=zh-CN
    homepage: https://dqxiu.github.io/
    last_name: Dong
    name: Qingxiu Dong
    semantic_scholar_id: https://www.semanticscholar.org/author/Qingxiu-Dong/2047143813
    username: ~Qingxiu_Dong1
  - dblp_id: https://dblp.org/pid/25/624
    emails: '****@pku.edu.cn'
    first_name: Jingjing
    last_name: Xu
    name: Jingjing Xu
    username: ~Jingjing_Xu1
  - dblp_id: https://dblp.org/pid/57/8451
    emails: '****@nju.edu.cn'
    first_name: Shujian
    google_scholar_id: https://scholar.google.com/citations?user=HF3-E9kAAAAJ&hl=en
    homepage: http://nlp.nju.edu.cn/huangsj/
    institution: Nanjing University
    last_name: Huang
    name: Shujian Huang
    username: ~Shujian_Huang1
  - dblp_id: https://dblp.org/pid/144/7656
    emails: '****@cs.hku.hk'
    first_name: Lingpeng
    google_scholar_id: https://scholar.google.com/citations?user=f1hBi5wAAAAJ&hl=en
    homepage: https://ikekonglp.github.io/
    institution: Department of Computer Science, The University of Hong Kong
    last_name: Kong
    name: Lingpeng Kong
    username: ~Lingpeng_Kong1
  - emails: '****@nju.edu.cn'
    first_name: Jiajun
    google_scholar_id: https://scholar.google.com.tw/citations?user=WIF7VaoAAAAJ
    homepage: https://cs.nju.edu.cn/chenjiajun/index_en.htm
    institution: Nanjing University
    last_name: Chen
    name: Jiajun Chen
    username: ~Jiajun_Chen1
  - dblp_id: https://dblp.org/pid/13/7007-5.html
    emails: '****@cs.cmu.edu'
    first_name: Lei
    google_scholar_id: https://scholar.google.com/citations?user=BYXqAlwAAAAJ&hl=en
    homepage: https://www.cs.cmu.edu/~leili
    institution: School of Computer Science, Carnegie Mellon University
    last_name: Li
    name: Lei Li
    orcid: https://orcid.org/0000-0003-3095-9776
    semantic_scholar_id: https://www.semanticscholar.org/author/Lei-Li/143900005
    username: ~Lei_Li11
  decision: toFindings
  end_page: 2780
  file: 672.pdf
  id: 672
  num_pages: 17
  openreview_id: VuQJIH5Y6Z
  pdf_file: 424169ae12e0e2e70c71e53688236d6199a03c6b.pdf
  start_page: 2764
  title: 'Multilingual Machine Translation with Large Language Models: Empirical Results
    and Analysis'
- abstract: "Court View Generation (CVG) plays a vital role in the realm of legal\
    \ artificial intelligence, which aims to support judges in crafting legal judgment\
    \ documents. \nThe court view consists of three essential judgment parts: the\
    \ charge-related, law article-related, and prison term-related parts, each requiring\
    \ specialized legal knowledge, rendering CVG a challenging task.\nAlthough Large\
    \ Language Models (LLMs) have made remarkable strides in language generation,\
    \ they encounter difficulties in the knowledge-intensive legal domain.\nActually,\
    \ there can be two types of knowledge: internal knowledge stored within LLMs'\
    \ parameters and external knowledge sourced from legal documents outside the models.\n\
    In this paper, we decompose court views into different parts, stimulate internal\
    \ knowledge, and incorporate external information to unleash the power of LLMs\
    \ in the CVG task.\nTo validate our method, we conduct a series of experiment\
    \ results on two real-world datasets LAIC2021 and CJO2022. The experiments demonstrate\
    \ that our method is capable of generating more accurate and reliable court views."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - emails: '****@zju.edu.cn'
    first_name: Yifei
    last_name: Liu
    name: Yifei Liu
    username: ~Yifei_Liu1
  - dblp_id: https://dblp.org/pid/43/2784
    emails: '****@zju.edu.cn'
    first_name: Yiquan
    google_scholar_id: https://scholar.google.com/citations?user=ZTampvYAAAAJ&hl=en&oi=ao
    homepage: https://wuyiquan.github.io/
    institution: Zhejiang University
    last_name: Wu
    name: Yiquan Wu
    semantic_scholar_id: https://www.semanticscholar.org/author/Yiquan-Wu/2115290080
    username: ~Yiquan_Wu3
  - emails: '****@zju.edu.cn'
    first_name: Ang
    google_scholar_id: https://scholar.google.cz/citations?user=evbApx4AAAAJ&hl=zh-CN
    last_name: Li
    name: Ang Li
    orcid: https://orcid.org/0000-0002-6286-8687
    username: ~Ang_Li22
  - dblp_id: https://dblp.org/pid/29/5889
    emails: '****@alibaba-inc.com'
    first_name: Yating
    last_name: Zhang
    name: Yating Zhang
    username: ~Yating_Zhang1
  - dblp_id: https://dblp.uni-trier.de/pers/hd/s/Sun:Changlong
    emails: '****@taobao.com'
    first_name: Changlong
    google_scholar_id: https://scholar.google.com/citations?&user=MvoNzW0AAAAJ
    institution: Alibaba Group
    last_name: Sun
    name: Changlong Sun
    username: ~Changlong_Sun2
  - dblp_id: https://dblp.org/pid/41/5305-1.html
    emails: '****@zju.edu.cn'
    first_name: Weiming
    homepage: https://person.zju.edu.cn/lwm
    institution: Zhejiang University
    last_name: Lu
    name: Weiming Lu
    semantic_scholar_id: https://www.semanticscholar.org/author/Weiming-Lu/1776903
    username: ~Weiming_Lu1
  - dblp_id: https://dblp.org/pid/84/3254-1
    emails: '****@cs.zju.edu.cn'
    first_name: Fei
    google_scholar_id: https://scholar.google.com/citations?user=XJLn4MYAAAAJ&hl=zh-CN&oi=ao
    homepage: https://person.zju.edu.cn/wufei
    institution: Zhejiang University
    last_name: Wu
    name: Fei Wu
    username: ~Fei_Wu2
  - dblp_id: https://dblp.org/pid/194/4245
    emails: '****@zju.edu.cn'
    first_name: Kun
    google_scholar_id: https://scholar.google.com.hk/citations?user=FOsNiMQAAAAJ&hl=en
    homepage: http://kunkuang.github.io
    institution: Zhejiang University
    last_name: Kuang
    name: Kun Kuang
    username: ~Kun_Kuang1
  decision: toFindings
  end_page: 2791
  file: 673.pdf
  id: 673
  num_pages: 11
  openreview_id: DN9XI1ZXSk
  pdf_file: 186cc6465b9dd0971f785d2fdc53b8a88f75fea4.pdf
  start_page: 2781
  title: Unleashing the Power of LLMs in Court View Generation by Stimulating Internal
    Knowledge and Incorporating External Knowledge
- abstract: Referring Expression Generation (REG) is the task of generating a description
    that unambiguously identifies a given target in the scene. Different from Image
    Captioning (IC), REG requires learning fine-grained characteristics of not only
    the scene objects but also their surrounding context. Referring expressions are
    usually not singular; an object can often be uniquely referenced in numerous ways,
    for instance, by color, by location, or by relationship with other objects. Most
    prior works, however, have not explored this 'aspect-based multiplicity' of referring
    expressions. Hence, in this work, we focus on the Aspect-Controlled REG task,
    which requires generating a referring expression conditioned on the input aspect(s),
    where an aspect captures a style of reference. By changing the input aspect such
    as color, location, action etc., one can generate multiple distinct expressions
    per target region. To solve this new task, we first modify BLIP for aligning image-regions
    and text-expressions. We achieve this through a novel approach for feeding the
    input by drawing a bounding box around the target image-region and prompting the
    model to generate the referring expression. Our base REG model already beats all
    prior works in CIDEr score. To tackle Aspect-Controlled REG, we append `aspect
    tokens' to the prompt and show that distinct expressions can be generated by just
    changing the prompt. Finally, to prove the high-quality and diversity of the data
    generated by our proposed aspect-controlled REG model, we also perform data-augmentation-based
    evaluation on the downstream Referring Expression Comprehension (REC) task. With
    just half of the real data augmented with the generated synthetic data, we achieve
    performance comparable to training with 100\% of real data, using a SOTA REC model.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multimodality and Language Grounding to Vision, Robotics and Beyond
  authors:
  - dblp_id: https://dblp.org/pid/123/7071.html
    emails: '****@ucla.edu'
    first_name: Danfeng
    last_name: Guo
    name: DANFENG GUO
    orcid: https://orcid.org/0000-0002-9023-6169
    username: ~DANFENG_GUO1
  - dblp_id: https://dblp.org/pid/70/4006
    emails: '****@gmail.com'
    first_name: Sanchit
    google_scholar_id: https://scholar.google.com/citations?user=ypBfdSoAAAAJ&hl=en
    institution: Amazon
    last_name: Agarwal
    name: Sanchit Agarwal
    semantic_scholar_id: https://www.semanticscholar.org/author/Sanchit-Agarwal/150290293
    username: ~Sanchit_Agarwal1
  - emails: '****@gmail.com'
    first_name: Arpit
    google_scholar_id: https://scholar.google.com/citations?user=XXVjLVgAAAAJ&hl=en
    institution: Amazon
    last_name: Gupta
    name: Arpit Gupta
    username: ~Arpit_Gupta1
  - dblp_id: https://dblp.org/pid/158/9756
    emails: '****@gmail.com'
    first_name: Jiun-Yu
    institution: Amazon Alexa AI
    last_name: Kao
    name: Jiun-Yu Kao
    semantic_scholar_id: https://www.semanticscholar.org/author/Jiun-Yu-Kao/38705864?sort=pub-date
    username: ~Jiun-Yu_Kao1
  - dblp_id: https://dblp.org/pid/141/7475.html
    emails: '****@amazon.com'
    first_name: Emre
    homepage: https://www.amazon.science/author/emre-barut
    institution: Amazon
    last_name: Barut
    name: Emre Barut
    semantic_scholar_id: https://www.semanticscholar.org/author/Emre-Barut/34974515
    username: ~Emre_Barut1
  - dblp_id: https://dblp.org/pid/02/8178
    emails: '****@amazon.com'
    first_name: Tagyoung
    google_scholar_id: https://scholar.google.com/citations?user=_-egoNcAAAAJ&hl=en
    institution: Amazon
    last_name: Chung
    name: Tagyoung Chung
    semantic_scholar_id: https://www.semanticscholar.org/author/Tagyoung-Chung/2878984
    username: ~Tagyoung_Chung2
  - dblp_id: https://dblp.org/pid/14/4834-19
    emails: '****@gmail.com'
    first_name: Jing
    google_scholar_id: https://scholar.google.com/citations?user=ocPXoIkAAAAJ&hl=en
    institution: Amazon Alexa AI
    last_name: Huang
    name: Jing Huang
    orcid: https://orcid.org/0000-0001-8769-9130
    semantic_scholar_id: https://www.semanticscholar.org/author/Jing-Huang/49025046
    username: ~Jing_Huang3
  - dblp_id: https://dblp.org/pid/32/5243.html
    emails: '****@cs.unc.edu'
    first_name: Mohit
    google_scholar_id: https://scholar.google.com/citations?user=DN8QtscAAAAJ&hl=en
    homepage: https://www.cs.unc.edu/~mbansal/
    institution: University of North Carolina at Chapel Hill
    last_name: Bansal
    name: Mohit Bansal
    username: ~Mohit_Bansal2
  decision: toFindings
  end_page: 2806
  file: 674.pdf
  id: 674
  num_pages: 15
  openreview_id: bgMURKqkJ0
  pdf_file: f4fcbeb3f6c6148ce049d2148a39829d1277345b.pdf
  start_page: 2792
  title: Prompting Vision-Language Models For Aspect-Controlled Generation of Referring
    Expressions
- abstract: Textual backdoor attacks pose significant security threats. Current detection
    approaches, typically relying on intermediate feature representation or reconstructing
    potential triggers, are task-specific and less effective beyond sentence classification,
    struggling with tasks like question answering and named entity recognition. We
    introduce TABDet (Task-Agnostic Backdoor Detector), a pioneering task-agnostic
    method for backdoor detection. TABDet leverages final layer logits combined with
    an efficient pooling technique, enabling unified logit representation across three
    prominent NLP tasks. TABDet can jointly learn from diverse task-specific models,
    demonstrating superior detection efficacy over traditional task-specific methods.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - dblp_id: https://dblp.org/pid/241/6097
    emails: '****@stonybrook.edu'
    first_name: Weimin
    last_name: Lyu
    name: Weimin Lyu
    username: ~Weimin_Lyu1
  - dblp_id: https://dblp.org/pid/09/1280
    emails: '****@sri.com'
    first_name: Xiao
    institution: SRI International
    last_name: Lin
    name: Xiao Lin
    username: ~Xiao_Lin2
  - dblp_id: https://dblp.org/pid/226/4925
    emails: '****@morganstanley.com'
    first_name: Songzhu
    google_scholar_id: https://scholar.google.com/citations?user=vq0hpV4AAAAJ&hl=zh-CN
    institution: Morgan Stanley
    last_name: Zheng
    name: Songzhu Zheng
    username: ~Songzhu_Zheng1
  - dblp_id: https://dblp.org/pid/191/4669-1
    emails: '****@stonybrook.edu'
    first_name: Lu
    google_scholar_id: https://scholar.google.com/citations?hl=zh-CN&user=yzClUtoAAAAJ
    institution: State University of New York at Stony Brook
    last_name: Pang
    name: Lu Pang
    username: ~Lu_Pang2
  - dblp_id: https://dblp.org/pid/93/3488
    emails: '****@gmail.com'
    first_name: Haibin
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=v3w4IYUAAAAJ
    homepage: https://www3.cs.stonybrook.edu/~hling/
    institution: State University of New York, Stony Brook
    last_name: Ling
    name: Haibin Ling
    username: ~Haibin_Ling1
  - dblp_id: https://dblp.uni-trier.de/pers/j/Jha:Susmit
    emails: '****@sri.com'
    first_name: Susmit
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=wPNgzO4AAAAJ
    homepage: http://susmitjha.github.io/
    institution: SRI International
    last_name: Jha
    name: Susmit Jha
    orcid: https://orcid.org/0000-0001-5983-9095
    username: ~Susmit_Jha1
  - dblp_id: https://dblp.org/pid/66/3019-12
    emails: '****@stonybrook.edu'
    first_name: Chao
    google_scholar_id: https://scholar.google.com/citations?user=J-iIIFAAAAAJ&hl=en
    homepage: https://chaochen.github.io/
    institution: State University of New York at Stony Brook
    last_name: Chen
    name: Chao Chen
    username: ~Chao_Chen1
  decision: toFindings
  end_page: 2821
  file: 679.pdf
  id: 679
  num_pages: 15
  openreview_id: H6rxYyTy8w
  pdf_file: 5af3e87a09f94fdc0d92a6faf2260694246ead19.pdf
  start_page: 2807
  title: Task-Agnostic Detector for Insertion-Based Backdoor Attacks
- abstract: 'Sequential labeling is a task predicting labels for each token in a sequence,
    such as Named Entity Recognition (NER). NER tasks aim to extract entities and
    predict their labels given a text, which is important in information extraction.
    Although previous works have shown great progress in improving NER performance,
    uncertainty estimation on NER (UE-NER) is still underexplored but essential. This
    work focuses on UE-NER, which aims to estimate uncertainty scores for the NER
    predictions. Previous uncertainty estimation models often overlook two unique
    characteristics of NER: the connection between entities (i.e., one entity embedding
    is learned based on the other ones) and wrong span cases in the entity extraction
    subtask. Therefore, we propose a Sequential Labeling Posterior Network (SLPN)
    to estimate uncertainty scores for the extracted entities, considering uncertainty
    transmitted from other tokens. Moreover, we have defined an evaluation strategy
    to address the specificity of wrong-span cases. Our SLPN has achieved significant
    improvements on three datasets, such as a 5.54-point improvement in AUPR on the
    MIT-Restaurant dataset. Our code is available at \url{https://github.com/he159ok/UncSeqLabeling_SLPN}.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Extraction
  authors:
  - dblp_id: https://dblp.org/pid/93/8352
    emails: '****@vt.edu'
    first_name: Jianfeng
    google_scholar_id: https://scholar.google.com/citations?user=_gAf96sAAAAJ&hl=en
    homepage: https://jianfenghe-vt.netlify.app/
    institution: Virginia Tech
    last_name: He
    name: Jianfeng He
    username: ~Jianfeng_He1
  - dblp_id: https://dblp.org/pid/204/9716
    emails: '****@utdallas.edu'
    first_name: Linlin
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=3pu55HoAAAAJ
    last_name: Yu
    name: Linlin Yu
    username: ~Linlin_Yu1
  - dblp_id: https://dblp.org/pid/183/5433
    emails: '****@vt.edu'
    first_name: Shuo
    google_scholar_id: https://scholar.google.com/citations?user=vm368LkAAAAJ&hl=en
    homepage: https://slei109.github.io/
    institution: Sony Coporation of America
    last_name: Lei
    name: Shuo Lei
    semantic_scholar_id: https://www.semanticscholar.org/author/Shuo-Lei/3433489
    username: ~Shuo_Lei1
  - dblp_id: https://dblp.uni-trier.de/pers/l/Lu:Chang=Tien.html
    emails: '****@vt.edu'
    first_name: Chang-Tien
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=TJI8JW0AAAAJ
    homepage: http://people.cs.vt.edu/~ctlu/
    institution: Virginia Tech
    last_name: Lu
    name: Chang-Tien Lu
    orcid: https://orcid.org/0000-0003-3675-0199
    username: ~Chang-Tien_Lu1
  - dblp_id: https://dblp.org/pid/21/3047-1
    emails: '****@utdallas.edu'
    first_name: Feng
    google_scholar_id: https://scholar.google.com/citations?user=KOQ-SSYAAAAJ&hl=en
    homepage: https://personal.utdallas.edu/~fxc190007/
    institution: University of Texas, Dallas
    last_name: Chen
    name: Feng Chen
    username: ~Feng_Chen7
  decision: toFindings
  end_page: 2834
  file: 680.pdf
  id: 680
  num_pages: 13
  openreview_id: qoewdkqBld
  pdf_file: f98810cffd18df59b5a4c760c705aead82796ae0.pdf
  start_page: 2822
  title: Uncertainty Estimation on Sequential Labeling via Uncertainty Transmission
- abstract: 'Auxiliary function is a helpful component to improve language model''s
    code generation ability. However, a systematic exploration of how they affect
    has yet to be done. In this work, we comprehensively evaluate the ability to utilize
    auxiliary functions encoded in recent code-pretrained language models. First,
    we construct a human-crafted evaluation set, called HumanExtension, which contains
    examples of two functions where one function assists the other.

    With HumanExtension, we design several experiments to examine their ability in
    a multifaceted way. Our evaluation processes enable a comprehensive understanding
    of including auxiliary functions in the prompt in terms of effectiveness and robustness.
    An additional implementation style analysis captures the models'' various implementation
    patterns when they access the auxiliary function. Through this analysis, we discover
    the models'' promising ability to utilize auxiliary functions including their
    self-improving behavior by implementing the two functions step-by-step. However,
    our analysis also reveals the model''s underutilized behavior to call the auxiliary
    function, suggesting the future direction to enhance their implementation by eliciting
    the auxiliary function call ability encoded in the models. We release our code
    and dataset to facilitate this research direction.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - dblp_id: https://dblp.org/pid/251/9511
    emails: '****@postech.ac.kr'
    first_name: Seonghyeon
    google_scholar_id: https://scholar.google.com/citations?user=VZpKsNEAAAAJ&hl=ko
    last_name: Lee
    name: Seonghyeon Lee
    semantic_scholar_id: https://www.semanticscholar.org/author/72490748
    username: ~Seonghyeon_Lee1
  - dblp_id: https://dblp.org/pid/336/3880
    emails: '****@postech.ac.kr'
    first_name: Sanghwan
    google_scholar_id: https://scholar.google.com/citations?user=iuvgvowAAAAJ
    institution: POSTECH
    last_name: Jang
    name: Sanghwan Jang
    semantic_scholar_id: https://www.semanticscholar.org/author/Sanghwan-Jang/2194236168
    username: ~Sanghwan_Jang1
  - dblp_id: https://dblp.org/pid/259/5132
    emails: '****@postech.ac.kr'
    first_name: Seongbo
    google_scholar_id: https://scholar.google.com/citations?user=Ai4-wrgAAAAJ&hl=ko
    institution: Pohang University of Science and Technology
    last_name: Jang
    name: Seongbo Jang
    orcid: https://orcid.org/0009-0008-3398-0144
    semantic_scholar_id: https://www.semanticscholar.org/author/Seongbo-Jang/1523619467
    username: ~Seongbo_Jang1
  - dblp_id: https://dblp.org/pid/12/760
    emails: '****@yonsei.ac.kr'
    first_name: Dongha
    google_scholar_id: https://scholar.google.com/citations?user=driVwKwAAAAJ
    homepage: https://donalee.github.io
    institution: Yonsei University
    last_name: Lee
    name: Dongha Lee
    orcid: https://orcid.org/0000-0003-2173-3476
    username: ~Dongha_Lee1
  - dblp_id: https://dblp.org/pid/80/6889
    emails: '****@postech.ac.kr'
    first_name: Hwanjo
    google_scholar_id: https://scholar.google.com/citations?user=LbrCa7EAAAAJ
    homepage: http://di.postech.ac.kr/hwanjoyu
    institution: POSTECH
    last_name: Yu
    name: Hwanjo Yu
    semantic_scholar_id: https://www.semanticscholar.org/author/Hwanjo-Yu/1723357
    username: ~Hwanjo_Yu2
  decision: toFindings
  end_page: 2847
  file: 687.pdf
  id: 687
  num_pages: 13
  openreview_id: BsRAp5PNhY
  pdf_file: cb16d3bd8395137593df90720fee9af2d6477621.pdf
  start_page: 2835
  title: Exploring Language Model's Code Generation Ability with Auxiliary Functions
- abstract: Recent advancements in large language models (LLMs) have underscored their
    importance in the evolution of artificial intelligence. However, despite extensive
    pretraining on multilingual datasets, available open-sourced LLMs exhibit limited
    effectiveness in processing Vietnamese. The challenge is exacerbated by the absence
    of systematic benchmark datasets and metrics tailored for Vietnamese LLM evaluation.
    To mitigate these issues, we have finetuned LLMs specifically for Vietnamese and
    developed a comprehensive evaluation framework encompassing 10 tasks and 31 metrics.
    We observe that finetuning can help LLMs transfer knowledge across languages,
    serving as an efficient way to bolster their capabilities in non-English languages.
    Moreover, our analysis indicates that larger models can introduce more biases
    and uncalibrated outputs and the key factor influencing LLM performance is the
    quality of the training or finetuning datasets. These insights underscore the
    significance of meticulous finetuning with high-quality datasets in enhancing
    LLM performance.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - dblp_id: https://dblp.org/pid/301/9134
    emails: '****@cs.stanford.edu'
    first_name: Sang
    google_scholar_id: https://scholar.google.com/citations?user=oXPm0dAAAAAJ&hl=en#
    homepage: https://cs.stanford.edu/~sttruong
    last_name: Truong
    middle_name: T.
    name: Sang T. Truong
    username: ~Sang_T._Truong1
  - emails: '****@hcmut.edu.vn'
    first_name: Duc
    google_scholar_id: https://scholar.google.com/citations?user=w1o63XAAAAAJ
    homepage: https://martinakaduc.github.io
    last_name: Nguyen
    middle_name: Quang
    name: Duc Quang Nguyen
    orcid: https://orcid.org/0000-0001-9638-8289
    semantic_scholar_id: https://www.semanticscholar.org/author/Duc-Quang-Nguyen/144898418
    username: ~Duc_Quang_Nguyen1
  - emails: '****@hcmut.edu.vn'
    first_name: Toan
    homepage: https://www.linkedin.com/in/toan-nguyen-058506180/
    last_name: Nguyen
    name: Toan Nguyen
    username: ~Toan_Nguyen4
  - emails: '****@hcmut.edu.vn'
    first_name: Dong
    last_name: Le
    name: Dong Le
    username: ~Dong_Le1
  - emails: '****@gmail.com'
    first_name: Nhi
    homepage: https://github.com/nhintruong
    last_name: Truong
    middle_name: Ngoc
    name: Nhi Ngoc Truong
    username: ~Nhi_Ngoc_Truong1
  - dblp_id: https://dblp.org/pid/26/8327.html
    emails: '****@hcmut.edu.vn'
    first_name: Tho
    google_scholar_id: https://scholar.google.com/citations?user=IlW-MrAAAAAJ&hl=en
    homepage: http://www.cse.hcmut.edu.vn/qttho/doku.php
    last_name: Quan
    name: Tho Quan
    orcid: https://orcid.org/0000-0003-0467-6254
    username: ~Tho_Quan1
  - dblp_id: https://dblp.org/pid/14/8885
    emails: '****@cs.stanford.edu'
    first_name: Sanmi
    google_scholar_id: https://scholar.google.com/citations?user=EaaOeJwAAAAJ
    homepage: https://cs.stanford.edu/~sanmi/
    institution: Stanford University and Google
    last_name: Koyejo
    name: Sanmi Koyejo
    orcid: https://orcid.org/0000-0002-4023-419X
    username: ~Sanmi_Koyejo1
  decision: toFindings
  end_page: 2899
  file: 689.pdf
  id: 689
  num_pages: 52
  openreview_id: GwfHcWpALj
  pdf_file: 01cf629f9dddd2190cf4d52de30e05df7cad072f.pdf
  start_page: 2848
  title: 'Crossing Linguistic Horizons: Finetuning and Comprehensive Evaluation of
    Vietnamese Large Language Models'
- abstract: With the widespread use of language models (LMs) in NLP tasks, researchers
    have discovered the potential of Chain-of-thought (CoT) to assist LMs in accomplishing
    complex reasoning tasks by generating intermediate steps. However, human thought
    processes are often non-linear, rather than simply sequential chains of thoughts.
    Therefore, we propose Graph-of-Thought (GoT) reasoning, which models human thought
    processes not only as a chain but also as a graph. By representing thought units
    as nodes and connections between them as edges, our approach captures the non-sequential
    nature of human thinking and allows for a more realistic modeling of thought processes.
    GoT adopts a two-stage framework with an additional GoT encoder for thought graph
    representation and fuses the graph representation with the original input representation
    through a gated fusion mechanism. We evaluate GoT's performance on a text-only
    reasoning task (AQUA-RAT) and a multimodal reasoning task (ScienceQA). Our model
    achieves significant improvement over the strong CoT baseline on the AQUA-RAT
    test set and boosts accuracy from 85.19\% to 87.59\% using the T5-base model over
    the state-of-the-art Multimodal-CoT on the ScienceQA test set. Our code is publicly
    available at https://github.com/Zoeyyao27/Graph-of-Thought
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Generation
  authors:
  - emails: '****@sjtu.edu.cn'
    first_name: Yao
    google_scholar_id: https://scholar.google.com/citations?user=tLMP3IkAAAAJ
    homepage: https://github.com/Zoeyyao27
    last_name: Yao
    name: Yao Yao
    username: ~Yao_Yao8
  - dblp_id: https://dblp.org/pid/198/9339
    emails: '****@whu.edu.cn'
    first_name: Zuchao
    google_scholar_id: https://scholar.google.com/citations?user=PyzBf5oAAAAJ
    homepage: https://zcli-charlie.github.io/
    last_name: Li
    name: Zuchao Li
    username: ~Zuchao_Li1
  - dblp_id: https://dblp.org/pid/25/1145
    emails: '****@cs.sjtu.edu.cn'
    first_name: Hai
    google_scholar_id: https://scholar.google.com.tw/citations?user=4dU5KS0AAAAJ
    homepage: http://bcmi.sjtu.edu.cn/~zhaohai/
    institution: Shanghai Jiao Tong University
    last_name: Zhao
    name: hai zhao
    username: ~hai_zhao1
  decision: toFindings
  end_page: 2920
  file: 690.pdf
  id: 690
  num_pages: 21
  openreview_id: ppeY6shDAg
  pdf_file: 7da2e0952a799701f29d19532e95216b58d7509f.pdf
  start_page: 2900
  title: 'GoT: Effective Graph-of-Thought Reasoning in Language Models'
- abstract: "Open-source pre-trained Large Language Models (LLMs) exhibit strong language\
    \ understanding and generation capabilities, making them highly successful in\
    \ a variety of tasks. \n However, when used as agents for dealing with complex\
    \ problems in the real world, their performance is far inferior to large commercial\
    \ models such as ChatGPT and GPT-4. \n As intelligent agents, LLMs need to have\
    \ the capabilities of task planning, long-term memory, and the ability to leverage\
    \ external tools to achieve satisfactory performance. \n Various methods have\
    \ been proposed to enhance the agent capabilities of LLMs. On the one hand, methods\
    \ involve constructing agent-specific data and fine-tuning the models. \n On the\
    \ other hand, some methods focus on designing prompts that effectively activate\
    \ the reasoning abilities of the LLMs. We explore both strategies on the 7B and\
    \ 13B models. \n We propose a comprehensive method for constructing agent-specific\
    \ data using GPT-4. \n Through supervised fine-tuning with constructed data, we\
    \ find that for these models with a relatively small number of parameters, supervised\
    \ fine-tuning can significantly reduce hallucination outputs and formatting errors\
    \ in agent tasks. \n Furthermore, techniques such as multi-path reasoning and\
    \ task decomposition can effectively decrease problem complexity and enhance the\
    \ performance of LLMs as agents. \n We evaluate our method on five agent tasks\
    \ of AgentBench and achieve satisfactory results."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Summarization
  authors:
  - emails: '****@hust.edu.cn'
    first_name: Qinhao
    homepage: https://github.com/lzwqjh
    last_name: Zhou
    name: Qinhao Zhou
    username: ~Qinhao_Zhou1
  - emails: '****@hust.edu.cn'
    first_name: Zihan
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=PmKcszYAAAAJ
    last_name: Zhang
    name: Zihan Zhang
    username: ~Zihan_Zhang6
  - dblp_id: https://dblp.org/pid/83/2686
    emails: '****@hust.edu.cn'
    first_name: Xiang
    google_scholar_id: https://scholar.google.com/citations?user=-D5k5ioAAAAJ&hl=en
    homepage: http://faculty.hust.edu.cn/XIANGXIANG
    institution: Huazhong University of Science and Technology
    last_name: Xiang
    name: Xiang Xiang
    orcid: https://orcid.org/0000-0003-0606-6193
    semantic_scholar_id: https://www.semanticscholar.org/author/Xiang-Xiang/144422820
    username: ~Xiang_Xiang1
  - emails: '****@gmail.com'
    first_name: Ke
    google_scholar_id: https://scholar.google.com/citations?user=1xuDUvkAAAAJ
    homepage: https://scholar.google.com/citations?user=1xuDUvkAAAAJ
    institution: Alibaba Group
    last_name: Wang
    name: Ke Wang
    username: ~Ke_Wang17
  - emails: '****@gmail.com'
    first_name: Yuchuan
    google_scholar_id: https://scholar.google.com/citations?user=y-cwb-MAAAAJ&hl=zh-CN
    institution: Alibaba Group
    last_name: Wu
    name: Yuchuan Wu
    username: ~Yuchuan_Wu1
  - dblp_id: https://dblp.org/pid/16/4349
    emails: '****@alibaba-inc.com'
    first_name: Yongbin
    google_scholar_id: https://scholar.google.com/citations?user=xF5VrokAAAAJ&hl=zh-CN
    homepage: https://yongbin-li.github.io/
    institution: Alibaba Group
    last_name: Li
    name: Yongbin Li
    semantic_scholar_id: https://www.semanticscholar.org/author/Yongbin-Li/1527090216
    username: ~Yongbin_Li2
  decision: toFindings
  end_page: 2930
  file: 693.pdf
  id: 693
  num_pages: 10
  openreview_id: debvl42NtE
  pdf_file: 989858e1d4f736ee4ffd4276a522dad43d245ad0.pdf
  start_page: 2921
  title: Enhancing the General Agent Capabilities of Low-Paramter LLMs through Tuning
    and Multi-Branch Reasoning
- abstract: 'Recently, the tool-use Large Language Models (LLMs) that integrate with
    external Python interpreters have significantly enhanced mathematical reasoning
    capabilities for open-source LLMs. However, these models fall short in demonstrating
    the calculation process, which compromises user-friendliness and understanding
    of problem-solving steps. Conversely, while tool-free methods offer a clear display
    of the problem-solving process, their accuracy leaves room for improvement.

    These tool-free methods typically employ a somewhat narrow range of augmentation
    techniques such as rephrasing and difficulty enhancement to boost performance.
    In response to this issue, we have amalgamated and further refined these strengths
    while broadening the scope of augmentation methods to construct a **mu**lti-perspective
    augmentation dataset for **math**ematics---termed **MuMath** ($\mu$-Math) Dataset.

    Subsequently, we finetune LLaMA-2 on the MuMath dataset to derive the MuMath model.
    Our experiments indicate that our MuMath-70B model achieves new state-of-the-art
    performance among tool-free methods---achieving 88.3\% on GSM8K and 34.5\% on
    MATH .

    We release the MuMath dataset along with its corresponding models and code for
    public use.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Learning for NLP
  authors:
  - emails: '****@tal.com'
    first_name: Weihao
    google_scholar_id: https://scholar.google.com/citations?user=R1Co1REAAAAJ
    last_name: You
    name: Weihao You
    username: ~Weihao_You1
  - emails: '****@foxmail.com'
    first_name: Shuo
    google_scholar_id: https://scholar.google.com/citations?user=7HPIzc4AAAAJ&hl=en&oi=sra
    last_name: Yin
    name: Shuo Yin
    orcid: https://orcid.org/0000-0003-0240-0764
    username: ~Shuo_Yin1
  - emails: '****@ecjtu.edu.cn'
    first_name: Xudong
    homepage: https://github.com/z18256199275
    last_name: Zhao
    name: Xudong Zhao
    username: ~Xudong_Zhao1
  - dblp_id: https://dblp.org/pid/263/6772.html
    emails: '****@hotmail.com'
    first_name: Zhilong
    institution: Tomorrow Advancing Life
    last_name: Ji
    name: Zhilong Ji
    username: ~Zhilong_Ji1
  - dblp_id: https://dblp.org/pid/15/3875
    emails: '****@ouc.edu.cn'
    first_name: Guoqiang
    google_scholar_id: https://scholar.google.com.hk/citations?hl=zh-CN&user=HqKD-fwAAAAJ
    homepage: http://cvpr.ouc.edu.cn/people/zhonggq.html
    institution: Ocean University of China
    last_name: Zhong
    name: Guoqiang Zhong
    semantic_scholar_id: https://www.semanticscholar.org/author/G.-Zhong/2421012
    username: ~Guoqiang_Zhong1
  - dblp_id: https://dblp.org/pid/120/7270.html
    emails: '****@gmail.com'
    first_name: Jinfeng
    last_name: Bai
    name: Jinfeng Bai
    username: ~Jinfeng_Bai1
  decision: toFindings
  end_page: 2957
  file: 694.pdf
  id: 694
  num_pages: 27
  openreview_id: 6Du1MIwLg4
  pdf_file: 36d2cf6820f933f36230c0955a3c394b5ddfd3dd.pdf
  start_page: 2931
  title: 'MuMath: Multi-perspective Data Augmentation for Mathematical Reasoning in
    Large Language Models'
- abstract: Automatically generating human-readable text describing the functionality
    of a program is the intent of source code summarization. Although neural language
    models achieve significant performance in this field, they are limited by their
    inability to access external knowledge. To address this limitation, an emerging
    trend is combining neural models with external knowledge through retrieval methods.
    Previous methods have relied on the sentence-level retrieval paradigm on the encoder
    side. However, this paradigm is coarse-grained, noise-filled and cannot directly
    take advantage of the high-quality retrieved summary tokens on the decoder side.
    In this paper, we propose a fine-grained Token-level retrieval-augmented mechanism
    (Tram) on the decoder side rather than the encoder side to enhance the performance
    of neural models and produce more low-frequency tokens in generating summaries.
    Furthermore, to overcome the challenge of token-level retrieval in capturing contextual
    code semantics, we also propose integrating code semantics into individual summary
    tokens. The results of extensive experiments and human evaluation show that our
    token-level retrieval-augmented approach significantly improves performance and
    is more interpretable.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - emails: '****@zju.edu.cn'
    first_name: Tong
    homepage: https://tongye98.github.io/
    institution: Zhejiang University
    last_name: Ye
    name: Tong Ye
    username: ~Tong_Ye1
  - dblp_id: https://dblp.org/pid/27/9060
    emails: '****@gmail.com'
    first_name: Lingfei
    google_scholar_id: https://scholar.google.com/citations?hl=en&view_op=list_works&gmla=AP6z3ObkuZbY_PY0Ax9X6t4_96hOZV89Vg8IyDOUM1oA4VeJqlPHcwvYOKE1b9b-wbfCPPsAzD9bhsvzf-zNUrpjHpXd&user=VYi6qHMAAAAJ
    homepage: https://sites.google.com/view/teddy-lfwu/
    institution: Anytime AI and Pinterest
    last_name: Wu
    name: Lingfei Wu
    username: ~Lingfei_Wu1
  - dblp_id: https://dblp.org/pid/94/9023-1
    emails: '****@gmail.com'
    first_name: Tengfei
    google_scholar_id: https://scholar.google.com/citations?user=9OvNakkAAAAJ&hl=en
    homepage: https://sites.google.com/site/matf0123/
    institution: State University of New York at Stony Brook
    last_name: Ma
    name: Tengfei Ma
    orcid: https://orcid.org/0000-0002-1086-529X
    semantic_scholar_id: https://www.semanticscholar.org/author/Tengfei-Ma/40411766
    username: ~Tengfei_Ma1
  - dblp_id: https://dblp.org/pid/139/6932-2.html
    emails: '****@zju.edu.cn'
    first_name: Xuhong
    google_scholar_id: https://scholar.google.com/citations?user=bWLpm3sAAAAJ&hl=en
    homepage: https://person.zju.edu.cn/zhangxuhong
    institution: Zhejiang University
    last_name: Zhang
    name: Xuhong Zhang
    username: ~Xuhong_Zhang1
  - dblp_id: https://dblp.org/pid/260/9581
    emails: '****@outlook.com'
    first_name: Yangkai
    google_scholar_id: https://scholar.google.com/citations?user=2dx1fzQAAAAJ&hl=zh-CN&oi=ao
    last_name: Du
    name: Yangkai Du
    semantic_scholar_id: https://www.semanticscholar.org/author/Yangkai-Du/2111936104
    username: ~Yangkai_Du1
  - dblp_id: https://dblp.uni-trier.de/pid/85/670-3
    emails: '****@zju.edu.cn'
    first_name: Peiyu
    homepage: https://nesa.zju.edu.cn/webpage/people.html
    last_name: Liu
    name: Peiyu Liu
    orcid: https://orcid.org/0000-0001-7793-7633
    semantic_scholar_id: https://www.semanticscholar.org/author/Peiyu-Liu/2113250310
    username: ~Peiyu_Liu2
  - dblp_id: https://dblp.org/pid/07/8388
    emails: '****@zju.edu.cn'
    first_name: Shouling
    google_scholar_id: https://scholar.google.com.vn/citations?hl=en&user=5HoF_9oAAAAJ
    homepage: https://nesa.zju.edu.cn/
    institution: Zhejiang University
    last_name: Ji
    name: Shouling Ji
    username: ~Shouling_Ji1
  - emails: '****@zju.edu.cn'
    first_name: Wenhai
    homepage: https://person.zju.edu.cn/wangweihai
    last_name: Wang
    name: Wenhai Wang
    username: ~Wenhai_Wang3
  decision: toFindings
  end_page: 2970
  file: 699.pdf
  id: 699
  num_pages: 13
  openreview_id: C1tyGxOKwP
  pdf_file: bb87bb90847ca06035b1c1da8a9a4672de8a003b.pdf
  start_page: 2958
  title: 'Tram: A Token-level Retrieval-augmented Mechanism for Source Code Summarization'
- abstract: 'Previous zero-shot dialogue state tracking (DST) methods only apply transfer
    learning, but ignore unlabelled data in the target domain.

    We transform zero-shot DST into few-shot DST by utilising such unlabelled data
    via joint and self-training methods. Our method incorporates auxiliary tasks that
    generate slot types as inverse prompts for main tasks, creating slot values during
    joint training.  Cycle consistency between these two tasks enables the generation
    and selection of quality samples in unknown target domains for subsequent fine-tuning.
    This approach also facilitates automatic label creation, thereby optimizing the
    training and fine-tuning of DST models. We demonstrate this method''s effectiveness
    on general language models in zero-shot scenarios, improving average joint goal
    accuracy by 8\% across all domains in MultiWOZ.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Dialogue and Interactive Systems
  authors:
  - emails: '****@u.nus.edu'
    first_name: Chuang
    last_name: Li
    name: Chuang Li
    username: ~Chuang_Li2
  - emails: '****@global.tencent.com'
    first_name: Yan
    google_scholar_id: https://scholar.google.com/citations?user=-oIMVnUAAAAJ&hl=en
    institution: Tencent
    last_name: Zhang
    name: Yan Zhang
    semantic_scholar_id: https://www.semanticscholar.org/author/Yan-Zhang/39831806
    username: ~Yan_Zhang12
  - dblp_id: https://dblp.org/pid/k/MinYenKan
    emails: '****@comp.nus.edu.sg'
    first_name: Min-Yen
    google_scholar_id: https://scholar.google.com.tw/citations?user=aNVcd3EAAAAJ
    homepage: https://www.comp.nus.edu.sg/~kanmy/
    institution: National University of Singapore
    last_name: Kan
    name: Min-Yen Kan
    semantic_scholar_id: https://www.semanticscholar.org/author/Min-Yen-Kan/37596605
    username: ~Min-Yen_Kan1
  - dblp_id: https://dblp.org/pid/36/4118
    emails: '****@cuhk.edu.cn'
    first_name: Haizhou
    google_scholar_id: https://scholar.google.com.sg/citations?user=z8_x7C8AAAAJ&hl=en
    homepage: https://colips.org/~eleliha/
    institution: The Chinese University of Hong Kong (Shenzhen); National University
      of Singapore and National University of Singapore
    last_name: Li
    name: Haizhou Li
    orcid: https://orcid.org/0000-0001-9158-9401
    semantic_scholar_id: https://www.semanticscholar.org/author/Haizhou-Li/1711271
    username: ~Haizhou_Li3
  decision: toFindings
  end_page: 2982
  file: 701.pdf
  id: 701
  num_pages: 12
  openreview_id: Q3gYZv01nY
  pdf_file: be6b0cac537459aef5e25581c6cab4bf0e86aa25.pdf
  start_page: 2971
  title: 'UNO-DST: Leveraging Unlabelled Data in Zero-Shot Dialogue State Tracking'
- abstract: "Pre-trained language models (LMs) have shown remarkable reasoning performance\
    \ using explanations or chain-of-thoughts (CoT)) for in-context learning. On the\
    \ other hand, these reasoning tasks are usually presumed to be more approachable\
    \ for symbolic programming. \nTo understand the mechanism of reasoning of LMs,\
    \ we curate synthetic datasets containing equivalent (natural, symbolic) data\
    \ pairs, where symbolic examples contain first-order logic rules and predicates\
    \ from non-parametric knowledge bases (KBs), supporting automated verification\
    \ of intermediate reasoning results. Then we revisit neuro-symbolic approaches\
    \ and propose to learn from demonstrations containing logic rules and corresponding\
    \ examples to iteratively reason over KBs, recovering Prolog\u2019s backward chaining\
    \ algorithm and supporting automated verification of LMs' outputs. Comprehensive\
    \ experiments are included to systematically compare LMLP with CoT in deductive\
    \ reasoning settings, showing that LMLP enjoys more than 25% higher accuracy than\
    \ CoT on length generalization benchmarks even with smaller model sizes."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - dblp_id: https://dblp.org/pid/57/4707
    emails: '****@gmail.com'
    first_name: YiFan
    google_scholar_id: https://scholar.google.com/citations?user=lUnt8X4AAAAJ&hl=zh-CN
    homepage: https://yfzhang114.github.io/
    institution: Institute of automation, Chinese academy of science
    last_name: Zhang
    name: YiFan Zhang
    semantic_scholar_id: https://www.semanticscholar.org/author/Yi-Fan-Zhang/2145062298
    username: ~YiFan_Zhang8
  - emails: '****@gmail.com'
    first_name: Hanlin
    google_scholar_id: https://scholar.google.com/citations?user=h5IXxToAAAAJ&hl=en#
    homepage: https://hanlin-zhang.com/
    institution: Harvard University
    last_name: Zhang
    name: Hanlin Zhang
    username: ~Hanlin_Zhang1
  - dblp_id: https://dblp.org/pid/l/ErranLLi.html
    emails: '****@gmail.com'
    first_name: Li
    google_scholar_id: https://scholar.google.com/citations?user=GkMfzy4AAAAJ&hl=en
    homepage: http://www.cs.columbia.edu/~lierranli/
    institution: Amazon and Columbia University
    last_name: Li
    middle_name: Erran
    name: Li Erran Li
    username: ~Li_Erran_Li1
  - dblp_id: https://dblp.org/pid/36/3855
    emails: '****@petuum.com'
    first_name: Eric
    google_scholar_id: https://scholar.google.com.tw/citations?user=5pKTRxEAAAAJ
    homepage: http://www.cs.cmu.edu/~epxing/
    institution: Mohamed bin Zayed Univeristy of AI and School of Computer Science,
      Carnegie Mellon University
    last_name: Xing
    middle_name: P.
    name: Eric P. Xing
    username: ~Eric_P._Xing1
  decision: toFindings
  end_page: 3001
  file: 706.pdf
  id: 706
  num_pages: 19
  openreview_id: GW735dQH9T
  pdf_file: 9b7aaf4fec35008f8aabdfb14083d7bdc7fbe818.pdf
  start_page: 2983
  title: Evaluating Step-by-Step Reasoning through Symbolic Verification
- abstract: 'Grounded text generation, encompassing tasks such as long-form question-answering
    and summarization, necessitates both content selection and content consolidation.
    Current end-to-end methods are difficult to control and interpret due to their
    opaqueness.

    Accordingly, recent works have proposed a modular approach, with separate components
    for each step. Specifically, we focus on the second subtask, of generating coherent
    text given pre-selected content in a multi-document setting. Concretely, we formalize
    Fusion-in-Context (FiC) as a standalone task, whose input consists of source texts
    with highlighted spans of targeted content. A model then needs to generate a coherent
    passage that includes all and only the target information.

    Our work includes the development of a curated dataset of 1000 instances in the
    reviews domain, alongside a novel evaluation framework for assessing the faithfulness
    and coverage of highlights, which strongly correlate to human judgment. Several
    baseline models exhibit promising outcomes and provide insightful analyses.

    This study lays the groundwork for further exploration of modular text generation
    in the multi-document setting, offering potential improvements in the quality
    and reliability of generated content. Our benchmark, FuseReviews, including the
    dataset, evaluation framework, and designated leaderboard, can be found at https://fusereviews.github.io/.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Generation
  authors:
  - dblp_id: https://dblp.org/pid/290/2100.html
    emails: '****@gmail.com'
    first_name: Aviv
    google_scholar_id: https://scholar.google.com/citations?user=oAy77cgAAAAJ&hl=en
    homepage: https://lovodkin93.github.io/
    last_name: Slobodkin
    name: Aviv Slobodkin
    semantic_scholar_id: https://www.semanticscholar.org/author/Aviv-Slobodkin/2074098656
    username: ~Aviv_Slobodkin2
  - dblp_id: https://dblp.org/pid/205/9013
    emails: '****@gmail.com'
    first_name: Ori
    google_scholar_id: https://scholar.google.com/citations?user=s7djZnUAAAAJ
    homepage: https://orishapira.wordpress.com/
    institution: Amazon
    last_name: Shapira
    name: Ori Shapira
    semantic_scholar_id: https://www.semanticscholar.org/author/Ori-Shapira/15392843
    username: ~Ori_Shapira1
  - dblp_id: https://dblp.org/pid/146/3128-1
    emails: '****@amazon.com'
    first_name: Ran
    google_scholar_id: https://scholar.google.com/citations?user=6HcOMAgAAAAJ&hl=th
    institution: Amazon
    last_name: Levy
    name: Ran Levy
    orcid: https://orcid.org/0009-0008-7352-2586
    semantic_scholar_id: https://www.semanticscholar.org/author/Ran-Levy/48496836
    username: ~Ran_Levy1
  - dblp_id: https://dblp.org/pid/95/284
    emails: '****@cs.biu.ac.il'
    first_name: Ido
    google_scholar_id: https://scholar.google.com.tw/citations?user=YzGAGtoAAAAJ
    homepage: http://u.cs.biu.ac.il/~dagan/
    institution: Bar-Ilan University
    last_name: Dagan
    name: Ido Dagan
    username: ~Ido_Dagan1
  decision: toFindings
  end_page: 3020
  file: 717.pdf
  id: 717
  num_pages: 19
  openreview_id: KmJJwCN7B0
  pdf_file: ef946320a945b03cf598f00b5058f18b11d6dfae.pdf
  start_page: 3002
  title: Multi-Review Fusion-in-Context
- abstract: 'Retrieval-Augmented Neural Machine Translation (RAMT) architectures retrieve
    examples from memory to guide the generation process. While most works in this
    trend explore new ways to exploit the retrieved examples, the upstream retrieval
    step is mostly unexplored. In this paper, we study the effect of varying retrieval
    methods for several translation architectures to better understand the interplay
    between these two processes.

    We conduct experiments in two language pairs in a multi-domain setting and consider
    several downstream architectures based on a standard autoregressive model, an
    edit-based model, and a large language model with in-context learning. Our experiments
    show that the choice of the retrieval technique impacts the translation scores,
    with variance across architectures. We also discuss the effects of increasing
    the number and diversity of examples, which are mostly positive across the board.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Translation
  authors:
  - emails: '****@gmail.com'
    first_name: Maxime
    last_name: Bouthors
    name: Maxime Bouthors
    orcid: https://orcid.org/0000-0003-1400-4902
    username: ~Maxime_Bouthors1
  - dblp_id: https://dblp.org/pid/94/361
    emails: '****@systrangroup.com'
    first_name: Josep
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=lw_aQqQAAAAJ
    institution: SYSTRAN
    last_name: Crego
    name: Josep Crego
    semantic_scholar_id: https://www.semanticscholar.org/author/J.-Crego/2209023
    username: ~Josep_Crego1
  - dblp_id: http://dblp.uni-trier.de/pers/hd/y/Yvon:Fran=ccedil=ois
    emails: '****@isir.upmc.fr'
    first_name: "Fran\xE7ois"
    google_scholar_id: https://scholar.google.fr/citations?hl=fr&user=JjfDvawAAAAJ
    homepage: http://cv.archives-ouvertes.fr/francois-yvon
    institution: "Universit\xE9 Pierre et Marie Curie - Paris 6, Sorbonne Universit\xE9\
      \ - Facult\xE9 des Sciences (Paris VI)"
    last_name: Yvon
    name: "Fran\xE7ois Yvon"
    orcid: https://orcid.org/0000-0002-7972-7442
    semantic_scholar_id: https://www.semanticscholar.org/author/Fran%C3%A7ois-Yvon/1846431
    username: "~Fran\xE7ois_Yvon2"
  decision: toFindings
  end_page: 3038
  file: 718.pdf
  id: 718
  num_pages: 18
  openreview_id: vh9y4TiwXY
  pdf_file: 26b8a58b4e51c92b3846d1b695b5599f004b61e2.pdf
  start_page: 3021
  title: 'Retrieving Examples from Memory for Retrieval Augmented Neural Machine Translation:
    A Systematic Comparison'
- abstract: "Effectively training language models on long\ninputs poses many technical\
    \ challenges. As a\ncost consideration, languages models are pre-\ntrained on\
    \ a fixed sequence length before being\nadapted to longer sequences. We explore\
    \ var-\nious methods for adapting models to longer\ninputs by training on segmented\
    \ sequences and\nan interpolation-based method for extending\nabsolute positional\
    \ embeddings. We develop\na training procedure to extend the input con-\ntext\
    \ size of pretrained models with no architec-\ntural changes and no additional\
    \ memory costs\nthan training on the original input lengths. By\nsub-sampling\
    \ segments from long inputs while\nmaintaining their original position the model\
    \ is\nable to learn new positional interactions. Our\nmethod benefits both models\
    \ trained with abso-\nlute positional embeddings, by extending their\ninput contexts,\
    \ as well as popular relative posi-\ntional embedding methods showing a reduced\n\
    perplexity on sequences longer than they were\ntrained on. We demonstrate our\
    \ method can\nextend input contexts by a factor of 4\xD7 while\nimproving perplexity."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Efficient/Low-Resource Methods for NLP
  authors:
  - emails: '****@umn.edu'
    first_name: Petros
    institution: University of California, San Diego
    last_name: Karypis
    name: Petros Karypis
    username: ~Petros_Karypis1
  - dblp_id: https://dblp.org/pid/29/3483
    emails: '****@cs.ucsd.edu'
    first_name: Julian
    google_scholar_id: https://scholar.google.com/citations?user=icbo4M0AAAAJ&hl=en
    homepage: http://cseweb.ucsd.edu/~jmcauley/
    institution: University of California, San Diego, University of California, San
      Diego
    last_name: McAuley
    name: Julian McAuley
    username: ~Julian_McAuley1
  - emails: '****@cs.umn.edu'
    first_name: George
    google_scholar_id: https://scholar.google.com/citations?user=ElqwScwAAAAJ&hl=en
    institution: University of Minnesota, Minneapolis
    last_name: Karypis
    name: George Karypis
    username: ~George_Karypis1
  decision: toFindings
  end_page: 3051
  file: 719.pdf
  id: 719
  num_pages: 13
  openreview_id: gaIxtt1MTU
  pdf_file: a49c9188b219f78d20bb5163cbdf82edf452c460.pdf
  start_page: 3039
  title: Extending Input Contexts of Language Models through Training on Segmented
    Sequences
- abstract: Large Language Models (LLMs) have demonstrated good performance in many
    reasoning tasks, but they still struggle with some complicated reasoning tasks
    including logical reasoning. One non-negligible reason for LLMs' suboptimal performance
    on logical reasoning is their overlooking of understanding logical fallacies correctly.
    To evaluate LLMs' capability of logical fallacy understanding (LFU), we propose
    five concrete tasks from three cognitive dimensions of WHAT, WHY, and HOW in this
    paper. Towards these LFU tasks, we have successfully constructed a new dataset
    LFUD based on GPT-4 accompanied by a little human effort. Our extensive experiments
    justify that our LFUD can be used not only to evaluate LLMs' LFU capability, but
    also to fine-tune LLMs to obtain significantly enhanced performance on logical
    reasoning.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Generation
  authors:
  - emails: '****@m.fudan.edu.cn'
    first_name: Yanda
    homepage: https://www.fudan.edu.cn/
    last_name: Li
    name: Yanda Li
    username: ~Yanda_Li2
  - emails: '****@m.fudan.edu.cn'
    first_name: Dixuan
    institution: Fudan University
    last_name: Wang
    name: Dixuan Wang
    username: ~Dixuan_Wang1
  - dblp_id: https://dblp.org/pid/177/7508
    emails: '****@gmail.com'
    first_name: Jiaqing
    google_scholar_id: https://scholar.google.com/citations?user=g5ZnsAYAAAAJ&hl=zh-CN
    homepage: http://kw.fudan.edu.cn/people/liangjiaqing/
    institution: Fudan University
    last_name: Liang
    name: Jiaqing Liang
    username: ~Jiaqing_Liang1
  - emails: '****@qq.com'
    first_name: Guochao
    homepage: https://github.com/jiangguochaoGG
    last_name: Jiang
    name: Guochao Jiang
    username: ~Guochao_Jiang1
  - dblp_id: https://dblp.org/pid/317/0033
    emails: '****@gmail.com'
    first_name: Qianyu
    google_scholar_id: https://scholar.google.com/citations?user=X4l87TgAAAAJ&hl=zh-CN
    institution: Fudan University
    last_name: He
    name: Qianyu He
    orcid: https://orcid.org/0000-0002-1575-314X
    semantic_scholar_id: https://www.semanticscholar.org/author/Qi-He/2152880833
    username: ~Qianyu_He1
  - dblp_id: https://dblp.org/pid/96/999
    emails: '****@fudan.edu.cn'
    first_name: Yanghua
    google_scholar_id: https://scholar.google.com/citations?hl=zh-CN&user=odFW4FoAAAAJ
    institution: Fudan University
    last_name: Xiao
    name: Yanghua Xiao
    username: ~Yanghua_Xiao1
  - dblp_id: https://dblp.org/pid/01/2462
    emails: '****@fudan.edu.cn'
    first_name: Deqing
    google_scholar_id: https://scholar.google.com/citations?user=uZdQxkwAAAAJ&hl=zh-CN
    homepage: http://kw.fudan.edu.cn/people/yangdeqing/
    institution: Fudan University
    last_name: Yang
    name: Deqing Yang
    orcid: https://orcid.org/0000-0002-1390-3861
    semantic_scholar_id: https://www.semanticscholar.org/author/Deqing-Yang/2089312
    username: ~Deqing_Yang1
  decision: toFindings
  end_page: 3065
  file: 720.pdf
  id: 720
  num_pages: 14
  openreview_id: axXG1ZxeZo
  pdf_file: e9162c1c6bfd3dd4f5b49a02d968cee4dd41e971.pdf
  start_page: 3052
  title: 'Reason from Fallacy: Enhancing Large Language Models'' Logical Reasoning
    through Logical Fallacy Understanding'
- abstract: Multiple-choice questions (MCQs) are ubiquitous in almost all levels of
    education since they are easy to administer, grade, and are a reliable format
    in assessments and practices. One of the most important aspects of MCQs is the
    distractors, i.e., incorrect options that are designed to target common errors
    or misconceptions among real students. To date, the task of crafting high-quality
    distractors largely remains a labor and time-intensive process for teachers and
    learning content designers, which has limited scalability. In this work, we study
    the task of automated distractor generation in the domain of math MCQs and explore
    a wide variety of large language model (LLM)-based approaches, from in-context
    learning to fine-tuning. We conduct extensive experiments using a real-world math
    MCQ dataset and find that although LLMs can generate some mathematically valid
    distractors, they are less adept at anticipating common errors or misconceptions
    among real students.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - emails: '****@umass.edu'
    first_name: Wanyong
    last_name: Feng
    name: Wanyong Feng
    username: ~Wanyong_Feng1
  - emails: '****@umass.edu'
    first_name: Jaewook
    last_name: Lee
    name: Jaewook Lee
    username: ~Jaewook_Lee5
  - emails: '****@umass.edu'
    first_name: Hunter
    homepage: https://huntercodes.com/
    last_name: McNichols
    name: Hunter McNichols
    username: ~Hunter_McNichols1
  - dblp_id: https://dblp.org/pid/275/8415
    emails: '****@gmail.com'
    first_name: Alexander
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=9dVKHM0AAAAJ
    homepage: https://people.cs.umass.edu/~ajscarlatos/
    institution: Department of Computer Science, University of Massachusetts at Amherst
    last_name: Scarlatos
    name: Alexander Scarlatos
    semantic_scholar_id: https://www.semanticscholar.org/author/Alexander-Scarlatos/1976128281
    username: ~Alexander_Scarlatos1
  - emails: '****@eedi.co.uk'
    first_name: Digory
    homepage: https://eedi.com/
    institution: Eedi
    last_name: Smith
    name: Digory Smith
    username: ~Digory_Smith1
  - dblp_id: https://dblp.org/pid/52/10854-2
    emails: '****@eedi.co.uk'
    first_name: Simon
    google_scholar_id: https://scholar.google.com/citations?user=tpdJPAkAAAAJ&hl=en
    homepage: https://www.eedi.com
    institution: Eedi
    last_name: Woodhead
    name: Simon Woodhead
    orcid: https://orcid.org/0000-0002-2192-9797
    username: ~Simon_Woodhead1
  - emails: '****@gmail.com'
    first_name: Nancy
    homepage: http://www.nancyotero.net/
    last_name: Ornelas
    middle_name: Otero
    name: Nancy Otero Ornelas
    username: ~Nancy_Otero_Ornelas1
  - dblp_id: https://dblp.org/pid/127/6503
    emails: '****@cs.umass.edu'
    first_name: Andrew
    institution: University of Massachusetts, Amherst
    last_name: Lan
    name: Andrew Lan
    username: ~Andrew_Lan1
  decision: toFindings
  end_page: 3081
  file: 726.pdf
  id: 726
  num_pages: 16
  openreview_id: tEAJEfiyYf
  pdf_file: 4301c5e0edd5702572f31035d218ddd65ae0d71c.pdf
  start_page: 3066
  title: Exploring Automated Distractor Generation for Math Multiple-choice Questions
    via Large Language Models
- abstract: Given a sentence and a particular aspect term, aspect-based sentiment
    analysis (ABSA) aims to predict the sentiment polarity towards this aspect term,
    which provides fine-grained analysis on sentiment understanding and it has attracted
    much attention in recent years. In order to achieve a good performance on ABSA,
    it is important for a model to appropriately encode contextual information, especially
    identifying salient features and eliminating noise in the context. To make incorrect
    predictions, most existing approaches employ powerful text encoders to locate
    important context features, as well as noises that mislead ABSA models. These
    approaches determine the noise in the text for ABSA by assigning low weights to
    context features or directly removing them from model input, which runs the risk
    of computing wrong weights or eliminating important context information. In this
    paper, we propose to improve ABSA with context denoising, where three types of
    word-level information are regarded as noise, namely, lexicographic noise, bag-of-words
    noise, and syntax noise. We utilize diffusion networks to perform the denoising
    process to gradually eliminate them so as to better predict sentiment polarities
    for given aspect terms. Our approach uses task-specific noise rather than the
    standard stochastic Gaussian noise in the diffusion networks. The experimental
    results on five widely used ABSA datasets demonstrate the validity and effectiveness
    of our approach.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Sentiment Analysis, Stylistic Analysis, and Argument Mining
  authors:
  - dblp_id: https://dblp.uni-trier.de/pid/246/0133
    emails: '****@uw.edu'
    first_name: Yuanhe
    google_scholar_id: https://scholar.google.com/citations?user=5GCwWZ8AAAAJ&hl=en
    institution: University of Washington, Seattle
    last_name: Tian
    name: Yuanhe Tian
    username: ~Yuanhe_Tian1
  - emails: '****@mail.ustc.edu.cn'
    first_name: Chang
    google_scholar_id: https://scholar.google.com/citations?user=Y3NKd1wAAAAJ&hl=zh-CN
    homepage: http://none.com
    last_name: Liu
    name: Chang Liu
    orcid: https://orcid.org/0009-0003-1751-6206
    semantic_scholar_id: https://www.semanticscholar.org/author/Chang-Liu/2144546556
    username: ~Chang_Liu24
  - dblp_id: https://dblp.org/pid/09/1398
    emails: '****@gmail.com'
    first_name: Yan
    homepage: https://clksong.github.io
    institution: University of Science and Technology of China
    last_name: Song
    name: Yan Song
    username: ~Yan_Song1
  - dblp_id: https://dblp.org/pid/79/1081.html
    emails: '****@u.washington.edu'
    first_name: Fei
    google_scholar_id: https://scholar.google.com/citations?user=BnVLQFwAAAAJ
    homepage: https://faculty.washington.edu/fxia/
    institution: University of Washington, Seattle
    last_name: Xia
    name: Fei Xia
    semantic_scholar_id: https://www.semanticscholar.org/author/Fei-Xia/144956443
    username: ~Fei_Xia2
  - dblp_id: https://dblp.org/pid/z/YongdongZhang
    emails: '****@ustc.edu.cn'
    first_name: Yongdong
    institution: University of Science and Technology of China
    last_name: Zhang
    name: Yongdong Zhang
    username: ~Yongdong_Zhang2
  decision: toFindings
  end_page: 3094
  file: 741.pdf
  id: 741
  num_pages: 13
  openreview_id: oBbDfKW922
  pdf_file: 2d76baa990f0380c0f2a50adbab1d321b7e305bf.pdf
  start_page: 3082
  title: Aspect-based Sentiment Analysis with Context Denoising
- abstract: 'Tamil, a Dravidian language of South Asia, is a highly diglossic language
    with two very different registers in everyday use: Literary Tamil (preferred in
    writing and formal communication) and Spoken Tamil (confined to speech and informal
    media). Spoken Tamil is under-studied in modern NLP systems compared to Literary
    Tamil written in the Tamil script, as evidenced by a lack of datasets explicitly
    targetting the Spoken variety. In this paper, we release IruMozhi, a human-translated
    dataset of parallel text in Literary and Spoken Tamil. Using IruMozhi, we train
    classifiers on the task of identifying which Tamil variety a text belongs to.
    We use these models to gauge the availability of pretraining data in Spoken Tamil,
    to audit the composition of existing labelled datasets for Tamil, and to encourage
    future work on the variety.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multilinguality and Language Diversity
  authors:
  - emails: '****@gmail.com'
    first_name: Kabilan
    last_name: Prasanna
    name: Kabilan Prasanna
    username: ~Kabilan_Prasanna1
  - dblp_id: https://dblp.uni-trier.de/pid/263/6933
    emails: '****@stanford.edu'
    first_name: Aryaman
    google_scholar_id: https://scholar.google.com/citations?user=0-4GKw8AAAAJ
    homepage: https://aryaman.io/
    last_name: Arora
    name: Aryaman Arora
    orcid: https://orcid.org/0000-0002-4977-8206
    semantic_scholar_id: https://www.semanticscholar.org/author/Aryaman-Arora/1575802390
    username: ~Aryaman_Arora1
  decision: toFindings
  end_page: 3102
  file: 742.pdf
  id: 742
  num_pages: 8
  openreview_id: rG4TtFaQzg
  pdf_file: 50afeb74e9d7df0207be9a478e6998406eea6aab.pdf
  start_page: 3095
  title: 'IruMozhi: Automatically classifying diglossia in Tamil'
- abstract: "Norm violations occur when individuals fail to conform to culturally\
    \ accepted behaviors, which may lead to potential conflicts. Remediating norm\
    \ violations requires social awareness and cultural sensitivity of the nuances\
    \ at play. To equip interactive AI systems with a remediation ability, we offer\
    \ ReNoVi \u2014 a large-scale corpus of 9,258 multi-turn dialogues annotated with\
    \ social norms, as well as define a sequence of tasks to help understand and remediate\
    \ norm violations step by step. ReNoVi consists of two parts: 512 human-authored\
    \ dialogues (real data), and 8,746 synthetic conversations generated by ChatGPT\
    \ through prompt learning. While collecting sufficient human-authored data is\
    \ costly, synthetic conversations provide suitable amounts of data to help mitigate\
    \ the scarcity of training data, as well as the chance to assess the alignment\
    \ between LLMs and humans in the awareness of social norms. We thus harness the\
    \ power of ChatGPT to generate synthetic training data for our task. To ensure\
    \ the quality of both human-authored and synthetic data, we follow a quality control\
    \ protocol during data collection. Our experimental results demonstrate the importance\
    \ of remediating norm violations in socio-cultural conversations, as well as the\
    \ improvement in performance obtained from synthetic data."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Computational Social Science and Cultural Analytics
  authors:
  - dblp_id: https://dblp.org/pid/243/3387
    emails: '****@gmail.com'
    first_name: Haolan
    google_scholar_id: https://scholar.google.com/citations?user=nZc5CU4AAAAJ&hl=zh-CN
    institution: Monash University
    last_name: Zhan
    name: Haolan Zhan
    semantic_scholar_id: https://www.semanticscholar.org/author/Haolan-Zhan/146950185
    username: ~Haolan_Zhan1
  - emails: '****@monash.edu'
    first_name: Zhuang
    institution: Monash University
    last_name: Li
    name: Zhuang Li
    username: ~Zhuang_Li1
  - emails: '****@monash.edu'
    first_name: Xiaoxi
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=COEl8WsAAAAJ&view_op=list_works&sortby=pubdate
    last_name: Kang
    name: Xiaoxi Kang
    orcid: https://orcid.org/0000-0003-3869-4741
    username: ~Xiaoxi_Kang1
  - emails: '****@monash.edu'
    first_name: Tao
    google_scholar_id: https://scholar.google.com/citations?user=bWDIBXwAAAAJ&hl=en
    institution: Monash University
    last_name: Feng
    name: Tao Feng
    orcid: https://orcid.org/0009-0001-9490-7353
    semantic_scholar_id: https://www.semanticscholar.org/author/Tao-Feng/2056029883
    username: ~Tao_Feng2
  - dblp_id: https://dblp.org/pid/162/1637.html
    emails: '****@monash.edu'
    first_name: Yuncheng
    google_scholar_id: https://scholar.google.com.au/citations?hl=en&user=yPwnJCkAAAAJ
    last_name: Hua
    name: YUNCHENG HUA
    orcid: https://orcid.org/0000-0002-4238-5071
    username: ~YUNCHENG_HUA2
  - dblp_id: https://dblp.org/pid/58/3601
    emails: '****@monash.edu'
    first_name: Lizhen
    google_scholar_id: https://scholar.google.com.au/citations?user=cHXZgHUAAAAJ&hl=en
    homepage: https://research.monash.edu/en/persons/lizhen-qu
    institution: Monash University
    last_name: Qu
    name: Lizhen Qu
    orcid: https://orcid.org/0000-0002-7764-431X
    semantic_scholar_id: https://www.semanticscholar.org/author/Lizhen-Qu/14564042
    username: ~Lizhen_Qu2
  - emails: '****@binus.edu'
    first_name: Yi Ying
    google_scholar_id: https://scholar.google.co.id/citations?user=EZXS79wAAAAJ&hl=en
    homepage: https://www.researchgate.net/profile/Yi-Ying-3
    institution: Binus University
    last_name: ''
    name: 'Yi Ying '
    orcid: https://orcid.org/0000-0002-8658-3838
    username: ~Yi_Ying1
  - emails: '****@binus.ac.id'
    first_name: Mei Rianto
    google_scholar_id: https://scholar.google.com/citations?user=FXNcxvYAAAAJ&hl=zh-CN&oi=ao
    institution: Binus University
    last_name: Chandra
    name: Mei Rianto Chandra
    username: ~Mei_Rianto_Chandra1
  - emails: '****@binus.ac.id'
    first_name: Kelly
    google_scholar_id: https://scholar.google.co.id/citations?user=OUL5eEAAAAAJ&hl=id
    last_name: Rosalin
    name: Kelly Rosalin
    orcid: https://orcid.org/0000-0001-5832-6394
    username: ~Kelly_Rosalin2
  - emails: '****@binus.edu'
    first_name: Jureynolds
    google_scholar_id: https://scholar.google.com/citations?user=SknpJcIAAAAJ&hl=en
    institution: Binus University
    last_name: Jureynolds
    name: JUREYNOLDS JUREYNOLDS
    orcid: https://orcid.org/0000-0002-3415-7036
    username: ~JUREYNOLDS_JUREYNOLDS1
  - emails: '****@csun.edu'
    first_name: Suraj
    homepage: https://www.linkedin.com/in/suraj-sharma-5ab191229
    last_name: Sharma
    name: Suraj Sharma
    username: ~Suraj_Sharma1
  - emails: '****@monash.edu'
    first_name: Shilin
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=Y2Feb1EAAAAJ
    last_name: QU
    name: SHILIN QU
    username: ~SHILIN_QU1
  - dblp_id: https://dblp.org/pid/251/5530.html
    emails: '****@monash.edu'
    first_name: Linhao
    google_scholar_id: https://scholar.google.com.hk/citations?user=RO46HpcAAAAJ&hl=zh-CN
    homepage: https://rmanluo.github.io/
    last_name: Luo
    name: LINHAO LUO
    orcid: https://orcid.org/0000-0003-0027-942X
    username: ~LINHAO_LUO1
  - emails: '****@monash.edu'
    first_name: Ingrid
    google_scholar_id: https://scholar.google.com.tw/citations?user=DFyOHW4AAAAJ
    homepage: http://users.monash.edu/~ingrid/
    institution: Monash University
    last_name: Zukerman
    name: Ingrid Zukerman
    orcid: https://orcid.org/0000-0003-2237-5017
    username: ~Ingrid_Zukerman1
  - dblp_id: https://dblp.org/pid/47/4141
    emails: '****@monash.edu'
    first_name: Lay-Ki
    google_scholar_id: https://scholar.google.com.my/citations?user=SiVW3HkAAAAJ&hl=en
    institution: Monash University
    last_name: Soon
    name: Lay-Ki Soon
    orcid: https://orcid.org/0000-0002-8072-242X
    username: ~Lay-Ki_Soon1
  - emails: '****@gmail.com'
    first_name: Zhaleh
    google_scholar_id: https://scholar.google.com/citations?user=zFebFdMAAAAJ
    homepage: https://www.csun.edu/management/zhaleh-semnaniazad
    institution: California State University, Northridge
    last_name: Semnani Azad
    name: Zhaleh Semnani Azad
    username: ~Zhaleh_Semnani_Azad1
  - dblp_id: https://dblp.org/pid/57/5129.html
    emails: '****@monash.edu'
    first_name: Reza
    google_scholar_id: https://scholar.google.com.tw/citations?user=Perjx5EAAAAJ
    homepage: http://users.monash.edu.au/~gholamrh/
    institution: Monash University
    last_name: Haf
    name: Reza Haf
    semantic_scholar_id: https://www.semanticscholar.org/author/Gholamreza-Haffari/2561045
    username: ~Reza_Haf1
  decision: toFindings
  end_page: 3116
  file: 748.pdf
  id: 748
  num_pages: 14
  openreview_id: xkzDGR5M1Y
  pdf_file: ca25fefd081032b159e15912827604b9b073fb3f.pdf
  start_page: 3103
  title: 'RENOVI: A Benchmark Towards Remediating Norm Violations in Socio-Cultural
    Conversations'
- abstract: Data augmentation techniques apply transformations to existing texts to
    generate additional data. The transformations may produce low-quality texts, where
    the meaning of the text is changed and the text may even be mangled beyond human
    comprehension. Analyzing the synthetically generated texts and their corresponding
    labels is slow and demanding. To winnow out texts with incorrect labels, we develop
    INSPECTOR, a human-in-the-loop data inspection technique. INSPECTOR combines the
    strengths of provenance tracking techniques with assistive labeling. INSPECTOR
    allows users to group related texts by their $\textit{transformation provenance}$,
    i.e., the transformations applied to the original text, or $\textit{feature provenance}$,
    the linguistic features of the original text. For assistive labeling, INSPECTOR
    computes metrics that approximate data quality, and allows users to compare the
    corresponding label of each text against the predictions of a large language model.
    In a user study, INSPECTOR increases the number of texts with correct labels identified
    by $3\times$ on a sentiment analysis task and by $4\times$ on a hate speech detection
    task. The participants found grouping the synthetically generated texts by their
    common transformation to be the most useful technique. Surprisingly, grouping
    texts by common linguistic features was perceived to be unhelpful. Contrary to
    prior work, our study finds that no single technique obviates the need for human
    inspection effort. This validates the design of INSPECTOR which combines both
    analysis of data provenance and assistive labeling to reduce human inspection
    effort.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - emails: '****@cs.ucla.edu'
    first_name: Hong Jin
    homepage: https://kanghj.github.io/
    institution: University of California, Los Angeles
    last_name: Kang
    name: Hong Jin Kang
    username: ~Hong_Jin_Kang1
  - dblp_id: https://dblp.org/pid/274/6818.html
    emails: '****@cs.ucla.edu'
    first_name: Fabrice
    google_scholar_id: https://scholar.google.com/citations?user=AY9hnu8AAAAJ&hl=en&oi=ao
    homepage: https://fabrice.harel-canada.com/
    last_name: Harel-Canada
    middle_name: Y
    name: Fabrice Y Harel-Canada
    semantic_scholar_id: https://www.semanticscholar.org/author/Fabrice-Harel-Canada/1576480073
    username: ~Fabrice_Y_Harel-Canada1
  - emails: '****@cs.vt.edu'
    first_name: Muhammad Ali
    google_scholar_id: https://scholar.google.com/citations?user=k9M-nccAAAAJ
    homepage: https://people.cs.vt.edu/~gulzar
    institution: Virginia Tech
    last_name: Gulzar
    name: Muhammad Ali Gulzar
    username: ~Muhammad_Ali_Gulzar1
  - dblp_id: https://dblp.org/pid/117/4036
    emails: '****@cs.ucla.edu'
    first_name: Nanyun
    google_scholar_id: https://scholar.google.com/citations?user=XxRXvX0AAAAJ&hl=en
    homepage: http://vnpeng.net/
    institution: University of California, Los Angeles
    last_name: Peng
    name: Nanyun Peng
    semantic_scholar_id: https://www.semanticscholar.org/author/Nanyun-Peng/3157053
    username: ~Nanyun_Peng1
  - dblp_id: https://dblp.org/pid/56/5320.html
    emails: '****@cs.ucla.edu'
    first_name: Miryung
    google_scholar_id: https://scholar.google.com.tw/citations?user=a_50FBsAAAAJ
    homepage: http://www.cs.ucla.edu/~miryung/
    institution: University of California, Los Angeles
    last_name: Kim
    name: Miryung Kim
    username: ~Miryung_Kim1
  decision: toFindings
  end_page: 3128
  file: 750.pdf
  id: 750
  num_pages: 12
  openreview_id: lE5tKg6CUS
  pdf_file: 6cd73a80e978a36cc2d4b7f813f21b44e1999fa1.pdf
  start_page: 3117
  title: Human-in-the-Loop Synthetic Text Data Inspection with Provenance Tracking
- abstract: "Recently, instruction-tuned large language models (LLMs) are showing\
    \ prominent performance on various tasks, such as question answering. However,\
    \ the majority of instruction-tuned LLMs are English-centric, which hinders their\
    \ application to low-resource language QA. In this paper, we propose COde-Mixed\
    \ Multilingual Instruction Tuning (COMMIT) to adapt English-centric LLM to low-resource\
    \ language QA. We point out two main causes of English-centricness: imbalance\
    \ of unlabeled data, and English-centric instruction tuning datasets. To deviate\
    \ from English-centric instruction tuning, we propose to specialize code-mixing\
    \ for instruction tuning, which blocks code-mixing in English templates, to leverage\
    \ the potential of its superiority. To overcome data imbalance, we perform cross-lingual\
    \ alignment. The majority of cross-lingual alignment works focused on making representations\
    \ similar, which is not desirable to decoder-based LLMs, such as LLaMA. \nTherefore,\
    \ we propose code-mixed continual causal language modeling to align the decoder.\
    \ COMMIT improves the exact match score of low-resourced language QA by up to\
    \ 32x. Code is publicly available."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multilinguality and Language Diversity
  authors:
  - dblp_id: https://dblp.org/pid/141/9456-2
    emails: '****@snu.ac.kr'
    first_name: Jaeseong
    institution: Seoul National University
    last_name: Lee
    name: Jaeseong Lee
    semantic_scholar_id: https://www.semanticscholar.org/author/Jaeseong-Lee/2125053163
    username: ~Jaeseong_Lee1
  - emails: '****@snu.ac.kr'
    first_name: YeonJoon
    google_scholar_id: https://scholar.google.com/citations?user=HNBHFSEAAAAJ&hl=ko
    homepage: http://dilab.yonsei.ac.kr/
    institution: Seoul National University
    last_name: Jung
    name: YeonJoon Jung
    username: ~YeonJoon_Jung1
  - dblp_id: https://dblp.org/pid/h/SeungwonHwang
    emails: '****@snu.ac.kr'
    first_name: Seung-won
    google_scholar_id: https://scholar.google.com/citations?user=63bBmc3mYrAC&hl=ko
    homepage: http://seungwonh.github.io
    institution: Seoul National University
    last_name: Hwang
    name: seung-won hwang
    semantic_scholar_id: https://www.semanticscholar.org/author/Seung-won-Hwang/1716415
    username: ~seung-won_hwang2
  decision: toFindings
  end_page: 3136
  file: 753.pdf
  id: 753
  num_pages: 8
  openreview_id: yYEoioqmD6
  pdf_file: 87dfb33e9d2060a2c836ba2802924a77e04a6067.pdf
  start_page: 3129
  title: 'COMMIT: Code-Mixing English-Centric Large Language Model for Multilingual
    Instruction Tuning'
- abstract: Dataset distillation aims to compress a training dataset by creating a
    small number of informative synthetic samples such that neural networks trained
    on them perform as well as those trained on the original training dataset. Current
    text dataset distillation methods create each synthetic sample as a sequence of
    word embeddings instead of a text to apply gradient-based optimization; however,
    such embedding-level distilled datasets cannot be used for training other models
    whose word embedding weights are different from the model used for distillation.
    To address this issue, we propose a novel text dataset distillation approach,
    called Distilling dataset into Language Model (DiLM), which trains a language
    model to generate informative synthetic training samples as text data, instead
    of directly optimizing synthetic samples. We evaluated DiLM on various text classification
    datasets and showed that distilled synthetic datasets from DiLM outperform those
    from current coreset selection methods. DiLM achieved remarkable generalization
    performance in training different types of models and in-context learning of large
    language models. Our code will be available at https://github.com/arumaekawa/DiLM.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Learning for NLP
  authors:
  - dblp_id: https://dblp.org/pid/346/1537
    emails: '****@lr.pi.titech.ac.jp'
    first_name: Aru
    google_scholar_id: https://scholar.google.co.jp/citations?user=D7xUvR4AAAAJ&hl=ja
    homepage: https://sites.google.com/view/aru-maekawa
    institution: Tokyo Institute of Technology, Tokyo Institute of Technology
    last_name: Maekawa
    name: Aru Maekawa
    semantic_scholar_id: https://www.semanticscholar.org/author/Aru-Maekawa/2215615664
    username: ~Aru_Maekawa1
  - dblp_id: https://dblp.org/pid/247/1236
    emails: '****@m.titech.ac.jp'
    first_name: Satoshi
    google_scholar_id: https://scholar.google.com/citations?user=3RoLn48AAAAJ
    homepage: https://satoshi-kosugi.github.io/
    institution: Tokyo Institute of Technology, Tokyo Institute of Technology
    last_name: Kosugi
    name: Satoshi Kosugi
    username: ~Satoshi_Kosugi1
  - dblp_id: https://dblp.org/pid/21/3705
    emails: '****@lr.pi.titech.ac.jp'
    first_name: Kotaro
    google_scholar_id: https://scholar.google.co.jp/citations?user=UhXijg8AAAAJ
    homepage: http://www.lr.pi.titech.ac.jp/~funakoshi/
    institution: Institute of Innovative Research, Tokyo Institute of Technology
    last_name: Funakoshi
    name: Kotaro Funakoshi
    orcid: https://orcid.org/0000-0002-4529-4634
    semantic_scholar_id: https://www.semanticscholar.org/author/Kotaro-Funakoshi/1747395
    username: ~Kotaro_Funakoshi1
  - dblp_id: https://dblp.org/pid/79/125
    emails: '****@pi.titech.ac.jp'
    first_name: Manabu
    google_scholar_id: https://scholar.google.com/citations?user=NpQMX_8AAAAJ&hl=ja
    homepage: http://lr-www.pi.titech.ac.jp/wp/
    institution: Tokyo Institute of Technology, Tokyo Institute of Technology
    last_name: Okumura
    name: Manabu Okumura
    orcid: https://orcid.org/0009-0001-7730-1536
    semantic_scholar_id: https://www.semanticscholar.org/author/M.-Okumura/144859189
    username: ~Manabu_Okumura2
  decision: toFindings
  end_page: 3152
  file: 763.pdf
  id: 763
  num_pages: 16
  openreview_id: AwsCYdrzhZ
  pdf_file: 2fb09cebe6048fabd69cb8c91445a41d5604aa00.pdf
  start_page: 3137
  title: 'DiLM: Distilling Dataset into Language Model for Text-level Dataset Distillation'
- abstract: "Large Foundation Models (LFMs) can perform complex scheduling in a multi-agent\
    \ system and can coordinate agents to complete sophisticated tasks that require\
    \ extensive collaboration.\nHowever, despite the introduction of numerous gaming\
    \ frameworks, the community lacks adequate benchmarks that support the implementation\
    \ of a general multi-agent infrastructure encompassing collaboration between LFMs\
    \ and human-NPCs. We propose a novel infrastructure---Mindagent---for evaluating\
    \ planning and coordination capabilities in the context of gaming interaction.\
    \ In particular, our infrastructure leverages an existing gaming framework to\
    \ (i) act as the coordinator for a multi-agent system, (ii) collaborate with human\
    \ players via instructions, and (iii) enable in-context learning based on few-shot\
    \ prompting with feedback.\nFurthermore, we introduce \u201CCuisineworld\u201D\
    , a new gaming scenario and its related benchmark that supervises multiple agents\
    \ playing the game simultaneously and measures multi-agent collaboration efficiency.\
    \ We have conducted comprehensive evaluations with a new auto-metric Collaboration\
    \ Score: CoS for assessing the collaboration efficiency. \nFinally, Mindagent\
    \ can be deployed in real-world gaming scenarios in a customized VR version of\
    \ Cuisineworld and adapted in the \"Minecraft'' domain. Our work involving LFMs\
    \ within our new infrastructure for general-purpose scheduling and coordination\
    \ can elucidate how such skills may be obtained by learning from large language\
    \ corpora."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - emails: '****@ucla.edu'
    first_name: Ran
    homepage: https://nikepupu.github.io/
    institution: University of California, Los Angeles
    last_name: Gong
    name: Ran Gong
    username: ~Ran_Gong1
  - emails: '****@microsoft.com'
    first_name: Qiuyuan
    google_scholar_id: https://scholar.google.com/citations?user=U7Mmyc8AAAAJ&hl=en
    homepage: https://www.microsoft.com/en-us/research/people/qihua/
    institution: Microsoft Research, Redmond
    last_name: Huang
    name: Qiuyuan Huang
    username: ~Qiuyuan_Huang1
  - dblp_id: https://dblp.org/pid/68/7758
    emails: '****@gmail.com'
    first_name: Xiaojian
    google_scholar_id: https://scholar.google.com/citations?user=xZec9goAAAAJ
    homepage: https://jeasinema.github.io
    last_name: Ma
    name: Xiaojian Ma
    username: ~Xiaojian_Ma1
  - emails: '****@microsoft.com'
    first_name: Yusuke
    institution: Microsoft
    last_name: Noda
    name: Yusuke Noda
    username: ~Yusuke_Noda1
  - emails: '****@stanford.edu'
    first_name: Zane
    google_scholar_id: https://scholar.google.com/citations?user=qxH2dTsAAAAJ&hl=en&oi=ao
    last_name: Durante
    name: Zane Durante
    orcid: https://orcid.org/0000-0001-9038-8915
    username: ~Zane_Durante1
  - dblp_id: https://dblp.org/pid/218/5234
    emails: '****@ucla.edu'
    first_name: Zilong
    google_scholar_id: https://scholar.google.com/citations?user=9sDx70IAAAAJ&hl=en
    homepage: http://web.cs.ucla.edu/~zilongzheng/
    institution: Beijing Institute for General Artificial Intelligence
    last_name: Zheng
    name: Zilong Zheng
    username: ~Zilong_Zheng1
  - dblp_id: https://dblp.org/pid/85/4738
    emails: '****@cs.ucla.edu'
    first_name: Demetri
    google_scholar_id: https://scholar.google.com/citations?user=pKuBFaQAAAAJ
    homepage: https://web.cs.ucla.edu/~dt
    institution: University of California, Los Angeles
    last_name: Terzopoulos
    name: Demetri Terzopoulos
    username: ~Demetri_Terzopoulos1
  - dblp_id: https://dblp.org/pid/79/2528
    emails: '****@cs.princeton.edu'
    first_name: Li
    google_scholar_id: https://scholar.google.com/citations?user=rDfyQnIAAAAJ&hl=en
    homepage: http://vision.stanford.edu/feifeili/
    institution: Stanford University and Stanford University
    last_name: Fei-Fei
    name: Li Fei-Fei
    username: ~Li_Fei-Fei1
  - dblp_id: https://dblp.org/pid/92/5339
    emails: '****@microsoft.com'
    first_name: Jianfeng
    homepage: https://www.microsoft.com/en-us/research/people/jfgao/
    institution: Microsoft Research
    last_name: Gao
    name: Jianfeng Gao
    username: ~Jianfeng_Gao1
  - emails: '****@microsoft.com'
    first_name: Hoi
    homepage: https://www.linkedin.com/in/hoi-vo-193420/
    last_name: Vo
    name: Hoi Vo
    username: ~Hoi_Vo1
  decision: toFindings
  end_page: 3182
  file: 767.pdf
  id: 767
  num_pages: 30
  openreview_id: gSxDM4z6n1
  pdf_file: 6c8d72cf0313d8642e876ee1e14d662a202ca9ed.pdf
  start_page: 3153
  title: 'MindAgent: Emergent Gaming Interaction'
- abstract: 'In the realm of modern Large Language Models (LLMs), facilitating high-quality,
    multi-turn dialogues with humans represents a cornerstone feature. However, human-based
    evaluation of such a capability involves substantial manual effort. This study
    offers a formative assessment of current LLMs'' proficiency in emulating human-like,
    multi-turn conversations using an LLM-centric approach. The evaluation encompasses
    three key elements in the evaluation pipeline: utterance generation, evaluation
    protocol, and judgement, and we delve deeply into each aspect. GPT-4, both as
    an utterance generator and as a judge, exhibits exceptional performance. As a
    generator, GPT-4 crafts dialogues indistinguishable from human interactions in
    terms of style and flow. When judging, it shows a heightened alignment with human
    evaluative standards and consistency. Conversely, other LLMs face challenges in
    producing quality multi-turn dialogues, hindered by inadequate instruction-following
    abilities, a propensity for prolix utterances, and overall limited capabilities.
    Notably, generating extensive dialogues (e.g., spanning tens of turns) remains
    a formidable task for most LLMs, particularly in Chinese contexts. We hope that
    our work can serve as a valuable resource for evaluating the multi-turn chatting
    capabilities of LLMs. Related resources are available at https://github.com/open-compass/BotChat.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - dblp_id: https://dblp.org/pid/211/7919
    emails: '****@pku.edu.cn'
    first_name: Haodong
    google_scholar_id: https://scholar.google.com/citations?user=vi3W-m8AAAAJ&hl=en&oi=ao
    homepage: https://kennymckormick.github.io
    institution: The Chinese University of Hong Kong
    last_name: Duan
    name: Haodong Duan
    semantic_scholar_id: https://www.semanticscholar.org/author/Haodong-Duan/31463937
    username: ~Haodong_Duan1
  - dblp_id: https://dblp.org/pid/355/7959.html
    emails: '****@qq.com'
    first_name: Jueqi
    homepage: https://frankweijue.github.io/
    last_name: Wei
    name: Jueqi Wei
    semantic_scholar_id: https://www.semanticscholar.org/author/Jueqi-Wei/2233546929
    username: ~Jueqi_Wei1
  - emails: '****@qq.com'
    first_name: Chonghua
    homepage: https://philipwangovo.github.io/
    institution: Shanghai Jiaotong University
    last_name: Wang
    name: Chonghua Wang
    username: ~Chonghua_Wang1
  - emails: '****@m.fudan.edu.cn'
    first_name: Hongwei
    homepage: https://www.cnblogs.com/liushz-blog/
    last_name: Liu
    name: Hongwei Liu
    username: ~Hongwei_Liu2
  - emails: '****@outlook.com'
    first_name: Yixiao
    homepage: https://github.com/fangyixiao18
    institution: Tencent
    last_name: Fang
    name: Yixiao Fang
    username: ~Yixiao_Fang1
  - dblp_id: https://dblp.org/pid/152/9228
    emails: '****@pjlab.org.cn'
    first_name: Songyang
    google_scholar_id: https://scholar.google.com/citations?user=8XQPi7YAAAAJ
    homepage: https://www.zhangsongyang.com/
    institution: Shanghai AI Laboratory
    last_name: Zhang
    name: Songyang Zhang
    username: ~Songyang_Zhang1
  - emails: '****@ie.cuhk.edu.hk'
    first_name: Dahua
    google_scholar_id: https://scholar.google.com/citations?user=GMzzRRUAAAAJ
    homepage: http://dahua.site
    institution: The Chinese University of Hong Kong
    last_name: Lin
    name: Dahua Lin
    username: ~Dahua_Lin1
  - dblp_id: https://dblp.org/pid/181/2839-26
    emails: '****@pjlab.org.cn'
    first_name: Kai
    google_scholar_id: https://scholar.google.com.hk/citations?user=eGD0b7IAAAAJ&hl=en
    homepage: https://chenkai.site/
    institution: Shanghai AI Laboratory
    last_name: Chen
    name: Kai Chen
    orcid: https://orcid.org/0000-0002-6820-2325
    semantic_scholar_id: https://www.semanticscholar.org/author/Kai-Chen/152568027
    username: ~Kai_Chen4
  decision: toFindings
  end_page: 3199
  file: 771.pdf
  id: 771
  num_pages: 17
  openreview_id: vVuI4bRthf
  pdf_file: b84b0f7b08c272dda47a232c57d1235627fb8de8.pdf
  start_page: 3183
  title: 'BotChat: Evaluating LLMs'' Capabilities of Having Multi-Turn Dialogues'
- abstract: Most pretrained language models rely on subword tokenization, which processes
    text as a sequence of subword tokens. However, different granularities of text,
    such as characters, subwords, and words, can contain different kinds of information.
    Previous studies have shown that incorporating multiple input granularities improves
    model generalization, yet very few of them outputs useful representations for
    each granularity. In this paper, we introduce the entanglement model, aiming to
    combine character and subword language models. Inspired by vision-language models,
    our model treats characters and subwords as separate modalities, and it generates
    mutually informed representations for both granularities as output. We evaluate
    our model on text classification, named entity recognition,  POS-tagging, and
    character-level sequence labeling (intraword code-switching). Notably, the entanglement
    model outperforms its backbone language models, particularly in the presence of
    noisy texts and low-resource languages. Furthermore, the entanglement model even
    outperforms larger pre-trained models on all English sequence labeling tasks and
    classification tasks. We make our code publically available.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multilinguality and Language Diversity
  authors:
  - emails: '****@alumni.cmu.edu'
    first_name: Yilin
    homepage: https://github.com/TonyW42
    institution: School of Engineering and Applied Sciences, Harvard University and
      Carnegie Mellon University
    last_name: Wang
    name: Yilin Wang
    username: ~Yilin_Wang14
  - emails: '****@gmail.com'
    first_name: Xinyi
    homepage: https://www.linkedin.com/in/xinyi-cindy-hu/
    institution: School of Computer Science, Carnegie Mellon University
    last_name: Hu
    name: Xinyi Hu
    username: ~Xinyi_Hu3
  - dblp_id: https://dblp.org/pid/116/0475
    emails: '****@cs.cmu.edu'
    first_name: Matthew
    google_scholar_id: https://scholar.google.com/citations?user=GU0SZmYAAAAJ&hl=en
    homepage: http://www.cs.cmu.edu/~mgormley/
    institution: School of Computer Science, Carnegie Mellon University and 3M
    last_name: Gormley
    middle_name: R.
    name: Matthew R. Gormley
    semantic_scholar_id: https://www.semanticscholar.org/author/Matthew-R.-Gormley/1762110
    username: ~Matthew_R._Gormley1
  decision: toFindings
  end_page: 3212
  file: 772.pdf
  id: 772
  num_pages: 13
  openreview_id: IQHihJKiOC
  pdf_file: 90b15522c45a3147ba3ae0ad780782824cc338f2.pdf
  start_page: 3200
  title: Learning Mutually Informed Representations for Characters and Subwords
- abstract: Existing transfer learning methods for neural machine translation typically
    use a well-trained translation model (i.e., a parent model) of a high-resource
    language pair to directly initialize a translation model (i.e., a child model)
    of a low-resource language pair, and the child model is then fine-tuned with corresponding
    datasets. In this paper, we propose a novel two-step fine-tuning (TSFT) framework
    for transfer learning in low-resource neural machine translation. In the first
    step, we adjust the parameters of the parent model to fit the child language by
    using the child source data. In the second step, we transfer the adjusted parameters
    to the child model and fine-tune it with a proposed distillation loss for efficient
    optimization. Our experimental results on five low-resource translations demonstrate
    that our framework yields significant improvements over various strong transfer
    learning baselines. Further analysis demonstrated the effectiveness of different
    components in our framework.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Translation
  authors:
  - emails: '****@massey.ac.nz'
    first_name: Yuan
    last_name: Gao
    name: Yuan Gao
    orcid: https://orcid.org/0009-0002-6397-2854
    username: ~Yuan_Gao25
  - dblp_id: https://dblp.org/pid/65/8064
    emails: '****@massey.ac.nz'
    first_name: Feng
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=BQBZ82oAAAAJ
    institution: Massey University
    last_name: Hou
    name: Feng Hou
    orcid: https://orcid.org/my-orcid?orcid=0000-0001-5776-9177
    semantic_scholar_id: https://www.semanticscholar.org/author/Feng-Hou/49272105
    username: ~Feng_Hou1
  - emails: '****@massey.ac.nz'
    first_name: Ruili
    homepage: https://www.massey.ac.nz/massey/expertise/profile.cfm?stref=980830
    institution: Massey University
    last_name: Wang
    name: Ruili Wang
    username: ~Ruili_Wang1
  decision: toFindings
  end_page: 3223
  file: 773.pdf
  id: 773
  num_pages: 11
  openreview_id: wVoGXCjrKs
  pdf_file: 31350b6d685ea8406dcbbb7d883e948bca1c99ba.pdf
  start_page: 3213
  title: A Novel Two-step Fine-tuning Framework for Transfer Learning in Low-Resource
    Neural Machine Translation
- abstract: 'The field of cross-lingual sentence embeddings has recently experienced
    significant advancements, but research concerning low-resource languages has lagged
    due to the scarcity of parallel corpora. This paper shows that cross-lingual word
    representation in low-resource languages is notably under-aligned with that in
    high-resource languages in current models. To address this, we introduce a novel
    framework that explicitly aligns words between English and eight low-resource
    languages, utilizing off-the-shelf word alignment models. This framework incorporates
    three primary training objectives: aligned word prediction and word translation
    ranking, along with the widely used translation ranking. We evaluate our approach
    through experiments on the bitext retrieval task, which demonstrate substantial
    improvements on sentence embeddings in low-resource languages. In addition, the
    competitive performance of the proposed model across a broader range of tasks
    in high-resource languages underscores its practicality.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multilinguality and Language Diversity
  authors:
  - emails: '****@g.ecc.u-tokyo.ac.jp'
    first_name: Zhongtao
    google_scholar_id: https://scholar.google.com/citations?user=sFzgKNoAAAAJ
    institution: The University of Tokyo
    last_name: Miao
    name: Zhongtao Miao
    username: ~Zhongtao_Miao1
  - dblp_id: https://dblp.org/pid/197/6506-1
    emails: '****@g.ecc.u-tokyo.ac.jp'
    first_name: Qiyu
    google_scholar_id: https://scholar.google.com/citations?user=oDn0AnwAAAAJ&hl=zh-CN
    homepage: https://qiyuw.github.io/
    institution: The University of Tokyo, Tokyo Institute of Technology and Peking
      University
    last_name: Wu
    name: Qiyu Wu
    semantic_scholar_id: https://www.semanticscholar.org/author/Qiyu-Wu/2112240600
    username: ~Qiyu_Wu2
  - emails: '****@gmail.com'
    first_name: Kaiyan
    last_name: Zhao
    name: Kaiyan Zhao
    username: ~Kaiyan_Zhao2
  - emails: '****@g.ecc.u-tokyo.ac.jp'
    first_name: Zilong
    last_name: Wu
    name: Zilong Wu
    username: ~Zilong_Wu1
  - dblp_id: https://dblp.org/pid/18/3787
    emails: '****@g.ecc.u-tokyo.ac.jp'
    first_name: Yoshimasa
    google_scholar_id: https://scholar.google.com/citations?user=J2CkFngAAAAJ
    homepage: https://www.logos.t.u-tokyo.ac.jp/~tsuruoka/
    institution: The University of Tokyo
    last_name: Tsuruoka
    name: Yoshimasa Tsuruoka
    semantic_scholar_id: https://www.semanticscholar.org/author/Yoshimasa-Tsuruoka/143946906
    username: ~Yoshimasa_Tsuruoka1
  decision: toFindings
  end_page: 3235
  file: 777.pdf
  id: 777
  num_pages: 12
  openreview_id: Mx2Ezghq8G
  pdf_file: f18582d7e0fce0edba48437fb07b65292a1fe8ea.pdf
  start_page: 3224
  title: Enhancing Cross-lingual Sentence Embedding for Low-resource Languages with
    Word Alignment
- abstract: Aspect-based Sentiment Analysis (ABSA) is a fine-grained task. Recently,
    using graph convolutional networks (GCNs) to model syntactic information has become
    a popular topic. In addition, a growing consensus exists to enhance sentence representation
    using contrastive learning. However, when modeling syntactic information, incorrect
    syntactic structure may introduce additional noise. Meanwhile, we believe that
    contrastive learning implicitly introduce label information as priori. Therefore,
    we propose C$^{3}$LPGCN, which integrates Contrastive Learning and Cooperative
    Learning with Prompt into GCN. Specifically, to alleviate the noise when modeling
    syntactic information, we propose mask-aware aspect information filter, which
    combines prompt information of template with aspect information to filter the
    syntactic information. Besides, we propose prompt-based contrastive learning and
    cooperative learning to utilise the label information further. On the one hand,
    we construct prompts containing labels for contrastive learning, by which the
    model can focus more on task-relevant features. On the other hand, cooperative
    learning further extracts label information by aligning input samples' representation
    and output distribution with label samples. Extensive experiments on three datasets
    demonstrate that our method significantly improves the model's performance compared
    to traditional contrastive learning methods. Moreover, our C$^{3}$LPGCN outperforms
    state-of-the-art methods. Our source code and final models are publicly available
    at github
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Sentiment Analysis, Stylistic Analysis, and Argument Mining
  authors:
  - emails: '****@163.com'
    first_name: Ye
    institution: Chongqing University of Technology
    last_name: He
    name: Ye He
    orcid: https://orcid.org/0000-0003-1815-7383
    username: ~Ye_He2
  - dblp_id: https://dblp.org/pid/223/4696
    emails: '****@163.com'
    first_name: Shihao
    institution: Chongqing University of Technology
    last_name: Zou
    name: Shihao Zou
    orcid: https://orcid.org/0000-0002-6648-3995
    username: ~Shihao_Zou3
  - emails: '****@163.com'
    first_name: YuzheChen
    last_name: YuzheChen
    name: YuzheChen
    orcid: https://orcid.org/0000-0001-8884-7780
    username: ~YuzheChen1
  - dblp_id: https://dblp.org/pid/08/11486
    emails: '****@163.com'
    first_name: Xianying
    institution: Chongqing University of Technology
    last_name: Huang
    name: Xianying Huang
    orcid: https://orcid.org/0000-0002-3667-6198
    username: ~Xianying_Huang1
  decision: toFindings
  end_page: 3246
  file: 778.pdf
  id: 778
  num_pages: 11
  openreview_id: gSwb6LlrLr
  pdf_file: 983bbc406db9c98604588e49abbf20e8f88aa7ef.pdf
  start_page: 3236
  title: C$^{3}$LPGCN:Integrating Contrastive Learning and Cooperative Learning with
    Prompt into Graph Convolutional Network for Aspect-based Sentiment Analysis
- abstract: 'MultiModal Summarization (MMS) aims to generate a concise summary based
    on multimodal data like texts and images and has wide application in multimodal
    fields.

    Previous works mainly focus on the coarse-level textual and visual features in
    which the overall features of the image interact with the whole sentence.

    However, the entities of the input text and the objects of the image may be underutilized,
    limiting the performance of current MMS models.

    In this paper, we propose a novel Visual Enhanced Entity-Level Interaction Network
    (VE-ELIN) to address the problem of underutilization of multimodal inputs at a
    fine-grained level in two ways.

    We first design a cross-modal entity interaction module to better fuse the entity
    information in text and the object information in vision.

    Then, we design an object-guided visual enhancement module to fully extract the
    visual features and enhance the focus of the image on the object area.

    We evaluate VE-ELIN on two MMS datasets and propose new metrics to measure the
    factual consistency of entities in the output.

    Finally, experimental results demonstrate that VE-ELIN is effective and outperforms
    previous methods under both traditional metrics and ours.

    The source code is available at https://github.com/summoneryhl/VE-ELIN.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Summarization
  authors:
  - emails: '****@bupt.edu.cn'
    first_name: Haolong
    homepage: https://github.com/summoneryhl
    last_name: Yan
    name: Haolong Yan
    username: ~Haolong_Yan1
  - emails: '****@bupt.edu.cn'
    first_name: Binghao
    homepage: https://github.com/TangBinghao
    last_name: Tang
    name: Binghao Tang
    username: ~Binghao_Tang1
  - dblp_id: https://dblp.org/pid/305/9830
    emails: '****@bupt.edu.cn'
    first_name: Boda
    google_scholar_id: https://scholar.google.com.hk/citations?user=OTbLS30AAAAJ&hl=zh-CN
    homepage: https://github.com/TimeLessLing
    last_name: Lin
    name: Boda Lin
    orcid: https://orcid.org/0000-0002-6627-7200
    semantic_scholar_id: https://www.semanticscholar.org/author/Boda-Lin/10144804
    username: ~Boda_Lin1
  - emails: '****@bupt.edu.cn'
    first_name: Gang
    last_name: Zhao
    name: Gang Zhao
    orcid: https://orcid.org/0000-0001-7965-8531
    username: ~Gang_Zhao2
  - dblp_id: https://dblp.org/pid/54/6603-1
    emails: '****@bupt.edu.cn'
    first_name: Si
    homepage: http://www.pris.net.cn/introduction/teacher/lisi
    institution: Beijing University of Posts and Telecommunications
    last_name: Li
    name: Si Li
    semantic_scholar_id: https://www.semanticscholar.org/author/Si-Li/2118153856
    username: ~Si_Li5
  decision: toFindings
  end_page: 3259
  file: 780.pdf
  id: 780
  num_pages: 13
  openreview_id: aCATLumo4m
  pdf_file: 49843e7204871e87ab81f7710168d797c08a56f4.pdf
  start_page: 3247
  title: Visual Enhanced Entity-Level Interaction Network for Multimodal Summarization
- abstract: "Large language models (LLMs) enable in-context learning (ICL) by conditioning\
    \ on a few labeled training examples as a text-based prompt, eliminating the need\
    \ for parameter updates and achieving competitive performance. In this paper,\
    \ we demonstrate that factual knowledge is imperative for the performance of ICL\
    \ in three core facets: the inherent knowledge learned in LLMs, the factual knowledge\
    \ derived from the selected in-context examples, and the knowledge biases in LLMs\
    \ for output generation. To unleash the power of LLMs in few-shot learning scenarios,\
    \ we introduce a novel Knowledgeable In-Context Tuning (KICT) framework to further\
    \ improve the performance of ICL:\n1) injecting knowledge into LLMs during continual\
    \ self-supervised pre-training, \n2) judiciously selecting the examples for ICL\
    \ with high knowledge relevance, and \n3) calibrating the prediction results based\
    \ on prior knowledge.\n\nWe evaluate the proposed approaches on autoregressive\
    \ models (e.g., GPT-style LLMs) over multiple text classification and question-answering\
    \ tasks.  Experimental results demonstrate that KICT substantially outperforms\
    \ strong baselines and improves by more than 13\\% and 7\\% on text classification\
    \ and question-answering tasks, respectively."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - dblp_id: https://dblp.org/pid/85/1466
    emails: '****@gmail.com'
    first_name: Jianing
    google_scholar_id: https://scholar.google.com/citations?user=ccaimI8AAAAJ&hl=zh-CN
    homepage: https://github.com/wjn1996
    last_name: Wang
    name: Jianing Wang
    orcid: https://orcid.org/0000-0001-6006-053X
    semantic_scholar_id: https://www.semanticscholar.org/author/Jianing-Wang/2140049394
    username: ~Jianing_Wang4
  - dblp_id: https://dblp.org/pid/135/5147-1
    emails: '****@gmail.com'
    first_name: Chengyu
    google_scholar_id: https://scholar.google.com/citations?user=_AVfRnQAAAAJ
    homepage: https://chywang.github.io/
    institution: Alibaba Group
    last_name: Wang
    name: Chengyu Wang
    username: ~Chengyu_Wang1
  - dblp_id: https://dblp.org/pid/148/4497
    emails: '****@alibaba-inc.com'
    first_name: Chuanqi
    google_scholar_id: https://scholar.google.com/citations?user=tOfo4ncAAAAJ&hl=en
    institution: Alibaba Group
    last_name: Tan
    name: Chuanqi Tan
    orcid: https://orcid.org/0000-0002-6676-3057
    semantic_scholar_id: https://www.semanticscholar.org/author/Chuanqi-Tan/2111727840
    username: ~Chuanqi_Tan3
  - dblp_id: https://dblp.org/pid/51/5022
    emails: '****@aliyun.com'
    first_name: Jun
    last_name: Huang
    name: Jun Huang
    username: ~Jun_Huang4
  - dblp_id: https://dblp.org/pid/71/4173-1
    emails: '****@dase.ecnu.edu.cn'
    first_name: Ming
    google_scholar_id: https://scholar.google.com/citations?hl=zh-CN&user=vwI3qB0AAAAJ
    homepage: http://dase.ecnu.edu.cn/mgao/
    last_name: Gao
    name: Ming Gao
    orcid: https://orcid.org/0000-0002-5603-2680
    username: ~Ming_Gao1
  decision: toFindings
  end_page: 3279
  file: 783.pdf
  id: 783
  num_pages: 20
  openreview_id: DKBsmAQpHI
  pdf_file: d74d6bc6c8250c1ed2f3eb731ffb1c0c49a2b687.pdf
  start_page: 3260
  title: 'Knowledgeable In-Context Tuning: Exploring and Exploiting Factual Knowledge
    for In-Context Learning'
- abstract: 'Large language models (LLMs) are often trained on extensive, temporally
    indiscriminate text corpora, reflecting the lack of datasets with temporal metadata.
    This approach is not aligned with the evolving nature of language. Conventional
    methods for creating temporally adapted language models often depend on further
    pre-training static models on time-specific data. This paper presents a new approach:
    a series of point-in-time LLMs called TimeMachineGPT (TiMaGPT), specifically designed
    to be nonprognosticative. This ensures they remain uninformed about future factual
    information and linguistic changes. This strategy is beneficial for understanding
    language evolution and is of critical importance when applying models in dynamic
    contexts, such as time-series forecasting, where foresight of future information
    can prove problematic. We provide access to both the models and training datasets.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - emails: '****@eng.ox.ac.uk'
    first_name: Felix
    institution: University of Oxford
    last_name: Drinkall
    name: Felix Drinkall
    semantic_scholar_id: https://www.semanticscholar.org/author/2111315119
    username: ~Felix_Drinkall1
  - emails: '****@manchester.ac.uk'
    first_name: Eghbal
    google_scholar_id: https://scholar.google.com/citations?user=-iIpAsUAAAAJ&hl=en&oi=ao
    homepage: https://www.rahimikia.com/
    institution: University of Manchester
    last_name: Rahimikia
    name: Eghbal Rahimikia
    username: ~Eghbal_Rahimikia1
  - dblp_id: https://dblp.org/pid/60/5814
    emails: '****@oerc.ox.ac.uk'
    first_name: Janet
    google_scholar_id: https://scholar.google.com/citations?user=ebzKiiwAAAAJ&hl=en&oi=ao
    homepage: https://eng.ox.ac.uk/people/janet-pierrehumbert/
    institution: University of Oxford
    last_name: Pierrehumbert
    middle_name: B.
    name: Janet B. Pierrehumbert
    orcid: https://orcid.org/0000-0002-5989-3574
    username: ~Janet_B._Pierrehumbert1
  - dblp_id: ''
    emails: '****@robots.ox.ac.uk'
    first_name: Stefan
    google_scholar_id: ''
    homepage: ''
    institution: University of Oxford
    last_name: Zohren
    name: Stefan Zohren
    orcid: ''
    username: ~Stefan_Zohren1
  decision: toFindings
  end_page: 3291
  file: 794.pdf
  id: 794
  num_pages: 12
  openreview_id: uC8uXJmi5D
  pdf_file: bc3f13024e116c1216eb88216034dcdd72bdffcd.pdf
  start_page: 3280
  title: Time Machine GPT
- abstract: Recent advancements in natural language tasks leverage the emergent In-Context
    Learning (ICL) ability of pretrained Large Language Models (LLMs). ICL enables
    LLMs to perform new tasks by utilizing a limited number of input-output examples
    as prompts. While ICL circumvents the costly step of finetuning LLMs, its effectiveness
    is heavily dependent on the quality and ordering of provided examples (called
    exemplars). In this work, we propose a two-stage data-efficient framework $\textit{Div-S3}$
    for exemplar selection for ICL. The first stage focuses on data annotation and
    employs a pool-based active learning approach to select a set of $\textit{Div}$erse
    and informative exemplars from the target tasks' unlabeled pool. Given a test
    input/query, the second stage uses Submodular Span Summarization ($\textit{S3}$)
    to select the most relevant and non-redundant exemplars from the annotated pool
    of a limited budget. On $7$ different NLP datasets and $5$ LLMs of varying complexities,
    we show $\textit{Div-S3}$ outperforms (1) existing active learning-based methods
    for data annotation for ICL and (2) similarity-based methods for test query-specific
    exemplars retrieval.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Learning for NLP
  authors:
  - dblp_id: https://dblp.org/pid/177/2456
    emails: '****@uw.edu'
    first_name: Lilly
    google_scholar_id: https://scholar.google.com/citations?user=eoGxOusAAAAJ&hl=en&oi=ao
    homepage: https://lillykumari8.github.io/
    institution: University of Washington, Seattle
    last_name: Kumari
    name: Lilly Kumari
    username: ~Lilly_Kumari1
  - dblp_id: https://dblp.org/pers/w/Wang:Shengjie.html
    emails: '****@nyu.edu'
    first_name: Shengjie
    homepage: https://sheng-jie-wang.github.io/
    institution: University of Washington, University of Illinois, Urbana Champaign
      and New York University, Shanghai
    last_name: Wang
    name: Shengjie Wang
    username: ~Shengjie_Wang1
  - dblp_id: https://dblp.org/pid/263/7747.html
    emails: '****@uw.edu'
    first_name: Arnav
    google_scholar_id: https://scholar.google.com/citations?user=rnRml4EAAAAJ&hl=en#
    institution: University of Washington
    last_name: Das
    middle_name: Mohanty
    name: Arnav Mohanty Das
    username: ~Arnav_Mohanty_Das1
  - dblp_id: https://dblp.org/pid/88/8205-1
    emails: '****@umd.edu'
    first_name: Tianyi
    google_scholar_id: https://scholar.google.com/citations?user=OKvgizMAAAAJ&hl=en
    homepage: https://tianyizhou.github.io/
    institution: University of Maryland, College Park
    last_name: Zhou
    name: Tianyi Zhou
    orcid: https://orcid.org/0000-0001-5348-0632
    semantic_scholar_id: https://www.semanticscholar.org/author/Tianyi-Zhou/1805655
    username: ~Tianyi_Zhou2
  - dblp_id: https://dblp.org/pid/b/JeffABilmes
    emails: '****@uw.edu'
    first_name: Jeff
    google_scholar_id: https://scholar.google.com/citations?user=L9QufAsAAAAJ&hl=en
    homepage: http://melodi.ee.washington.edu/people/bilmes
    institution: University of Washington, Seattle
    last_name: Bilmes
    name: Jeff Bilmes
    username: ~Jeff_Bilmes1
  decision: toFindings
  end_page: 3307
  file: 795.pdf
  id: 795
  num_pages: 16
  openreview_id: ay7ON8aKTJ
  pdf_file: ead3daf7e2449cf9e7526b77d12be65a89c42193.pdf
  start_page: 3292
  title: An End-to-End Submodular Framework for Data-Efficient In-Context Learning
- abstract: This paper explores cost-efficient methods to adapt pretrained Large Language
    Models (LLMs) to new lower-resource languages, with a specific focus on Estonian.
    Leveraging the Llama 2 model, we investigate the impact of combining cross-lingual
    instruction-tuning with additional monolingual pretraining. Our results demonstrate
    that even a relatively small amount of additional monolingual pretraining followed
    by cross-lingual instruction-tuning significantly enhances results on Estonian.
    Furthermore, we showcase cross-lingual knowledge transfer from high-quality English
    instructions to Estonian, resulting in improvements in commonsense reasoning and
    multi-turn conversation capabilities. Our best model, named Llammas, represents
    the first open-source instruction-following LLM for Estonian. Additionally, we
    publish Alpaca-est, the first general task instruction dataset for Estonia. These
    contributions mark the initial progress in the direction of developing open-source
    LLMs for Estonian.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Efficient/Low-Resource Methods for NLP
  authors:
  - emails: '****@ut.ee'
    first_name: Hele-Andra
    homepage: https://tartunlp.ai/
    institution: University of Tartu
    last_name: Kuulmets
    name: Hele-Andra Kuulmets
    username: ~Hele-Andra_Kuulmets1
  - dblp_id: https://dblp.org/pid/321/5282
    emails: '****@ut.ee'
    first_name: Taido
    last_name: Purason
    name: Taido Purason
    semantic_scholar_id: https://www.semanticscholar.org/author/Taido-Purason/2167124642
    username: ~Taido_Purason1
  - emails: '****@ut.ee'
    first_name: Agnes
    google_scholar_id: https://scholar.google.es/citations?user=EiMNnN8AAAAJ&hl=en
    institution: institute of computer science, University of Tartu
    last_name: Luhtaru
    name: Agnes Luhtaru
    username: ~Agnes_Luhtaru1
  - dblp_id: https://dblp.org/pid/18/8157
    emails: '****@ut.ee'
    first_name: Mark
    google_scholar_id: https://scholar.google.com/citations?user=K6jhzXcAAAAJ&hl=en
    homepage: https://tartunlp.ai
    institution: University of Tartu
    last_name: Fishel
    name: Mark Fishel
    username: ~Mark_Fishel1
  decision: toFindings
  end_page: 3324
  file: 802.pdf
  id: 802
  num_pages: 17
  openreview_id: Phwq4INVBE
  pdf_file: b8afcd24c6bdac6309ee8cfd0bb3eeb413953d43.pdf
  start_page: 3308
  title: Teaching Llama a New Language Through Cross-Lingual Knowledge Transfer
- abstract: 'Accurately simulating human opinion dynamics is crucial for understanding
    a variety of societal phenomena, including polarization and the spread of misinformation.
    However, the agent-based models (ABMs) commonly used for such simulations often
    over-simplify human behavior. We propose a new approach to simulating opinion
    dynamics based on populations of Large Language Models (LLMs). Our findings reveal
    a strong inherent bias in LLM agents towards producing accurate information, leading
    simulated agents to consensus in line with scientific reality. This bias limits
    their utility for understanding resistance to consensus views on issues like climate
    change. After inducing confirmation bias through prompt engineering, however,
    we observed opinion fragmentation in line with existing agent-based modeling and
    opinion dynamics research. These insights highlight the promise and limitations
    of LLM agents in this domain and suggest a path forward: refining LLMs with real-world
    discourse to better simulate the evolution of human beliefs.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Computational Social Science and Cultural Analytics
  authors:
  - emails: '****@wisc.edu'
    first_name: Yun-Shiuan
    google_scholar_id: https://scholar.google.com/citations?user=XO6Nt0AAAAAJ&hl=en
    institution: University of Wisconsin - Madison
    last_name: Chuang
    name: Yun-Shiuan Chuang
    username: ~Yun-Shiuan_Chuang1
  - emails: '****@wisc.edu'
    first_name: Agam
    google_scholar_id: https://scholar.google.com/citations?user=lpqh8B0AAAAJ&hl=en
    homepage: https://agoyal0512.github.io/
    last_name: Goyal
    name: Agam Goyal
    orcid: https://orcid.org/0009-0009-5989-2887
    semantic_scholar_id: https://www.semanticscholar.org/author/Agam-Goyal/2266839000
    username: ~Agam_Goyal1
  - dblp_id: https://dblp.org/pid/361/0079
    emails: '****@wisc.edu'
    first_name: Nikunj
    google_scholar_id: https://scholar.google.com/citations?user=0CYKCKUAAAAJ&hl=en
    homepage: https://niks64.github.io/
    last_name: Harlalka
    name: Nikunj Harlalka
    semantic_scholar_id: https://www.semanticscholar.org/author/Nikunj-Harlalka/97283515
    username: ~Nikunj_Harlalka1
  - dblp_id: https://dblp.org/pid/262/0748
    emails: '****@wisc.edu'
    first_name: Siddharth
    google_scholar_id: https://scholar.google.com/citations?user=xsyrntwAAAAJ&hl=en
    homepage: https://www.sidsuresh.com/
    last_name: Suresh
    name: Siddharth Suresh
    username: ~Siddharth_Suresh1
  - dblp_id: https://dblp.org/pid/168/8718.html
    emails: '****@princeton.edu'
    first_name: Robert
    google_scholar_id: https://scholar.google.com/citations?user=7EPsnxEAAAAJ&hl=en
    homepage: https://www.rxdhawkins.com
    institution: Princeton University
    last_name: Hawkins
    middle_name: D.
    name: Robert D. Hawkins
    semantic_scholar_id: https://www.semanticscholar.org/author/Robert-D.-Hawkins/8932668
    username: ~Robert_D._Hawkins1
  - dblp_id: https://dblp.org/pid/212/7594
    emails: '****@alumni.upenn.edu'
    first_name: Sijia
    google_scholar_id: https://scholar.google.com/citations?user=PQJnIWkAAAAJ&hl=en&oi=ao
    homepage: https://journalism.wisc.edu/news/staff/yang-sijia/
    institution: University of Wisconsin - Madison
    last_name: Yang
    name: Sijia Yang
    orcid: https://orcid.org/0000-0003-4209-9881
    semantic_scholar_id: https://www.semanticscholar.org/author/Sijia-Yang/2108973978
    username: ~Sijia_Yang1
  - emails: '****@wisc.edu'
    first_name: Dhavan
    google_scholar_id: https://scholar.google.com/citations?user=9_GqLnwAAAAJ&hl=en&oi=ao
    homepage: https://dshah.journalism.wisc.edu/
    institution: University of Wisconsin - Madison
    last_name: Shah
    middle_name: V.
    name: Dhavan V. Shah
    orcid: https://orcid.org/0000-0001-5034-2816
    semantic_scholar_id: https://www.semanticscholar.org/author/Dhavan-V.-Shah/34355283
    username: ~Dhavan_V._Shah1
  - dblp_id: https://dblp.org/pid/123/0773-1.html
    emails: '****@wisc.edu'
    first_name: Junjie
    google_scholar_id: https://scholar.google.com/citations?user=j-42gHYAAAAJ
    homepage: https://junjiehu.github.io/
    institution: University of Wisconsin, Madison
    last_name: Hu
    name: Junjie Hu
    username: ~Junjie_Hu2
  - dblp_id: https://dblp.org/pid/25/7229
    emails: '****@wisc.edu'
    first_name: Timothy
    google_scholar_id: https://scholar.google.com/citations?user=7u_uyOsAAAAJ&hl=en&oi=ao
    homepage: http://concepts.psych.wisc.edu/
    last_name: Rogers
    middle_name: T.
    name: Timothy T. Rogers
    username: ~Timothy_T._Rogers1
  decision: toFindings
  end_page: 3345
  file: 813.pdf
  id: 813
  num_pages: 21
  openreview_id: 17k2XRixZF
  pdf_file: c6c38264208c7b58ed045528d81d1d20c9f5c63f.pdf
  start_page: 3325
  title: Simulating Opinion Dynamics with Networks of LLM-based Agents
- abstract: 'We investigate how pretrained language models (PLM) encode the grammatical
    category of verbal aspect in Russian.  Encoding of aspect in transformer LMs has
    not been studied previously in any language.  A particular challenge is posed
    by ''''alternative contexts'''': where either the perfective or the imperfective
    aspect is suitable grammatically and semantically.  We perform probing using BERT
    and RoBERTa on alternative and non-alternative contexts.  First, we assess the
    models'' performance on aspect prediction, via behavioral probing.  Next, we examine
    the models'' performance when their contextual representations are substituted
    with counterfactual representations, via causal probing.  These counterfactuals
    alter the value of the "boundedness'''' feature---a semantic feature, which characterizes
    the action in the context.  Experiments show that BERT and RoBERTa do encode aspect---mostly
    in their final layers. The counterfactual interventions affect perfective and
    imperfective in opposite ways, which is consistent with grammar: perfective is
    positively affected by adding the meaning of boundedness, and vice versa.  The
    practical implications of our probing results are that fine-tuning only the last
    layers of BERT on predicting aspect is faster and more effective than fine-tuning
    the whole model.  The model has high predictive uncertainty about aspect in alternative
    contexts, which tend to lack explicit hints about the boundedness of the described
    action.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - dblp_id: https://dblp.org/pid/204/1361
    emails: '****@helsinki.fi'
    first_name: Anisia
    google_scholar_id: https://scholar.google.com/citations?user=Fefa8gkAAAAJ&hl=en&oi=ao
    homepage: https://researchportal.helsinki.fi/en/persons/anisia-katinskaia
    last_name: Katinskaia
    name: Anisia Katinskaia
    semantic_scholar_id: https://www.semanticscholar.org/author/Anisia-Katinskaia/35602279
    username: ~Anisia_Katinskaia1
  - emails: '****@helsinki.fi'
    first_name: Roman
    google_scholar_id: https://scholar.google.com/citations?user=jb5mzrwAAAAJ&hl=en&oi=ao
    institution: University of Helsinki
    last_name: Yangarber
    name: Roman Yangarber
    orcid: https://orcid.org/0000-0001-5264-9870
    username: ~Roman_Yangarber2
  decision: toFindings
  end_page: 3365
  file: 815.pdf
  id: 815
  num_pages: 20
  openreview_id: SJhXjpKC60
  pdf_file: 1429d71a2bf594fefff5a5fea24dcb291b5f8ed5.pdf
  start_page: 3346
  title: Probing the Category of Verbal Aspect in Transformer Language Models
- abstract: Typologically diverse benchmarks are increasingly created to track the
    progress achieved in multilingual NLP. Linguistic diversity of these data sets
    is typically measured as the number of languages or language families included
    in the sample, but such measures do not consider structural properties of the
    included languages. In this paper, we propose assessing linguistic diversity of
    a data set against a reference language sample as a means of maximising linguistic
    diversity in the long run. We represent languages as sets of features and apply
    a version of the Jaccard index suitable for comparing sets of measures. In addition
    to the features extracted from typological data bases, we propose an automatic
    text-based measure, which can be used as a means of overcoming the well-known
    problem of data sparsity in manually collected features. Our diversity score is
    interpretable in terms of linguistic features and can identify the types of languages
    that are not represented in a data set. Using our method, we analyse a range of
    popular multilingual data sets (UD, Bible100, mBERT, XTREME, XGLUE, XNLI, XCOPA,
    TyDiQA, XQuAD). In addition to ranking these data sets, we find, for example,
    that (poly)synthetic languages are missing in almost all of them.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multilinguality and Language Diversity
  authors:
  - dblp_id: https://dblp.org/pid/117/4006
    emails: '****@uzh.ch'
    first_name: Tanja
    google_scholar_id: https://scholar.google.ch/citations?hl=en&user=fMt7jzMAAAAJ&view_op=list_works&sortby=pubdate
    homepage: http://www.spur.uzh.ch/samardzic
    institution: University of Zurich
    last_name: Samardzic
    name: Tanja Samardzic
    username: ~Tanja_Samardzic1
  - dblp_id: https://dblp.org/pid/165/0767
    emails: '****@ciencias.unam.mx'
    first_name: Ximena
    google_scholar_id: https://scholar.google.com.mx/citations?user=fgPTAocAAAAJ&hl=es
    homepage: https://scholar.google.com.mx/citations?user=fgPTAocAAAAJ
    institution: "Universidad Nacional Aut\xF3noma de M\xE9xico"
    last_name: Gutierrez
    name: Ximena Gutierrez
    semantic_scholar_id: https://www.semanticscholar.org/author/Ximena-Gutierrez-Vasques/
    username: ~Ximena_Gutierrez1
  - dblp_id: https://dblp.org/pid/142/5112.html
    emails: '****@christianbentz.de'
    first_name: Christian
    google_scholar_id: https://scholar.google.com/citations?user=oUEFb0EAAAAJ&hl=en
    homepage: http://www.christianbentz.de/
    institution: "Eberhard-Karls-Universit\xE4t T\xFCbingen"
    last_name: Bentz
    name: Christian Bentz
    semantic_scholar_id: https://www.semanticscholar.org/author/C.-Bentz/3255675
    username: ~Christian_Bentz1
  - dblp_id: https://dblp.org/pid/82/9971
    emails: '****@gmail.com'
    first_name: Steven
    google_scholar_id: https://scholar.google.com/citations?user=PpTOh08AAAAJ&hl=en
    homepage: https://unine.ch/evolang/home/team/steven-moran.html
    institution: University of Miami
    last_name: Moran
    name: Steven Moran
    orcid: https://orcid.org/0000-0002-3969-6549
    semantic_scholar_id: https://www.semanticscholar.org/author/S.-Moran/2113459789
    username: ~Steven_Moran1
  - emails: '****@uzh.ch'
    first_name: Olga
    google_scholar_id: https://scholar.google.com/citations?user=wUXZLmkAAAAJ&hl=en
    last_name: Pelloni
    name: Olga Pelloni
    username: ~Olga_Pelloni1
  decision: toFindings
  end_page: 3381
  file: 817.pdf
  id: 817
  num_pages: 16
  openreview_id: d0UZtDS7vy
  pdf_file: 3f1bd328fdb966cb1d330c6c67cb439ff8176b76.pdf
  start_page: 3366
  title: A Measure for Transparent Comparison of Linguistic Diversity in Multilingual
    NLP Data Sets
- abstract: Text-to-SQL aims to convert natural language into structured query language,
    which is a challenging task. Current research focuses mainly on read operations
    and ignores other aspects of database operations such as create, update, and delete
    operations. The benchmark datasets as well as models that have been proposed also
    fail to cover these operations, limiting the development and practical applications
    in the field. To bridge this gap, we propose CRUDSQL, a large-scale cross-domain
    single-table CRUD operations Chinese Text-to-SQL dataset. The dataset contains
    10,000 question/SQL pairs involving 625 tables from different domains. To support
    further research on this dataset, we also propose a baseline method, CRUDParser,
    which employs a two-phase approach based on BERT and T5 for SQL generation and
    incorporates two strategies, value matching, and value prompting, for interacting
    with databases to further improve the performance. The experimental results show
    that the new operation types bring different challenges for future research, and
    our approach achieves 67.08% and 83.8% exact set matching accuracy under both
    read and delete operations in the test set, but only 49.6% and 61.8% under create
    and update operations. We believe that the proposal of CRUDSQL as well as CRUDParser
    can provide new directions and possibilities for research and practical applications
    in the field of Text-to-SQL. The dataset is published at https://github.com/bizard-lab/CRUDSQL.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - emails: '****@stu.kust.edu.cn'
    first_name: Xi
    last_name: Chen
    name: Xi Chen
    orcid: https://orcid.org/0009-0009-6946-4034
    username: ~Xi_Chen53
  - emails: '****@126.com'
    first_name: Jinguo
    homepage: https://www.scholat.com/jgyou
    institution: Kunmimg University of Science and Technology
    last_name: You
    name: Jinguo You
    username: ~Jinguo_You1
  - emails: '****@huawei.com'
    first_name: Likun
    last_name: Likun
    name: Likun
    orcid: https://orcid.org/0000-0002-6156-8164
    username: ~Likun1
  - emails: '****@stu.kust.edu.cn'
    first_name: Xiang
    homepage: https://github.com/
    last_name: Li
    name: Xiang Li
    username: ~Xiang_Li77
  decision: toFindings
  end_page: 3392
  file: 818.pdf
  id: 818
  num_pages: 11
  openreview_id: uGBSXTle0A
  pdf_file: 7925c34fc57552e196565146c6e55d76d233412b.pdf
  start_page: 3382
  title: 'Beyond Read-Only: Crafting a Comprehensive Chinese Text-to-SQL Dataset for
    Database Manipulation and Query'
- abstract: Conservation of historical documents benefits from computational methods
    by alleviating the manual labor related to digitization and modernization of textual
    content. Languages usually evolve over time and keeping historical wordforms is
    crucial for diachronic studies and digital humanities. However, spelling conventions
    did not necessarily exist when texts were originally written and orthographic
    variations are commonly observed depending on scribes and time periods. In this
    study, we propose to automatically normalize orthographic wordforms found in historical
    archives written in Middle French during the 16th century without fully modernizing
    textual content. We leverage pre-trained models in a low resource setting based
    on a manually curated parallel corpus and produce additional resources with artificial
    data generation approaches. Results show that causal language models and knowledge
    distillation improve over a strong baseline, thus validating the proposed methods.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - dblp_id: https://dblp.org/pid/18/9125.html
    emails: '****@gmail.com'
    first_name: Raphael
    google_scholar_id: https://scholar.google.com/citations?user=bSrDt54AAAAJ
    institution: University of Geneva
    last_name: Rubino
    name: Raphael Rubino
    semantic_scholar_id: https://www.semanticscholar.org/author/Rapha%C3%ABl-Rubino/1731383
    username: ~Raphael_Rubino1
  - dblp_id: https://dblp.org/pid/80/8154
    emails: '****@unige.ch'
    first_name: Johanna
    institution: University of Geneva
    last_name: Gerlach
    name: Johanna Gerlach
    orcid: https://orcid.org/ 0000-0002-3371-4021
    username: ~Johanna_Gerlach1
  - emails: '****@unige.ch'
    first_name: Jonathan
    google_scholar_id: https://scholar.google.com/citations?user=5tajzc0AAAAJ&hl=en&oi=ao
    homepage: https://www.unige.ch/fti/en/faculte/departements/dtim/membrestim/mutal/
    last_name: Mutal
    middle_name: David
    name: Jonathan David Mutal
    username: ~Jonathan_David_Mutal1
  - emails: '****@unige.ch'
    first_name: Pierrette
    homepage: https://www.unige.ch/fti/fr/faculte/departements/dtim/membrestim/bouillon/
    institution: University of Geneva
    last_name: Bouillon
    name: Pierrette Bouillon
    username: ~Pierrette_Bouillon2
  decision: toFindings
  end_page: 3401
  file: 822.pdf
  id: 822
  num_pages: 9
  openreview_id: mUzq6S2o1r
  pdf_file: 1b64c068a5da97f7c9dd538891a411e98de68ca9.pdf
  start_page: 3393
  title: 'Normalizing without Modernizing: Keeping Historical Wordforms of Middle
    French while Reducing Spelling Variants'
- abstract: Zero-shot In-context learning is the phenomenon where models can perform
    a task given only the instructions. However, pre-trained large language models
    are known to be poorly calibrated for zero-shot tasks. One of the most effective
    approaches to handling this bias is to adopt a contrastive decoding objective,
    which accounts for the prior probability of generating the next token by conditioning
    on a context. This work introduces an Anti-Language Model objective with a decay
    factor designed to address the weaknesses of In-context Machine Translation. We
    conduct our experiments across 3 model types and sizes, 3 language directions,
    and for both greedy decoding and beam search. The proposed method outperforms
    other state-of-the-art decoding objectives, with up to 20 BLEU point improvement
    from the default objective in some settings.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Translation
  authors:
  - dblp_id: https://dblp.org/pid/264/0109
    emails: '****@gmail.com'
    first_name: Suzanna
    homepage: https://suzyahyah.github.io
    last_name: Sia
    name: Suzanna Sia
    semantic_scholar_id: https://www.semanticscholar.org/author/Suzanna-Sia/1666941674
    username: ~Suzanna_Sia1
  - dblp_id: https://dblp.org/pid/276/1319
    emails: '****@jhu.edu'
    first_name: Alexandra
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=gLBPalMAAAAJ&view_op=list_works&sortby=pubdate
    homepage: https://alexandradelucia.com
    last_name: DeLucia
    name: Alexandra DeLucia
    orcid: https://orcid.org/0000-0003-4325-9170
    semantic_scholar_id: https://www.semanticscholar.org/author/Alexandra-DeLucia/34989844?sort=velocity
    username: ~Alexandra_DeLucia1
  - dblp_id: https://dblp.org/pid/58/3217
    emails: '****@cs.jhu.edu'
    first_name: Kevin
    google_scholar_id: https://scholar.google.com/citations?user=M3BSiiQAAAAJ
    homepage: https://cs.jhu.edu/~kevinduh/
    institution: Johns Hopkins University
    last_name: Duh
    name: Kevin Duh
    semantic_scholar_id: https://www.semanticscholar.org/author/Kevin-Duh/1800354
    username: ~Kevin_Duh1
  decision: toFindings
  end_page: 3419
  file: 829.pdf
  id: 829
  num_pages: 18
  openreview_id: 9vYnJYeOzP
  pdf_file: 7d43774bdf8dd545314de9de26d62ff37df83cec.pdf
  start_page: 3402
  title: Anti-LM Decoding for Zero-shot In-context Machine Translation
- abstract: Recently, various parameter-efficient fine-tuning (PEFT) strategies for
    application to language models have been proposed and successfully implemented.
    However, this raises the question of whether PEFT, which only updates a limited
    set of model parameters, constitutes security vulnerabilities when confronted
    with weight-poisoning backdoor attacks. In this study, we show that PEFT is more
    susceptible to weight-poisoning backdoor attacks compared to the full-parameter
    fine-tuning method, with pre-defined triggers remaining exploitable and pre-defined
    targets maintaining high confidence, even after fine-tuning. Motivated by this
    insight, we developed a  Poisoned Sample Identification Module (PSIM) leveraging
    PEFT, which identifies poisoned samples through confidence, providing robust defense
    against weight-poisoning backdoor attacks. Specifically, we leverage PEFT to train
    the PSIM with randomly reset sample labels. During the inference process, extreme
    confidence serves as an indicator for poisoned samples, while others are clean.
    We conduct experiments on text classification tasks, five fine-tuning strategies,
    and three weight-poisoning backdoor attack methods. Experiments show near 100\%
    success rates for weight-poisoning backdoor attacks when utilizing PEFT. Furthermore,
    our defensive approach exhibits overall competitive performance in mitigating
    weight-poisoning backdoor attacks.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Ethics, Bias, and Fairness
  authors:
  - dblp_id: https://dblp.org/pid/116/8682
    emails: '****@ntu.edu.sg'
    first_name: Shuai
    google_scholar_id: https://scholar.google.com/citations?user=upbsFBAAAAAJ&hl=zh-CN
    institution: Jinan University
    last_name: Zhao
    name: Shuai Zhao
    orcid: https://orcid.org/0000-0001-5174-5182
    semantic_scholar_id: https://www.semanticscholar.org/author/Shuai-Zhao/2111307323
    username: ~Shuai_Zhao2
  - dblp_id: https://dblp.uni-trier.de/pid/213/9040
    emails: '****@zju.edu.cn'
    first_name: Leilei
    google_scholar_id: https://scholar.google.com/citations?user=7P0RI0cAAAAJ&hl=zh-CN
    institution: Zhejiang University
    last_name: Gan
    name: Leilei Gan
    semantic_scholar_id: https://www.semanticscholar.org/author/Leilei-Gan/35618308
    username: ~Leilei_Gan1
  - dblp_id: https://dblp.org/pid/81/8329
    emails: '****@ntu.edu.sg'
    first_name: Anh Tuan
    google_scholar_id: https://scholar.google.com.sg/citations?hl=en&user=d6ixOGYAAAAJ&view_op=list_works
    homepage: https://tuanluu.github.io/
    institution: Nanyang Technological University
    last_name: Luu
    name: Anh Tuan Luu
    semantic_scholar_id: https://www.semanticscholar.org/author/Anh-Tuan-Luu/26336902
    username: ~Anh_Tuan_Luu2
  - dblp_id: https://dblp.org/pid/16/7565.html
    emails: '****@gmail.com'
    first_name: Jie
    google_scholar_id: https://scholar.google.com/citations?user=66osleIAAAAJ&hl=en
    homepage: https://bigaidream.github.io/
    institution: Hong Kong University of Science and Technology
    last_name: Fu
    name: Jie Fu
    orcid: https://orcid.org/0000-0002-4494-843X
    username: ~Jie_Fu2
  - dblp_id: https://dblp.org/pid/178/9876
    emails: '****@gmail.com'
    first_name: Lingjuan
    institution: Sony Research
    last_name: Lyu
    name: Lingjuan Lyu
    username: ~Lingjuan_Lyu1
  - dblp_id: https://dblp.org/pid/150/3494
    emails: '****@bit.edu.cn'
    first_name: Meihuizi
    google_scholar_id: https://scholar.google.com/citations?user=9f9c0J0AAAAJ&hl=zh-CN&oi=sra
    last_name: Jia
    name: Meihuizi Jia
    semantic_scholar_id: https://www.semanticscholar.org/author/Meihuizi-Jia/1762766
    username: ~Meihuizi_Jia1
  - dblp_id: https://dblp.org/pid/36/8492.html
    emails: '****@mail.mcgill.ca'
    first_name: Jinming
    google_scholar_id: https://scholar.google.com/citations?user=L_ssfM4AAAAJ&hl=en
    homepage: https://scholar.google.com/citations?user=L_ssfM4AAAAJ&hl=en
    last_name: Wen
    name: Jinming Wen
    username: ~Jinming_Wen1
  decision: toFindings
  end_page: 3437
  file: 831.pdf
  id: 831
  num_pages: 18
  openreview_id: LIsAMz9Ypv
  pdf_file: b768f01e00c1956122496d89d2d28c0ddab6662e.pdf
  start_page: 3420
  title: Defending Against Weight-Poisoning Backdoor Attacks for Parameter-Efficient
    Fine-Tuning
- abstract: Abstractive summarization for long-form narrative texts such as movie
    scripts is challenging due to the computational and memory constraints of current
    language models. A movie script typically comprises a large number of scenes;
    however, only a fraction of these scenes are salient, i.e., important for understanding
    the overall narrative. The salience of a scene can be operationalized by considering
    it as salient if it is mentioned in the summary. Automatically identifying salient
    scenes is difficult due to the lack of suitable datasets. In this work, we introduce
    a scene saliency dataset that consists of human-annotated salient scenes for 100
    movies. We propose a two-stage abstractive summarization approach which first
    identifies the salient scenes in script and then generates a summary using only
    those scenes. Using QA-based evaluation, we show that our model outperforms previous
    state-of-the-art summarization methods and reflects the information content of
    a movie more accurately than a model that takes the whole movie script as input.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Summarization
  authors:
  - dblp_id: https://dblp.uni-trier.de/pid/168/9577
    emails: '****@ed.ac.uk'
    first_name: Rohit
    google_scholar_id: https://scholar.google.co.in/citations?user=tc8C32QAAAAJ&hl=en
    institution: University of Edinburgh, University of Edinburgh
    last_name: Saxena
    name: Rohit Saxena
    username: ~Rohit_Saxena3
  - dblp_id: https://dblp.org/pid/30/4872
    emails: '****@inf.ed.ac.uk'
    first_name: Frank
    google_scholar_id: https://scholar.google.co.uk/citations?user=-lbtnAgAAAAJ&hl=en
    homepage: https://homepages.inf.ed.ac.uk/keller/
    institution: University of Edinburgh
    last_name: Keller
    name: Frank Keller
    orcid: https://orcid.org/0000-0002-8242-4362
    semantic_scholar_id: https://www.semanticscholar.org/author/Frank-Keller/143694777
    username: ~Frank_Keller1
  decision: toFindings
  end_page: 3454
  file: 834.pdf
  id: 834
  num_pages: 17
  openreview_id: J9nKg8kyhM
  pdf_file: 115a8b93ea850f8f22921ed94b05ceccc19ac74f.pdf
  start_page: 3438
  title: 'Select and Summarize: Scene Saliency for Movie Script Summarization'
- abstract: Offensive language detection is an important task for filtering out abusive
    expressions and improving online user experiences. However, malicious users often
    attempt to avoid filtering systems through the involvement of textual noises.
    In this paper, we propose these evasions as user-intended adversarial attacks
    that insert special symbols or leverage the distinctive features of the Korean
    language. Furthermore, we introduce simple yet effective pooling strategies in
    a layer-wise manner to defend against the proposed attacks, focusing on the preceding
    layers not just the last layer to capture both offensiveness and token embeddings.
    We demonstrate that these pooling strategies are more robust to performance degradation
    even when the attack rate is increased, without directly training of such patterns.
    Notably, we found that models pre-trained on clean texts could achieve a comparable
    performance in detecting attacked offensive language, to models pre-trained on
    noisy texts by employing these pooling strategies.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - emails: '****@gmail.com'
    first_name: Seunguk
    homepage: https://github.com/seungukyu
    last_name: Yu
    name: Seunguk Yu
    username: ~Seunguk_Yu1
  - dblp_id: https://dblp.org/pid/174/4879
    emails: '****@cau.ac.kr'
    first_name: Juhwan
    google_scholar_id: https://scholar.google.com/citations?view_op=list_works&hl=ko&user=6WRunk0AAAAJ
    homepage: https://github.com/c-juhwan
    institution: Chung-Ang University
    last_name: Choi
    name: Juhwan Choi
    semantic_scholar_id: https://www.semanticscholar.org/author/Juhwan-Choi/2190681194
    username: ~Juhwan_Choi1
  - dblp_id: https://dblp.org/pid/89/8603
    emails: '****@cau.ac.kr'
    first_name: YoungBin
    google_scholar_id: https://scholar.google.co.kr/citations?user=If6P518AAAAJ&hl=ko
    homepage: https://sites.google.com/view/iiplcau/
    institution: Chung-Ang University
    last_name: Kim
    name: YoungBin Kim
    semantic_scholar_id: https://www.semanticscholar.org/author/Youngbin-Kim/2135771346
    username: ~YoungBin_Kim1
  decision: toFindings
  end_page: 3466
  file: 839.pdf
  id: 839
  num_pages: 12
  openreview_id: qpz0vMdoWb
  pdf_file: fbb619a6ea15faf3b572d97a59d2250ef7ac7ec2.pdf
  start_page: 3455
  title: 'Don''t be a Fool: Pooling Strategies in Offensive Language Detection from
    User-Intended Adversarial Attacks'
- abstract: "Despite recent significant progress, Multi-Object Tracking (MOT) faces\
    \ limitations such as reliance on prior knowledge and predefined categories and\
    \ struggles with unseen objects. To address these issues, Generic Multiple Object\
    \ Tracking (GMOT) has emerged as an alternative approach, requiring less prior\
    \ information. However, current GMOT methods often rely on initial bounding boxes\
    \ and struggle to handle variations in factors such as viewpoint, lighting, occlusion,\
    \ and scale, among others. \nOur contributions commence with the introduction\
    \ of the Referring GMOT dataset a collection of videos, each accompanied by detailed\
    \ textual descriptions of their attributes. Subsequently, we propose Z-GMOT, a\
    \ cutting-edge tracking solution capable of tracking objects from never-seen categories\
    \ without the need of initial bounding boxes or predefined categories. Within\
    \ our Z-GMOT framework, we introduce two novel components: (i) iGLIP, an improved\
    \ Grounded language-image pretraining, for accurately detecting unseen objects\
    \ with specific characteristics. (ii) MA-SORT, a novel object association approach\
    \ that adeptly integrates motion and appearance-based matching strategies to tackle\
    \ the complex task of tracking objects with high similarity. Our contributions\
    \ are benchmarked through extensive experiments conducted on the Referring GMOT\
    \ dataset for GMOT task. Additionally, to assess the generalizability of the proposed\
    \ Z-GMOT, we conduct ablation studies on the DanceTrack and MOT20 datasets for\
    \ the MOT task. Our dataset, code, and models are released at: https://fsoft-aic.github.io/Z-GMOT"
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multimodality and Language Grounding to Vision, Robotics and Beyond
  authors:
  - emails: '****@gmail.com'
    first_name: Kim
    google_scholar_id: https://scholar.google.com/citations?user=SQXHftAAAAAJ&hl=en
    institution: FPT Software
    last_name: Tran
    middle_name: Hoang
    name: Kim Hoang Tran
    username: ~Kim_Hoang_Tran1
  - emails: '****@fpt.com'
    first_name: Anh Duy
    institution: FPT Software AI Center
    last_name: Le Dinh
    name: Anh Duy Le Dinh
    username: ~Anh_Duy_Le_Dinh1
  - emails: '****@selab.hcmus.edu.vn'
    first_name: Tien-Phat
    institution: ' John von Neumann'
    last_name: Nguyen
    name: Tien-Phat Nguyen
    username: ~Tien-Phat_Nguyen1
  - dblp_id: https://dblp.org/pid/253/9015.html
    emails: '****@uark.edu'
    first_name: Thinh
    homepage: https://sciprofiles.com/profile/1628355
    last_name: Phan
    name: Thinh Phan
    username: ~Thinh_Phan1
  - dblp_id: https://dblp.org/pid/270/6213
    emails: '****@uark.edu'
    first_name: Pha
    google_scholar_id: https://scholar.google.com/citations?user=YOzpM0AAAAAJ
    homepage: https://pha-nguyen.github.io/
    institution: University of Arkansas - Fayetteville
    last_name: Nguyen
    name: Pha Nguyen
    username: ~Pha_Nguyen1
  - dblp_id: https://dblp.org/pid/43/8092
    emails: '****@uark.edu'
    first_name: Khoa
    google_scholar_id: https://scholar.google.com/citations?user=JPAl8-gAAAAJ
    homepage: https://computer-science-and-computer-engineering.uark.edu/directory/index/uid/khoaluu/name/Khoa+Luu/
    institution: University of Arkansas, Fayetteville
    last_name: Luu
    name: Khoa Luu
    orcid: https://orcid.org/0000-0003-2104-0901
    username: ~Khoa_Luu2
  - dblp_id: https://dblp.org/pid/30/1579.html
    emails: '****@mail.wvu.edu'
    first_name: Donald
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=WfzNXGIAAAAJ&view_op=list_works&sortby=pubdate
    homepage: https://community.wvu.edu/~daadjeroh/
    institution: West Virginia University
    last_name: Adjeroh
    name: Donald Adjeroh
    username: ~Donald_Adjeroh1
  - dblp_id: https://dblp.org/pid/61/3857
    emails: '****@mail.wvu.edu'
    first_name: Gianfranco
    google_scholar_id: https://scholar.google.com/citations?user=ReEG0FwAAAAJ&hl=en
    homepage: https://vision.csee.wvu.edu/~doretto/
    institution: West Virginia University
    last_name: Doretto
    name: Gianfranco Doretto
    orcid: https://orcid.org/0000-0002-8921-6646
    semantic_scholar_id: https://www.semanticscholar.org/author/Gianfranco-Doretto/1736352
    username: ~Gianfranco_Doretto1
  - dblp_id: https://dblp.org/pid/37/245
    emails: '****@uark.edu'
    first_name: Ngan
    google_scholar_id: https://scholar.google.com/citations?user=8ck0k_UAAAAJ&hl=en
    homepage: https://computer-science-and-computer-engineering.uark.edu/directory/index/uid/thile/name/Thi+Hoang+Ngan+Le/
    institution: University of Arkansas, Fayetteville
    last_name: Le
    middle_name: Hoang
    name: Ngan Hoang Le
    username: ~Ngan_Hoang_Le1
  decision: toFindings
  end_page: 3478
  file: 842.pdf
  id: 842
  num_pages: 12
  openreview_id: 12eJ1fCL9q
  pdf_file: 54eefdd0a412f083de4038064bfc0d94fff47223.pdf
  start_page: 3467
  title: 'Z-GMOT: Zero-shot Generic Multiple Object Tracking'
- abstract: In recent years, counterspeech has emerged as one of the most promising
    strategies to fight online hate. These non-escalatory responses tackle online
    abuse while preserving the freedom of speech of the users, and can have a tangible
    impact in reducing online and offline violence. Recently, there has been growing
    interest from the Natural Language Processing (NLP) community in addressing the
    challenges of analysing, collecting, classifying, and automatically generating
    counterspeech, to reduce the huge burden of manually producing it. In particular,
    researchers have taken different directions in addressing these challenges, thus
    providing a variety of related tasks and resources. In this paper, we provide
    a guide for doing research on counterspeech, by describing - with detailed examples
    - the steps to undertake, and providing best practices that can be learnt from
    the NLP studies on this topic. Finally, we discuss open challenges and future
    directions of counterspeech research in NLP.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - dblp_id: https://dblp.org/pid/297/5466
    emails: '****@fbk.eu'
    first_name: Helena
    institution: Fondazione Bruno Kessler and University of Trento
    last_name: Bonaldi
    name: Helena Bonaldi
    semantic_scholar_id: https://www.semanticscholar.org/author/2120042648
    username: ~Helena_Bonaldi1
  - dblp_id: https://dblp.org/pid/176/1859.html
    emails: '****@turing.ac.uk'
    first_name: Yi-Ling
    google_scholar_id: https://scholar.google.it/citations?hl=en&user=276kTuoAAAAJ
    homepage: https://yilingchung.github.io
    institution: Alan Turing Institute
    last_name: Chung
    name: Yi-Ling Chung
    semantic_scholar_id: https://www.semanticscholar.org/author/Yi-Ling-Chung/3365740
    username: ~Yi-Ling_Chung1
  - dblp_id: https://dblp.org/pid/184/8685
    emails: '****@hw.ac.uk'
    first_name: Gavin
    institution: Heriot-Watt University
    last_name: Abercrombie
    name: Gavin Abercrombie
    semantic_scholar_id: https://www.semanticscholar.org/author/Gavin-Abercrombie/17038002
    username: ~Gavin_Abercrombie1
  - dblp_id: https://dblp.org/pid/68/2913
    emails: '****@fbk.eu'
    first_name: Marco
    google_scholar_id: https://scholar.google.it/citations?user=dt-Fys0AAAAJ
    homepage: https://www.marcoguerini.eu/
    institution: Fondazione Bruno Kessler
    last_name: Guerini
    name: Marco Guerini
    orcid: https://orcid.org/0000-0003-1582-6617
    semantic_scholar_id: https://www.semanticscholar.org/author/Marco-Guerini/1912357
    username: ~Marco_Guerini1
  decision: toFindings
  end_page: 3498
  file: 849.pdf
  id: 849
  num_pages: 20
  openreview_id: vTwFaHt0rv
  pdf_file: eea9ed69a729d43435a3f5102f2863f63e0f9458.pdf
  start_page: 3479
  title: 'NLP for Counterspeech against Hate: A Survey and How-To Guide'
- abstract: Providing dialogue agents with a profile representation can improve their
    consistency and coherence, leading to better conversations. However, current profile-based
    dialogue datasets for training such agents contain either explicit profile representations
    that are simple and dialogue-specific, or implicit representations that are difficult
    to collect. In this work, we introduce the PRODIGy (PROfile-based DIalogue Generation)
    dataset, which brings diverse representations together, providing a more comprehensive
    profile dimension set for each speaker. This resource comprises more than 20k
    dialogues, sourced from movie scripts, aligned with speaker representations such
    as communication style, biography, personality and gender. Initial experiments
    with diverse baselines show that providing generative language models with these
    aspects of a profile, both separately and jointly, enhances models' performance.
    This improvement holds true in both in-domain and cross-domain settings, for both
    fine-tuned and instruction-based LLMs.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Dialogue and Interactive Systems
  authors:
  - dblp_id: https://dblp.org/pid/281/0329
    emails: '****@fbk.eu'
    first_name: Daniela
    google_scholar_id: https://scholar.google.com/citations?user=yPrpiQQAAAAJ&hl=it
    homepage: https://phd.fbk.eu/people/detail/daniela-occhipinti/
    last_name: Occhipinti
    name: Daniela Occhipinti
    username: ~Daniela_Occhipinti1
  - dblp_id: https://dblp.org/pid/153/9505
    emails: '****@gmail.com'
    first_name: Serra
    google_scholar_id: https://scholar.google.it/citations?user=pbhIF3cAAAAJ&hl=en
    last_name: Tekiroglu
    middle_name: Sinem
    name: Serra Sinem Tekiroglu
    semantic_scholar_id: https://www.semanticscholar.org/author/Serra-Sinem-Tekiroglu/2034636
    username: ~Serra_Sinem_Tekiroglu1
  - dblp_id: https://dblp.org/pid/68/2913
    emails: '****@fbk.eu'
    first_name: Marco
    google_scholar_id: https://scholar.google.it/citations?user=dt-Fys0AAAAJ
    homepage: https://www.marcoguerini.eu/
    institution: Fondazione Bruno Kessler
    last_name: Guerini
    name: Marco Guerini
    orcid: https://orcid.org/0000-0003-1582-6617
    semantic_scholar_id: https://www.semanticscholar.org/author/Marco-Guerini/1912357
    username: ~Marco_Guerini1
  decision: toFindings
  end_page: 3513
  file: 850.pdf
  id: 850
  num_pages: 15
  openreview_id: Nwy2TXbz14
  pdf_file: ba827fd7030d0f83735e2b602c8a67d6fc32b05c.pdf
  start_page: 3499
  title: 'PRODIGy: a PROfile-based DIalogue Generation dataset'
- abstract: Watermarking generative-AI systems, such as LLMs, has gained considerable
    interest, driven by their enhanced capabilities across a wide range of tasks.
    Although current approaches have demonstrated that small, context-dependent shifts
    in the word distributions can be used to apply and detect watermarks, there has
    been little work in analyzing the impact that these perturbations have on the
    quality of generated texts. Balancing high detectability with minimal performance
    degradation is crucial in terms of selecting the appropriate watermarking setting;
    therefore this paper proposes a simple analysis framework where comparative assessment,
    a flexible NLG evaluation framework, is used to assess the quality degradation
    caused by a particular watermark setting. We demonstrate that our framework provides
    easy visualization of the quality-detection trade-off of watermark settings, enabling
    a simple solution to find an LLM watermark operating point that provides a well-balanced
    performance. This approach is applied to two different summarization systems and
    a translation system, enabling cross-model analysis for a task, and cross-task
    analysis.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - emails: '****@gmail.com'
    first_name: Piotr
    last_name: Molenda
    name: Piotr Molenda
    username: ~Piotr_Molenda1
  - dblp_id: https://dblp.org/pid/333/0793
    emails: '****@cam.ac.uk'
    first_name: Adian
    google_scholar_id: https://scholar.google.com/citations?user=dYtKMOgAAAAJ&hl=en&oi=ao
    institution: University of Cambridge
    last_name: Liusie
    name: Adian Liusie
    semantic_scholar_id: https://www.semanticscholar.org/author/Adian-Liusie/2190750613
    username: ~Adian_Liusie1
  - dblp_id: https://dblp.org/pid/74/4419.html
    emails: '****@eng.cam.ac.uk'
    first_name: Mark
    google_scholar_id: https://scholar.google.co.uk/citations?hl=en&user=RSFlmjIAAAAJ&view_op=list_works&sortby=pubdate
    homepage: http://mi.eng.cam.ac.uk/~mjfg/index.html
    institution: University of Cambridge
    last_name: Gales
    name: Mark Gales
    semantic_scholar_id: https://www.semanticscholar.org/author/M.-Gales/1740397
    username: ~Mark_Gales1
  decision: toFindings
  end_page: 3524
  file: 853.pdf
  id: 853
  num_pages: 11
  openreview_id: opq40b0iwU
  pdf_file: a507997d385ff3471b897dc97bd028a7b281cddd.pdf
  start_page: 3514
  title: 'WaterJudge: Quality-Detection Trade-off when Watermarking Large Language
    Models'
- abstract: While large language models (LLMs) have demonstrated increasing power,
    they have also called upon studies on their vulnerabilities. As representatives,
    jailbreak attacks can provoke harmful or unethical responses from LLMs, even after
    safety alignment. In this paper, we investigate a novel category of jailbreak
    attacks specifically designed to target the cognitive structure and processes
    of LLMs. Specifically, we analyze the safety vulnerability of LLMs in the face
    of 1) multilingual cognitive overload, 2) veiled expression, and 3) effect-to-
    cause reasoning. Different from previous jailbreak attacks, our proposed cognitive
    overload is a black-box attack with no need for knowledge of model architecture
    or access to model weights. Experiments conducted on AdvBench and MasterKey reveal
    that various LLMs, including both popular open-source model Llama 2 and the proprietary
    model ChatGPT, can be compromised through cognitive overload. Motivated by cognitive
    psychology work on managing cognitive load, we further investigate defending cognitive
    overload attack from two perspectives. Empirical studies show that our cognitive
    overload from three perspectives can jailbreak all studied LLMs successfully,
    while existing defense strategies can hardly mitigate the caused malicious uses
    effectively.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Ethics, Bias, and Fairness
  authors:
  - emails: '****@usc.edu'
    first_name: Nan
    google_scholar_id: https://scholar.google.co.uk/citations?hl=en&user=iysoxM4AAAAJ
    homepage: https://sites.google.com/site/xunannancy
    institution: University of Southern California
    last_name: Xu
    name: Nan Xu
    username: ~Nan_Xu2
  - dblp_id: https://dblp.org/pid/52/3194-60
    emails: '****@gmail.com'
    first_name: Fei
    google_scholar_id: https://scholar.google.com/citations?user=N1O2KT8AAAAJ
    homepage: https://feiwang96.github.io/
    institution: University of Southern California
    last_name: Wang
    name: Fei Wang
    semantic_scholar_id: https://www.semanticscholar.org/author/F.-Wang/47939052
    username: ~Fei_Wang12
  - dblp_id: https://dblp.org/pid/219/5276
    emails: '****@seas.upenn.edu'
    first_name: Ben
    google_scholar_id: https://scholar.google.com/citations?user=0Cb4mtIAAAAJ
    homepage: http://xuanyu.me
    institution: University of Pennsylvania
    last_name: Zhou
    name: Ben Zhou
    semantic_scholar_id: https://www.semanticscholar.org/author/Ben-Zhou/145360756
    username: ~Ben_Zhou1
  - emails: '****@gmail.com'
    first_name: Bangzheng
    google_scholar_id: https://scholar.google.com/citations?user=UcegV-cAAAAJ&hl=en
    institution: University of Southern California
    last_name: Li
    name: Bangzheng Li
    username: ~Bangzheng_Li1
  - dblp_id: https://dblp.org/pid/150/3317
    emails: '****@umich.edu'
    first_name: Chaowei
    google_scholar_id: https://scholar.google.com/citations?user=Juoqtj8AAAAJ&hl=en
    homepage: https://xiaocw11.github.io/
    institution: University of Wisconsin - Madison and NVIDIA
    last_name: Xiao
    name: Chaowei Xiao
    username: ~Chaowei_Xiao2
  - dblp_id: https://dblp.org/pid/173/2608
    emails: '****@ucdavis.edu'
    first_name: Muhao
    google_scholar_id: https://scholar.google.com/citations?user=k79yEZkAAAAJ&hl=en
    homepage: https://muhaochen.github.io/
    institution: University of California, Davis and University of Southern California
    last_name: Chen
    name: Muhao Chen
    orcid: https://orcid.org/0000-0003-0118-3147
    semantic_scholar_id: https://www.semanticscholar.org/author/Muhao-Chen/1998918
    username: ~Muhao_Chen1
  decision: toFindings
  end_page: 3547
  file: 854.pdf
  id: 854
  num_pages: 23
  openreview_id: gv7WTgNPPt
  pdf_file: 181209e969cfce5e177036a50a36f95d9b38b11f.pdf
  start_page: 3525
  title: 'Cognitive Overload: Jailbreaking Large Language Models with Overloaded Logical
    Thinking'
- abstract: 'We introduce PAELLA, a Parameter-Efficient Lightweight Language-Agnostic
    image captioning model designed to be both parameter and data-efficient using
    retrieval augmentation. The model is trained by learning a small mapping network
    with 34M parameters between a pre-trained visual model and a multilingual language
    model that is conditioned on two types of input: (i) the image itself, and (ii)
    a set of retrieved captions in the target language. The retrieved examples play
    a key role in guiding the model to generate captions across languages. Through
    retrieval, the model can be lightweight in terms of the number of trainable parameters,
    which only exist in its mapping network, and also in the amount of multilingual
    training data that is required. Experiments on the XM3600 dataset, featuring 36
    languages, show that PAELLA can outperform or compete against some models with
    3--77$\times$ more learned parameters and 35--863$\times$ more data, particularly
    in low-resource languages.  We also find that PAELLA can be trained on only monolingual
    data and still show strong zero-shot abilities in other languages.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multimodality and Language Grounding to Vision, Robotics and Beyond
  authors:
  - emails: '****@tecnico.ulisboa.pt'
    first_name: Rita
    google_scholar_id: https://scholar.google.com/citations?user=1AslBsAAAAAJ&hl=en
    institution: "Instituto Superior T\xE9cnico"
    last_name: Ramos
    name: Rita Ramos
    username: ~Rita_Ramos1
  - dblp_id: https://dblp.org/pid/241/9497
    emails: '****@google.com'
    first_name: Emanuele
    google_scholar_id: https://scholar.google.com/citations?user=9yc1aXYAAAAJ&hl=en
    homepage: http://e-bug.github.io/
    institution: Google
    last_name: Bugliarello
    name: Emanuele Bugliarello
    orcid: https://orcid.org/0000-0002-2999-7081
    semantic_scholar_id: https://www.semanticscholar.org/author/Emanuele-Bugliarello/83574123
    username: ~Emanuele_Bugliarello1
  - dblp_id: https://dblp.org/pid/m/BrunoMartins
    emails: '****@gmail.com'
    first_name: Bruno
    google_scholar_id: https://scholar.google.com/citations?user=VQMUt8kAAAAJ&hl=en
    homepage: http://web.ist.utl.pt/bruno.g.martins/
    institution: "Instituto Superior T\xE9cnico"
    last_name: Martins
    name: Bruno Martins
    orcid: https://orcid.org/0000-0002-3856-2936
    semantic_scholar_id: https://www.semanticscholar.org/author/Bruno-Martins/144694868
    username: ~Bruno_Martins1
  - dblp_id: https://dblp.org/pid/46/7536
    emails: '****@di.ku.dk'
    first_name: Desmond
    institution: ' and University of Copenhagen'
    last_name: Elliott
    name: Desmond Elliott
    semantic_scholar_id: https://www.semanticscholar.org/author/Desmond-Elliott/50369944
    username: ~Desmond_Elliott1
  decision: toFindings
  end_page: 3563
  file: 858.pdf
  id: 858
  num_pages: 16
  openreview_id: GEliPYCRqa
  pdf_file: 76bac90f12107991abd85c2a36bdc7a12628687e.pdf
  start_page: 3548
  title: 'PAELLA: Parameter-Efficient Lightweight Language-Agnostic Captioning Model'
- abstract: The capability of intelligent models to extrapolate and comprehend changes
    in object states is a crucial yet demanding aspect of AI research, particularly
    through the lens of human interaction in real-world settings. This task involves
    describing complex visual environments, identifying active objects, and interpreting
    their changes as conveyed through language. Traditional methods, which isolate
    object captioning and state change detection, offer a limited view of dynamic
    environments. Moreover, relying on a small set of symbolic words to represent
    changes has restricted the expressiveness of language. To address these challenges,
    in this paper, we introduce the Object State Captioning and State Change Representation
    (OSCaR) dataset and benchmark. OSCaR consists of 14,084 annotated video segments
    with nearly 1,000 unique objects from various egocentric video collections. It
    sets a new testbed for evaluating Multimodal Large Language Models (MLLMs). Our
    experiments demonstrate that while MLLMs show some skill, they lack a full understanding
    of object state changes. The benchmark includes a fine-tuned model that, despite
    initial capabilities, requires significant improvements in accuracy and generalization
    ability for effective understanding of these changes. Our code and dataset are
    available at https://github.com/nguyennm1024/OSCaR.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multimodality and Language Grounding to Vision, Robotics and Beyond
  authors:
  - dblp_id: https://dblp.org/pid/38/6441
    emails: '****@gmail.com'
    first_name: Nguyen
    google_scholar_id: https://scholar.google.com/citations?user=kYok1lsAAAAJ&hl=en
    homepage: https://nguyennm1024.github.io
    last_name: Nguyen
    name: Nguyen Nguyen
    username: ~Nguyen_Nguyen1
  - dblp_id: https://dblp.org/pid/79/3146
    emails: '****@ur.rochester.edu'
    first_name: Jing
    google_scholar_id: https://scholar.google.com/citations?user=ZyCYhUkAAAAJ&hl=en
    homepage: https://jing-bi.github.io/
    last_name: Bi
    name: Jing Bi
    username: ~Jing_Bi1
  - dblp_id: https://dblp.org/pid/207/6403
    emails: '****@ece.rochester.edu'
    first_name: Ali
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=uyqE3LEAAAAJ&view_op=list_works&sortby=pubdate
    homepage: https://alivosoughi.com/
    institution: University of Rochester
    last_name: Vosoughi
    name: Ali Vosoughi
    username: ~Ali_Vosoughi1
  - dblp_id: https://dblp.org/pid/176/4020
    emails: '****@utdallas.edu'
    first_name: Yapeng
    google_scholar_id: https://scholar.google.com/citations?user=lxCqdpoAAAAJ&hl=en
    homepage: http://www.yapengtian.com/
    institution: University of Texas at Dallas
    last_name: Tian
    name: Yapeng Tian
    username: ~Yapeng_Tian1
  - dblp_id: https://dblp.org/pid/35/3763
    emails: '****@gmail.com'
    first_name: Pooyan
    google_scholar_id: https://scholar.google.com/citations?user=A35BH40AAAAJ&hl=en
    homepage: https://www.pooyanfazli.com/
    institution: Arizona State University
    last_name: Fazli
    name: Pooyan Fazli
    username: ~Pooyan_Fazli1
  - dblp_id: https://dblp.org/pid/117/4770
    emails: '****@rochester.edu'
    first_name: Chenliang
    google_scholar_id: https://scholar.google.com.tw/citations?user=54HfyDIAAAAJ
    homepage: https://www.cs.rochester.edu/~cxu22/
    institution: University of Rochester, University of Rochester and University of
      Rochester
    last_name: Xu
    name: Chenliang Xu
    username: ~Chenliang_Xu1
  decision: toFindings
  end_page: 3575
  file: 866.pdf
  id: 866
  num_pages: 12
  openreview_id: lIkkMRO5Tf
  pdf_file: 98a7072cd3b910dd2719eb061c14ba4f0a145b63.pdf
  start_page: 3564
  title: 'OSCaR: Object State Captioning and State Change Representation'
- abstract: Sentence embedding models are typically trained using contrastive learning
    (CL), either using human annotations directly or by repurposing other annotated
    datasets. In this work, we explore the recently introduced paradigm of generating
    CL data using generative language models (LM). In CL for computer vision (CV),
    compositional transformations (series of operations applied over an image. e.g.
    cropping + color distortion) which modify the input/image to retain minimal information
    were shown to be very effective. We show that composition of a `Summary' transformation
    with diverse paraphrasing/contradicting transformations accomplishes the same
    and works very well in CL for sentence embeddings. Our final generated dataset
    (using Vicuna-13B) significantly outperforms the previous best unsupervised method
    (using ChatGPT) by 1.8 points, and SimCSE, a strong supervised baseline by 0.3
    points on the semantic text similarity (STS) benchmark.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Efficient/Low-Resource Methods for NLP
  authors:
  - dblp_id: https://dblp.org/pid/205/0092.html
    emails: '****@duke.edu'
    first_name: Raghuveer
    homepage: https://raghavlite.github.io/
    last_name: Thirukovalluru
    name: Raghuveer Thirukovalluru
    semantic_scholar_id: https://www.semanticscholar.org/author/Raghuveer-Thirukovalluru/6476826
    username: ~Raghuveer_Thirukovalluru1
  - emails: '****@megagon.ai'
    first_name: Xiaolan
    google_scholar_id: https://scholar.google.com/citations?user=tdwzy4EAAAAJ&hl=en
    institution: Megagon Labs
    last_name: Wang
    name: Xiaolan Wang
    username: ~Xiaolan_Wang2
  - emails: '****@gmail.com'
    first_name: Jun
    google_scholar_id: https://scholar.google.com/citations?hl=zh-CN&user=SH8aC7MAAAAJ
    institution: Meta Platform
    last_name: Chen
    name: Jun Chen
    username: ~Jun_Chen16
  - dblp_id: https://dblp.org/pid/29/9471
    emails: '****@gmail.com'
    first_name: Shuyang
    google_scholar_id: https://scholar.google.com/citations?user=mZrs5XMAAAAJ&hl=en
    homepage: https://shuyangli.me/
    institution: Meta AI
    last_name: Li
    name: Shuyang Li
    orcid: https://orcid.org/0000-0003-1503-7351
    semantic_scholar_id: https://www.semanticscholar.org/author/Shuyang-Li/2733699
    username: ~Shuyang_Li2
  - emails: '****@gmail.com'
    first_name: Jie
    google_scholar_id: https://scholar.google.com/citations?user=SZN9FLIAAAAJ&hl=en
    homepage: https://jayleicn.github.io/
    last_name: Lei
    name: Jie Lei
    username: ~Jie_Lei3
  - emails: '****@gmail.com'
    first_name: Rong
    google_scholar_id: https://scholar.google.com/citations?user=CS5uNscAAAAJ&hl=zh-CN
    institution: Twitter
    last_name: Jin
    name: Rong Jin
    username: ~Rong_Jin3
  - dblp_id: https://dblp.org/pid/180/5692
    emails: '****@google.com'
    first_name: Bhuwan
    google_scholar_id: https://scholar.google.com/citations?user=2W2ttrQAAAAJ&hl=en&authuser=1&oi=ao
    homepage: https://users.cs.duke.edu/~bdhingra/
    institution: Duke University
    last_name: Dhingra
    name: Bhuwan Dhingra
    semantic_scholar_id: https://www.semanticscholar.org/author/Bhuwan-Dhingra/34994191
    username: ~Bhuwan_Dhingra1
  decision: toFindings
  end_page: 3587
  file: 867.pdf
  id: 867
  num_pages: 12
  openreview_id: plsmH90CwF
  pdf_file: c6fc1abbd72843a6f42f77bcc9a8229c4361cef3.pdf
  start_page: 3576
  title: 'SumCSE: Summary as a transformation for Contrastive Learning'
- abstract: This study investigates the consequences of training language models on
    synthetic data generated by their predecessors, an increasingly prevalent practice
    given the prominence of powerful generative models. Diverging from the usual emphasis
    on performance metrics, we focus on the impact of this training methodology on
    linguistic diversity, especially when conducted recursively over time. To assess
    this, we adapt and develop a set of novel metrics targeting lexical, syntactic,
    and semantic diversity, applying them in recursive finetuning experiments across
    various natural language generation tasks in English. Our findings reveal a consistent
    decrease in the diversity of the model outputs through successive iterations,
    especially remarkable for tasks demanding high levels of creativity. This trend
    underscores the potential risks of training language models on synthetic text,
    particularly concerning the preservation of linguistic richness. Our study highlights
    the need for careful consideration of the long-term effects of such training approaches
    on the linguistic capabilities of language models.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - emails: '****@polytechnique.edu'
    first_name: Yanzhu
    google_scholar_id: https://scholar.google.com/citations?user=v_fvWzQAAAAJ&hl=en
    last_name: Guo
    name: Yanzhu Guo
    username: ~Yanzhu_Guo1
  - dblp_id: https://dblp.org/pid/220/3989
    emails: '****@hotmail.com'
    first_name: Guokan
    google_scholar_id: https://scholar.google.com/citations?user=EcBibPkAAAAJ
    homepage: https://shang.tech
    institution: Mohamed bin Zayed University of Artificial Intelligence
    last_name: Shang
    name: Guokan Shang
    semantic_scholar_id: https://www.semanticscholar.org/author/Guokan-Shang/46222231
    username: ~Guokan_Shang1
  - dblp_id: https://dblp.org/pid/v/MVazirgiannis
    emails: '****@lix.polytechnique.fr'
    first_name: Michalis
    google_scholar_id: https://scholar.google.gr/citations?user=aWGJYcMAAAAJ&hl=el
    institution: Ecole Polytechnique, France
    last_name: Vazirgiannis
    name: Michalis Vazirgiannis
    username: ~Michalis_Vazirgiannis1
  - dblp_id: https://dblp.org/pid/50/2768
    emails: '****@telecom-paristech.fr'
    first_name: "Chlo\xE9"
    google_scholar_id: https://scholar.google.fr/citations?user=TAZbfksAAAAJ&hl=en
    homepage: https://clavel.wp.imt.fr/
    institution: "INRIA and T\xE9l\xE9com Paris"
    last_name: Clavel
    name: "Chlo\xE9 Clavel"
    username: "~Chlo\xE9_Clavel2"
  decision: toFindings
  end_page: 3602
  file: 868.pdf
  id: 868
  num_pages: 15
  openreview_id: XfzZXfnUHQ
  pdf_file: 882d035b5caf426e1b0022e7bcd7a8cecc59190e.pdf
  start_page: 3588
  title: 'The Curious Decline of Linguistic Diversity: Training Language Models on
    Synthetic Text'
- abstract: Despite the many use cases for large language models (LLMs) in creating
    personalized chatbots, there has been limited research on evaluating the extent
    to which the behaviors of personalized LLMs accurately and consistently reflect
    specific personality traits. We consider studying the behavior of LLM-based agents
    which we refer to as LLM personas and present a case study with GPT-3.5 and GPT-4
    to investigate whether LLMs can generate content that aligns with their assigned
    personality profiles. To this end, we simulate distinct LLM personas based on
    the Big Five personality model, have them complete the 44-item Big Five Inventory
    (BFI) personality test and a story writing task, and then assess their essays
    with automatic and human evaluations. Results show that LLM personas' self-reported
    BFI scores are consistent with their designated personality types, with large
    effect sizes observed across five traits. Additionally, LLM personas' writings
    have emerging representative linguistic patterns for personality traits when compared
    with a human writing corpus. Furthermore, human evaluation shows that humans can
    perceive some personality traits with an accuracy of up to 80\%. Interestingly,
    the accuracy drops significantly when the annotators were informed of AI authorship.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Linguistic theories, Cognitive Modeling and Psycholinguistics
  authors:
  - emails: '****@mit.edu'
    first_name: Hang
    google_scholar_id: https://scholar.google.com/citations?user=9qvTNQ4AAAAJ&hl=en
    homepage: https://www.mit.edu/~hjian42/
    last_name: Jiang
    name: Hang Jiang
    semantic_scholar_id: https://www.semanticscholar.org/author/Hang-Jiang/48579520
    username: ~Hang_Jiang1
  - emails: '****@mit.edu'
    first_name: Xiajie
    homepage: https://www.media.mit.edu/people/xiajie
    institution: Massachusetts Institute of Technology
    last_name: Zhang
    name: Xiajie Zhang
    username: ~Xiajie_Zhang1
  - emails: '****@stanford.edu'
    first_name: Xubo
    last_name: Cao
    name: Xubo Cao
    orcid: https://orcid.org/0000-0002-8792-4382
    username: ~Xubo_Cao1
  - emails: '****@media.mit.edu'
    first_name: Cynthia
    institution: Massachusetts Institute of Technology
    last_name: Breazeal
    name: Cynthia Breazeal
    username: ~Cynthia_Breazeal1
  - dblp_id: https://dblp.org/pid/16/1529
    emails: '****@mit.edu'
    first_name: Deb
    google_scholar_id: https://scholar.google.com/citations?user=btoec6QAAAAJ&hl=en&oi=ao
    homepage: https://www.ccc.mit.edu/person/deb-roy/
    institution: Massachusetts Institute of Technology
    last_name: Roy
    name: Deb Roy
    orcid: https://orcid.org/0000-0002-4333-7194
    semantic_scholar_id: https://www.semanticscholar.org/author/D.-Roy/145364504
    username: ~Deb_Roy1
  - dblp_id: https://dblp.org/pid/148/9943
    emails: '****@mit.edu'
    first_name: Jad
    homepage: http://www.mit.edu/~jkabbara/
    institution: Massachusetts Institute of Technology
    last_name: Kabbara
    name: Jad Kabbara
    semantic_scholar_id: https://www.semanticscholar.org/author/Jad-Kabbara/2631301
    username: ~Jad_Kabbara1
  decision: toFindings
  end_page: 3625
  file: 870.pdf
  id: 870
  num_pages: 23
  openreview_id: ndzwTYdbDr
  pdf_file: 9734a296f28152621579e2a7a62ec4d52f581147.pdf
  start_page: 3603
  title: 'PersonaLLM: Investigating the Ability of Large Language Models to Express
    Personality Traits'
- abstract: As a crucial task in the task-oriented dialogue systems, spoken language
    understanding (SLU) has garnered increasing attention. However, errors from automatic
    speech recognition (ASR) often hinder the performance of understanding. To tackle
    this problem, we propose MoE-SLU, an ASR-Robust SLU framework based on the mixture-of-experts
    technique. Specifically, we first introduce three strategies to generate additional
    transcripts from clean transcripts. Then, we apply the mixture-of-experts technique
    to weight the representations of the generated transcripts, ASR transcripts, and
    the corresponding clean manual transcripts. Additionally, we regularize the weighted
    average of predictions and the predictions of ASR transcripts by minimizing the
    Jensen-Shannon Divergence (JSD) between the two output distributions. Experiment
    results on three benchmark SLU datasets demonstrate that our MoE-SLU achieves
    state-of-the-art performance. Further analysis also verifies the superiority of
    our method.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Speech recognition, text-to-speech and spoken language understanding
  authors:
  - dblp_id: https://dblp.org/pid/239/5637
    emails: '****@stu.pku.edu.cn'
    first_name: Xuxin
    homepage: https://www.linkedin.com/in/chengxx/
    last_name: Cheng
    name: Xuxin Cheng
    username: ~Xuxin_Cheng3
  - dblp_id: https://dblp.org/pid/304/1173
    emails: '****@stu.pku.edu.cn'
    first_name: Zhihong
    homepage: https://github.com/Zhihong-Zhu
    last_name: Zhu
    name: Zhihong Zhu
    semantic_scholar_id: https://www.semanticscholar.org/author/Zhihong-Zhu/2190278563
    username: ~Zhihong_Zhu1
  - dblp_id: https://dblp.org/pid/339/2318
    emails: '****@stu.pku.edu.cn'
    first_name: Xianwei
    google_scholar_id: https://scholar.google.com/citations?user=A1TGx8kAAAAJ&hl=zh-CN
    last_name: Zhuang
    name: Xianwei Zhuang
    username: ~Xianwei_Zhuang2
  - emails: '****@stu.pku.edu.cn'
    first_name: Zhanpeng
    homepage: https://www.zhihu.com/people/sakura-71-72-38
    last_name: Chen
    name: Zhanpeng Chen
    username: ~Zhanpeng_Chen1
  - dblp_id: https://dblp.org/pid/71/5479
    emails: '****@pku.edu.cn'
    first_name: Zhiqi
    google_scholar_id: https://scholar.google.com/citations?user=5JGMGCsAAAAJ
    homepage: https://zhiqi-huang.github.io/
    institution: Tencent Game
    last_name: Huang
    name: Zhiqi Huang
    semantic_scholar_id: https://www.semanticscholar.org/author/Zhiqi-Huang/1490968632
    username: ~Zhiqi_Huang2
  - emails: '****@gmail.com'
    first_name: Yuexian
    google_scholar_id: https://scholar.google.com/citations?user=sfyr7zMAAAAJ&hl=zh-CN
    institution: Peking University
    last_name: Zou
    name: Yuexian Zou
    username: ~Yuexian_Zou4
  decision: toFindings
  end_page: 3636
  file: 875.pdf
  id: 875
  num_pages: 11
  openreview_id: BAU2tLlKGw
  pdf_file: 6121db1fc8db68e8757b20a831293e3b6ab3e199.pdf
  start_page: 3626
  title: 'MoE-SLU: Towards ASR-Robust Spoken Language Understanding via Mixture-of-Experts'
- abstract: This paper introduces FIRE (**FI**nancial **R**elation **E**xtraction),
    a sentence-level dataset of named entities and relations within the financial
    sector. Comprising 3,025 instances, the dataset encapsulates 13 named entity types
    along with 18 relation types. Sourced from public financial reports and financial
    news articles, FIRE captures a wide array of financial information about a business
    including, but not limited to, corporate structure, business model, revenue streams,
    and market activities such as acquisitions. The full dataset was labeled by a
    single annotator to minimize labeling noise. The labeling time for each sentence
    was recorded during the labeling process. We show how this feature, along with
    curriculum learning techniques, can be used to improved a model's performance.
    The FIRE dataset is designed to serve as a valuable resource for training and
    evaluating machine learning algorithms in the domain of financial information
    extraction. The dataset and the code to reproduce our experimental results are
    available at https://github.com/hmhamad/FIRE. The repository for the labeling
    tool can be found at https://github.com/abhinav-kumar-thakur/relation-extraction-annotator.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Extraction
  authors:
  - emails: '****@usc.edu'
    first_name: Hassan
    google_scholar_id: https://scholar.google.com/citations?user=HHjfW3UAAAAJ&hl=en&authuser=1
    homepage: https://www.hassanhamad.com/
    last_name: Hamad
    name: Hassan Hamad
    username: ~Hassan_Hamad1
  - emails: '****@usc.edu'
    first_name: Abhinav Kumar
    homepage: https://www.abhinavkumarthakur.com/
    last_name: Thakur
    name: Abhinav Kumar Thakur
    username: ~Abhinav_Kumar_Thakur1
  - emails: '****@v-labs.ai'
    first_name: Nijil
    last_name: Kolleri
    name: Nijil Kolleri
    username: ~Nijil_Kolleri1
  - emails: '****@v-labs.ai'
    first_name: Sujith
    google_scholar_id: https://scholar.google.com/citations?user=wtP6ZVoAAAAJ&hl=en
    last_name: Pulikodan
    name: Sujith Pulikodan
    username: ~Sujith_Pulikodan1
  - emails: '****@usc.edu'
    first_name: Keith
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=0jawlUUAAAAJ
    homepage: https://hal.usc.edu/chugg/
    institution: University of Southern California
    last_name: Chugg
    middle_name: M.
    name: Keith M. Chugg
    username: ~Keith_M._Chugg1
  decision: toFindings
  end_page: 3651
  file: 876.pdf
  id: 876
  num_pages: 15
  openreview_id: 2kkwrI1NUV
  pdf_file: bbda07238e97b28ae1947316587c7dcf0c2eebd3.pdf
  start_page: 3637
  title: 'FIRE: A Dataset for Financial Relation Extraction'
- abstract: Large Language Models (LLMs) have shown immense potential in multimodal
    applications, yet the convergence of textual and musical domains remains not well-explored.
    To address this gap, we present MusiLingo, a novel system for music caption generation
    and music-related query responses. MusiLingo employs a single projection layer
    to align music representations from the pre-trained frozen music audio model MERT~\cite{li2023mert}
    with a frozen LLM, bridging the gap between music audio and textual contexts.
    We train it on an extensive music caption dataset and fine-tune it with instructional
    data. Due to the scarcity of high-quality music Q\&A datasets, we created the
    MusicInstruct (MI) dataset from captions in the MusicCaps datasets, tailored for
    open-ended music inquiries. Empirical evaluations demonstrate its competitive
    performance in generating music captions and composing music-related Q\&A pairs.
    Our introduced dataset enables notable advancements beyond previous ones.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multimodality and Language Grounding to Vision, Robotics and Beyond
  authors:
  - emails: '****@gmail.com'
    first_name: Zihao
    last_name: Deng
    name: Zihao Deng
    username: ~Zihao_Deng2
  - emails: '****@qmul.ac.uk'
    first_name: Yinghao
    google_scholar_id: https://scholar.google.com/citations?user=RiYt9toAAAAJ
    homepage: https://nicolaus625.github.io/
    institution: Queen Mary University of London
    last_name: Ma
    name: Yinghao Ma
    orcid: https://orcid.org/0009-0009-9500-4015
    username: ~Yinghao_Ma1
  - emails: '****@andrew.cmu.edu'
    first_name: Yudong
    homepage: https://yudongl2000.github.io/
    last_name: Liu
    name: Yudong Liu
    username: ~Yudong_Liu3
  - emails: '****@utexas.edu'
    first_name: Rongchen
    homepage: https://rongchenguo.github.io/
    last_name: Guo
    name: Rongchen Guo
    orcid: https://orcid.org/0000-0002-1870-6541
    username: ~Rongchen_Guo1
  - emails: '****@umich.edu'
    first_name: Ge
    google_scholar_id: https://scholar.google.com/citations?user=qyTrq4kAAAAJ&hl=zh-CN
    last_name: Zhang
    name: Ge Zhang
    username: ~Ge_Zhang5
  - dblp_id: https://dblp.uni-trier.de/pers/c/Chen:Wenhu.html
    emails: '****@gmail.com'
    first_name: Wenhu
    google_scholar_id: https://scholar.google.co.jp/citations?user=U8ShbhUAAAAJ&hl=en
    homepage: https://wenhuchen.github.io/
    institution: University of Waterloo and Google
    last_name: Chen
    name: Wenhu Chen
    semantic_scholar_id: https://www.semanticscholar.org/author/Wenhu-Chen/2928777
    username: ~Wenhu_Chen3
  - dblp_id: https://dblp.org/pid/51/11
    emails: '****@gmail.com'
    first_name: Wenhao
    last_name: Huang
    name: Wenhao Huang
    username: ~Wenhao_Huang1
  - dblp_id: https://dblp.org/pid/23/5543.html
    emails: '****@qmul.ac.uk'
    first_name: Emmanouil
    google_scholar_id: https://scholar.google.com.tw/citations?user=Wg49oI4AAAAJ
    homepage: http://www.eecs.qmul.ac.uk/~emmanouilb/
    institution: Queen Mary, University of London
    last_name: Benetos
    name: Emmanouil Benetos
    orcid: https://orcid.org/0000-0002-6820-6764
    username: ~Emmanouil_Benetos1
  decision: toFindings
  end_page: 3664
  file: 880.pdf
  id: 880
  num_pages: 13
  openreview_id: JEnxUBQplG
  pdf_file: bc54a6906e4a5c6514cce9c3d411593d167d7729.pdf
  start_page: 3652
  title: 'MusiLingo: Bridging Music and Text with Pre-trained Language Models for
    Music Captioning and Query Response'
- abstract: Large Language Models (LLMs) have achieved remarkable performance across
    a wide variety of tasks; however, their large size makes their inference slow
    and computationally expensive. Focusing on this problem, we study instruction
    tuning LLMs with additional explicit Losses from the Intermediate layers (LITE)
    and show that it enables these layers to acquire 'good' generation ability without
    affecting the generation ability of the final layer. We then perform 'dynamic
    confidence-based early exiting' at token level from the intermediate layers which
    improves the computational efficiency of text generation without sacrificing the
    quality of the generation. We conduct comprehensive experiments by instruction
    tuning LLaMA-2 models on the Alpaca dataset and evaluate on four different instruction
    test sets. We show that dynamic early exiting achieves consistent and considerable
    inference cost improvements (37.86% for 7B and 46.35% for 13B model) while maintaining
    the generation quality. We further conduct a thorough analysis of the results
    and dissect the efficiency improvements which reveals several important findings.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Efficient/Low-Resource Methods for NLP
  authors:
  - dblp_id: https://dblp.org/pid/139/3970
    emails: '****@asu.edu'
    first_name: Neeraj
    google_scholar_id: https://scholar.google.com/citations?user=Ju9nR0IAAAAJ&hl=en
    homepage: https://nrjvarshney.github.io/
    last_name: Varshney
    name: Neeraj Varshney
    semantic_scholar_id: https://www.semanticscholar.org/author/2067056655
    username: ~Neeraj_Varshney1
  - emails: '****@asu.edu'
    first_name: Agneet
    homepage: https://agneetchatterjee.com/
    institution: Arizona State University
    last_name: Chatterjee
    name: Agneet Chatterjee
    username: ~Agneet_Chatterjee1
  - dblp_id: https://dblp.org/pid/253/6105
    emails: '****@asu.edu'
    first_name: Mihir
    google_scholar_id: https://scholar.google.com/citations?user=2UPwJC4AAAAJ&hl=en
    last_name: Parmar
    name: Mihir Parmar
    semantic_scholar_id: https://www.semanticscholar.org/author/Mihir-Parmar/1423660254
    username: ~Mihir_Parmar1
  - dblp_id: https://dblp.uni-trier.de/pers/hd/b/Baral:Chitta
    emails: '****@asu.edu'
    first_name: Chitta
    google_scholar_id: https://scholar.google.com/citations?user=9Yd716IAAAAJ&hl=en&oi=ao
    homepage: http://www.public.asu.edu/~cbaral
    institution: Arizona State University, Arizona State University and Arizona State
      University
    last_name: Baral
    name: Chitta Baral
    orcid: https://orcid.org/0000-0002-7549-723X
    semantic_scholar_id: https://www.semanticscholar.org/author/Chitta-Baral/1760291
    username: ~Chitta_Baral1
  decision: toFindings
  end_page: 3686
  file: 882.pdf
  id: 882
  num_pages: 22
  openreview_id: 7CMTPgCIIU
  pdf_file: 37d7f4bdfcc21ad0f3e36fe78a447d757ee2ec8b.pdf
  start_page: 3665
  title: Investigating Acceleration of LLaMA Inference by Enabling Intermediate Layer
    Decoding via Instruction Tuning with 'LITE'
- abstract: While instruction-tuned models have shown remarkable success in various
    natural language processing tasks, accurately evaluating their ability to follow
    instructions remains challenging. Existing benchmarks primarily focus on common
    instructions that align well with what the model learned during training. However,
    proficiency in responding to these instructions does not necessarily imply strong
    ability in instruction following. In this paper, we propose a novel instruction-following
    evaluation protocol called verbalizer manipulation. It instructs the model to
    verbalize the task label with words aligning with model priors to different extents,
    adopting verbalizers from highly aligned (e.g., outputting "positive" for positive
    sentiment), to minimally aligned (e.g., outputting "negative" for positive sentiment).
    Verbalizer manipulation can be seamlessly integrated with any classification benchmark
    to examine the model's reliance on priors and its ability to override them to
    accurately follow the instructions. We conduct a comprehensive evaluation of four
    major model families across nine datasets, employing twelve sets of verbalizers
    for each of them. We observe that the instruction-following abilities of models,
    across different families and scales, are significantly distinguished by their
    performance on less natural verbalizers. Even the strongest GPT-4 model struggles
    to perform better than random guessing on the most challenging verbalizer, emphasizing
    the need for continued advancements to improve their instruction-following abilities.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - dblp_id: https://dblp.org/pid/138/8064
    emails: '****@ucsb.edu'
    first_name: Shiyang
    google_scholar_id: https://scholar.google.com/citations?user=4zli0KkAAAAJ&hl=en
    institution: Amazon
    last_name: Li
    name: Shiyang Li
    username: ~Shiyang_Li1
  - dblp_id: https://dblp.org/pid/89/5901-12
    emails: '****@usc.edu'
    first_name: Jun
    google_scholar_id: https://scholar.google.com/citations?user=rhNj2RcAAAAJ
    last_name: Yan
    name: Jun Yan
    semantic_scholar_id: https://www.semanticscholar.org/author/Jun-Yan/49781448
    username: ~Jun_Yan5
  - emails: '****@ttic.edu'
    first_name: Hai
    google_scholar_id: https://scholar.google.com/citations?user=sOF6iA4AAAAJ&hl=en
    homepage: http://ttic.uchicago.edu/~haiwang/
    institution: Samsung
    last_name: Wang
    name: Hai Wang
    username: ~Hai_Wang1
  - emails: '****@gmail.com'
    first_name: Zheng
    homepage: http://ztang.info
    institution: Samsung
    last_name: Tang
    name: Zheng Tang
    username: ~Zheng_Tang3
  - dblp_id: https://dblp.org/pid/36/360-1
    emails: '****@usc.edu'
    first_name: Xiang
    google_scholar_id: https://scholar.google.com/citations?user=_moJlrIAAAAJ&hl=en
    homepage: https://shanzhenren.github.io/
    institution: University of Southern California, University of Southern California
      and University of Southern California
    last_name: Ren
    name: Xiang Ren
    username: ~Xiang_Ren1
  - dblp_id: https://dblp.org/pid/77/6327
    emails: '****@gmail.com'
    first_name: Vijay
    google_scholar_id: https://scholar.google.com/citations?user=jqaSvfEAAAAJ&hl=en&authuser=1
    last_name: Srinivasan
    name: Vijay Srinivasan
    orcid: https://orcid.org/0009-0006-6572-6787
    username: ~Vijay_Srinivasan1
  - dblp_id: https://dblp.org/pid/55/2789
    emails: '****@samsung.com'
    first_name: Hongxia
    institution: Samsung Research America AI center
    last_name: Jin
    name: Hongxia Jin
    username: ~Hongxia_Jin1
  decision: toFindings
  end_page: 3701
  file: 890.pdf
  id: 890
  num_pages: 15
  openreview_id: sCeQOBtPmp
  pdf_file: 78b1de4c93f48f9cba71e1751e4d5b2acd251875.pdf
  start_page: 3687
  title: Instruction-following Evaluation through Verbalizer Manipulation
- abstract: This paper investigates using Large Language Models (LLMs) to automatically
    perform web software tasks using click, scroll, and text in- put operations. Previous
    approaches, such as reinforcement learning (RL) or imitation learning, are inefficient
    to train and task-specific. Our method uses filtered Document Object Model (DOM)
    elements as observations and performs tasks step-by-step, sequentially generating
    small programs based on the current observations. We use in-context learning,
    either benefiting from a single manually provided example, or an automatically
    generated example based on a successful zero-shot trial. We evaluate our proposed
    method on the MiniWob++ benchmark. With only one in-context example, our WebWISE
    method using gpt-3.5-turbo achieves similar or better performance than other methods
    that require many demonstrations or trials.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - emails: '****@illinois.edu'
    first_name: Heyi
    last_name: Tao
    name: Heyi Tao
    username: ~Heyi_Tao1
  - dblp_id: https://dblp.org/pid/280/0581.html
    emails: '****@gmail.com'
    first_name: Sethuraman
    google_scholar_id: https://scholar.google.com/citations?user=Ufg3TDgAAAAJ&hl=en
    homepage: https://sethuramantv001.wixsite.com/sethu
    institution: Department of Computer Science
    last_name: T V
    name: Sethuraman T V
    username: ~Sethuraman_T_V1
  - dblp_id: https://dblp.org/pid/269/4751
    emails: '****@illinois.edu'
    first_name: Michal
    google_scholar_id: https://scholar.google.com/citations?user=x9szIWsAAAAJ&hl=en
    homepage: https://michalmsr.web.illinois.edu/
    institution: University of Illinois, Urbana Champaign
    last_name: Shlapentokh-Rothman
    name: Michal Shlapentokh-Rothman
    username: ~Michal_Shlapentokh-Rothman1
  - dblp_id: https://dblp.org/pid/62/1086
    emails: '****@allenai.org'
    first_name: Tanmay
    google_scholar_id: https://scholar.google.co.in/citations?user=zblQKM8AAAAJ&hl=en
    homepage: http://tanmaygupta.info/
    institution: Allen Institute for Artificial Intelligence
    last_name: Gupta
    name: Tanmay Gupta
    semantic_scholar_id: https://www.semanticscholar.org/author/Tanmay-Gupta/1911972
    username: ~Tanmay_Gupta1
  - emails: '****@illinois.edu'
    first_name: Heng
    google_scholar_id: https://scholar.google.com/citations?user=z7GCqT4AAAAJ&hl=en
    homepage: http://blender.cs.illinois.edu/hengji.html
    institution: University of Illinois, Urbana-Champaign
    last_name: Ji
    name: Heng Ji
    username: ~Heng_Ji3
  - dblp_id: https://dblp.org/pid/08/6948
    emails: '****@illinois.edu'
    first_name: Derek
    google_scholar_id: https://scholar.google.com/citations?user=8Sfj7q8AAAAJ
    homepage: http://dhoiem.cs.illinois.edu/
    institution: Department of Computer Science, Reconstruct and University of Illinois,
      Urbana Champaign
    last_name: Hoiem
    name: Derek Hoiem
    semantic_scholar_id: https://www.semanticscholar.org/author/Derek-Hoiem/2433269
    username: ~Derek_Hoiem1
  decision: toFindings
  end_page: 3720
  file: 891.pdf
  id: 891
  num_pages: 19
  openreview_id: H7ifNypFhJ
  pdf_file: 615b2b06d43771e0ff8ccaa2e479f61f06d82f5b.pdf
  start_page: 3702
  title: 'WebWISE: Unlocking Web Interface Control for LLMs via Sequential Exploration'
- abstract: "Instruction tuning has emerged as the key in aligning large language\
    \ models (LLMs) with specific task instructions, thereby mitigating the discrepancy\
    \ between the next-token prediction objective and users' actual goals. To reduce\
    \ the labor and time cost to collect or annotate data by humans, researchers start\
    \ to explore the use of LLMs to generate instruction-aligned synthetic data. \n\
    Recent works focus on generating diverse instructions and applying LLM to increase\
    \ instruction complexity, often neglecting downstream use cases. It remains unclear\
    \ how to tailor high-quality data to elicit better instruction-following abilities\
    \ in different target instruction distributions and LLMs. To this end, we introduce\
    \ CodecLM, a general framework for adaptively generating high-quality synthetic\
    \ data for LLM alignment with different downstream instruction distributions and\
    \ LLMs. Drawing on the Encode-Decode principles, we use LLMs as codecs to guide\
    \ the data generation process. \nWe first encode seed instructions into metadata,\
    \ which are concise keywords generated on-the-fly to capture the target instruction\
    \ distribution, and then decode metadata to create tailored instructions. We also\
    \ introduce Self-Rubrics and Contrastive Filtering during decoding to tailor data-efficient\
    \ samples. Extensive experiments on four open-domain instruction following benchmarks\
    \ validate the effectiveness of CodecLM over the current state-of-the-arts."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Generation
  authors:
  - dblp_id: https://dblp.org/pid/43/7716-2
    emails: '****@google.com'
    first_name: Zifeng
    google_scholar_id: https://scholar.google.co.il/citations?user=N1uBekcAAAAJ&hl=en
    homepage: https://kingspencer.github.io/
    institution: Google
    last_name: Wang
    name: Zifeng Wang
    username: ~Zifeng_Wang1
  - emails: '****@cs.cmu.edu'
    first_name: Chun-Liang
    homepage: https://chunliangli.github.io/
    institution: Google
    last_name: Li
    name: Chun-Liang Li
    username: ~Chun-Liang_Li1
  - dblp_id: https://dblp.org/pid/227/2509
    emails: '****@google.com'
    first_name: Vincent
    institution: Google
    last_name: Perot
    name: Vincent Perot
    username: ~Vincent_Perot1
  - emails: '****@google.com'
    first_name: Long
    google_scholar_id: https://scholar.google.com/citations?user=C__97UYAAAAJ&hl=en
    institution: Google
    last_name: Le
    name: Long Le
    username: ~Long_Le2
  - emails: '****@utexas.edu'
    first_name: Jin
    google_scholar_id: https://scholar.google.com/citations?user=ObQniKIAAAAJ&hl=en
    institution: Google
    last_name: Miao
    name: Jin Miao
    username: ~Jin_Miao1
  - dblp_id: http://dblp.uni-trier.de/pers/hd/z/Zhang:Zizhao
    emails: '****@google.com'
    first_name: Zizhao
    google_scholar_id: https://scholar.google.dk/citations?hl=en&pli=1&user=lGrbH60AAAAJ
    homepage: https://sites.google.com/corp/view/zizhaozhang
    institution: Google
    last_name: Zhang
    name: Zizhao Zhang
    username: ~Zizhao_Zhang3
  - dblp_id: https://dblp.org/pid/04/656
    emails: '****@gmail.com'
    first_name: Chen-Yu
    google_scholar_id: https://scholar.google.com/citations?user=uWPUSEgAAAAJ&hl=en
    homepage: https://chl260.github.io/
    institution: Google
    last_name: Lee
    name: Chen-Yu Lee
    username: ~Chen-Yu_Lee1
  - dblp_id: https://dblp.org/pid/14/8360
    emails: '****@google.com'
    first_name: Tomas
    google_scholar_id: https://scholar.google.com/citations?user=ahSpJOAAAAAJ&hl=en
    homepage: http://tomas.pfister.fi
    institution: Google
    last_name: Pfister
    name: Tomas Pfister
    orcid: https://orcid.org/0009-0004-4088-8718
    semantic_scholar_id: https://www.semanticscholar.org/author/Tomas-Pfister/1945962
    username: ~Tomas_Pfister1
  decision: toFindings
  end_page: 3738
  file: 893.pdf
  id: 893
  num_pages: 18
  openreview_id: fkg34PEwEQ
  pdf_file: aa07b2979f66bacd786dbee0cc21d30570ad2f84.pdf
  start_page: 3721
  title: 'CodecLM: Aligning Language Models with Tailored Synthetic Data'
- abstract: Given several documents, multi-hop question generation (MQG) is a task
    aims to generate complicated questions that require reasoning over multiple pieces
    of these documents to find the answer. To perform this task, existing studies
    focus on designing advanced architectures to locate essential keywords or sentences
    in multiple documents and then generate questions accordingly, where they normally
    do not note that question types could provide crucial hints for extracting key
    information from the documents for MQG. In general, supervised approaches are
    used that rely on large annotated data, which is not available in many low-resource
    scenarios and thus makes MQG hard in these domains. Consider the recent success
    of large language models (LLMs) on natural language processing tasks using limited
    labeled data under few-shot settings, in this paper, we propose an approach named
    type-aware semantics extraction-based chain-of-thought method (TASE-CoT) for few-shot
    MQG. Specifically, our approach firstly extracts question types and essential
    semantic phrases from the given documents and the answer. Then, we design a three-step
    CoT template to leverage the extracted question type and semantic phrases to predict
    multi-hop questions. Extensive experiments and the results demonstrate the effectiveness
    of our approach and the proposed modules.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Generation
  authors:
  - emails: '****@mail.ustc.edu.cn'
    first_name: Zefeng
    homepage: https://github.com/John-Lin98
    institution: University of Science and Technology of China
    last_name: Lin
    name: Zefeng Lin
    username: ~Zefeng_Lin1
  - dblp_id: https://dblp.org/rec/conf/mm/ChenHQHWQHL22
    emails: '****@ustc.edu.cn'
    first_name: Weidong
    google_scholar_id: https://scholar.google.com/citations?user=Z-vKGdoAAAAJ&hl=en
    last_name: Chen
    name: Weidong Chen
    orcid: https://orcid.org/0000-0003-2774-2875
    username: ~Weidong_Chen1
  - dblp_id: https://dblp.org/pid/09/1398
    emails: '****@gmail.com'
    first_name: Yan
    homepage: https://clksong.github.io
    institution: University of Science and Technology of China
    last_name: Song
    name: Yan Song
    username: ~Yan_Song1
  - dblp_id: https://dblp.org/pid/z/YongdongZhang
    emails: '****@ustc.edu.cn'
    first_name: Yongdong
    institution: University of Science and Technology of China
    last_name: Zhang
    name: Yongdong Zhang
    username: ~Yongdong_Zhang2
  decision: toFindings
  end_page: 3749
  file: 900.pdf
  id: 900
  num_pages: 11
  openreview_id: qsoTxGpD3e
  pdf_file: 9079a98ae11b23bb43ec83a05e53ddc356214b26.pdf
  start_page: 3739
  title: Prompting Few-shot Multi-hop Question Generation via Comprehending Type-aware
    Semantics
- abstract: 'Recent studies suggest that self-reflective prompting can significantly
    enhance the reasoning capabilities of Large Language Models (LLMs). However, the
    use of external feedback as a stop criterion raises doubts about the true extent
    of LLMs'' ability to emulate human-like self-reflection. In this paper, we set
    out to clarify these capabilities under a more stringent evaluation setting in
    which we disallow any kind of external feedback. Our findings under this setting
    show a split: while self-reflection enhances performance in TruthfulQA, it adversely
    affects results in HotpotQA.

    We conduct follow-up analyses to clarify the contributing factors in these patterns,
    and find that the influence of self-reflection is impacted both by reliability
    of accuracy in models'' initial responses, and by overall question difficulty:
    specifically, self-reflection shows the most benefit when models are less likely
    to be correct initially, and when overall question difficulty is higher. We also
    find that self-reflection reduces tendency toward majority voting. Based on our
    findings, we propose guidelines for decisions on when to implement self-reflection.
    We release the codebase for reproducing our experiments at https://github.com/yanhong-lbh/LLM-SelfReflection-Eval.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - dblp_id: https://dblp.org/pid/29/240
    emails: '****@uchicago.edu'
    first_name: Yanhong
    last_name: Li
    name: Yanhong Li
    username: ~Yanhong_Li1
  - dblp_id: https://dblp.org/pid/229/4179
    emails: '****@gmail.com'
    first_name: Chenghao
    google_scholar_id: https://scholar.google.com/citations?user=B28fiOAAAAAJ&hl=zh-CN
    homepage: https://yangalan123.github.io/
    institution: University of Chicago
    last_name: Yang
    name: Chenghao Yang
    username: ~Chenghao_Yang1
  - dblp_id: https://dblp.org/pid/165/0758
    emails: '****@allenai.org'
    first_name: Allyson
    homepage: https://aetting.github.io
    institution: Allen Institute for Artificial Intelligence
    last_name: Ettinger
    name: Allyson Ettinger
    semantic_scholar_id: https://www.semanticscholar.org/author/Allyson-Ettinger/37907837
    username: ~Allyson_Ettinger1
  decision: toFindings
  end_page: 3762
  file: 904.pdf
  id: 904
  num_pages: 13
  openreview_id: 2R1DFB1JLU
  pdf_file: 1cdea8fbd1991884fe875ab71969747f8685b971.pdf
  start_page: 3750
  title: 'When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large
    Language Models'
- abstract: We present CoDa (**Co**nstrained Generation based **Da**ta Augmentation),
    a controllable, effective, and *training-free* data augmentation technique for
    low-resource (data-scarce) NLP. Our approach is based on prompting off-the-shelf
    instruction-following Large Language Models (LLMs) for generating text that satisfies
    a set of constraints. Precisely, we extract a set of simple constraints from every
    instance in the low-resource dataset and verbalize them to prompt an LLM to generate
    novel and diverse training instances. Our findings reveal that synthetic data
    that follows simple constraints in the downstream dataset act as highly effective
    augmentations, and CoDa can achieve this without intricate decoding-time constrained
    generation techniques or fine-tuning with complex algorithms that eventually make
    the model biased toward the small number of training instances. Additionally,
    CoDa is the first framework that provides users explicit control over the augmentation
    generation process, thereby also allowing easy adaptation to several domains.
    We demonstrate the effectiveness of CoDa across 11 datasets spanning 3 tasks and
    3 low-resource settings. CoDa outperforms all our baselines, qualitatively and
    quantitatively, with improvements of 0.12%-7.19%. Code is available.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Efficient/Low-Resource Methods for NLP
  authors:
  - emails: '****@umd.edu'
    first_name: Chandra Kiran
    last_name: Evuru
    middle_name: Reddy
    name: Chandra Kiran Reddy Evuru
    username: ~Chandra_Kiran_Reddy_Evuru1
  - dblp_id: https://dblp.org/pid/173/5626
    emails: '****@umd.edu'
    first_name: Sreyan
    google_scholar_id: https://scholar.google.com/citations?user=5HKZJHAAAAAJ&hl=en
    homepage: https://sreyan88.github.io/
    last_name: Ghosh
    name: Sreyan Ghosh
    username: ~Sreyan_Ghosh1
  - emails: '****@gmail.com'
    first_name: Sonal
    google_scholar_id: https://scholar.google.com/citations?user=jiJ2DcEAAAAJ&hl=en&oi=sra
    last_name: Kumar
    name: Sonal Kumar
    username: ~Sonal_Kumar1
  - emails: '****@gmail.com'
    first_name: Ramaneswaran
    google_scholar_id: https://scholar.google.com/citations?user=YIhHxbwAAAAJ&hl=en
    last_name: S
    name: Ramaneswaran S
    username: ~Ramaneswaran_S1
  - emails: '****@umd.edu'
    first_name: Utkarsh
    google_scholar_id: https://scholar.google.co.in/citations?user=RLjKaTwAAAAJ&hl=en
    homepage: https://utkarsh4430.github.io
    last_name: Tyagi
    name: Utkarsh Tyagi
    username: ~Utkarsh_Tyagi1
  - dblp_id: https://dblp.org/pers/hd/m/Manocha:Dinesh
    emails: '****@umd.edu'
    first_name: Dinesh
    google_scholar_id: https://scholar.google.com/citations?user=X08l_4IAAAAJ
    homepage: https://www.cs.umd.edu/people/dmanocha
    institution: University of Maryland, College Park
    last_name: Manocha
    name: Dinesh Manocha
    orcid: https://orcid.org/0000-0001-7047-9801
    username: ~Dinesh_Manocha3
  decision: toFindings
  end_page: 3778
  file: 905.pdf
  id: 905
  num_pages: 16
  openreview_id: O5jNMEmc41
  pdf_file: fc3d784e4e9c2a26bf1e3238cd859ce9a643af4f.pdf
  start_page: 3763
  title: 'CoDa: Constrained Generation based Data Augmentation for Low-Resource NLP'
- abstract: 'We analyze whether object detectors trained on vision-language data learn
    effective visual representations for synonyms. Since many current vision-language
    models accept user-provided textual input, we highlight the need for such models
    to learn feature representations that are robust to changes in how such input
    is provided. Specifically, we analyze changes in synonyms used to refer to objects.
    Here, we study object detectors trained on vision-language data and investigate
    how to make their performance less dependent on whether synonyms are used to refer
    to an object. We propose two approaches to achieve this goal: data augmentation
    by back-translation and class embedding enrichment. We show the promise of such
    approaches, reporting improved performance on synonyms from mAP@0.5=33.87% to
    37.93%.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multimodality and Language Grounding to Vision, Robotics and Beyond
  authors:
  - emails: '****@pitt.edu'
    first_name: Giacomo
    last_name: Nebbia
    name: Giacomo Nebbia
    orcid: https://orcid.org/0000-0002-4766-6278
    username: ~Giacomo_Nebbia1
  - dblp_id: http://dblp.uni-trier.de/pers/hd/k/Kovashka:Adriana
    emails: '****@cs.pitt.edu'
    first_name: Adriana
    google_scholar_id: https://scholar.google.com/citations?user=Dl949GoAAAAJ
    homepage: http://people.cs.pitt.edu/~kovashka/
    institution: University of Pittsburgh
    last_name: Kovashka
    name: Adriana Kovashka
    username: ~Adriana_Kovashka1
  decision: toFindings
  end_page: 3785
  file: 908.pdf
  id: 908
  num_pages: 7
  openreview_id: dogFpoIyAN
  pdf_file: 30cf34a45e6aef4534d4d01cc69479563182142f.pdf
  start_page: 3779
  title: Synonym relations affect object detection learned on vision-language data
- abstract: Neural Text-to-Speech (TTS) systems find broad applications in voice assistants,
    e-learning, and audiobook creation. The pursuit of modern models, like Diffusion
    Models (DMs), holds promise for achieving high-fidelity, real-time speech synthesis.
    Yet, the efficiency of multi-step sampling in Diffusion Models presents challenges.
    Efforts have been made to integrate GANs with DMs, speeding up inference by approximating
    denoising distributions, but this introduces issues with model convergence due
    to adversarial training. To overcome this, we introduce CM-TTS, a novel architecture
    grounded in consistency models (CMs). Drawing inspiration from continuous-time
    diffusion models, CM-TTS achieves top-quality speech synthesis in fewer steps
    without adversarial training or pre-trained model dependencies. We further design
    weighted samplers to incorporate different sampling positions into model training
    with dynamic probabilities, ensuring unbiased learning throughout the entire training
    process. We present a real-time mel-spectrogram generation consistency model,
    validated through comprehensive evaluations. Experimental results underscore CM-TTS's
    superiority over existing single-step speech synthesis systems, representing a
    significant advancement in the field.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Speech recognition, text-to-speech and spoken language understanding
  authors:
  - emails: '****@bupt.edu.cn'
    first_name: Xiang
    institution: Beijing University of Posts and Telecommunications
    last_name: Li
    name: Xiang Li
    orcid: https://orcid.org/0009-0009-2453-3390
    username: ~Xiang_Li76
  - emails: '****@bupt.edu.cn'
    first_name: FanBu
    institution: Beijing University of Posts and Telecommunications
    last_name: FanBu
    name: FanBu
    orcid: https://orcid.org/0000-0002-5564-9574
    username: ~FanBu1
  - emails: '****@iiitd.ac.in'
    first_name: Ambuj
    google_scholar_id: https://scholar.google.com/citations?user=4q8VxIIAAAAJ&hl=en
    last_name: Mehrish
    name: Ambuj Mehrish
    username: ~Ambuj_Mehrish1
  - emails: '****@bupt.edu.cn'
    first_name: Yingting
    last_name: Li
    name: Yingting Li
    username: ~Yingting_Li1
  - dblp_id: https://dblp.uni-trier.de/pid/266/2826.html
    emails: '****@bupt.edu.cn'
    first_name: Jiale
    google_scholar_id: https://scholar.google.com/citations?user=itFycmoAAAAJ&hl=en
    homepage: https://hanjiale.github.io/
    last_name: Han
    name: Jiale Han
    orcid: https://orcid.org/0000-0001-6477-0424
    semantic_scholar_id: https://www.semanticscholar.org/author/Jiale-Han/12212068
    username: ~Jiale_Han1
  - emails: '****@bupt.edu.cn'
    first_name: Bo
    homepage: https://scs.bupt.edu.cn/info/1292/2714.htm
    institution: Beijing University of Posts and Telecommunications
    last_name: Cheng
    name: Bo Cheng
    username: ~Bo_Cheng3
  - dblp_id: https://dblp.org/pid/116/4904
    emails: '****@sutd.edu.sg'
    first_name: Soujanya
    google_scholar_id: https://scholar.google.co.in/citations?user=oS6gRc4AAAAJ&hl=en
    homepage: https://sporia.info
    institution: Singapore University of Technology and Design
    last_name: Poria
    name: Soujanya Poria
    username: ~Soujanya_Poria1
  decision: toFindings
  end_page: 3803
  file: 909.pdf
  id: 909
  num_pages: 18
  openreview_id: NzzDS4EYue
  pdf_file: 19ce2a1bdac636d79f27e0d7d23760bd16cab3dd.pdf
  start_page: 3786
  title: 'CM-TTS: Enhancing Real Time Text-to-Speech Synthesis Efficiency through
    Weighted Samplers and Consistency Models'
- abstract: Pre-trained language models (PLMs) have consistently demonstrated outstanding
    performance across a diverse spectrum of natural language processing tasks. Nevertheless,
    despite their success with unseen data, current PLM-based representations often
    exhibit poor robustness in adversarial settings. In this paper, we introduce RobustSentEmbed,
    a self-supervised sentence embedding framework designed to improve both generalization
    and robustness in diverse text representation tasks and against a diverse set
    of adversarial attacks. Through the generation of high-risk adversarial perturbations
    and their utilization in a novel objective function, RobustSentEmbed adeptly learns
    high-quality and robust sentence embeddings. Our experiments confirm the superiority
    of RobustSentEmbed over state-of-the-art representations. Specifically, Our framework
    achieves a significant reduction in the success rate of various adversarial attacks,
    notably reducing the BERTAttack success rate by almost half (from 75.51% to 38.81%).
    The framework also yields improvements of 1.59% and 0.23% in semantic textual
    similarity tasks and various transfer tasks, respectively.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Learning for NLP
  authors:
  - dblp_id: https://dblp.org/pid/204/0907.html
    emails: '****@student.gsu.edu'
    first_name: Javad
    google_scholar_id: https://scholar.google.com/citations?user=rXfZo-8AAAAJ&hl=en
    homepage: https://www.linkedin.com/in/javad-rafiei-asl-034b22a2/
    last_name: Rafiei Asl
    name: Javad Rafiei Asl
    orcid: https://orcid.org/0000-0002-6154-1068
    username: ~Javad_Rafiei_Asl1
  - emails: '****@student.gsu.edu'
    first_name: Prajwal
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=dGnbNA0AAAAJ&view_op=list_works&sortby=pubdate
    homepage: https://www.prajwalpanzade.com/
    institution: Georgia State University
    last_name: Panzade
    name: Prajwal Panzade
    username: ~Prajwal_Panzade1
  - dblp_id: https://dblp.org/pid/32/369-2
    emails: '****@arizona.edu'
    first_name: Eduardo
    google_scholar_id: https://scholar.google.com/citations?user=AqGa3-MAAAAJ&hl=en
    homepage: https://eduardoblanco.github.io/
    institution: University of Arizona
    last_name: Blanco
    name: Eduardo Blanco
    semantic_scholar_id: https://www.semanticscholar.org/author/Eduardo-Blanco/145186180
    username: ~Eduardo_Blanco1
  - dblp_id: https://dblp.org/pid/254/2719
    emails: '****@odu.edu'
    first_name: Daniel
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=8OTNELUAAAAJ
    homepage: https://www.odu.edu/cyber
    institution: Old Dominion University
    last_name: Takabi
    name: Daniel Takabi
    username: ~Daniel_Takabi1
  - dblp_id: https://dblp.uni-trier.de/pid/14/5155-1.html
    emails: '****@gsu.edu'
    first_name: Zhipeng
    google_scholar_id: https://scholar.google.com/citations?user=tq-LVzIAAAAJ&hl=en
    homepage: http://cai.csgsu.org/
    institution: Georgia State University
    last_name: Cai
    name: Zhipeng Cai
    username: ~Zhipeng_Cai1
  decision: toFindings
  end_page: 3818
  file: 916.pdf
  id: 916
  num_pages: 15
  openreview_id: 9dEAg4lJEA
  pdf_file: 849e2e30980435ea49c13e65b510c2c4083b7112.pdf
  start_page: 3804
  title: 'RobustSentEmbed: Robust Sentence Embeddings Using Adversarial Self-Supervised
    Contrastive Learning'
- abstract: Recent advancements in large language models' (LLMs) capabilities have
    yielded few-shot, human-comparable performance on a range of tasks. At the same
    time, researchers expend significant effort and resources gathering human annotations.  At
    some point, LLMs may be able to perform some simple annotation tasks, but studies
    of LLM annotation accuracy and behavior are sparse. In this paper, we characterize
    OpenAI's GPT-3.5's judgment on a behavioral task for implicit object categorization.
    We characterize the embedding spaces of models trained on human vs. GPT responses
    and give similarities and differences between them, finding many similar dimensions.
    We also find that despite these similar dimensions, augmenting humans' responses
    with GPT ones drives model divergence across the sizes of datasets tested.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Linguistic theories, Cognitive Modeling and Psycholinguistics
  authors:
  - emails: '****@ualberta.ca'
    first_name: D
    google_scholar_id: https://scholar.google.ca/citations?user=LV-LPiYAAAAJ
    homepage: https://www.demcknight.com
    last_name: McKnight
    middle_name: Estes
    name: D Estes McKnight
    username: ~D_Estes_McKnight1
  - dblp_id: https://dblp.org/pid/30/3660
    emails: '****@ualberta.ca'
    first_name: Alona
    google_scholar_id: https://scholar.google.ca/citations?user=Vw8z7qwAAAAJ&hl=en
    homepage: http://webdocs.cs.ualberta.ca/~alona/
    institution: University of Alberta
    last_name: Fyshe
    name: Alona Fyshe
    orcid: https://orcid.org/0000-0003-4367-0306
    semantic_scholar_id: https://www.semanticscholar.org/author/Alona-Fyshe/2655967
    username: ~Alona_Fyshe1
  decision: toFindings
  end_page: 3837
  file: 919.pdf
  id: 919
  num_pages: 19
  openreview_id: UwlzjtEWtJ
  pdf_file: 7795570d07dbaa5ca8fc827214ae3a292ced841e.pdf
  start_page: 3819
  title: Characterizing Human and Zero-Shot GPT-3.5 Object-Similarity Judgments
- abstract: Large language models (LLMs) have shown promising abilities of in-context
    learning (ICL), adapting swiftly to new tasks with only few-shot demonstrations.
    However, current few-shot methods heavily depend on high-quality, query-specific
    demos, which are often lacking. When faced with out-of-demonstration (OOD) queries,
    methods that rely on hand-crafted demos or external retrievers might fail. To
    bridge the gap between limited demos and OOD queries, we propose Self-Demos, a
    novel prompting method that elicits the inherent generalizability in LLMs by query-aware
    demo generation. The generated demos strategically interpolate between existing
    demos and the given query, transforming the query from OOD to ID. To evaluate
    the effectiveness of our approach, we manually constructed OOD-Toolset, a dataset
    in the tool-using scenario with over 300 real-world APIs and 1000 instances, each
    consisting of three tool-use cases as demos and an OOD query. Thorough experiments
    on our dataset and two public math benchmarks have shown that our method can outperform
    state-of-the-art baselines in the OOD setting. Moreover, we conduct a range of
    analyses to validate Self-Demos's generalization and provide more insights.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Efficient/Low-Resource Methods for NLP
  authors:
  - emails: '****@m.fudan.edu.cn'
    first_name: Wei
    homepage: https://hwcoder.top
    institution: Fudan University
    last_name: He
    name: Wei He
    orcid: https://orcid.org/0009-0002-9240-5330
    semantic_scholar_id: https://www.semanticscholar.org/author/Wei-He/2222752705
    username: ~Wei_He14
  - dblp_id: https://dblp.org/pid/342/9419
    emails: '****@gmail.com'
    first_name: Shichun
    google_scholar_id: https://scholar.google.com/citations?user=XiLg-t0AAAAJ
    homepage: https://github.com/Shichun-Liu
    last_name: Liu
    name: Shichun Liu
    semantic_scholar_id: https://www.semanticscholar.org/author/Shichun-Liu/2211905760
    username: ~Shichun_Liu2
  - emails: '****@fudan.edu.cn'
    first_name: Jun
    google_scholar_id: https://scholar.google.com/citations?hl=zh-CN&view_op=list_works&gmla=AJsN-F7dji7uR79NklThiaCHzZop5hZ9YSgjlArWvB7IOMW0aHW_P3zhnLOnyAA0ZHwYoz8wgCz8UwNr70D9QTQAugAVkcg32VAFodA8vtCH3qW8fggUR96gVMsPpxs751vyjcCV2QEJsUA2gaUQ-mdsgLPrEuraHg&user=I0UfoywAAAAJ
    last_name: Zhao
    name: Jun Zhao
    semantic_scholar_id: https://www.semanticscholar.org/author/Jun-Zhao/40245629
    username: ~Jun_Zhao5
  - emails: '****@m.fudan.edu.cn'
    first_name: Yiwen
    last_name: Ding
    name: Yiwen Ding
    orcid: https://orcid.org/0009-0003-0670-9221
    username: ~Yiwen_Ding3
  - emails: '****@gmail.com'
    first_name: Yi
    google_scholar_id: https://scholar.google.com/citations?user=WK62eYQAAAAJ&hl=zh-CN
    last_name: Lu
    name: Yi Lu
    semantic_scholar_id: https://www.semanticscholar.org/author/Yi-Lu/2143773562
    username: ~Yi_Lu7
  - dblp_id: https://dblp.uni-trier.de/pid/333/4268
    emails: '****@m.fudan.edu.cn'
    first_name: Zhiheng
    google_scholar_id: https://scholar.google.com.hk/citations?user=zSVLkqAAAAAJ&hl=zh-CN
    homepage: https://woooodyy.github.io/
    last_name: Xi
    name: Zhiheng Xi
    username: ~Zhiheng_Xi1
  - dblp_id: https://dblp.org/pid/135/6973
    emails: '****@fudan.edu.cn'
    first_name: Tao
    institution: Fudan University
    last_name: Gui
    name: Tao Gui
    username: ~Tao_Gui1
  - dblp_id: https://dblp.org/pid/52/323-1
    emails: '****@fudan.edu.cn'
    first_name: Qi
    google_scholar_id: https://scholar.google.com/citations?user=XfqR3yYAAAAJ&hl=en
    homepage: http://qizhang.info
    institution: Fudan University
    last_name: Zhang
    name: Qi Zhang
    username: ~Qi_Zhang8
  - dblp_id: https://dblp.org/pid/05/6735-1.html
    emails: '****@fudan.edu.cn'
    first_name: Xuanjing
    google_scholar_id: https://scholar.google.com/citations?user=RGsMgZA4H78C
    homepage: https://xuanjing-huang.github.io/
    institution: Fudan University
    last_name: Huang
    name: Xuanjing Huang
    semantic_scholar_id: https://www.semanticscholar.org/author/Xuanjing-Huang/1790227
    username: ~Xuanjing_Huang1
  decision: toFindings
  end_page: 3854
  file: 929.pdf
  id: 929
  num_pages: 17
  openreview_id: PBxniM3PUj
  pdf_file: 0d65c236996917e24c13d555aecd04482c567262.pdf
  start_page: 3838
  title: 'Self-Demos: Eliciting Out-of-Demonstration Generalizability in Large Language
    Models'
- abstract: Event temporal reasoning aims at identifying the temporal relations between
    two or more events from narratives. However, knowledge conflicts arise when there
    is a mismatch between the actual temporal relations of events in the context and
    the prior knowledge or biases learned by the model. In this paper, we propose
    to detect knowledge-conflict examples in event temporal reasoning using bias indicators,
    which include event relation prior bias, tense bias, narrative bias, and dependency
    bias. We define conflict examples as those where event relations are opposite
    to biased or prior relations. To mitigate event-related knowledge conflicts, we
    introduce a Counterfactual Data Augmentation (CDA) based method that can be applied
    to both Pre-trained Language Models (PLMs) and Large Language Models (LLMs) either
    as additional training data or demonstrations for In- Context Learning. Experiments
    suggest both PLMs and LLMs suffer from knowledge conflicts in event temporal reasoning,
    and CDA has the potential for reducing hallucination and improving model performance.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Extraction
  authors:
  - dblp_id: https://dblp.org/pid/283/4921
    emails: '****@connect.ust.hk'
    first_name: Tianqing
    google_scholar_id: https://scholar.google.com.hk/citations?user=Tb3rc34AAAAJ&hl=en
    homepage: http://fangtq.com/
    last_name: Fang
    name: Tianqing Fang
    semantic_scholar_id: https://www.semanticscholar.org/author/2044202073
    username: ~Tianqing_Fang1
  - dblp_id: https://dblp.org/pid/120/1278-3
    emails: '****@cse.ust.hk'
    first_name: Zhaowei
    google_scholar_id: https://scholar.google.com/citations?user=5dzojAsAAAAJ&hl=en
    homepage: https://zhaowei-wang-nlp.github.io/
    institution: Department of Computer Science and Engineering, Hong Kong University
      of Science and Technology
    last_name: Wang
    name: Zhaowei Wang
    orcid: https://orcid.org/0000-0001-5539-8181
    semantic_scholar_id: https://www.semanticscholar.org/author/Zhaowei-Wang/2187830802
    username: ~Zhaowei_Wang2
  - dblp_id: https://dblp.org/pid/78/9975.html
    emails: '****@gmail.com'
    first_name: Wenxuan
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=Td0j6cUAAAAJ&view_op=list_works&authuser=2
    homepage: https://wzhouad.github.io/
    institution: Zoom
    last_name: Zhou
    name: Wenxuan Zhou
    semantic_scholar_id: https://www.semanticscholar.org/author/Wenxuan-Zhou/2203076
    username: ~Wenxuan_Zhou2
  - dblp_id: https://dblp.org/pid/48/859.html
    emails: '****@cse.ust.hk'
    first_name: Hongming
    google_scholar_id: https://scholar.google.com/citations?user=i5ETuuQAAAAJ&hl=en
    homepage: http://www.cse.ust.hk/~hzhangal/
    last_name: Zhang
    name: Hongming Zhang
    username: ~Hongming_Zhang2
  - dblp_id: https://dblp.org/pid/86/2159
    emails: '****@cse.ust.hk'
    first_name: Yangqiu
    google_scholar_id: https://scholar.google.com.tw/citations?user=MdQZ-q8AAAAJ
    homepage: https://www.cse.ust.hk/~yqsong/
    institution: The Hong Kong University of Science and Technology
    last_name: Song
    name: Yangqiu Song
    username: ~Yangqiu_Song1
  - dblp_id: https://dblp.org/pid/173/2608
    emails: '****@ucdavis.edu'
    first_name: Muhao
    google_scholar_id: https://scholar.google.com/citations?user=k79yEZkAAAAJ&hl=en
    homepage: https://muhaochen.github.io/
    institution: University of California, Davis and University of Southern California
    last_name: Chen
    name: Muhao Chen
    orcid: https://orcid.org/0000-0003-0118-3147
    semantic_scholar_id: https://www.semanticscholar.org/author/Muhao-Chen/1998918
    username: ~Muhao_Chen1
  decision: toFindings
  end_page: 3877
  file: 933.pdf
  id: 933
  num_pages: 23
  openreview_id: YZ0aKGBNc3
  pdf_file: 5c73e1a5111be75fa98a237550294d2591213e9f.pdf
  start_page: 3855
  title: Getting Sick After Seeing a Doctor? Diagnosing and Mitigating Knowledge Conflicts
    in Event Temporal Reasoning
- abstract: Event coreference resolution (ECR) is a critical task in information extraction
    of natural language processing, aiming to identify and link event mentions across
    multiple documents. Despite recent progress, existing datasets for ECR primarily
    focus on within-document event coreference and English text, lacking cross-document
    ECR datasets for multiple languages beyond English. To address this issue, this
    work presents the first multiligual dataset for cross-document ECR, called MCECR
    (Multilingual Cross-Document Event Coreference Resolution), that manually annotates
    a diverse collection of documents for event mentions and coreference in five languages,
    i.e., English, Spanish, Hindi, Turkish, and Ukrainian. Using sampled articles
    from Wikinews over various topics as the seeds, our dataset fetches related news
    articles from the Google search engine to increase the number of non-singleton
    event clusters. In total, we annotate 5,802 news articles, providing a substantial
    and varied dataset for multilingual ECR in both within-document and cross-document
    scenarios. Extensive analysis of the proposed dataset reveals the challenging
    nature of multilingual event coreference resolution tasks, promoting MCECR as
    a strong benchmark dataset for future research in this area.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Extraction
  authors:
  - emails: '****@cs.uoregon.edu'
    first_name: Amir
    google_scholar_id: https://scholar.google.com/citations?user=CrK5xTwAAAAJ&hl=en
    last_name: Pouran Ben Veyseh
    name: Amir Pouran Ben Veyseh
    username: ~Amir_Pouran_Ben_Veyseh2
  - dblp_id: https://dblp.org/pid/251/8546
    emails: '****@kensho.com'
    first_name: Viet
    google_scholar_id: https://scholar.google.com/citations?user=TtxmNccAAAAJ&hl=en
    homepage: http://laiviet.github.io
    institution: Kensho Technologies
    last_name: Lai
    middle_name: Dac
    name: Viet Dac Lai
    username: ~Viet_Dac_Lai1
  - emails: '****@uoregon.edu'
    first_name: Chien
    google_scholar_id: https://scholar.google.com/citations?user=fW5HEnEAAAAJ&hl=vi
    homepage: https://chiennv2000.github.io/
    institution: University of Oregon
    last_name: Nguyen
    middle_name: Van
    name: Chien Van Nguyen
    username: ~Chien_Van_Nguyen1
  - dblp_id: https://dblp.org/pid/132/4043
    emails: '****@gmail.com'
    first_name: Franck
    google_scholar_id: https://scholar.google.com/citations?user=kz2aIc8AAAAJ&hl=en
    homepage: http://francky.me
    institution: Adobe Systems
    last_name: Dernoncourt
    name: Franck Dernoncourt
    orcid: https://orcid.org/0000-0002-1119-1346
    semantic_scholar_id: https://www.semanticscholar.org/author/Franck-Dernoncourt/2462276
    username: ~Franck_Dernoncourt1
  - dblp_id: https://dblp.org/pid/17/9407
    emails: '****@cs.uoregon.edu'
    first_name: Thien
    google_scholar_id: https://scholar.google.com/citations?user=Da2FhegAAAAJ&hl=en&oi=ao
    homepage: http://ix.cs.uoregon.edu/~thien
    institution: ', University of Oregon'
    last_name: Nguyen
    middle_name: Huu
    name: Thien Huu Nguyen
    username: ~Thien_Huu_Nguyen1
  decision: toFindings
  end_page: 3889
  file: 936.pdf
  id: 936
  num_pages: 12
  openreview_id: kpzGsZC6Is
  pdf_file: 4cb9ea35f2966a89c2759ebc25150fb8a5160f79.pdf
  start_page: 3878
  title: 'MCECR: A Novel Dataset for Multilingual Cross-Document Event Coreference
    Resolution'
- abstract: Sentiment analysis (SA) has been a long-standing research area in natural
    language processing. With the recent advent of large language models (LLMs), there
    is great potential for their employment on SA problems. However, the extent to
    which current LLMs can be leveraged for different sentiment analysis tasks remains
    unclear. This paper aims to provide a comprehensive investigation into the capabilities
    of LLMs in performing various sentiment analysis tasks, from conventional sentiment
    classification to aspect-based sentiment analysis and multifaceted analysis of
    subjective texts. We evaluate performance across 13 tasks on 26 datasets and compare
    the results against small language models (SLMs) trained on domain-specific datasets.
    Our study reveals that while LLMs demonstrate satisfactory performance in simpler
    tasks, they lag behind in more complex tasks requiring a deeper understanding
    of specific sentiment phenomena or structured sentiment information. However,
    LLMs significantly outperform SLMs in few-shot learning settings, suggesting their
    potential when annotation resources are limited. We also highlight the limitations
    of current evaluation practices in assessing LLMs' SA abilities and propose a
    novel benchmark, \textsc{SentiEval}, for a more comprehensive and realistic evaluation.
    Data and code are available at \url{https://github.com/DAMO-NLP-SG/LLM-Sentiment}.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Sentiment Analysis, Stylistic Analysis, and Argument Mining
  authors:
  - dblp_id: https://dblp.org/pid/85/1177
    emails: '****@gmail.com'
    first_name: Wenxuan
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=tr7MWA4AAAAJ
    homepage: https://isakzhang.github.io/
    last_name: Zhang
    name: Wenxuan Zhang
    semantic_scholar_id: https://www.semanticscholar.org/author/Wenxuan-Zhang/150341144
    username: ~Wenxuan_Zhang1
  - dblp_id: https://dblp.org/pid/35/8109-10
    emails: '****@e.ntu.edu.sg'
    first_name: Yue
    homepage: https://ntudy.github.io/
    institution: School of Computer Science and  Engineering, Nanyang Technological
      University
    last_name: Deng
    name: Yue Deng
    orcid: https://orcid.org/0009-0006-3682-8047
    semantic_scholar_id: https://www.semanticscholar.org/author/Yue-Deng/2162024594
    username: ~Yue_Deng3
  - dblp_id: https://dblp.org/pid/l/BingLiu1.html
    emails: '****@uic.edu'
    first_name: Bing
    google_scholar_id: https://scholar.google.com/citations?user=Kt1bjZoAAAAJ&hl=en
    homepage: https://www.cs.uic.edu/~liub/
    institution: University of Illinois at Chicago
    last_name: Liu
    name: Bing Liu
    username: ~Bing_Liu1
  - dblp_id: https://dblp.org/pid/80/5412
    emails: '****@cuhk.edu.hk'
    first_name: Sinno
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=P6WcnfkAAAAJ
    homepage: http://www.cse.cuhk.edu.hk/~sinnopan/
    institution: Nanyang Technological University and The Chinese University of Hong
      Kong
    last_name: Pan
    middle_name: Jialin
    name: Sinno Jialin Pan
    username: ~Sinno_Jialin_Pan1
  - dblp_id: https://dblp.org/pid/53/6625
    emails: '****@gmail.com'
    first_name: Lidong
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=_oYzrzAAAAAJ
    homepage: https://lidongbing.github.io/
    institution: Alibaba Group
    last_name: Bing
    name: Lidong Bing
    semantic_scholar_id: https://www.semanticscholar.org/author/Lidong-Bing/1996394
    username: ~Lidong_Bing2
  decision: toFindings
  end_page: 3915
  file: 948.pdf
  id: 948
  num_pages: 26
  openreview_id: XEIhIOfviQ
  pdf_file: eed8bb51e7cc2d4ab64b0c9726265af4a82961d7.pdf
  start_page: 3890
  title: 'Sentiment Analysis in the Era of Large Language Models: A Reality Check'
- abstract: "The recent success of large language models (LLMs) has been predominantly\
    \ driven by curating the training dataset composition, scaling of model architectures\
    \ and dataset sizes and advancements in pretraining objectives, leaving tokenizer\
    \ influence as a blind spot.\nShedding light on this underexplored area, we conduct\
    \ a comprehensive study on the influence of tokenizer choice on LLM downstream\
    \ performance by training 24 mono- and multilingual LLMs at a 2.6B parameter scale,\
    \ ablating different tokenizer algorithms and parameterizations. Our studies highlight\
    \ that the tokenizer choice can significantly impact the model's downstream performance\
    \ and training costs. \nIn particular, we find that the common tokenizer evaluation\
    \ metrics fertility and parity are not always predictive of model downstream performance,\
    \ rendering these metrics a questionable proxy for the model's downstream performance.\
    \ Furthermore, we show that multilingual tokenizers trained on the five most frequent\
    \ European languages require vocabulary size increases of factor three in comparison\
    \ to English. \nWhile English-centric tokenizers have been applied to the training\
    \ of multi-lingual LLMs in the past, we find that this approach results in a severe\
    \ downstream performance degradation and additional training costs of up to 68%,\
    \ due to an inefficient tokenization vocabulary."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multilinguality and Language Diversity
  authors:
  - emails: '****@iais.fraunhofer.de'
    first_name: Mehdi
    google_scholar_id: https://scholar.google.com/citations?user=quD3c8cAAAAJ&hl=de&oi=ao
    institution: Fraunhofer Institute IAIS, Fraunhofer IAIS
    last_name: Ali
    name: Mehdi Ali
    username: ~Mehdi_Ali1
  - dblp_id: https://dblp.org/pid/14/3833-1.html
    emails: '****@iais.fraunhofer.de'
    first_name: Michael
    google_scholar_id: https://scholar.google.de/citations?user=NL5yVhYAAAAJ&hl=de&oi=sra
    homepage: https://fromm-m.github.io/fromm/
    institution: Fraunhofer Institute IAIS, Fraunhofer IAIS
    last_name: Fromm
    name: Michael Fromm
    orcid: https://orcid.org/0000-0002-7244-4191
    semantic_scholar_id: https://www.semanticscholar.org/author/Michael-Fromm/1485668969
    username: ~Michael_Fromm2
  - dblp_id: https://dblp.org/pid/169/3503
    emails: '****@tu-dresden.de'
    first_name: Klaudia
    google_scholar_id: https://scholar.google.de/citations?user=ljgVUsAAAAAJ&hl=de
    institution: TU Dresden
    last_name: Thellmann
    name: Klaudia Thellmann
    username: ~Klaudia_Thellmann1
  - emails: '****@iais.fraunhofer.de'
    first_name: Richard
    homepage: https://github.com/rrutmann
    institution: Fraunhofer Institute IAIS, Fraunhofer IAIS
    last_name: Rutmann
    name: Richard Rutmann
    username: ~Richard_Rutmann1
  - emails: '****@iais.fraunhofer.de'
    first_name: Max
    google_scholar_id: https://scholar.google.de/citations?user=8WrA_AIAAAAJ&hl=en
    institution: Fraunhofer IAIS
    last_name: "L\xFCbbering"
    name: "Max L\xFCbbering"
    username: "~Max_L\xFCbbering1"
  - dblp_id: https://dblp.org/pid/01/4157
    emails: '****@iais.fraunhofer.de'
    first_name: Johannes
    google_scholar_id: https://scholar.google.com/citations?user=Ncxiu-kAAAAJ&hl=en
    institution: Fraunhofer Institute IAIS, Fraunhofer IAIS
    last_name: Leveling
    name: Johannes Leveling
    orcid: https://orcid.org/0000-0003-0603-4191
    semantic_scholar_id: https://www.semanticscholar.org/author/Johannes-Leveling/1725514
    username: ~Johannes_Leveling1
  - emails: '****@iais.fraunhofer.de'
    first_name: Katrin
    institution: Fraunhofer Institute IAIS, Fraunhofer IAIS
    last_name: Klug
    name: Katrin Klug
    username: ~Katrin_Klug1
  - emails: '****@posteo.de'
    first_name: Jan
    google_scholar_id: https://scholar.google.com/citations?user=qB0K3vQAAAAJ
    homepage: https://jan-ebert.com
    institution: "Forschungszentrum J\xFClich GmbH"
    last_name: Ebert
    name: Jan Ebert
    orcid: https://orcid.org/0000-0001-7118-0481
    username: ~Jan_Ebert1
  - emails: '****@iais.fraunhofer.de'
    first_name: Niclas
    institution: Fraunhofer Institute IAIS, Fraunhofer IAIS
    last_name: Doll
    name: Niclas Doll
    username: ~Niclas_Doll1
  - emails: '****@iais.fraunhofer.de'
    first_name: Jasper
    institution: Fraunhofer Institute IAIS, Fraunhofer IAIS
    last_name: Buschhoff
    middle_name: Schulze
    name: Jasper Schulze Buschhoff
    username: ~Jasper_Schulze_Buschhoff1
  - emails: '****@gmail.com'
    first_name: Charvi
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=npdDKn4AAAAJ
    last_name: Jain
    name: Charvi Jain
    username: ~Charvi_Jain1
  - emails: '****@iais.fraunhofer.de'
    first_name: Alexander
    google_scholar_id: https://scholar.google.com/citations?user=NRKxDhkAAAAJ
    institution: Fraunhofer Institute IAIS, Fraunhofer IAIS
    last_name: Weber
    middle_name: Arno
    name: Alexander Arno Weber
    username: ~Alexander_Arno_Weber1
  - emails: '****@tu-dresden.de'
    first_name: Lena
    homepage: https://tu-dresden.de/zih/die-einrichtung/struktur/lena-jurkschat
    institution: "Technische Universit\xE4t Dresden"
    last_name: Jurkschat
    name: Lena Jurkschat
    username: ~Lena_Jurkschat1
  - emails: '****@iais.fraunhofer.de'
    first_name: Hammam
    google_scholar_id: https://scholar.google.com/citations?view_op=new_articles&hl=en&imq=Hammam+Abdelwahab#
    homepage: https://hammamwahab.github.io/
    institution: Fraunhofer Institute IAIS, Fraunhofer IAIS
    last_name: Abdelwahab
    name: Hammam Abdelwahab
    username: ~Hammam_Abdelwahab1
  - emails: '****@fz-juelich.de'
    first_name: Chelsea
    google_scholar_id: https://scholar.google.com/citations?user=WC_pJ6YAAAAJ&hl=en
    homepage: https://www.fz-juelich.de/profile/john_c
    institution: Forschungszentrum Juelich GmbH
    last_name: John
    middle_name: Maria
    name: Chelsea Maria John
    orcid: https://orcid.org/0000-0003-3777-7393
    username: ~Chelsea_Maria_John1
  - dblp_id: https://dblp.uni-trier.de/pid/254/0860
    emails: '****@commoncrawl.org'
    first_name: Pedro
    google_scholar_id: https://scholar.google.fr/citations?user=5sNdyvkAAAAJ&hl=en
    homepage: https://portizs.eu
    institution: Common Crawl Foundation
    last_name: Ortiz Suarez
    name: Pedro Ortiz Suarez
    orcid: https://orcid.org/0000-0003-0343-8852
    semantic_scholar_id: "https://www.semanticscholar.org/author/Pedro-Javier-Ortiz-Su\xE1\
      rez/147846651"
    username: ~Pedro_Ortiz_Suarez1
  - dblp_id: https://dblp.org/pid/249/2629
    emails: '****@i.mieo.de'
    first_name: Malte
    google_scholar_id: https://scholar.google.de/citations?user=8WfhSIcAAAAJ
    homepage: https://ostendorff.org
    institution: German Research Center for AI
    last_name: Ostendorff
    name: Malte Ostendorff
    semantic_scholar_id: https://www.semanticscholar.org/author/Malte-Ostendorff/1389569281
    username: ~Malte_Ostendorff1
  - dblp_id: https://dblp.org/pid/278/8408
    emails: '****@gmail.com'
    first_name: Samuel
    homepage: https://aleph-alpha.com
    institution: Aleph Alpha GmbH
    last_name: Weinbach
    name: Samuel Weinbach
    orcid: https://orcid.org/0000-0001-9481-5363
    username: ~Samuel_Weinbach1
  - emails: '****@iais.fraunhofer.de'
    first_name: Rafet
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=4jBgqpcAAAAJ&view_op=list_works&sortby=pubdate
    homepage: https://www.b-it-center.de/research-groups/applied-machine-learning-group
    institution: "Rheinische Friedrich-Wilhelms Universit\xE4t Bonn"
    last_name: Sifa
    name: Rafet Sifa
    username: ~Rafet_Sifa1
  - emails: '****@fz-juelich.de'
    first_name: Stefan
    google_scholar_id: https://scholar.google.com/citations?user=5k2rdewAAAAJ&hl=de
    homepage: http://go.fzj.de/stefankesselheim
    institution: "Forschungszentrum J\xFClich"
    last_name: Kesselheim
    name: Stefan Kesselheim
    orcid: https://orcid.org/0000-0003-0940-5752
    username: ~Stefan_Kesselheim1
  - emails: '****@iais.fraunhofer.de'
    first_name: Nicolas
    google_scholar_id: https://scholar.google.com/citations?hl=de&user=twcjkYUAAAAJ
    homepage: https://www.iais.fraunhofer.de/de/geschaeftsfelder/speech-technologies/conversational-ai.html
    institution: Max-Planck Institute and Fraunhofer Institute IAIS, Fraunhofer IAIS
    last_name: Flores-Herr
    name: Nicolas Flores-Herr
    username: ~Nicolas_Flores-Herr1
  decision: toFindings
  end_page: 3933
  file: 951.pdf
  id: 951
  num_pages: 18
  openreview_id: b8xPwc02my
  pdf_file: 46f56087a40602c33e80a81f1a7c4ca7d54b4cdf.pdf
  start_page: 3916
  title: 'Tokenizer Choice For LLM Training: Negligible or Crucial?'
- abstract: 'The emergence of large language models (LLMs) further improves the capabilities
    of open-domain dialogue systems and can generate fluent, coherent, and diverse
    responses. However, LLMs still lack a crucial ability: communication skills. This
    limitation renders them more like information seeking tools rather than anthropomorphic
    chatbots. Communication skills, such as topic transition, proactively asking questions,
    concept guidance, empathy, and summarising often should be taken into consideration,
    to make LLMs more anthropomorphic and proactive during the conversation, thereby
    increasing the interest of users and attracting them to chat for longer. However,
    enabling these communication skills in black-box LLMs remains a key challenge
    because they do not have the same utterance formation mode as real people: think
    before speaking. Inspired by linguistics and cognitive science, we empower LLMs
    with communication skills through inner monologues. To evaluate various communication
    skills, we construct a benchmark named Cskills, which can also more comprehensively
    evaluate the dialogue generation ability of the model. Experimental results show
    that the proposed CSIM strategy improves the backbone models and outperforms the
    baselines.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Dialogue and Interactive Systems
  authors:
  - dblp_id: https://dblp.org/pid/313/3671.html
    emails: '****@ict.ac.cn'
    first_name: Junkai
    homepage: https://github.com/934865517zjk
    last_name: Zhou
    name: Junkai Zhou
    semantic_scholar_id: https://www.semanticscholar.org/author/Junkai-Zhou/13297687
    username: ~Junkai_Zhou1
  - dblp_id: https://dblp.org/pid/37/11078
    emails: '****@ict.ac.cn'
    first_name: Liang
    google_scholar_id: https://scholar.google.com/citations?user=1dgQHBkAAAAJ&hl=en
    homepage: https://pl8787.github.io/
    institution: Institute of Computing Technology, Chinese Academy of Sciences
    last_name: Pang
    name: Liang Pang
    semantic_scholar_id: https://www.semanticscholar.org/author/Liang-Pang/48537499
    username: ~Liang_Pang1
  - dblp_id: https://dblp.org/pid/98/917
    emails: '****@ict.ac.cn'
    first_name: Huawei
    institution: Institute of Computing Technology, Chinese Academy of Sciences
    last_name: Shen
    name: Huawei Shen
    username: ~Huawei_Shen1
  - dblp_id: https://dblp.org/pid/44/912
    emails: '****@ict.ac.cn'
    first_name: Xueqi
    homepage: http://www.ict.cas.cn/sourcedb_2018_ict_cas/cn/jssrck/200909/t20090917_2496598.html
    institution: ', Chinese Academy of Sciences'
    last_name: Cheng
    name: Xueqi Cheng
    username: ~Xueqi_Cheng1
  decision: toFindings
  end_page: 3960
  file: 955.pdf
  id: 955
  num_pages: 27
  openreview_id: SafoQbwbEI
  pdf_file: 8a3e3bb04ec4959d1cf3f8c1b8de253df35fb288.pdf
  start_page: 3934
  title: 'Think Before You Speak: Cultivating Communication Skills of Large Language
    Models via Inner Monologue'
- abstract: 'The performance cost of differential privacy has, for some applications,
    been shown to be higher for minority groups; fairness, conversely, has been shown
    to disproportionally compromise the privacy of members of such groups. Most work
    in this area has been restricted to computer vision and risk assessment. In response,
    we evaluate the impact of differential privacy on fairness across four diverse
    tasks, focusing on how attempts to mitigate privacy violations and between-group
    performance differences interact: Does privacy inhibit attempts to ensure fairness?
    To this end, we train $(\varepsilon,\delta)$-differentially private models with
    empirical risk minimization and group distributionally robust training objectives.
    Consistent with previous findings, we find that differential privacy increases
    between-group performance differences in the baseline setting; more interestingly,
    differential privacy {\em reduces} between-group performance differences in the
    robust setting. We explain this by interpreting differential privacy as regularization.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Ethics, Bias, and Fairness
  authors:
  - dblp_id: https://dblp.org/pid/240/3529.html
    emails: '****@di.ku.dk'
    first_name: Victor
    google_scholar_id: https://scholar.google.com/citations?user=rBmVgUsAAAAJ
    last_name: Hansen
    middle_name: "Petr\xE9n Bach"
    name: "Victor Petr\xE9n Bach Hansen"
    orcid: https://orcid.org/0000-0002-4715-1389
    username: "~Victor_Petr\xE9n_Bach_Hansen1"
  - dblp_id: https://dblp.org/pid/315/4311
    emails: '****@gmail.com'
    first_name: Atula
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=227xMUsAAAAJ
    homepage: https://atutej.github.io/
    institution: University of Texas at Austin
    last_name: Neerkaje
    middle_name: Tejaswi
    name: Atula Tejaswi Neerkaje
    semantic_scholar_id: https://www.semanticscholar.org/author/A.-Neerkaje/2157860264
    username: ~Atula_Tejaswi_Neerkaje1
  - dblp_id: https://dblp.org/pid/222/5147
    emails: '****@gmail.com'
    first_name: Ramit
    homepage: https://sites.google.com/iiitd.ac.in/ramitsawhney/
    institution: Georgia Institute of Technology
    last_name: Sawhney
    name: Ramit Sawhney
    semantic_scholar_id: https://www.semanticscholar.org/author/Ramit-Sawhney/51042088
    username: ~Ramit_Sawhney1
  - dblp_id: https://dblp.org/pid/143/9405
    emails: '****@bit.uni-bonn.de'
    first_name: Lucie
    google_scholar_id: https://scholar.google.com/citations?user=qZCZFp0AAAAJ
    homepage: https://caisa-lab.github.io
    institution: "Rheinische Friedrich-Wilhelms Universit\xE4t Bonn"
    last_name: Flek
    name: Lucie Flek
    orcid: https://orcid.org/0000-0002-5995-8454
    semantic_scholar_id: https://www.semanticscholar.org/author/Lucie-Flekova/2192277
    username: ~Lucie_Flek1
  - dblp_id: https://dblp.org/pid/30/2756
    emails: '****@di.ku.dk'
    first_name: Anders
    google_scholar_id: https://scholar.google.com.tw/citations?user=x3I4CrYAAAAJ
    homepage: https://anderssoegaard.github.io/
    institution: Copenhagen University
    last_name: "S\xF8gaard"
    name: "Anders S\xF8gaard"
    username: "~Anders_S\xF8gaard1"
  decision: toFindings
  end_page: 3974
  file: 964.pdf
  id: 964
  num_pages: 14
  openreview_id: Z60Vl0bs1w
  pdf_file: 9cd2d0a5aaf5a15565e49279139a88775b551df6.pdf
  start_page: 3961
  title: The Impact of Differential Privacy on Group Disparity Mitigation
- abstract: Traditional Automatic Video Dubbing (AVD) pipeline consists of three key
    modules, namely, Automatic Speech Recognition (ASR), Neural Machine Translation
    (NMT), and Text-to-Speech (TTS). Within AVD pipelines, isometric-NMT algorithms
    are employed to regulate the length of the synthesized output text. This is done
    to guarantee synchronization with respect to the alignment of video and audio
    subsequent to the dubbing process. Previous approaches have focused on aligning
    the number of characters and words in the source and target language texts of
    Machine Translation models. However, our approach aims to align the number of
    phonemes instead, as they are closely associated with speech duration. In this
    paper, we present the development of an isometric NMT system using Reinforcement
    Learning (RL), with a focus on optimizing the alignment of phoneme counts in the
    source and target language sentence pairs. To evaluate our models, we propose
    the Phoneme Count Compliance (PCC) score,  which is a measure of length compliance.
    Our approach demonstrates a substantial improvement of approximately 36% in the
    PCC score compared to the state-of-the-art models when applied to English-Hindi
    language pairs. Moreover, we propose a student-teacher architecture within the
    framework of our RL approach to maintain a trade-off between the phoneme count
    and translation quality.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Translation
  authors:
  - dblp_id: https://dblp.org/pid/298/7348
    emails: '****@gmail.com'
    first_name: Shivam
    institution: Rakuten Mobile, Inc.
    last_name: Mhaskar
    middle_name: Ratnakant
    name: Shivam Ratnakant Mhaskar
    username: ~Shivam_Ratnakant_Mhaskar1
  - dblp_id: https://dblp.org/pid/136/3651.html
    emails: '****@sony.com'
    first_name: Nirmesh
    google_scholar_id: https://scholar.google.co.in/citations?user=iB_UFIYAAAAJ&hl=en
    homepage: https://www.linkedin.com/in/nirmesh-shah/
    institution: Sony Research India
    last_name: Shah
    middle_name: J.
    name: Nirmesh J. Shah
    username: ~Nirmesh_J._Shah1
  - dblp_id: https://dblp.org/pid/153/0722
    emails: '****@iisc.ac.in'
    first_name: Mohammadi
    google_scholar_id: https://scholar.google.com/citations?user=H3ji_pAAAAAJ&hl=nl
    homepage: https://sites.google.com/site/mohammadizaki52/home
    institution: Sony Research India, Bangalore
    last_name: Zaki
    name: Mohammadi Zaki
    username: ~Mohammadi_Zaki1
  - emails: '****@sony.com'
    first_name: Ashishkumar
    google_scholar_id: https://scholar.google.com/citations?user=xb6IwtoAAAAJ&hl=en
    last_name: Gudmalwar
    middle_name: Prabhakar
    name: Ashishkumar Prabhakar Gudmalwar
    orcid: https://orcid.org/0000-0003-3590-503X
    username: ~Ashishkumar_Prabhakar_Gudmalwar1
  - dblp_id: https://dblp.org/pid/184/0627
    emails: '****@sony.com'
    first_name: Pankaj
    google_scholar_id: https://scholar.google.com/citations?user=3C5r8MoAAAAJ&hl=en&oi=ao
    homepage: https://www.sonyresearchindia.com/
    institution: Sony Research India
    last_name: Wasnik
    name: Pankaj Wasnik
    orcid: https://orcid.org/0000-0001-5602-2901
    username: ~Pankaj_Wasnik1
  - dblp_id: https://dblp.org/pid/134/3502
    emails: '****@iiitd.ac.in'
    first_name: Rajiv
    google_scholar_id: https://scholar.google.com.sg/citations?hl=en&user=WAChZv4AAAAJ
    homepage: https://iiitd.ac.in/rajivratn
    institution: Indraprastha Institute of Information Technology, Delhi
    last_name: Shah
    middle_name: Ratn
    name: Rajiv Ratn Shah
    username: ~Rajiv_Ratn_Shah1
  decision: toFindings
  end_page: 3985
  file: 965.pdf
  id: 965
  num_pages: 11
  openreview_id: whxJwWwTYz
  pdf_file: 0c1cb8897dd484c7a90c2e06992ce528d62b124f.pdf
  start_page: 3975
  title: Isometric Neural Machine Translation using Phoneme Count Ratio Reward-based
    Reinforcement Learning
- abstract: While text summarization is a well-known NLP task, in this paper, we introduce
    a novel and useful variant of it called functionality extraction from Git README
    files. Though this task is a text2text  generation at an abstract level, it involves
    its own peculiarities and challenges making existing  text2text generation systems
    not very useful. The motivation behind this task stems from a recent surge in
    research and development activities around the use of large language models for
    code-related tasks, such as code refactoring, code summarization, etc. We also
    release a human-annotated dataset called FuncRead, and develop a battery of models
    for the task. Our exhaustive experimentation shows that small size fine-tuned
    models beat any baseline models that can be designed using popular black-box or
    white-box large language models (LLMs) such as  ChatGPT and Bard. Our best fine-tuned
    7 Billion CodeLlama model exhibit 70% and 20% gain on the F1 score against ChatGPT
    and Bard respectively.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Extraction
  authors:
  - emails: '****@gmail.com'
    first_name: Prince
    google_scholar_id: https://scholar.google.com/citations?user=8qDwuyEAAAAJ&hl=en
    institution: International Business Machines
    last_name: Kumar
    name: Prince Kumar
    username: ~Prince_Kumar1
  - dblp_id: https://dblp.org/pid/138/3209
    emails: '****@in.ibm.com'
    first_name: Srikanth
    google_scholar_id: https://scholar.google.co.in/citations?user=cFd7pr8AAAAJ&hl=en
    homepage: https://researcher.watson.ibm.com/researcher/view.php?person=in-srikanth.tamilselvam
    institution: International Business Machines
    last_name: Tamilselvam
    middle_name: G.
    name: Srikanth G. Tamilselvam
    username: ~Srikanth_G._Tamilselvam1
  - dblp_id: https://dblp.uni-trier.de/pers/g/Garg:Dinesh.html
    emails: '****@in.ibm.com'
    first_name: Dinesh
    google_scholar_id: https://scholar.google.com.tw/citations?user=YrU_ZDkAAAAJ
    homepage: https://researcher.watson.ibm.com/researcher/view.php?person=in-garg.dinesh
    last_name: Garg
    name: Dinesh Garg
    semantic_scholar_id: https://www.semanticscholar.org/author/Dinesh-Garg/40408530
    username: ~Dinesh_Garg1
  decision: toFindings
  end_page: 3999
  file: 969.pdf
  id: 969
  num_pages: 14
  openreview_id: o0bXtJfJIY
  pdf_file: e1c1d2a27ccbe9b1213d8dea8658fcd202349200.pdf
  start_page: 3986
  title: Read between the lines - Functionality Extraction From READMEs
- abstract: Cognitive research indicates that abstraction ability is essential in
    human intelligence, which remains under-explored in language models. In this paper,
    we present AbsPyramid, a unified entailment graph of 221K textual descriptions
    of abstraction knowledge. While existing resources only touch nouns or verbs within
    simplified events or specific domains, AbsPyramid collects abstract knowledge
    for three components of diverse events to comprehensively evaluate the abstraction
    ability of language models in the open domain. Experimental results demonstrate
    that current LLMs face challenges comprehending abstraction knowledge in zero-shot
    and few-shot settings. By training on our rich abstraction knowledge, we find
    LLMs can acquire basic abstraction abilities and generalize to unseen events.
    In the meantime, we empirically show that our benchmark is comprehensive to enhance
    LLMs across two previous abstraction tasks.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - dblp_id: https://dblp.org/pid/120/1278-3
    emails: '****@cse.ust.hk'
    first_name: Zhaowei
    google_scholar_id: https://scholar.google.com/citations?user=5dzojAsAAAAJ&hl=en
    homepage: https://zhaowei-wang-nlp.github.io/
    institution: Department of Computer Science and Engineering, Hong Kong University
      of Science and Technology
    last_name: Wang
    name: Zhaowei Wang
    orcid: https://orcid.org/0000-0001-5539-8181
    semantic_scholar_id: https://www.semanticscholar.org/author/Zhaowei-Wang/2187830802
    username: ~Zhaowei_Wang2
  - emails: '****@connect.ust.hk'
    first_name: Haochen
    last_name: Shi
    name: Haochen Shi
    username: ~Haochen_Shi4
  - dblp_id: https://dblp.org/pid/51/5775
    emails: '****@cse.ust.hk'
    first_name: Weiqi
    google_scholar_id: https://scholar.google.com/citations?hl=zh-CN&user=ZKgZ7jEAAAAJ
    homepage: https://mighty-weaver.github.io/
    institution: The Hong Kong University of Science and Technology
    last_name: Wang
    name: Weiqi Wang
    orcid: https://orcid.org/0000-0002-1617-9805
    semantic_scholar_id: https://www.semanticscholar.org/author/1587728690
    username: ~Weiqi_Wang1
  - dblp_id: https://dblp.org/pid/283/4921
    emails: '****@connect.ust.hk'
    first_name: Tianqing
    google_scholar_id: https://scholar.google.com.hk/citations?user=Tb3rc34AAAAJ&hl=en
    homepage: http://fangtq.com/
    last_name: Fang
    name: Tianqing Fang
    semantic_scholar_id: https://www.semanticscholar.org/author/2044202073
    username: ~Tianqing_Fang1
  - dblp_id: https://dblp.org/pid/48/859.html
    emails: '****@cse.ust.hk'
    first_name: Hongming
    google_scholar_id: https://scholar.google.com/citations?user=i5ETuuQAAAAJ&hl=en
    homepage: http://www.cse.ust.hk/~hzhangal/
    last_name: Zhang
    name: Hongming Zhang
    username: ~Hongming_Zhang2
  - emails: '****@cse.ust.hk'
    first_name: Sehyun
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=I_wmwsUAAAAJ
    homepage: https://syncdoth.github.io
    institution: Department of Computer Science and Engineering, Hong Kong University
      of Science and Technology
    last_name: Choi
    name: Sehyun Choi
    orcid: https://orcid.org/0009-0009-3225-9379
    semantic_scholar_id: https://www.semanticscholar.org/author/Sehyun-Choi/2127128777
    username: ~Sehyun_Choi1
  - dblp_id: https://dblp.org/pid/76/1820-39.html
    emails: '****@cse.ust.hk'
    first_name: Xin
    google_scholar_id: https://scholar.google.com.hk/citations?user=WvC4upQAAAAJ&hl=en
    homepage: https://www.cse.ust.hk/~xliucr/
    institution: Amazon
    last_name: Liu
    name: Xin Liu
    orcid: https://orcid.org/0000-0001-9610-9526
    semantic_scholar_id: https://www.semanticscholar.org/author/Xin-Liu/89121677
    username: ~Xin_Liu9
  - dblp_id: https://dblp.org/pid/86/2159
    emails: '****@cse.ust.hk'
    first_name: Yangqiu
    google_scholar_id: https://scholar.google.com.tw/citations?user=MdQZ-q8AAAAJ
    homepage: https://www.cse.ust.hk/~yqsong/
    institution: The Hong Kong University of Science and Technology
    last_name: Song
    name: Yangqiu Song
    username: ~Yangqiu_Song1
  decision: toFindings
  end_page: 4019
  file: 970.pdf
  id: 970
  num_pages: 20
  openreview_id: mSabHQDVAi
  pdf_file: 1ee2eecbb7a44f3de6a887675276dd80c7f30cc5.pdf
  start_page: 4000
  title: 'AbsPyramid: Benchmarking the Abstraction Ability of Language Models with
    a Unified Entailment Graph'
- abstract: Scientific texts are distinctive from ordinary texts in quite a few aspects
    like their vocabulary and discourse structure. Consequently, Information Extraction
    (IE) tasks for scientific texts come with their own set of challenges. The classical
    definition of Named Entities restricts the inclusion of all scientific terms under
    its hood, which is why previous works have used the terms Named Entities and Keyphrases
    interchangeably. We suggest the rechristening of Named Entities for the scientific
    domain as Typed Keyphrases (TK), broadening their scope. We advocate for exploring
    this task in the few-shot domain due to the scarcity of labeled scientific IE
    data. Currently, no dataset exists for few-shot scientific Typed Keyphrase Recognition.
    To address this gap,  we develop an annotation schema and present Few-TK, a dataset
    in the AI/ML field that includes scientific Typed Keyphrase annotations on abstracts
    of 500 research papers. To the best of our knowledge, this is the introductory
    few-shot Typed Keyphrase recognition dataset and only the second dataset structured
    specifically for few-shot NER, after Few-NERD. We report the results of several
    few-shot sequence-labelling models applied to our dataset. The data and code are
    available at https://github.com/AvishekLahiri/Few_TK.git
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Extraction
  authors:
  - dblp_id: https://dblp.org/pid/344/1879
    emails: '****@gmail.com'
    first_name: Avishek
    google_scholar_id: https://scholar.google.com/citations?user=yBXIpkAAAAAJ&hl=en
    institution: Indian Association for the Cultivation of Science
    last_name: Lahiri
    name: Avishek Lahiri
    orcid: https://orcid.org/0000-0001-5860-1026
    semantic_scholar_id: https://www.semanticscholar.org/author/Avishek-Lahiri/2061504374
    username: ~Avishek_Lahiri1
  - emails: '****@gmail.com'
    first_name: Pratyay
    institution: Indian Association for the Cultivation of Science
    last_name: Sarkar
    name: Pratyay Sarkar
    username: ~Pratyay_Sarkar1
  - emails: '****@yahoo.com'
    first_name: Medha
    last_name: Sen
    name: Medha Sen
    username: ~Medha_Sen1
  - dblp_id: https://dblp.org/pid/60/7052
    emails: '****@iacs.res.in'
    first_name: Debarshi
    google_scholar_id: https://scholar.google.com/citations?user=AevNQmMAAAAJ&hl=en
    institution: Indian Association for the Cultivation of Science
    last_name: Sanyal
    middle_name: Kumar
    name: Debarshi Kumar Sanyal
    orcid: https://orcid.org/0000-0001-8723-5002
    semantic_scholar_id: https://www.semanticscholar.org/author/Debarshi-Kumar-Sanyal/11575056
    username: ~Debarshi_Kumar_Sanyal1
  - dblp_id: https://dblp.org/pid/51/8909.html
    emails: '****@iiitkalyani.ac.in'
    first_name: Imon
    google_scholar_id: https://scholar.google.com/citations?user=3xcXNz0AAAAJ&hl=fr
    homepage: https://iiitkalyani.ac.in/php/facultymainpage/im.html
    last_name: Mukherjee
    name: Imon Mukherjee
    orcid: https://orcid.org/0000-0002-8598-148X
    username: ~Imon_Mukherjee1
  decision: toFindings
  end_page: 4034
  file: 972.pdf
  id: 972
  num_pages: 15
  openreview_id: dO32eH5pPO
  pdf_file: 62c3a22f9e039db8ed37b49fb3fca30b83bf9b41.pdf
  start_page: 4020
  title: 'Few-TK: A Dataset for Few-shot Scientific Typed Keyphrase Recognition'
- abstract: Logical reasoning is a fundamental aspect of human intelligence and a
    key component of tasks like problem-solving and decision-making. Recent advancements
    have enabled Large Language Models (LLMs) to potentially exhibit reasoning capabilities,
    but complex logical reasoning remains a challenge. The state-of-the-art, solver-augmented
    language models, use LLMs to parse natural language logical questions into symbolic
    representations first and then adopt external logical solvers to take in the symbolic
    representations and output the answers. Despite their impressive performance,
    any parsing errors will inevitably result in the failure of the execution of external
    logical solvers and no answer to the logical questions. In this paper, we introduce
    LoGiPT, a novel language model that directly internalizes and emulates the reasoning
    processes of logical solvers and avoids parsing errors by learning strict adherence
    to solver syntax and grammar. LoGiPT is fine-tuned on a newly constructed instruction-tuning
    dataset derived from revealing and refining the invisible reasoning process of
    deductive solvers. Experimental results on two public deductive reasoning benchmarks
    show that LoGiPT outperforms state-of-the-art solver-augmented LMs and few-shot
    prompting methods on competitive LLMs like GPT-4. This project is available in
    https://github.com/Cyril-JZ/LoGiPT.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Question Answering
  authors:
  - dblp_id: https://dblp.org/pid/242/9191
    emails: '****@pku.edu.cn'
    first_name: Jiazhan
    google_scholar_id: https://scholar.google.com/citations?user=uYHmew8AAAAJ&hl=en
    homepage: https://sites.google.com/view/jzfeng/home/
    last_name: Feng
    name: Jiazhan Feng
    orcid: https://orcid.org/0000-0002-5832-6199
    semantic_scholar_id: https://www.semanticscholar.org/author/Jiazhan-Feng/147062881
    username: ~Jiazhan_Feng1
  - dblp_id: https://dblp.uni-trier.de/pers/hd/x/Xu:Ruochen
    emails: '****@gmail.com'
    first_name: Ruochen
    google_scholar_id: https://scholar.google.com/citations?user=HTp5S00AAAAJ&hl=en
    homepage: https://www.microsoft.com/en-us/research/people/ruox/
    institution: Microsoft
    last_name: Xu
    name: Ruochen Xu
    username: ~Ruochen_Xu2
  - dblp_id: https://dblp.org/pid/192/4730
    emails: '****@microsoft.com'
    first_name: Junheng
    google_scholar_id: https://scholar.google.com/citations?user=GL1yyoEAAAAJ
    homepage: https://www.haojunheng.com/
    institution: Microsoft
    last_name: Hao
    name: Junheng Hao
    orcid: https://orcid.org/0000-0001-5292-9389
    username: ~Junheng_Hao1
  - dblp_id: https://dblp.org/pid/158/3418
    emails: '****@gmail.com'
    first_name: Hiteshi
    homepage: https://hiteshis.github.io/
    last_name: Sharma
    name: Hiteshi Sharma
    username: ~Hiteshi_Sharma1
  - emails: '****@gmail.com'
    first_name: Yelong
    google_scholar_id: https://scholar.google.com/citations?user=S6OFEFEAAAAJ&hl=en
    institution: Microsoft
    last_name: Shen
    name: yelong shen
    username: ~yelong_shen1
  - dblp_id: https://dblp.org/pid/63/1870
    emails: '****@pku.edu.cn'
    first_name: Dongyan
    google_scholar_id: https://scholar.google.com/citations?hl=zh-CN&user=lhR8-68AAAAJ&view_op=list_works
    homepage: http://www.wict.pku.edu.cn/zhaodongyan/en
    institution: Peking University
    last_name: Zhao
    name: Dongyan Zhao
    username: ~Dongyan_Zhao1
  - dblp_id: https://dblp.org/pid/79/2536
    emails: '****@microsoft.com'
    first_name: Weizhu
    institution: Microsoft GenAI
    last_name: Chen
    name: Weizhu Chen
    username: ~Weizhu_Chen1
  decision: toFindings
  end_page: 4051
  file: 982.pdf
  id: 982
  num_pages: 17
  openreview_id: aD8W7rt29O
  pdf_file: e81ab462414384b4fa2d10045d483c6632ec5494.pdf
  start_page: 4035
  title: Language Models can be Deductive Solvers
- abstract: "Users of natural language interfaces, frequently powered by Large Language\
    \ Models (LLMs), must often repeat their full set of preferences each time they\
    \ make a similar  request. We describe an approach to LLM-based dialogue modeling\
    \ in which persistent user constraints and preferences -- collectively termed\
    \ standing instructions -- are provided as additional context for such interfaces.\
    \ For example, when a user states \"I'm hungry\", a  previously expressed preference\
    \ for Persian food can be automatically added to the LLM prompt, influencing the\
    \ search for relevant restaurants.\nWe develop NLSI, a language-to-program dataset\
    \ consisting of over 2.4K English dialogues spanning 17 domains, in which each\
    \ dialogue is paired with a user profile (a set of user-specific standing instructions)\
    \ and corresponding structured representations (a sequence of API calls). A key\
    \ challenge in NLSI is to identify which subset of the standing instructions is\
    \ applicable to a given dialogue. NLSI contains diverse phenomena, from simple\
    \ preferences to interdependent instructions such as triggering a hotel search\
    \ whenever the user is booking tickets to an event. \nWe conduct experiments on\
    \ NLSI using prompting with large language models and various retrieval approaches,\
    \ achieving a maximum of 46% exact match on API prediction.  \nOur results demonstrate\
    \ the challenges in identifying the relevant standing instructions and their interpretation\
    \ into API calls"
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Dialogue and Interactive Systems
  authors:
  - dblp_id: https://dblp.org/pid/222/2610
    emails: '****@gmail.com'
    first_name: Nikita
    google_scholar_id: https://scholar.google.com/citations?user=pj7W-KYAAAAJ
    last_name: Moghe
    name: Nikita Moghe
    semantic_scholar_id: https://www.semanticscholar.org/author/Nikita-Moghe/51025891
    username: ~Nikita_Moghe1
  - dblp_id: https://dblp.org/pid/128/4897
    emails: '****@gmail.com'
    first_name: Patrick
    google_scholar_id: https://scholar.google.com/citations?user=k9IJYg8AAAAJ
    homepage: https://patrickxia.me
    institution: Microsoft
    last_name: Xia
    name: Patrick Xia
    semantic_scholar_id: https://www.semanticscholar.org/author/Patrick-Xia/2465658
    username: ~Patrick_Xia1
  - dblp_id: https://dblp.org/pid/97/8154
    emails: '****@mit.edu'
    first_name: Jacob
    google_scholar_id: https://scholar.google.com/citations?user=dnZ8udEAAAAJ
    homepage: http://web.mit.edu/jda/www
    institution: Massachusetts Institute of Technology and Microsoft
    last_name: Andreas
    name: Jacob Andreas
    semantic_scholar_id: https://www.semanticscholar.org/author/Jacob-Andreas/2112400
    username: ~Jacob_Andreas1
  - dblp_id: https://dblp.org/pid/37/3263
    emails: '****@cs.jhu.edu'
    first_name: Jason
    google_scholar_id: https://scholar.google.com/citations?user=tjb2UccAAAAJ&hl=en
    homepage: http://cs.jhu.edu/~jason
    institution: Microsoft and Johns Hopkins University
    last_name: Eisner
    name: Jason Eisner
    orcid: https://orcid.org/0000-0002-8861-0772
    semantic_scholar_id: https://www.semanticscholar.org/author/Jason-Eisner/145043214
    username: ~Jason_Eisner1
  - dblp_id: https://dblp.org/pid/06/4775
    emails: '****@cs.jhu.edu'
    first_name: Benjamin
    google_scholar_id: https://scholar.google.com.tw/citations?user=wIujAJoAAAAJ
    homepage: http://www.cs.jhu.edu/~vandurme/
    institution: Johns Hopkins University, Johns Hopkins University, Johns Hopkins
      University and Microsoft
    last_name: Van Durme
    name: Benjamin Van Durme
    username: ~Benjamin_Van_Durme2
  - dblp_id: https://dblp.org/pid/146/6263
    emails: '****@gmail.com'
    first_name: Harsh
    google_scholar_id: https://scholar.google.co.in/citations?user=YQe9pdUAAAAJ&hl=en
    homepage: https://sites.google.com/view/harshjhamtani/research
    institution: Microsoft
    last_name: Jhamtani
    name: Harsh Jhamtani
    semantic_scholar_id: https://www.semanticscholar.org/author/Harsh-Jhamtani/2006291
    username: ~Harsh_Jhamtani1
  decision: toFindings
  end_page: 4069
  file: 984.pdf
  id: 984
  num_pages: 18
  openreview_id: FbFtVl0i0a
  pdf_file: 0ed000731f147a1f36efdf35a72cdf584adb8473.pdf
  start_page: 4052
  title: Interpreting User Requests in the Context of Natural Language Standing Instructions
- abstract: Large language models (LLMs) have notably revolutionized many domains
    within natural language processing due to their exceptional performance. Their
    security has become increasingly vital. This study is centered on protecting LLMs
    against unauthorized access and potential theft. We propose a simple yet effective
    protective measure wherein a unique key prompt is embedded within the LLM. This
    mechanism enables the model to respond only when presented with the correct key
    prompt; otherwise, LLMs will refuse to react to any input instructions. This key
    prompt protection offers a robust solution to prevent the unauthorized use of
    LLMs, as the model becomes unusable without the correct key. We evaluated the
    proposed protection on multiple LLMs and NLP tasks. Results demonstrate that our
    method can successfully protect the LLM without significantly impacting the model's
    original function. Moreover, we demonstrate potential attacks that attempt to
    bypass the protection mechanism will adversely affect the model's performance,
    further emphasizing the effectiveness of the proposed protection method.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - dblp_id: https://dblp.org/pid/239/1928
    emails: '****@rice.edu'
    first_name: Ruixiang
    google_scholar_id: https://scholar.google.com/citations?user=T575jsoAAAAJ&hl=en
    homepage: https://www.ruixiangtang.net/
    last_name: Tang
    name: Ruixiang Tang
    username: ~Ruixiang_Tang1
  - dblp_id: https://dblp.org/pid/207/7875
    emails: '****@rice.edu'
    first_name: Yu-Neng
    institution: Rice University
    last_name: Chuang
    name: Yu-Neng Chuang
    username: ~Yu-Neng_Chuang1
  - emails: '****@gmail.com'
    first_name: Xuanting
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=Sp2HIl4AAAAJ
    last_name: Cai
    name: Xuanting Cai
    username: ~Xuanting_Cai1
  - dblp_id: https://dblp.org/pid/183/5606
    emails: '****@njit.edu'
    first_name: Mengnan
    google_scholar_id: https://scholar.google.com/citations?user=8uXBeaEAAAAJ&hl=en
    homepage: https://mengnandu.com/
    institution: New Jersey Institute of Technology
    last_name: Du
    name: Mengnan Du
    semantic_scholar_id: https://www.semanticscholar.org/author/Mengnan-Du/3432460
    username: ~Mengnan_Du1
  - dblp_id: https://dblp.org/pid/24/7536
    emails: '****@rice.edu'
    first_name: Xia
    google_scholar_id: https://scholar.google.com.tw/citations?user=pcCS60IAAAAJ
    homepage: http://faculty.cs.tamu.edu/xiahu/
    institution: Rice University
    last_name: Hu
    name: Xia Hu
    username: ~Xia_Hu4
  decision: toFindings
  end_page: 4082
  file: 986.pdf
  id: 986
  num_pages: 13
  openreview_id: RasdTh8LXe
  pdf_file: efab3c0eda4c477fb2f4062ac23b4338b0dc2b83.pdf
  start_page: 4070
  title: 'Secure Your Model: An Effective Key Prompt Protection Mechanism for Large
    Language Models'
- abstract: 'Large language models (LLMs) can achieve impressive performance on various
    reasoning tasks by incorporating chain-of-thought (CoT) prompting, where step-by-step
    reasoning is provided to guide LLMs to generate answers to questions, and the
    question-rationale-answer triplets are utilized as demonstration exemplars. However,
    the reasoning chains of demonstrations generated by LLMs are observed to be prone
    to errors, which can subsequently lead to incorrect reasoning during inference.
    Furthermore, inappropriate exemplars, e.g., overly simplistic or complex exemplars
    depending on the question''s difficulty level, can affect the LLM''s performance.
    To address these issues, we introduce Iter-CoT (Iterative bootstrapping in Chain-of-Thoughts
    prompting). Iter-CoT has two advantages: (1) it adopts iterative bootstrapping
    that enables LLMs to rectify errors autonomously, resulting in more precise and
    comprehensive reasoning chains. (2) it selects exemplars of challenging yet answerable
    (i.e., the LLM has the potential to answer correctly) questions, enhancing the
    LLMs'' generalizability to answer questions with varying difficulty levels. Experimental
    results exhibit Iter-CoT superior performance on three distinct reasoning tasks
    on ten datasets.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - dblp_id: https://dblp.org/pid/336/2528
    emails: '****@gmail.com'
    first_name: Jiashuo
    homepage: https://github.com/gasolsun36
    last_name: Sun
    name: Jiashuo Sun
    username: ~Jiashuo_Sun1
  - dblp_id: https://dblp.org/pid/56/4437
    emails: '****@stu.xmu.edu.cn'
    first_name: Yi
    homepage: https://github.com/YiLuo-roy
    last_name: Luo
    name: Yi Luo
    username: ~Yi_Luo7
  - emails: '****@microsoft.com'
    first_name: Yeyun
    google_scholar_id: https://scholar.google.com/citations?user=piUkwMYAAAAJ&hl=en
    last_name: Gong
    name: Yeyun Gong
    username: ~Yeyun_Gong2
  - dblp_id: https://dblp.org/pid/37/3102-1.html
    emails: '****@xmu.edu.cn'
    first_name: Chen
    google_scholar_id: https://scholar.google.com/citations?user=z1l2JSMAAAAJ&hl=en
    homepage: https://xmudm.github.io/publications/
    institution: Xiamen University
    last_name: Lin
    name: Chen Lin
    orcid: https://orcid.org/0000-0002-2275-997X
    username: ~Chen_Lin5
  - emails: '****@gmail.com'
    first_name: Yelong
    google_scholar_id: https://scholar.google.com/citations?user=S6OFEFEAAAAJ&hl=en
    institution: Microsoft
    last_name: Shen
    name: yelong shen
    username: ~yelong_shen1
  - dblp_id: https://dblp.org/pid/96/2596-2
    emails: '****@idea.edu.cn'
    first_name: Jian
    homepage: https://idea.edu.cn/person/guojian/
    institution: Hong Kong University of Science and Technology
    last_name: Guo
    name: Jian Guo
    username: ~Jian_Guo2
  - dblp_id: https://dblp.org/pid/30/8160
    emails: '****@microsoft.com'
    first_name: Nan
    google_scholar_id: https://scholar.google.com/citations?user=Qaa6OxIAAAAJ&hl=en
    homepage: https://nanduan.github.io/
    institution: Microsoft Research Asia
    last_name: Duan
    name: Nan Duan
    username: ~Nan_Duan1
  decision: toFindings
  end_page: 4110
  file: 993.pdf
  id: 993
  num_pages: 28
  openreview_id: UFPGB0XfO6
  pdf_file: 379633eaec16be2e69c64e2c351e85b99aa0ec0c.pdf
  start_page: 4083
  title: Enhancing Chain-of-Thoughts Prompting with Iterative Bootstrapping in Large
    Language Models
- abstract: Prompt-based models have gathered a lot of attention from researchers
    due to their remarkable advancements in the fields of zero-shot and few-shot learning.
    Developing an effective prompt template plays a critical role. However, prior
    studies have mainly focused on prompt vocabulary searching or embedding initialization
    within a predefined template with the prompt position fixed. In this empirical
    study, we conduct the most comprehensive analysis to date of prompt position for
    diverse Natural Language Processing (NLP) tasks. Our findings quantify the substantial
    impact prompt position has on model performance. We observe that the prompt positions
    used in prior studies are often sub-optimal, and this observation is consistent
    even in widely used instruction-tuned models. These findings suggest prompt position
    optimisation as a valuable research direction to augment prompt engineering methodologies
    and prompt position-aware instruction tuning as a potential way to build more
    robust models in the future.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Learning for NLP
  authors:
  - emails: '****@soton.ac.uk'
    first_name: Junyu
    google_scholar_id: https://scholar.google.com/citations?user=uLNfX1QAAAAJ&hl=en
    institution: University of Southampton
    last_name: Mao
    name: Junyu Mao
    username: ~Junyu_Mao1
  - dblp_id: https://dblp.org/pid/86/5912
    emails: '****@soton.ac.uk'
    first_name: Stuart
    google_scholar_id: https://scholar.google.co.uk/citations?user=VGRsDAIAAAAJ&hl=en
    homepage: https://www.southampton.ac.uk/~sem03/
    institution: University of Southampton
    last_name: Middleton
    middle_name: E.
    name: Stuart E. Middleton
    orcid: https://orcid.org/0000-0001-8305-8176
    semantic_scholar_id: https://www.semanticscholar.org/author/S.-Middleton/3097680
    username: ~Stuart_E._Middleton1
  - emails: '****@ecs.soton.ac.uk'
    first_name: Mahesan
    google_scholar_id: https://scholar.google.co.uk/citations?user=3kuILUIAAAAJ&hl=en
    homepage: http://www.ecs.soton.ac.uk/people/mn
    institution: University of Southampton
    last_name: Niranjan
    name: Mahesan Niranjan
    username: ~Mahesan_Niranjan1
  decision: toFindings
  end_page: 4139
  file: 1002.pdf
  id: 1002
  num_pages: 29
  openreview_id: EmJYREcgY1
  pdf_file: f3ed08fbdfa7a9206245ca8f7a3e4842bc8d0ff8.pdf
  start_page: 4111
  title: Do Prompt Positions Really Matter?
- abstract: How can we perform  computations over natural language representations
    to solve tasks that require symbolic and numeric reasoning? We propose natural
    language embedded programs (NLEP) as a unifying framework for  addressing math/symbolic
    reasoning, natural language understanding, and instruction following tasks. Our
    approach prompts a  language model to generate full Python programs that define
    functions over data structures which contain natural language representations
    of structured knowledge. A Python interpreter then executes the generated code
    and prints the output. Despite using a task-general prompt, we find that this
    approach can improve upon strong baselines across a range of different tasks including
    math and symbolic reasoning, text classification, question answering, and instruction
    following. We found that the generated programs are interpretable since they outline
    the exact reasoning process followed by the program interpreter.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Generation
  authors:
  - emails: '****@link.cuhk.edu.hk'
    first_name: Tianhua
    last_name: Zhang
    name: Tianhua Zhang
    semantic_scholar_id: https://www.semanticscholar.org/author/Tianhua-Zhang/2146333115
    username: ~Tianhua_Zhang2
  - emails: '****@berkeley.edu'
    first_name: Jiaxin
    homepage: https://github.com/para-lost
    last_name: Ge
    name: Jiaxin Ge
    username: ~Jiaxin_Ge1
  - dblp_id: https://dblp.org/pid/147/4317
    emails: '****@mit.edu'
    first_name: Hongyin
    institution: Massachusetts Institute of Technology
    last_name: Luo
    name: Hongyin Luo
    username: ~Hongyin_Luo1
  - dblp_id: https://dblp.org/pid/64/3095
    emails: '****@mit.edu'
    first_name: Yung-Sung
    google_scholar_id: https://scholar.google.com/citations?user=3ar1DOwAAAAJ
    homepage: https://people.csail.mit.edu/yungsung/
    institution: Massachusetts Institute of Technology
    last_name: Chuang
    name: Yung-Sung Chuang
    orcid: https://orcid.org/0000-0002-1723-5063
    semantic_scholar_id: https://www.semanticscholar.org/author/Yung-Sung-Chuang/2475831
    username: ~Yung-Sung_Chuang1
  - emails: '****@mit.edu'
    first_name: Mingye
    homepage: https://onelab.mit.edu/people
    last_name: Gao
    name: Mingye Gao
    username: ~Mingye_Gao1
  - dblp_id: https://dblp.org/pid/98/8660.html
    emails: '****@mit.edu'
    first_name: Yuan
    google_scholar_id: https://scholar.google.com/citations?user=MuhvvOkAAAAJ&hl=en
    institution: Massachusetts Institute of Technology
    last_name: Gong
    name: Yuan Gong
    username: ~Yuan_Gong3
  - dblp_id: https://dblp.org/pid/07/1501
    emails: '****@mit.edu'
    first_name: Yoon
    google_scholar_id: https://scholar.google.com/citations?user=n_ts4eYAAAAJ
    homepage: https://people.csail.mit.edu/yoonkim/
    institution: Massachusetts Institute of Technology
    last_name: Kim
    name: Yoon Kim
    semantic_scholar_id: https://www.semanticscholar.org/author/Yoon-Kim/38367242
    username: ~Yoon_Kim1
  - emails: '****@se.cuhk.edu.hk'
    first_name: Xixin
    homepage: https://www1.se.cuhk.edu.hk/~wuxx/
    institution: The Chinese University of Hong Kong
    last_name: Wu
    name: Xixin Wu
    username: ~Xixin_Wu1
  - dblp_id: https://dblp.org/pid/92/3270
    emails: '****@se.cuhk.edu.hk'
    first_name: Helen
    homepage: http://www.se.cuhk.edu.hk/people/academic-staff/prof-meng-mei-ling-helen/
    institution: The Chinese University of Hong Kong
    last_name: Meng
    middle_name: M.
    name: Helen M. Meng
    username: ~Helen_M._Meng1
  - dblp_id: https://dblp.org/pid/37/6580
    emails: '****@mit.edu'
    first_name: James
    google_scholar_id: https://scholar.google.com/citations?user=pfGI-KcAAAAJ&hl=en&oi=ao
    homepage: https://www.csail.mit.edu/person/jim-glass
    last_name: Glass
    middle_name: R.
    name: James R. Glass
    orcid: https://orcid.org/0000-0002-3097-360X
    semantic_scholar_id: https://www.semanticscholar.org/author/James-R.-Glass/145898106
    username: ~James_R._Glass1
  decision: toFindings
  end_page: 4164
  file: 1005.pdf
  id: 1005
  num_pages: 25
  openreview_id: N7mFFvuR8F
  pdf_file: 3633b69c5c8892d0d8efe3131a1237a0a040357b.pdf
  start_page: 4140
  title: Natural Language Embedded Programs for Hybrid Language Symbolic Reasoning
- abstract: 'Media framing is the study of strategically selecting and presenting
    specific aspects of political issues to shape public opinion. Despite its relevance
    to almost all societies around the world, research has been limited due to the
    lack of available datasets and other resources. This study explores the possibility
    of dataset creation through crowdsourcing, utilizing non-expert annotators to
    develop training corpora. We first extend framing analysis beyond English news
    to a multilingual context (12 typologically diverse languages) through automatic
    translation. We also present a novel benchmark in Bengali and Portuguese on the
    immigration and same-sex marriage domains.

    Additionally, we show that a system trained on our crowd-sourced dataset, combined
    with other existing ones, leads to a 5.32 percentage point increase from the baseline,
    showing that crowdsourcing is a viable option. Last, we study the performance
    of large language models (LLMs) for this task, finding that task-specific fine-tuning
    is a better approach than employing bigger non-specialized models.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Computational Social Science and Cultural Analytics
  authors:
  - emails: '****@gmu.edu'
    first_name: Syeda Sabrina
    google_scholar_id: https://scholar.google.com/citations?user=PHMT3scAAAAJ&hl=en
    homepage: https://syedasabrina.github.io/index.html
    institution: George Mason University
    last_name: Akter
    name: Syeda Sabrina Akter
    username: ~Syeda_Sabrina_Akter1
  - dblp_id: https://dblp.org/pid/148/9479
    emails: '****@gmu.edu'
    first_name: Antonios
    google_scholar_id: https://scholar.google.com/citations?user=g_G_SNAAAAAJ&hl=en
    homepage: http://www.cs.gmu.edu/~antonis/
    institution: Athena Research Center and George Mason University
    last_name: Anastasopoulos
    name: Antonios Anastasopoulos
    orcid: https://orcid.org/0000-0002-8544-246X
    semantic_scholar_id: https://www.semanticscholar.org/author/Antonios-Anastasopoulos/49513989
    username: ~Antonios_Anastasopoulos1
  decision: toFindings
  end_page: 4182
  file: 1007.pdf
  id: 1007
  num_pages: 18
  openreview_id: 4VQEyXLIBB
  pdf_file: 15a46bda9821aa08cea3ea18927decb07f600d2b.pdf
  start_page: 4165
  title: A Study on Scaling Up Multilingual News Framing Analysis
- abstract: 'As the number of language models has increased, various benchmarks have
    been suggested to assess the proficiency of the models in natural language understanding.
    However, there is a lack of such a benchmark in Vietnamese due to the difficulty
    in accessing natural language processing datasets or the scarcity of task-specific
    datasets. **ViGLUE**, the proposed dataset collection, is a **Vi**etnamese **G**eneral
    **L**anguage **U**nderstanding **E**valuation benchmark developed using three
    methods: translating an existing benchmark, generating new corpora, and collecting
    available datasets. ViGLUE contains twelve tasks and encompasses over ten areas
    and subjects, enabling it to evaluate models comprehensively over a broad spectrum
    of aspects. Baseline models utilizing multilingual language models are also provided
    for all tasks in the proposed benchmarks. In addition, the study of the available
    Vietnamese large language models is conducted to explore the language models''
    ability in the few-shot learning framework, leading to the exploration of the
    relationship between specific tasks and the number of shots.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - emails: '****@apcs.fitus.edu.vn'
    first_name: Minh-Nam
    last_name: Tran
    name: Minh-Nam Tran
    orcid: https://orcid.org/0009-0002-1279-9756
    username: ~Minh-Nam_Tran1
  - emails: '****@apcs.fitus.edu.vn'
    first_name: Phu-Vinh
    homepage: https://www.facebook.com/phuvinh.nguyen.75873/
    last_name: Nguyen
    name: Phu-Vinh Nguyen
    username: ~Phu-Vinh_Nguyen1
  - dblp_id: https://dblp.org/pid/192/3829
    emails: '****@gmail.com'
    first_name: Long
    google_scholar_id: https://scholar.google.com/citations?hl=en&authuser=1&user=jECXavQAAAAJ
    institution: Ho Chi Minh city University of Science, Vietnam National University
    last_name: Nguyen
    middle_name: HB
    name: Long HB Nguyen
    orcid: https://orcid.org/0000-0002-0884-1635
    username: ~Long_HB_Nguyen1
  - emails: '****@fit.hcmus.edu.vn'
    first_name: Dien
    homepage: http://www.clc.hcmus.edu.vn
    last_name: Dinh
    name: Dien Dinh
    username: ~Dien_Dinh1
  decision: toFindings
  end_page: 4198
  file: 1013.pdf
  id: 1013
  num_pages: 16
  openreview_id: qfCliTN908
  pdf_file: 55db8ace095caf499a96db50345591cfedc11e8e.pdf
  start_page: 4183
  title: 'ViGLUE: A Vietnamese General Language Understanding Benchmark and Analysis
    of Vietnamese Language Models'
- abstract: Saliency post-hoc explainability methods are important tools for understanding
    increasingly complex NLP models. While these methods can reflect the model's reasoning,
    they may not align with human intuition, making the explanations not plausible.
    In this work, we present a methodology for incorporating rationales, which are
    text annotations explaining human decisions, into text classification models.
    This incorporation enhances the plausibility of post-hoc explanations while preserving
    their faithfulness. Our approach is agnostic to model architectures and explainability
    methods. We introduce the rationales during model training by augmenting the standard
    cross-entropy loss with a novel loss function inspired by contrastive learning.
    By leveraging a multi-objective optimization algorithm, we explore the trade-off
    between the two loss functions and generate a Pareto-optimal frontier of models
    that balance performance and plausibility. Through extensive experiments involving
    diverse models, datasets, and explainability methods, we demonstrate that our
    approach significantly enhances the quality of model explanations without causing
    substantial (sometimes negligible) degradation in the original model's performance.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - dblp_id: https://dblp.org/pid/315/0757
    emails: '****@gmail.com'
    first_name: Lucas
    google_scholar_id: https://scholar.google.com.br/citations?user=ROLstoAAAAAJ
    homepage: https://lucasresck.github.io/
    institution: "Funda\xE7\xE3o Getulio Vargas"
    last_name: Resck
    middle_name: Emanuel
    name: Lucas Emanuel Resck
    orcid: https://orcid.org/0000-0001-9634-450X
    semantic_scholar_id: https://www.semanticscholar.org/author/Lucas-Emanuel-Resck/2164014168
    username: ~Lucas_Emanuel_Resck1
  - emails: '****@gmail.com'
    first_name: Marcos
    google_scholar_id: https://scholar.google.com.br/citations?user=ibe0jfQAAAAJ&hl=pt-BR
    institution: Universidade Estadual de Campinas
    last_name: M. Raimundo
    name: Marcos M. Raimundo
    username: ~Marcos_M._Raimundo1
  - dblp_id: https://dblp.org/pid/55/9845
    emails: '****@gmail.com'
    first_name: Jorge
    google_scholar_id: https://scholar.google.com/citations?user=S_88vX4AAAAJ&hl=en
    homepage: http://visualdslab.com/~jpocom/
    institution: "Funda\xE7\xE3o Getulio Vargas"
    last_name: Poco
    name: Jorge Poco
    orcid: https://orcid.org/0000-0001-9096-6287
    semantic_scholar_id: https://www.semanticscholar.org/author/Jorge-Poco/1998171
    username: ~Jorge_Poco1
  decision: toFindings
  end_page: 4225
  file: 1019.pdf
  id: 1019
  num_pages: 27
  openreview_id: 7W3xchroVH
  pdf_file: 1d2870d765099fa27f9592a00be992234de297df.pdf
  start_page: 4199
  title: Exploring the Trade-off Between Model Performance and Explanation Plausibility
    of Text Classifiers Using Human Rationales
- abstract: Parameter-efficient fine-tuning (PEFT) methods are increasingly vital
    in adapting large-scale pre-trained language models for diverse tasks, offering
    a balance between adaptability and computational efficiency. They are important
    in Low-Resource Language (LRL) Neural Machine Translation (NMT) to enhance translation
    accuracy with minimal resources. However, their practical effectiveness varies
    significantly across different languages. We conducted comprehensive empirical
    experiments with varying LRL domains and sizes to evaluate the performance of
    8 PEFT methods with in total of 15 architectures using the SacreBLEU score. We
    showed that 6 PEFT architectures outperform the baseline for both in-domain and
    out-domain tests and the Houlsby+Inversion adapter has the best performance overall,
    proving the effectiveness of PEFT methods.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Efficient/Low-Resource Methods for NLP
  authors:
  - emails: '****@mail.utoronto.ca'
    first_name: Tong
    homepage: https://sue-tong.github.io/
    last_name: Su
    name: Tong Su
    username: ~Tong_Su3
  - emails: '****@mail.utoronto.ca'
    first_name: Xin
    last_name: Peng
    name: Xin Peng
    username: ~Xin_Peng7
  - emails: '****@coli.uni-saarland.de'
    first_name: Sarubi
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=KszUKukAAAAJ
    homepage: https://www.sarubi.com/
    institution: "Universit\xE4t des Saarlandes"
    last_name: Thillainathan
    name: Sarubi Thillainathan
    orcid: https://orcid.org/0000-0001-8229-2497
    username: ~Sarubi_Thillainathan1
  - emails: '****@mail.utoronto.ca'
    first_name: David
    homepage: https://davidguzmanr.github.io/
    last_name: "Guzm\xE1n"
    name: "David Guzm\xE1n"
    orcid: https://orcid.org/0009-0004-5158-017X
    username: "~David_Guzm\xE1n1"
  - dblp_id: https://dblp.uni-trier.de/pid/46/10481.html
    emails: '****@massey.ac.nz'
    first_name: Surangika
    google_scholar_id: https://scholar.google.com/citations?user=TB4RYToAAAAJ&hl=en&oi=ao
    homepage: https://www.massey.ac.nz/massey/expertise/college-staff-lists/college-of-sciences/all-staff/all-staff_home.cfm?stref=319722
    institution: Massey University
    last_name: Ranathunga
    name: Surangika Ranathunga
    orcid: https://orcid.org/0000-0003-0701-0204
    semantic_scholar_id: https://www.semanticscholar.org/author/Surangika-Ranathunga/2594452
    username: ~Surangika_Ranathunga1
  - emails: '****@utoronto.ca'
    first_name: En-Shiun
    google_scholar_id: https://scholar.google.com/citations?user=H84vuJ0AAAAJ&hl=en
    homepage: https://www.cs.toronto.edu/~ealee/
    last_name: Lee
    middle_name: Annie
    name: En-Shiun Annie Lee
    username: ~En-Shiun_Annie_Lee1
  decision: toFindings
  end_page: 4234
  file: 1028.pdf
  id: 1028
  num_pages: 9
  openreview_id: jZ1jBDMxpX
  pdf_file: b1b4f19637ced31d6f58d922c619d0465138c249.pdf
  start_page: 4226
  title: Unlocking Parameter-Efficient Fine-Tuning for Low-Resource Language Translation
- abstract: 'Large Language Models (LLMs) are increasingly being used for interactive
    decision-making tasks requiring planning and adapting to the environment. Recent
    works employ LLMs-as-agents in broadly two ways: iteratively determining the next
    action (iterative executors) or generating plans and executing sub-tasks using
    LLMs (plan-and-execute). However, these methods struggle with task complexity,
    as the inability to execute any sub-task may lead to task failure. To address
    these shortcomings, we introduce As-Needed Decomposition and Planning for complex
    Tasks (ADaPT), an approach that explicitly plans and decomposes complex sub-tasks
    as-needed, i.e., when the LLM is unable to execute them. ADaPT recursively decomposes
    sub-tasks to adapt to both task complexity and LLM capability. Our results demonstrate
    that ADaPT substantially outperforms established strong baselines, achieving success
    rates up to 28.3% higher in ALFWorld, 27% in WebShop, and 33% in TextCraft --
    a novel compositional dataset that we introduce. Through extensive analysis, we
    illustrate the importance of multilevel decomposition and establish that ADaPT
    dynamically adjusts to the capabilities of the executor LLM as well as to task
    complexity.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Learning for NLP
  authors:
  - dblp_id: https://dblp.org/pid/264/2812
    emails: '****@cs.unc.edu'
    first_name: Archiki
    google_scholar_id: https://scholar.google.com/citations?user=Svcwv-IAAAAJ&hl=en
    homepage: https://archiki.github.io/
    last_name: Prasad
    name: Archiki Prasad
    semantic_scholar_id: https://www.semanticscholar.org/author/Archiki-Prasad/1677896557
    username: ~Archiki_Prasad1
  - dblp_id: https://dblp.org/pid/52/3406
    emails: '****@coli.uni-saarland.de'
    first_name: Alexander
    homepage: http://www.coli.uni-saarland.de/~koller/
    institution: Saarland University
    last_name: Koller
    name: Alexander Koller
    semantic_scholar_id: https://www.semanticscholar.org/author/Alexander-Koller/145542037
    username: ~Alexander_Koller2
  - dblp_id: https://dblp.org/pid/206/6859
    emails: '****@gmail.com'
    first_name: Mareike
    google_scholar_id: https://scholar.google.de/citations?user=f-NzaE8AAAAJ&hl=de&oi=ao
    institution: "Universit\xE4t des Saarlandes"
    last_name: Hartmann
    name: Mareike Hartmann
    semantic_scholar_id: https://www.semanticscholar.org/author/Mareike-Hartmann/34604611
    username: ~Mareike_Hartmann1
  - dblp_id: https://dblp.org/pid/34/1184
    emails: '****@allenai.org'
    first_name: Peter
    institution: Allen Institute for Artificial Intelligence
    last_name: Clark
    name: Peter Clark
    username: ~Peter_Clark1
  - dblp_id: http://dblp.uni-trier.de/pers/hd/s/Sabharwal:Ashish
    emails: '****@allenai.org'
    first_name: Ashish
    google_scholar_id: https://scholar.google.com/citations?user=7VspfeAAAAAJ&hl=en
    homepage: http://allenai.org/team/ashishs
    institution: Allen Institute for Artificial Intelligence
    last_name: Sabharwal
    name: Ashish Sabharwal
    semantic_scholar_id: https://www.semanticscholar.org/author/Ashish-Sabharwal/48229640
    username: ~Ashish_Sabharwal1
  - dblp_id: https://dblp.org/pid/32/5243.html
    emails: '****@cs.unc.edu'
    first_name: Mohit
    google_scholar_id: https://scholar.google.com/citations?user=DN8QtscAAAAJ&hl=en
    homepage: https://www.cs.unc.edu/~mbansal/
    institution: University of North Carolina at Chapel Hill
    last_name: Bansal
    name: Mohit Bansal
    username: ~Mohit_Bansal2
  - dblp_id: https://dblp.org/pid/83/8117
    emails: '****@allenai.org'
    first_name: Tushar
    google_scholar_id: https://scholar.google.com/citations?user=_8mkIjgAAAAJ&hl=en&oi=ao
    homepage: https://allenai.org/team/tushark/
    institution: Allen Institute for Artificial Intelligence
    last_name: Khot
    name: Tushar Khot
    semantic_scholar_id: https://www.semanticscholar.org/author/Tushar-Khot/2236429
    username: ~Tushar_Khot1
  decision: toFindings
  end_page: 4261
  file: 1029.pdf
  id: 1029
  num_pages: 27
  openreview_id: KVvLqRJBo4
  pdf_file: 9b3fb652e3f4d4f1de8c5174e47b293171df751d.pdf
  start_page: 4235
  title: 'ADaPT: As-Needed Decomposition and Planning with Language Models'
- abstract: Machine Translation (MT) remains one of the last NLP tasks where large
    language models (LLMs) have not yet replaced dedicated supervised systems. This
    work exploits the complementary strengths of LLMs and supervised MT by guiding
    LLMs to automatically post-edit MT with external feedback on its quality, derived
    from Multidimensional Quality Metric (MQM) annotations. Working with LLaMA-2 models,
    we consider prompting strategies varying the nature of feedback provided and then
    fine-tune the LLM to improve its ability to exploit the provided guidance. Through
    experiments on Chinese-English, English-German, and English-Russian MQM data,
    we demonstrate that prompting LLMs to post-edit MT improves TER, BLEU and COMET
    scores, although the benefits of fine-grained feedback are not clear. Fine-tuning
    helps integrate fine-grained feedback more effectively and further improves translation
    quality based on both automatic and human evaluation.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Translation
  authors:
  - emails: '****@umd.edu'
    first_name: Dayeon
    google_scholar_id: https://scholar.google.com/citations?user=1-pevpcAAAAJ&hl=ko
    homepage: https://dayeonki.github.io/
    institution: University of Maryland, College Park
    last_name: Ki
    name: Dayeon Ki
    username: ~Dayeon_Ki1
  - dblp_id: https://dblp.org/pid/71/1827
    emails: '****@umd.edu'
    first_name: Marine
    google_scholar_id: https://scholar.google.com/citations?user=iPAX6jcAAAAJ
    homepage: http://www.cs.umd.edu/~marine/
    institution: University of Maryland, College Park
    last_name: Carpuat
    name: Marine Carpuat
    username: ~Marine_Carpuat1
  decision: toFindings
  end_page: 4282
  file: 1034.pdf
  id: 1034
  num_pages: 21
  openreview_id: YH6GNPN4RH
  pdf_file: ef244f2d1e64adb73f23eb4a158bb4cf5ea12eda.pdf
  start_page: 4262
  title: Guiding Large Language Models to Post-Edit Machine Translation with Error
    Annotations
- abstract: Sample contrastive methods, typically referred to simply as contrastive
    are the foundation of most unsupervised methods to learn text and sentence embeddings.
    On the other hand, a different class of self-supervised non-contrastive loss functions
    and methods have been considered in the computer vision community and referred
    to as dimension contrastive. In this paper, we thoroughly compare this class of
    methods with the standard baseline for contrastive sentence embeddings, SimCSE.
    We find that self-supervised embeddings trained using dimension contrastive objectives
    can outperform SimCSE on downstream tasks without needing auxiliary loss functions.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Learning for NLP
  authors:
  - emails: '****@gmail.com'
    first_name: Duccio
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=_AezOKYAAAAJ
    institution: Bloomberg
    last_name: Pappadopulo
    name: Duccio Pappadopulo
    username: ~Duccio_Pappadopulo1
  - emails: '****@gmail.com'
    first_name: Marco
    google_scholar_id: https://scholar.google.com/citations?user=sZMy8iUAAAAJ&hl=en
    institution: Bloomberg
    last_name: Farina
    name: Marco Farina
    orcid: https://orcid.org/0000-0001-8500-0138
    semantic_scholar_id: https://www.semanticscholar.org/author/M.-Farina/50405470
    username: ~Marco_Farina1
  decision: toFindings
  end_page: 4293
  file: 1038.pdf
  id: 1038
  num_pages: 11
  openreview_id: 2bivvE4qNI
  pdf_file: 17cb126a6368621debcc64c5823262a018ffe4d8.pdf
  start_page: 4283
  title: Non-contrastive sentence representations via self-supervision
- abstract: 'Language-vision models like CLIP have made significant strides in vision
    tasks, such as zero-shot image classification (ZSIC). However, generating specific
    and expressive visual descriptions remains challenging; descriptions produced
    by current methods are often ambiguous and lacking in granularity. To tackle these
    issues, we propose V-GLOSS: Visual Glosses, a novel method built upon two key
    ideas. The first is Semantic Prompting, which conditions a language model on structured
    semantic knowledge. The second is a new contrastive algorithm that elicits fine-grained
    distinctions between similar concepts. With both ideas, we demonstrate that V-GLOSS
    improves visual descriptions and achieves strong results in the zero-shot setting
    on general and fine-grained image-classification datasets, including ImageNet,
    STL-10, FGVC Aircraft, and Flowers 102. Moreover, these descriptive capabilities
    contribute to enhancing image-generation performance. Finally, we introduce a
    quality-tested silver dataset with descriptions generated with V-GLOSS for all
    ImageNet classes.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multimodality and Language Grounding to Vision, Robotics and Beyond
  authors:
  - emails: '****@ualberta.ca'
    first_name: Michael
    homepage: https://mikeogezi.com
    last_name: Ogezi
    name: Michael Ogezi
    username: ~Michael_Ogezi1
  - dblp_id: https://dblp.org/pid/127/6967
    emails: '****@ualberta.ca'
    first_name: Bradley
    google_scholar_id: https://scholar.google.ca/citations?user=F3ZNBV4AAAAJ
    homepage: https://webdocs.cs.ualberta.ca/~bmhauer/
    institution: University of Alberta
    last_name: Hauer
    name: Bradley Hauer
    semantic_scholar_id: https://www.semanticscholar.org/author/B.-Hauer/39918547
    username: ~Bradley_Hauer1
  - dblp_id: https://dblp.org/pid/40/3774
    emails: '****@ualberta.ca'
    first_name: Grzegorz
    google_scholar_id: https://scholar.google.com.tw/citations?user=TV3Tl_sAAAAJ
    homepage: http://webdocs.cs.ualberta.ca/~kondrak/
    institution: University of Alberta
    last_name: Kondrak
    name: Grzegorz Kondrak
    semantic_scholar_id: https://www.semanticscholar.org/author/Grzegorz-Kondrak/46474310
    username: ~Grzegorz_Kondrak1
  decision: toFindings
  end_page: 4311
  file: 1039.pdf
  id: 1039
  num_pages: 18
  openreview_id: fw5Sv3Lkr4
  pdf_file: a5ac5af28a92b24b30f5b4b82a5123f3ffe10abd.pdf
  start_page: 4294
  title: Semantically-Prompted Language Models Improve Visual Descriptions
- abstract: 'The rapid advancements in large language models (LLMs) have ignited interest
    in the temporal knowledge graph (tKG) domain, where conventional embedding-based
    and rule-based methods dominate. The question remains open of whether pre-trained
    LLMs can understand structured temporal relational data and replace them as the
    foundation model for temporal relational forecasting. Therefore, we bring temporal
    knowledge forecasting into the generative setting. However, challenges occur in
    the huge chasms between complex temporal graph data structure and sequential natural
    expressions LLMs can handle, and between the enormous data sizes of tKGs and heavy
    computation costs of finetuning LLMs. To address these challenges, we propose
    a novel retrieval-augmented generation framework named GenTKG combining a temporal
    logical rule-based retrieval strategy and few-shot parameter-efficient instruction
    tuning to solve the above challenges, respectively. Extensive experiments have
    shown that GenTKG outperforms conventional methods of temporal relational forecasting
    with low computation resources using extremely limited training data as few as
    16 samples. GenTKG also highlights remarkable cross-domain generalizability with
    outperforming performance on unseen datasets without re-training, and in-domain
    generalizability regardless of time split in the same dataset. Our work reveals
    the huge potential of LLMs in the tKG domain and opens a new frontier for generative
    forecasting on tKGs. The code and data are released here:  \url{https://github.com/mayhugotong/GenTKG}.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Learning for NLP
  authors:
  - emails: '****@outlook.com'
    first_name: Ruotong
    last_name: Liao
    name: Ruotong Liao
    username: ~Ruotong_Liao1
  - emails: '****@outlook.com'
    first_name: Xu
    homepage: https://www.cit.tum.de/en/cit/home/
    last_name: Jia
    name: Xu Jia
    username: ~Xu_Jia3
  - emails: '****@tum.de'
    first_name: Yangzhe
    institution: "Technische Universit\xE4t M\xFCnchen"
    last_name: Li
    name: Yangzhe Li
    username: ~Yangzhe_Li1
  - dblp_id: https://dblp.org/pid/199/8143.html
    emails: '****@gmail.com'
    first_name: Yunpu
    google_scholar_id: https://scholar.google.com/citations?user=fj5DzgcAAAAJ&hl=en
    homepage: https://dblp.org/pid/199/8143.html
    institution: Siemens Corporate Research
    last_name: Ma
    name: Yunpu Ma
    username: ~Yunpu_Ma1
  - dblp_id: http://dblp.uni-trier.de/pers/hd/t/Tresp:Volker
    emails: '****@gmail.com'
    first_name: Volker
    google_scholar_id: https://scholar.google.com/citations?user=xIJHTUwAAAAJ&hl=en
    homepage: http://www.tresp.org
    institution: Ludwig Maximilian University of Munich and Siemens Corporate Research
    last_name: Tresp
    name: Volker Tresp
    orcid: https://orcid.org/0000-0001-9428-3686
    semantic_scholar_id: https://www.semanticscholar.org/author/Volker-Tresp/1700754
    username: ~Volker_Tresp1
  decision: toFindings
  end_page: 4326
  file: 1041.pdf
  id: 1041
  num_pages: 15
  openreview_id: oGbwVmlANQ
  pdf_file: 33bfef061174fd1b2a3837bb0f887ab9184dc792.pdf
  start_page: 4312
  title: 'GenTKG: Generative Forecasting on Temporal Knowledge Graph with Large Language
    Models'
- abstract: ''
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Learning for NLP
  authors:
  - dblp_id: https://dblp.org/pid/299/1900
    emails: '****@ai.ethz.ch'
    first_name: Jiaoda
    homepage: https://ai.ethz.ch/people/jiaoda-li.html
    institution: ETHZ - ETH Zurich
    last_name: Li
    name: Jiaoda Li
    orcid: https://orcid.org/0000-0002-7691-4269
    semantic_scholar_id: https://www.semanticscholar.org/author/Jiaoda-Li/2124945496
    username: ~Jiaoda_Li1
  - emails: '****@cam.ac.uk'
    first_name: Jennifer
    google_scholar_id: https://scholar.google.co.uk/citations?user=HR7sOUMAAAAJ
    homepage: https://jennifercw.github.io/
    last_name: White
    middle_name: C.
    name: Jennifer C. White
    username: ~Jennifer_C._White1
  - dblp_id: https://dblp.org/pid/86/10440.html
    emails: '****@inf.ethz.ch'
    first_name: Mrinmaya
    google_scholar_id: https://scholar.google.com/citations?user=Tpp9ZjoAAAAJ&hl=en
    homepage: https://sites.google.com/site/mrinsachan/
    institution: Swiss Federal Institute of Technology
    last_name: Sachan
    name: Mrinmaya Sachan
    username: ~Mrinmaya_Sachan3
  - dblp_id: https://dblp.org/pid/146/4361.html
    emails: '****@gmail.com'
    first_name: Ryan
    google_scholar_id: https://scholar.google.com/citations?user=DexOqtoAAAAJ&hl=en
    homepage: https://rycolab.io/
    institution: Swiss Federal Institute of Technology
    last_name: Cotterell
    name: Ryan Cotterell
    semantic_scholar_id: https://www.semanticscholar.org/author/Ryan-Cotterell/1750769
    username: ~Ryan_Cotterell1
  decision: toFindings
  end_page: 4339
  file: 1049.pdf
  id: 1049
  num_pages: 13
  openreview_id: Mr12SN1Tcs
  pdf_file: bfe6fb3ae5a7df8f34ba68f4d08d56a87c2e6ebf.pdf
  start_page: 4327
  title: A Transformer Language Model with Stack Attention
- abstract: In-context learning (ICL) performs tasks by prompting a large language
    model (LLM) using an instruction and a small set of annotated examples called
    demonstrations. Recent work has shown that precise details of the inputs used
    in the ICL prompt significantly impact performance, which has incentivized instruction
    selection algorithms. The effect of instruction-choice however is severely underexplored,
    with existing analyses restricted to shallow subsets of models and tasks, limiting
    the generalizability of their insights. We develop InstructEval, an ICL evaluation
    suite to conduct a thorough assessment of these techniques. The suite includes
    13 open-sourced LLMs of varying scales from four model families, and covers nine
    tasks across three categories. Using the suite, we evaluate the relative performance
    of seven popular instruction selection methods over five metrics relevant to ICL.
    Our experiments reveal that using curated manually-written instructions or simple
    instructions without any task-specific descriptions often elicits superior ICL
    performance overall than that of automatic instruction-induction methods, pointing
    to a lack of generalizability among the latter. We release our evaluation suite
    (at https://github.com/princeton-nlp/InstructEval) for benchmarking instruction
    selection approaches and enabling more generalizable methods in this space.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Learning for NLP
  authors:
  - dblp_id: https://dblp.org/pid/348/5792
    emails: '****@princeton.edu'
    first_name: Anirudh
    google_scholar_id: https://scholar.google.com/citations?user=KarsBWAAAAAJ
    homepage: https://anirudhajith.github.io
    last_name: Ajith
    name: Anirudh Ajith
    semantic_scholar_id: https://www.semanticscholar.org/author/Anirudh-Ajith/2218438150
    username: ~Anirudh_Ajith1
  - emails: '****@gmail.com'
    first_name: Chris
    last_name: Pan
    name: Chris Pan
    username: ~Chris_Pan1
  - dblp_id: https://dblp.org/pid/241/9329
    emails: '****@princeton.edu'
    first_name: Mengzhou
    google_scholar_id: https://scholar.google.com/citations?user=zyJn1IcAAAAJ&hl=en
    homepage: https://xiamengzhou.github.io/
    last_name: Xia
    name: Mengzhou Xia
    username: ~Mengzhou_Xia1
  - dblp_id: https://dblp.org/pid/220/4337
    emails: '****@princeton.edu'
    first_name: Ameet
    google_scholar_id: https://scholar.google.com/citations?user=332L1coAAAAJ&hl=en
    homepage: https://ameet-1997.github.io
    last_name: Deshpande
    name: Ameet Deshpande
    semantic_scholar_id: https://www.semanticscholar.org/author/Ameet-Deshpande/33341943
    username: ~Ameet_Deshpande1
  - dblp_id: https://dblp.org/pid/147/0322
    emails: '****@princeton.edu'
    first_name: Karthik
    google_scholar_id: https://scholar.google.com/citations?user=euc0GX4AAAAJ&hl=en
    homepage: http://www.karthiknarasimhan.com
    institution: Princeton University
    last_name: Narasimhan
    middle_name: R
    name: Karthik R Narasimhan
    semantic_scholar_id: https://www.semanticscholar.org/author/Karthik-Narasimhan/144958935
    username: ~Karthik_R_Narasimhan1
  decision: toFindings
  end_page: 4354
  file: 1050.pdf
  id: 1050
  num_pages: 15
  openreview_id: P0SqqrdalZ
  pdf_file: ed7a0b44f9fedd00150553f12afc12079e9f369a.pdf
  start_page: 4340
  title: 'InstructEval: Systematic Evaluation of Instruction Selection Methods'
- abstract: "While the recommendation system (RS) has advanced significantly through\
    \ deep learning, current RS approaches usually train and fine-tune models on task-specific\
    \ datasets, limiting their generalizability to new recommendation tasks and their\
    \ ability to leverage external knowledge due to model scale and data size constraints.\
    \ Thus, we designed an LLM-powered autonomous recommender agent, RecMind, which\
    \ is capable of leveraging external knowledge, utilizing tools with careful planning\
    \ to provide zero-shot personalized recommendations. We propose a Self-Inspiring\
    \ algorithm to improve the planning ability. At each intermediate step, the LLM\
    \ \u201Cself-inspires\u201D to consider all previously explored states to plan\
    \ for the next step. This mechanism greatly improves the model\u2019s ability\
    \ to comprehend and utilize historical information in planning for recommendation.\
    \ We evaluate RecMind's performance in various recommendation scenarios. Our experiment\
    \ shows that RecMind outperforms existing zero/few-shot LLM-based recommendation\
    \ baseline methods in various tasks and achieves comparable performance to a fully\
    \ trained recommendation model P5."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - dblp_id: https://dblp.org/pid/89/2805
    emails: '****@asu.edu'
    first_name: Yancheng
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=U5QZhDAAAAAJ
    institution: Arizona State University
    last_name: Wang
    name: Yancheng Wang
    username: ~Yancheng_Wang2
  - dblp_id: https://dblp.org/pid/02/10386
    emails: '****@gmail.com'
    first_name: Ziyan
    google_scholar_id: https://scholar.google.com/citations?user=wDJjd2IAAAAJ&hl=en
    homepage: https://xmhzz2018.github.io/
    institution: Amazon
    last_name: Jiang
    name: Ziyan Jiang
    orcid: https://orcid.org/0009-0005-4955-0737
    username: ~Ziyan_Jiang1
  - dblp_id: https://dblp.org/pid/33/2592-10
    emails: '****@drexel.edu'
    first_name: Zheng
    institution: Amazon
    last_name: Chen
    name: Zheng Chen
    username: ~Zheng_Chen5
  - emails: '****@gmail.com'
    first_name: Fan
    google_scholar_id: https://scholar.google.com/citations?user=S2kHIhMAAAAJ&hl=en
    institution: Amazon
    last_name: Yang
    name: Fan Yang
    username: ~Fan_Yang61
  - emails: '****@umn.edu'
    first_name: Yingxue
    google_scholar_id: https://scholar.google.com/citations?user=EEm_z9YAAAAJ&hl=en#
    homepage: https://sites.google.com/umn.edu/zhou0877/home
    institution: University of Minnesota, Minneapolis
    last_name: Zhou
    name: Yingxue Zhou
    username: ~Yingxue_Zhou1
  - dblp_id: https://dblp.org/pid/126/8748
    emails: '****@amazon.com'
    first_name: Eunah
    google_scholar_id: https://scholar.google.com/citations?user=QHOk-eUAAAAJ&hl=en
    last_name: Cho
    name: Eunah Cho
    username: ~Eunah_Cho1
  - emails: '****@gmail.com'
    first_name: Xing
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=W_NygtQAAAAJ&view_op=list_works&sortby=pubdate
    last_name: Fan
    name: Xing Fan
    username: ~Xing_Fan1
  - emails: '****@gmail.com'
    first_name: Yanbin
    google_scholar_id: https://scholar.google.com/citations?user=kaJrcokAAAAJ&hl=en&oi=sra
    last_name: Lu
    name: Yanbin Lu
    username: ~Yanbin_Lu1
  - dblp_id: https://dblp.org/pid/73/5550
    emails: '****@gmail.com'
    first_name: Xiaojiang
    last_name: Huang
    name: Xiaojiang Huang
    username: ~Xiaojiang_Huang1
  - dblp_id: http://dblp.uni-trier.de/pers/hd/y/Yang:Yingzhen
    emails: '****@gmail.com'
    first_name: Yingzhen
    homepage: http://yingzhenyang.com
    institution: Arizona State University
    last_name: Yang
    name: Yingzhen Yang
    username: ~Yingzhen_Yang1
  decision: toFindings
  end_page: 4368
  file: 1052.pdf
  id: 1052
  num_pages: 14
  openreview_id: Jpw0XnWEzU
  pdf_file: 3ade4e4580c8d08a3e2c580ca8c2cc21f25cd966.pdf
  start_page: 4355
  title: 'RecMind: Large Language Model Powered Agent For Recommendation'
- abstract: Knowledge distillation from LLMs is essential for the efficient deployment
    of language models. Prior works have proposed data generation using LLMs for preparing
    distilled models. We argue that generating data with LLMs is prone to sampling
    mainly from the center of original content distribution. This limitation hinders
    the distilled model from learning the true underlying data distribution and to
    forget the tails of the distributions (samples with lower probability). To this
    end, we propose GOLD, a task-agnostic data generation and knowledge distillation
    framework, which employs an iterative out-of-distribution-guided feedback mechanism
    for the LLM. As a result, the generated data improves the generalizability of
    distilled models. An energy-based OOD evaluation approach is also introduced to
    deal with noisy generated data. Our extensive experiments on 10 different classification
    and sequence-to-sequence tasks in NLP show that GOLD respectively outperforms
    prior arts and the LLM with an average improvement of 5% and 14%. We will also
    show that the proposed method is applicable to less explored and novel tasks.
    Code is available in the Appendix.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Learning for NLP
  authors:
  - dblp_id: https://dblp.org/pid/243/8520
    emails: '****@ece.ubc.ca'
    first_name: Mohsen
    google_scholar_id: https://scholar.google.com/citations?user=6zlnAJgAAAAJ&hl=en
    last_name: Gholami
    name: Mohsen Gholami
    username: ~Mohsen_Gholami1
  - emails: '****@huawei.com'
    first_name: Mohammad
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=-88eCXIAAAAJ
    institution: Huawei Technologies Ltd.
    last_name: Akbari
    name: Mohammad Akbari
    username: ~Mohammad_Akbari3
  - emails: '****@huawei.com'
    first_name: Tianxi
    homepage: https://github.com/tianxic-hu/
    institution: Huawei Technologies Ltd.
    last_name: Hu
    name: Tianxi Hu
    username: ~Tianxi_Hu1
  - dblp_id: https://dblp.org/pid/199/5404
    emails: '****@cs.ubc.ca'
    first_name: Vaden
    google_scholar_id: https://scholar.google.ca/citations?user=3m_6zUEAAAAJ&hl=en
    homepage: https://vmasrani.github.io/
    last_name: Masrani
    name: Vaden Masrani
    username: ~Vaden_Masrani1
  - dblp_id: https://dblp.org/pid/13/3672-1
    emails: '****@ece.ubc.ca'
    first_name: Z.
    google_scholar_id: https://scholar.google.ca/citations?user=W75uTm8AAAAJ&hl=en
    homepage: https://www.ece.ubc.ca/~zjanew
    institution: University of British Columbia
    last_name: Wang
    middle_name: Jane
    name: Z. Jane Wang
    username: ~Z._Jane_Wang1
  - dblp_id: https://dblp.org/pid/66/4615-4
    emails: '****@huawei.com'
    first_name: Yong
    google_scholar_id: https://scholar.google.com/citations?user=K2zamrwAAAAJ&hl=en
    homepage: https://sites.google.com/site/yongzhangai
    institution: Huawei Technologies Ltd.
    last_name: Zhang
    name: Yong Zhang
    orcid: https://orcid.org/0000-0002-0238-0719
    username: ~Yong_Zhang2
  decision: toFindings
  end_page: 4384
  file: 1053.pdf
  id: 1053
  num_pages: 16
  openreview_id: ipAeOYVRoo
  pdf_file: a5955712b62676bbd241f540f395aeaa9bd902d2.pdf
  start_page: 4369
  title: 'GOLD: Generalized Knowledge Distillation via Out-of-Distribution-Guided
    Language Data Generation'
- abstract: In contemporary machine learning approaches to bilingual lexicon induction
    (BLI), a model learns a mapping between the embedding spaces of a language pair.
    Recently, retrieve-and-rank approach to BLI has achieved state of the art results
    on the task. However, the problem remains challenging in low-resource settings,
    due to the paucity of data. The task is complicated by factors such as lexical
    variation across languages. We argue that the incorporation of additional lexical
    information into the recent retrieve-and-rank approach should improve lexicon
    induction. We demonstrate the efficacy of our proposed approach on XLING, improving
    over the previous state of the art by an average of 2% across all language pairs.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Learning for NLP
  authors:
  - emails: '****@gmail.com'
    first_name: Harsh
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=PmkygQYAAAAJ
    homepage: https://harshkohli.github.io/
    institution: Ohio State University, Columbus
    last_name: Kohli
    name: Harsh Kohli
    orcid: https://orcid.org/0000-0003-1431-6025
    username: ~Harsh_Kohli1
  - emails: '****@amazon.com'
    first_name: Helian
    google_scholar_id: https://scholar.google.com/citations?user=CteYHh0AAAAJ&hl=en
    institution: Amazon
    last_name: Feng
    name: Helian Feng
    orcid: https://orcid.org/0000-0002-6296-1902
    username: ~Helian_Feng1
  - emails: '****@gmail.com'
    first_name: Nicholas
    google_scholar_id: https://scholar.google.com/citations?user=NNwlJ7cAAAAJ&hl=en
    institution: Lightmatter
    last_name: Dronen
    middle_name: Andrew
    name: Nicholas Andrew Dronen
    username: ~Nicholas_Andrew_Dronen1
  - dblp_id: https://dblp.org/pid/19/10661
    emails: '****@gmail.com'
    first_name: Calvin
    google_scholar_id: https://scholar.google.com/citations?user=l19Vwl8AAAAJ
    homepage: http://calvinmccarter.com/
    last_name: McCarter
    name: Calvin McCarter
    orcid: https://orcid.org/0000-0002-7257-1350
    semantic_scholar_id: https://www.semanticscholar.org/author/3278598
    username: ~Calvin_McCarter1
  - emails: '****@gmail.com'
    first_name: Sina
    homepage: https://github.com/sinamoeini
    last_name: Moeini
    name: Sina Moeini
    username: ~Sina_Moeini1
  - emails: '****@gmail.com'
    first_name: Ali
    last_name: Kebarighotbi
    name: Ali Kebarighotbi
    username: ~Ali_Kebarighotbi1
  decision: toFindings
  end_page: 4390
  file: 1055.pdf
  id: 1055
  num_pages: 6
  openreview_id: kzVKY0DmXT
  pdf_file: 8b4ef91b26fa7fa4a568d59bd96359b1fd798f03.pdf
  start_page: 4385
  title: How Lexical is Bilingual Lexicon Induction?
- abstract: ChatGPT has recently emerged as a powerful NLP tool that can carry out
    a variety of tasks. However, the range of languages ChatGPT can handle remains
    largely a mystery. To uncover which languages ChatGPT 'knows', we investigate
    its language identification (LID) abilities. For this purpose, we compile Babel-670,
    a benchmark comprising $670$ languages representing $23$ language families spoken
    in five continents. Languages in Babel-670 run the gamut from the very high-resource
    to the very low-resource. We then study ChatGPT's (both GPT-3.5 and GPT-4) ability
    to (i) identify  language names and language codes (ii) under zero- and few-shot
    conditions (iii) with and without provision of a label set. When compared to smaller
    finetuned LID tools, we find that ChatGPT lags behind. For example, it has poor
    performance on African languages. We conclude that current large language models
    would benefit from further development before they can sufficiently serve diverse
    communities.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multilinguality and Language Diversity
  authors:
  - emails: '****@student.ubc.ca'
    first_name: Wei-Rui
    homepage: https://weiruichen01.github.io/
    institution: University of British Columbia
    last_name: Chen
    name: Wei-Rui Chen
    username: ~Wei-Rui_Chen1
  - dblp_id: https://dblp.org/pid/167/9928
    emails: '****@ubc.ca'
    first_name: Ife
    google_scholar_id: https://scholar.google.co.id/citations?user=TXMQQngAAAAJ&hl=en&oi=ao
    institution: University of British Columbia
    last_name: Adebara
    name: Ife Adebara
    semantic_scholar_id: https://www.semanticscholar.org/author/Ife-Adebara/1983323
    username: ~Ife_Adebara1
  - emails: '****@mbzuai.ac.ae'
    first_name: Khai
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=7dAiPEEAAAAJ
    last_name: Doan
    middle_name: Duy
    name: Khai Duy Doan
    orcid: https://orcid.org/0009-0004-9601-944X
    username: ~Khai_Duy_Doan1
  - emails: '****@nyu.edu'
    first_name: Qisheng
    homepage: https://qishengl.github.io/
    last_name: Liao
    name: Qisheng Liao
    username: ~Qisheng_Liao1
  - dblp_id: https://dblp.org/pid/49/9389.html
    emails: '****@ubc.ca'
    first_name: Muhammad
    google_scholar_id: https://scholar.google.com/citations?user=SOjQhl8AAAAJ&hl=en&oi=ao
    homepage: https://mageed.arts.ubc.ca
    institution: University of British Columbia
    last_name: Abdul-Mageed
    name: Muhammad Abdul-Mageed
    orcid: https://orcid.org/0000-0002-8590-2040
    semantic_scholar_id: https://www.semanticscholar.org/author/Muhammad-Abdul-Mageed/1388437494
    username: ~Muhammad_Abdul-Mageed2
  decision: toFindings
  end_page: 4417
  file: 1057.pdf
  id: 1057
  num_pages: 27
  openreview_id: 2Kz2mVfAXg
  pdf_file: 0eb3703eb31df424ee1cadcc7edf76a316e15e8c.pdf
  start_page: 4391
  title: 'Fumbling in Babel: An Investigation into ChatGPT''s Language Identification
    Ability'
- abstract: Addressing the challenge of low-resource information extraction remains
    an ongoing issue due to the inherent information scarcity within limited training
    examples. Existing data augmentation methods, considered potential solutions,
    struggle to strike a balance between weak augmentation (e.g., synonym augmentation)
    and drastic augmentation (e.g., conditional generation without proper guidance).
    This paper introduces a novel paradigm that employs targeted augmentation and
    back validation to produce augmented examples with enhanced diversity, polarity,
    accuracy, and coherence. Extensive experimental results demonstrate the effectiveness
    of the proposed paradigm. Furthermore, identified limitations are discussed, shedding
    light on areas for future improvement.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Efficient/Low-Resource Methods for NLP
  authors:
  - emails: '****@vt.edu'
    first_name: Sijia
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=raDkYB8AAAAJ
    last_name: Wang
    name: Sijia Wang
    semantic_scholar_id: https://www.semanticscholar.org/author/Sijia-Wang/2116423012
    username: ~Sijia_Wang1
  - dblp_id: https://dblp.org/pid/127/0072
    emails: '****@gmail.com'
    first_name: Lifu
    google_scholar_id: https://scholar.google.com/citations?user=76IEGtYAAAAJ&hl=en
    homepage: https://wilburone.github.io/
    institution: Virginia Tech
    last_name: Huang
    name: Lifu Huang
    username: ~Lifu_Huang1
  decision: toFindings
  end_page: 4432
  file: 1058.pdf
  id: 1058
  num_pages: 15
  openreview_id: pFZzoLOoKD
  pdf_file: 47f132c4b4f4389f7afc45e140b806704c532b59.pdf
  start_page: 4418
  title: Targeted Augmentation for Low-Resource Event Extraction
- abstract: When a model is trying to gather information in an interactive setting,
    it benefits from asking informative questions. However, in the case of a grounded
    multi-turn image identification task, previous studies have been constrained to
    polar yes/no questions (White et al., 2021), limiting how much information the
    model can gain in a single turn. We present an approach that formulates more informative,
    open-ended questions. In doing so, we discover that off-the-shelf visual question
    answering (VQA) models often make presupposition errors, which standard information
    gain question selection methods fail to account for. To address this issue, we
    propose a method that can incorporate presupposition handling into both question
    selection and belief updates. Specifically, we use a two-stage process, where
    the model first filters out images which are irrelevant to a given question, then
    updates its beliefs about which image the user intends. Through self-play and
    human evaluations, we show that our method is successful in asking informative
    open-ended questions, increasing accuracy over the past state-of-the-art by 14%,
    while resulting in 48% more efficient games in human evaluations.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multimodality and Language Grounding to Vision, Robotics and Beyond
  authors:
  - dblp_id: https://dblp.org/pid/244/9561
    emails: '****@gmail.com'
    first_name: Sedrick
    google_scholar_id: https://scholar.google.com/citations?user=IMYgXsYAAAAJ&hl=en
    homepage: https://sedrickkeh.github.io
    institution: Toyota Research Institute
    last_name: Keh
    name: Sedrick Keh
    semantic_scholar_id: https://www.semanticscholar.org/author/Sedrick-Scott-Keh/150299584
    username: ~Sedrick_Keh1
  - dblp_id: https://dblp.org/pid/278/2437.html
    emails: '****@gmail.com'
    first_name: Justin
    google_scholar_id: https://scholar.google.com/citations?user=043r6toAAAAJ&hl=en
    institution: Cornell University
    last_name: Chiu
    middle_name: T
    name: Justin T Chiu
    username: ~Justin_T_Chiu1
  - dblp_id: https://dblp.org/pid/117/4804
    emails: '****@andrew.cmu.edu'
    first_name: Daniel
    google_scholar_id: https://scholar.google.com/citations?user=sJDqACEAAAAJ
    homepage: https://dpfried.github.io/
    institution: Carnegie Mellon University
    last_name: Fried
    name: Daniel Fried
    semantic_scholar_id: https://www.semanticscholar.org/author/Daniel-Fried/47070750
    username: ~Daniel_Fried1
  decision: toFindings
  end_page: 4446
  file: 1059.pdf
  id: 1059
  num_pages: 14
  openreview_id: LQxYWiaLID
  pdf_file: 819c340eb351c853d92d2271c4d213fb28e6a5ce.pdf
  start_page: 4433
  title: Asking More Informative Questions for Grounded Retrieval
- abstract: "In recent years, there has been a growing interest in utilizing external\
    \ knowledge to reduce hallucinations in large language models (LLMs) and provide\
    \ them with updated information. Despite this improvement, a major challenge lies\
    \ in the lack of explicit citations, which hampers the ability to verify the information\
    \ generated by these models.\nThis paper focuses on providing models with citation\
    \ capabilities efficiently. By constructing a dataset of citations, we train two\
    \ model architectures: an FID-style FLAN-T5 model for efficient answer composition\
    \ and a 13B \\llama model known for its success in instruction following after\
    \ tuning. \nEvaluation on fluency, correctness, and citation quality is conducted\
    \ through human assessment and the newly introduced Automatic LLMs' Citation Evaluation\
    \ (ALCE) benchmark.\nResults demonstrate significant improvements in answer quality\
    \ and efficiency, surpassing the performance of the popular ChatGPT on some of\
    \ the metrics. The models exhibit exceptional out-of-domain generalization in\
    \ both human and automatic evaluation. Notably, the FID-style FLAN-T5 model with\
    \ only 3B parameters performs impressively compared to the 13B \\llama model."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Efficient/Low-Resource Methods for NLP
  authors:
  - dblp_id: https://dblp.org/pid/124/3059.html
    emails: '****@gmail.com'
    first_name: Marzieh
    google_scholar_id: https://scholar.google.ca/citations?user=i1ZWWjEAAAAJ&hl=en
    last_name: Tahaei
    middle_name: S.
    name: Marzieh S. Tahaei
    username: ~Marzieh_S._Tahaei1
  - emails: '****@uwaterloo.ca'
    first_name: Aref
    google_scholar_id: https://scholar.google.com/citations?user=HlKxEOEAAAAJ&hl=en
    institution: University of Waterloo and Huawei Technologies Ltd.
    last_name: Jafari
    name: Aref Jafari
    semantic_scholar_id: https://www.semanticscholar.org/author/A.-Jafari/31036999
    username: ~Aref_Jafari1
  - dblp_id: https://dblp.org/pid/239/5145
    emails: '****@gmail.com'
    first_name: Ahmad
    google_scholar_id: https://scholar.google.ca/citations?user=YPbaQkQAAAAJ&hl=en
    homepage: https://ahmadrash.github.io/
    institution: University of Waterloo
    last_name: Rashid
    name: Ahmad Rashid
    username: ~Ahmad_Rashid1
  - emails: '****@gmail.com'
    first_name: David
    homepage: https://www-ens.iro.umontreal.ca/~alfonsda/
    last_name: Alfonso-Hermelo
    name: David Alfonso-Hermelo
    username: ~David_Alfonso-Hermelo1
  - emails: '****@gmail.com'
    first_name: Khalil
    institution: Huawei Technologies Ltd.
    last_name: Bibi
    name: Khalil Bibi
    username: ~Khalil_Bibi1
  - dblp_id: https://dblp.org/pid/135/8903
    emails: '****@gmail.com'
    first_name: Yimeng
    google_scholar_id: https://scholar.google.com/citations?user=TrTASWoAAAAJ&hl=en
    last_name: Wu
    name: Yimeng Wu
    orcid: https://orcid.org/0000-0002-0237-5995
    semantic_scholar_id: https://www.semanticscholar.org/author/Yimeng-Wu/2000863903
    username: ~Yimeng_Wu1
  - dblp_id: http://dblp.uni-trier.de/pers/hd/g/Ghodsi:Ali
    emails: '****@uwaterloo.ca'
    first_name: Ali
    google_scholar_id: https://scholar.google.com/citations?user=WXbhp_4AAAAJ&hl=en
    homepage: https://uwaterloo.ca/data-analytics/
    last_name: Ghodsi
    name: Ali Ghodsi
    username: ~Ali_Ghodsi1
  - dblp_id: https://dblp.org/pid/12/1081
    emails: '****@gmail.com'
    first_name: Boxing
    google_scholar_id: https://scholar.google.com/citations?user=LiINs3gAAAAJ&hl=en
    homepage: https://sites.google.com/site/chenboxing/Home
    institution: Huawei Technologies Ltd.
    last_name: Chen
    name: Boxing Chen
    orcid: https://orcid.org/0000-0002-3170-4858
    username: ~Boxing_Chen1
  - emails: '****@gmail.com'
    first_name: Mehdi
    google_scholar_id: https://scholar.google.com/citations?user=MvXlF6kAAAAJ
    last_name: Rezagholizadeh
    name: Mehdi Rezagholizadeh
    username: ~Mehdi_Rezagholizadeh1
  decision: toFindings
  end_page: 4454
  file: 1074.pdf
  id: 1074
  num_pages: 8
  openreview_id: QhPz0BfNeQ
  pdf_file: a01599f356f230692bdfecc5195a338d03138126.pdf
  start_page: 4447
  title: 'Efficient Citer: Tuning Large Language Models for Enhanced Answer Quality
    and Verification'
- abstract: Recent studies have highlighted the issue of Pretrained Language Models
    (PLMs) inadvertently propagating social stigmas and stereotypes, a critical concern
    given their widespread use. This is particularly problematic in sensitive areas
    like healthcare, where such biases could lead to detrimental outcomes. Our research
    addresses this by adapting two intrinsic bias benchmarks to quantify racial and
    LGBTQ+ biases in prevalent PLMs. We also empirically evaluate the effectiveness
    of various debiasing methods in mitigating these biases. Furthermore, we assess
    the impact of debiasing on both Natural Language Understanding and specific biomedical
    applications. Our findings reveal that while PLMs commonly exhibit healthcare-related
    racial and LGBTQ+ biases, the applied debiasing techniques successfully reduce
    these biases without compromising the models' performance in downstream tasks.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Ethics, Bias, and Fairness
  authors:
  - dblp_id: https://dblp.org/pid/360/0267
    emails: '****@dartmouth.edu'
    first_name: Sean
    last_name: Xie
    name: Sean Xie
    username: ~Sean_Xie1
  - dblp_id: https://dblp.org/pid/41/7507
    emails: '****@dartmouth.edu'
    first_name: Saeed
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=vkFXULkAAAAJ
    homepage: https://www.hassanpourlab.com
    institution: Dartmouth College
    last_name: Hassanpour
    name: Saeed Hassanpour
    username: ~Saeed_Hassanpour1
  - dblp_id: https://dblp.org/pid/01/1709
    emails: '****@dartmouth.edu'
    first_name: Soroush
    google_scholar_id: https://scholar.google.com/citations?user=45DAXkwAAAAJ&hl=en&oi=ao
    homepage: https://www.cs.dartmouth.edu/~soroush/
    institution: Dartmouth College
    last_name: Vosoughi
    name: Soroush Vosoughi
    orcid: https://orcid.org/0000-0002-2564-8909
    semantic_scholar_id: https://www.semanticscholar.org/author/Soroush-Vosoughi/1918441
    username: ~Soroush_Vosoughi1
  decision: toFindings
  end_page: 4468
  file: 1075.pdf
  id: 1075
  num_pages: 14
  openreview_id: mqftTv5PWj
  pdf_file: 935303475c9e8ef8316b45d59e6684503b18c1bf.pdf
  start_page: 4455
  title: Addressing Healthcare-related Racial and LGBTQ+ Biases in Pretrained Language
    Models
- abstract: "Humans can develop new theorems to explore broader and more complex mathematical\
    \ results.\nWhile current generative language models (LMs) have achieved significant\
    \ improvement in automatically proving theorems, their ability to generate new\
    \ or reusable theorems is still under-explored. Without the new theorems, current\
    \ LMs struggle to prove harder theorems that are distant from the given hypotheses\
    \ with the exponentially growing search space.\nMore advanced theorem proving\
    \ is if an agent (for instance, a generative LM) can leverage its creativity to\
    \ generate new but also reasonable theorems that properly substitute part of a\
    \ proof and also be saved as reusable knowledge for future theorem proving.\n\
    Therefore, this paper proposes an Automated Theorem Generation (ATG) benchmark\
    \ that evaluates whether an agent can automatically generate valuable (and possibly\
    \ brand new) theorems that are applicable for downstream theorem proving as reusable\
    \ knowledge. \nSpecifically, we construct the ATG benchmark by splitting the Metamath\
    \ library into three sets: axioms, library, and problem based on their proving\
    \ depth.\nWe conduct extensive experiments to investigate whether current LMs\
    \ can generate theorems in the library and benefit the problem theorems proving.\
    \ \nThe results demonstrate that high-quality ATG data facilitates models\u2019\
    \ performances on downstream ATP. \nHowever, there is still room for current LMs\
    \ to develop better ATG and generate more advanced and human-like theorems. \n\
    We hope the new ATG challenge can shed some light on advanced complex theorem\
    \ proving."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Generation
  authors:
  - emails: '****@mail2.sysu.edu.cn'
    first_name: Xiaohan
    homepage: https://github.com/XiaoHLim
    last_name: Lin
    name: Xiaohan Lin
    username: ~Xiaohan_Lin2
  - dblp_id: https://dblp.org/pid/149/7615
    emails: '****@mail2.sysu.edu.cn'
    first_name: Qingxing
    google_scholar_id: https://scholar.google.com/citations?user=flOBrd8AAAAJ&hl=en
    institution: SUN YAT-SEN UNIVERSITY, Tsinghua University
    last_name: Cao
    name: Qingxing Cao
    semantic_scholar_id: https://www.semanticscholar.org/author/Qingxing-Cao/2826839
    username: ~Qingxing_Cao1
  - dblp_id: https://dblp.org/pid/282/1562
    emails: '****@gmail.com'
    first_name: Yinya
    google_scholar_id: https://scholar.google.com/citations?user=dWStaRIAAAAJ&hl=zh-CN
    homepage: https://eleanor-h.github.io/
    last_name: Huang
    name: Yinya Huang
    orcid: https://orcid.org/0000-0002-0686-0832
    semantic_scholar_id: https://www.semanticscholar.org/author/Yinya-Huang/153268218
    username: ~Yinya_Huang1
  - dblp_id: https://dblp.org/pid/21/10358
    emails: '****@gmail.com'
    first_name: Zhicheng
    google_scholar_id: https://scholar.google.com/citations?user=ZvXicO8AAAAJ&hl=zh-CN
    homepage: https://yangzhch6.github.io/
    institution: Hong Kong University of Science and Technology (Guangzhou)
    last_name: Yang
    name: Zhicheng YANG
    orcid: https://orcid.org/0000-0002-7580-5183
    username: ~Zhicheng_YANG5
  - dblp_id: https://dblp.org/pid/241/1782
    emails: '****@huawei.com'
    first_name: Zhengying
    google_scholar_id: http:// https://scholar.google.com/citations?user=DFme0joAAAAJ
    institution: Huawei Technologies Ltd.
    last_name: Liu
    name: Zhengying Liu
    username: ~Zhengying_Liu2
  - dblp_id: https://dblp.org/pid/23/6479
    emails: '****@huawei.com'
    first_name: Zhenguo
    google_scholar_id: https://scholar.google.com/citations?user=XboZC1AAAAAJ&hl=en
    homepage: http://www.ee.columbia.edu/~zgli/
    institution: Department of Computer Science and Engineering, Hong Kong University
      of Science and Technology and Huawei Noah's Ark Lab
    last_name: Li
    name: Zhenguo Li
    username: ~Zhenguo_Li1
  - dblp_id: https://dblp.org/pid/136/0900
    emails: '****@gmail.com'
    first_name: Xiaodan
    google_scholar_id: https://scholar.google.com/citations?user=voxznZAAAAAJ&hl=en
    homepage: https://lemondan.github.io/
    last_name: Liang
    name: Xiaodan Liang
    username: ~Xiaodan_Liang2
  decision: toFindings
  end_page: 4484
  file: 1082.pdf
  id: 1082
  num_pages: 16
  openreview_id: H0RzzhAxTv
  pdf_file: 885a51d2ccd6ce864e39a4ae91efd4e175abeefb.pdf
  start_page: 4469
  title: 'ATG: Benchmarking Automated Theorem Generation for Generative Language Models'
- abstract: While large language models (LLMs) can already achieve strong performance
    on standard generic summarization benchmarks, their performance on more complex
    summarization task settings is less studied. Therefore, we benchmark LLMs on instruction
    controllable text summarization, where the model input consists of both a source
    article and a natural language requirement for desired summary characteristics.
    To this end, we curate an evaluation-only dataset for this task setting and conduct
    human evaluations of five LLM-based systems to assess their instruction-following
    capabilities in controllable summarization. We then benchmark LLM-based automatic
    evaluation for this task with 4 different evaluation protocols and 11 LLMs, resulting
    in 40 evaluation methods. Our study reveals that instruction controllable text
    summarization remains a challenging task for LLMs, since (1) all LLMs evaluated
    still make factual and other types of errors in their summaries; (2) no LLM-based
    evaluation methods can achieve a strong alignment with human annotators when judging
    the quality of candidate summaries; (3) different LLMs show large performance
    gaps in summary generation and evaluation capabilities. We make our collected
    benchmark InstruSum publicly available to facilitate future research in this direction.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Summarization
  authors:
  - dblp_id: https://dblp.org/pid/140/7348.html
    emails: '****@yale.edu'
    first_name: Yixin
    google_scholar_id: https://scholar.google.com/citations?user=sFtxaMkAAAAJ&hl=en
    homepage: https://yixinl7.github.io/
    institution: Yale University
    last_name: Liu
    name: Yixin Liu
    semantic_scholar_id: https://www.semanticscholar.org/author/Yixin-Liu/2108176413
    username: ~Yixin_Liu2
  - dblp_id: https://dblp.org/pid/203/8539
    emails: '****@salesforce.com'
    first_name: Alexander
    google_scholar_id: https://scholar.google.com/citations?user=GgfJdhwAAAAJ&hl=en
    homepage: https://alex-fabbri.github.io
    institution: SalesForce.com
    last_name: Fabbri
    name: Alexander Fabbri
    username: ~Alexander_Fabbri1
  - emails: '****@yale.edu'
    first_name: Jiawen
    last_name: Chen
    name: Jiawen Chen
    username: ~Jiawen_Chen3
  - dblp_id: https://dblp.org/pid/271/8391
    emails: '****@yale.edu'
    first_name: Yilun
    google_scholar_id: https://scholar.google.com/citations?hl=zh-CN&user=yAQAXrIAAAAJ
    homepage: https://yilunzhao.github.io/
    institution: Yale University
    last_name: Zhao
    name: Yilun Zhao
    semantic_scholar_id: https://www.semanticscholar.org/author/Yilun-Zhao/46316984
    username: ~Yilun_Zhao1
  - emails: '****@yale.edu'
    first_name: Simeng
    google_scholar_id: https://scholar.google.com/citations?user=D0dpploAAAAJ&hl=en
    homepage: https://shirleyhan6.github.io/
    institution: Yale University
    last_name: Han
    name: SIMENG HAN
    username: ~SIMENG_HAN1
  - dblp_id: https://dblp.org/pid/62/2078
    emails: '****@salesforce.com'
    first_name: Shafiq
    google_scholar_id: https://scholar.google.com/citations?user=hR249csAAAAJ&hl=en
    homepage: https://raihanjoty.github.io/
    institution: SalesForce.com and Nanyang Technological University
    last_name: Joty
    name: Shafiq Joty
    username: ~Shafiq_Joty1
  - dblp_id: https://dblp.org/pid/34/3381-3
    emails: '****@gmail.com'
    first_name: Pengfei
    google_scholar_id: https://scholar.google.com/citations?user=oIz_CYEAAAAJ&hl=en
    homepage: http://pfliu.com/
    last_name: Liu
    name: Pengfei Liu
    semantic_scholar_id: https://www.semanticscholar.org/author/Pengfei-Liu/144118452
    username: ~Pengfei_Liu1
  - dblp_id: https://dblp.org/pid/r/DragomirRRadev
    emails: '****@yale.edu'
    first_name: Dragomir
    google_scholar_id: https://scholar.google.com/citations?user=vIqWvgwAAAAJ&hl=en
    homepage: http://www.cs.yale.edu/~radev
    institution: Yale University
    last_name: Radev
    name: Dragomir Radev
    orcid: https://orcid.org/0000-0002-0213-7487
    semantic_scholar_id: https://www.semanticscholar.org/author/Dragomir-R.-Radev/9215251
    username: ~Dragomir_Radev2
  - dblp_id: https://dblp.org/pid/180/5537
    emails: '****@gmail.com'
    first_name: Chien-Sheng
    google_scholar_id: https://scholar.google.com/citations?user=1G4GV2EAAAAJ&hl=en&oi=ao
    homepage: http://jasonwu0731.github.io
    institution: Salesforce AI
    last_name: Wu
    name: Chien-Sheng Wu
    semantic_scholar_id: https://www.semanticscholar.org/author/Chien-Sheng-Wu/30340989
    username: ~Chien-Sheng_Wu1
  - dblp_id: https://dblp.org/pid/160/1727
    emails: '****@yale.edu'
    first_name: Arman
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=baI7IY0AAAAJ
    homepage: http://www.armancohan.com
    institution: Yale University and Allen Institute for Artificial Intelligence
    last_name: Cohan
    name: Arman Cohan
    semantic_scholar_id: https://www.semanticscholar.org/author/Arman-Cohan/2527954
    username: ~Arman_Cohan1
  decision: toFindings
  end_page: 4505
  file: 1087.pdf
  id: 1087
  num_pages: 21
  openreview_id: JRph4aeGLu
  pdf_file: 8a0486a9b4ab68e8bf6340f7cc44f3a59af66add.pdf
  start_page: 4485
  title: Benchmarking Generation and Evaluation Capabilities of Large Language Models
    for Instruction Controllable Summarization
- abstract: 'Comparative knowledge (e.g., steel is stronger and heavier than styrofoam)
    is an essential component of our world knowledge, yet understudied in prior literature.
    In this paper, we harvest the dramatic improvements in knowledge capabilities
    of language models into a large-scale comparative knowledge base. While the ease
    of acquisition of such comparative knowledge is much higher from extreme-scale
    models like GPT-4, compared to their considerably smaller and weaker counterparts
    such as GPT-2, not even the most powerful models are exempt from making errors.
    We thus ask: to what extent are models at different scales able to generate valid
    and diverse comparative knowledge?


    We introduce NeuroComparatives, a novel framework for comparative knowledge distillation
    overgenerated from language models such as GPT-variants and LLaMA, followed by
    stringent filtering of the generated knowledge. Our framework acquires comparative
    knowledge between everyday objects, producing a corpus of up to 8.8M comparisons
    over 1.74M entity pairs - 10X larger and 30% more diverse than existing resources.
    Moreover, human evaluations show that NeuroComparatives outperform existing resources
    in terms of validity (up to 32% absolute improvement). Our acquired NeuroComparatives
    leads to performance improvements on five downstream tasks.

    We find that neuro-symbolic manipulation of smaller models offers complementary
    benefits to the currently dominant practice of prompting extreme-scale language
    models for knowledge distillation.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - emails: '****@intel.com'
    first_name: Phillip
    google_scholar_id: https://scholar.google.com/citations?user=EKh822gAAAAJ&hl=en
    institution: Intel
    last_name: Howard
    name: Phillip Howard
    username: ~Phillip_Howard1
  - emails: '****@duke.edu'
    first_name: Junlin
    google_scholar_id: https://scholar.google.com/citations?user=s1Qw2pAAAAAJ&hl=en
    homepage: https://isthatyou.github.io/
    last_name: Wang
    name: Junlin Wang
    semantic_scholar_id: https://www.semanticscholar.org/author/Junlin-Wang/49606614
    username: ~Junlin_Wang1
  - emails: '****@intel.com'
    first_name: Vasudev
    google_scholar_id: https://scholar.google.com/citations?user=Qbu4oKwAAAAJ&hl=en&oi=ao
    institution: Intel
    last_name: Lal
    name: Vasudev Lal
    orcid: https://orcid.org/0000-0002-5907-9898
    semantic_scholar_id: https://www.semanticscholar.org/author/Vasudev-Lal/95340164
    username: ~Vasudev_Lal1
  - emails: '****@intel.com'
    first_name: Gadi
    homepage: https://www.intel.com
    last_name: Singer
    name: Gadi Singer
    username: ~Gadi_Singer1
  - dblp_id: https://dblp.org/pid/89/579
    emails: '****@cs.washington.edu'
    first_name: Yejin
    google_scholar_id: https://scholar.google.com/citations?user=vhP-tlcAAAAJ&hl=en
    homepage: https://homes.cs.washington.edu/~yejin/
    institution: Department of Computer Science, University of Washington
    last_name: Choi
    name: Yejin Choi
    semantic_scholar_id: https://www.semanticscholar.org/author/Yejin-Choi/1699545?sort=year
    username: ~Yejin_Choi1
  - dblp_id: https://dblp.org/pid/121/2036
    emails: '****@usc.edu'
    first_name: Swabha
    google_scholar_id: https://scholar.google.com/citations?user=3uTVQt0AAAAJ&hl=en
    homepage: http://swabhs.com/
    institution: University of Southern California
    last_name: Swayamdipta
    name: Swabha Swayamdipta
    semantic_scholar_id: https://www.semanticscholar.org/author/Swabha-Swayamdipta/2705113
    username: ~Swabha_Swayamdipta1
  decision: toFindings
  end_page: 4524
  file: 1091.pdf
  id: 1091
  num_pages: 19
  openreview_id: tupFzDEFcU
  pdf_file: 48f12b4daca553622d32a30084f9854b49d057c1.pdf
  start_page: 4506
  title: 'NeuroComparatives: Neuro-Symbolic Distillation of Comparative Knowledge'
- abstract: Emotion Recognition in Conversation (ERC) involves detecting the underlying
    emotion behind each utterance within a conversation. Effectively generating representations
    for utterances remains a significant challenge in this task. Recent works propose
    various models to address this issue, but they still struggle with differentiating
    similar emotions such as excitement and happiness. To alleviate this problem,
    We propose an Emotion-Anchored Contrastive Learning (EACL) framework that can
    generate more distinguishable utterance representations for similar emotions.
    To achieve this, we utilize label encodings as anchors to guide the learning of
    utterance representations and design an auxiliary loss to ensure the effective
    separation of anchors for similar emotions. Moreover, an additional adaptation
    process is proposed to adapt anchors to serve as effective classifiers to improve
    classification performance. Across extensive experiments, our proposed EACL achieves
    state-of-the-art emotion recognition performance and exhibits superior performance
    on similar emotions. Our code is available at https://github.com/Yu-Fangxu/EACL.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Dialogue and Interactive Systems
  authors:
  - dblp_id: https://dblp.org/pid/299/2017.html
    emails: '****@gmail.com'
    first_name: Fangxu
    google_scholar_id: https://scholar.google.com/citations?hl=zh-CN&user=AQjUv4YAAAAJ
    homepage: https://yu-fangxu.github.io/
    last_name: Yu
    name: Fangxu Yu
    username: ~Fangxu_Yu1
  - emails: '****@smail.nju.edu.cn'
    first_name: Junjie
    homepage: https://github.com/teal0range
    institution: Nanjing University
    last_name: Guo
    name: Junjie Guo
    username: ~Junjie_Guo1
  - dblp_id: https://dblp.org/pid/16/4485
    emails: '****@nju.edu.cn'
    first_name: Zhen
    google_scholar_id: https://scholar.google.com/citations?user=IoGlgtoAAAAJ&hl
    homepage: https://wuzhen247.github.io/
    institution: Nanjing University
    last_name: Wu
    name: Zhen Wu
    semantic_scholar_id: https://www.semanticscholar.org/author/Zhen-Wu/2146267846
    username: ~Zhen_Wu2
  - dblp_id: https://dblp.org/pid/39/5815
    emails: '****@nju.edu.cn'
    first_name: Xinyu
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=zpWB1CgAAAAJ&view_op=list_works&sortby=pubdate
    homepage: http://cs.nju.edu.cn/daixinyu
    institution: Nanjing University
    last_name: Dai
    name: Xinyu Dai
    semantic_scholar_id: https://www.semanticscholar.org/author/Xin-Yu-Dai/3035069
    username: ~Xinyu_Dai1
  decision: toFindings
  end_page: 4538
  file: 1094.pdf
  id: 1094
  num_pages: 14
  openreview_id: D1ACN0J082
  pdf_file: 328a6b2c9493b01cfc27fb63faac5b7aabcb3d25.pdf
  start_page: 4525
  title: Emotion-Anchored Contrastive Learning Framework for Emotion Recognition in
    Conversation
- abstract: "While most conversational agents are grounded on either free-text or\
    \ structured knowledge, many knowledge corpora consist of hybrid sources.\nThis\
    \ paper presents the first conversational agent that supports the full generality\
    \ of hybrid data access for large knowledge corpora, through a language we developed\
    \ called SUQL ($\\textbf{S}$tructured and $\\textbf{U}$nstructured $\\textbf{Q}$uery\
    \ $\\textbf{L}$anguage). Specifically, SUQL extends SQL with free-text primitives\
    \ (${\\small \\text{SUMMARY}}$ and ${\\small \\text{ANSWER}}$), so information\
    \ retrieval can be composed with structured data accesses arbitrarily in a formal,\
    \ succinct, precise, and interpretable notation. With SUQL, we propose the first\
    \ semantic parser, an LLM with in-context learning, that can handle hybrid data\
    \ sources.\n\nOur in-context learning-based approach, when applied to the HybridQA\
    \ dataset, comes within 8.9% Exact Match and 7.1% F1 of the SOTA, which was trained\
    \ on 62K data samples. \nMore significantly, unlike previous approaches, our technique\
    \ is applicable to large databases and free-text corpora. \n\nWe introduce a dataset\
    \ consisting of crowdsourced questions and conversations on Yelp, a large, real\
    \ restaurant knowledge base with structured and unstructured data. \nWe show that\
    \ our few-shot conversational agent based on SUQL finds an entity satisfying all\
    \ user requirements 90.3% of the time, compared to 63.4% for a baseline based\
    \ on linearization."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Dialogue and Interactive Systems
  authors:
  - emails: '****@cs.stanford.edu'
    first_name: Shicheng
    homepage: https://george1459.github.io/
    institution: Stanford University
    last_name: Liu
    name: Shicheng Liu
    orcid: https://orcid.org/0000-0003-2697-1916
    username: ~Shicheng_Liu2
  - dblp_id: https://dblp.org/pid/233/6929
    emails: '****@gmail.com'
    first_name: Jialiang
    google_scholar_id: https://scholar.google.com/citations?user=S_mgVngAAAAJ
    homepage: http://liamjxu.github.io
    last_name: Xu
    name: Jialiang Xu
    semantic_scholar_id: https://www.semanticscholar.org/author/Jialiang-Xu/9097644
    username: ~Jialiang_Xu1
  - emails: '****@stanford.edu'
    first_name: Wesley
    last_name: Tjangnaka
    name: Wesley Tjangnaka
    username: ~Wesley_Tjangnaka1
  - dblp_id: https://dblp.org/pid/274/1427
    emails: '****@stanford.edu'
    first_name: Sina
    google_scholar_id: https://scholar.google.com/citations?user=ECn_7SYAAAAJ
    homepage: https://web.stanford.edu/~sinaj/
    institution: Stanford University
    last_name: Semnani
    name: Sina Semnani
    orcid: https://orcid.org/0000-0002-1472-5788
    semantic_scholar_id: https://www.semanticscholar.org/author/Sina-J.-Semnani/1922611796
    username: ~Sina_Semnani1
  - emails: '****@cs.stanford.edu'
    first_name: Chen
    homepage: https://jayyu.xyz/
    last_name: Yu
    middle_name: Jie
    name: Chen Jie Yu
    username: ~Chen_Jie_Yu1
  - dblp_id: https://dblp.org/pid/l/MonicaSLam
    emails: '****@cs.stanford.edu'
    first_name: Monica
    google_scholar_id: https://scholar.google.com.tw/citations?user=fVaS7a8AAAAJ
    homepage: https://cs.stanford.edu/~lam/
    institution: Stanford University
    last_name: Lam
    name: Monica Lam
    username: ~Monica_Lam1
  decision: toFindings
  end_page: 4559
  file: 1097.pdf
  id: 1097
  num_pages: 21
  openreview_id: DoSQeeVlUO
  pdf_file: 678362f3ab920b6fa494b6de3e707f0bdebec612.pdf
  start_page: 4539
  title: 'SUQL: Conversational Search over Structured and Unstructured Data with Large
    Language Models'
- abstract: 'This study introduces a new long-form database question answering dataset
    designed to evaluate how Large Language Models (LLMs) interact with a SQL interpreter.
    The task necessitates LLMs to strategically generate multiple SQL queries to retrieve
    sufficient data from a database, to reason with the acquired context, and to synthesize
    them into a comprehensive analytical narrative. Our findings highlight that this
    task poses great challenges even for the state-of-the-art **GPT-4** model. We
    propose and evaluate two interaction strategies, and provide a fine-grained analysis
    of the individual stages within the interaction. A key discovery is the identification
    of two primary bottlenecks hindering effective interaction: the capacity for planning
    and the ability to generate multiple SQL queries. To address the challenge of
    accurately assessing answer quality, we introduce a multi-agent evaluation framework
    that simulates the academic peer-review process, enhancing the precision and reliability
    of our evaluations. This framework allows for a more nuanced understanding of
    the strengths and limitations of current LLMs in complex retrieval and reasoning
    tasks.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Question Answering
  authors:
  - emails: '****@yale.edu'
    first_name: Linyong
    google_scholar_id: https://scholar.google.com/citations?user=b-HaNvYAAAAJ&hl=en
    homepage: https://linyongnan.github.io/
    last_name: Nan
    name: Linyong Nan
    semantic_scholar_id: https://www.semanticscholar.org/author/Linyong-Nan/51990260
    username: ~Linyong_Nan1
  - emails: '****@yale.edu'
    first_name: Ellen
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=Oxair2UAAAAJ
    last_name: Zhang
    name: Ellen Zhang
    username: ~Ellen_Zhang1
  - dblp_id: https://dblp.org/pid/321/1052
    emails: '****@yale.edu'
    first_name: Weijin
    institution: LinkedIn
    last_name: Zou
    name: Weijin Zou
    semantic_scholar_id: https://www.semanticscholar.org/author/Weijin-Zou/2166311011
    username: ~Weijin_Zou1
  - dblp_id: https://dblp.org/pid/271/8391
    emails: '****@yale.edu'
    first_name: Yilun
    google_scholar_id: https://scholar.google.com/citations?hl=zh-CN&user=yAQAXrIAAAAJ
    homepage: https://yilunzhao.github.io/
    institution: Yale University
    last_name: Zhao
    name: Yilun Zhao
    semantic_scholar_id: https://www.semanticscholar.org/author/Yilun-Zhao/46316984
    username: ~Yilun_Zhao1
  - emails: '****@gmail.com'
    first_name: Wenfei
    homepage: https://www.linkedin.com/in/wenfei-zhou-981011162/
    institution: NVIDIA
    last_name: Zhou
    name: Wenfei Zhou
    username: ~Wenfei_Zhou1
  - dblp_id: https://dblp.org/pid/160/1727
    emails: '****@yale.edu'
    first_name: Arman
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=baI7IY0AAAAJ
    homepage: http://www.armancohan.com
    institution: Yale University and Allen Institute for Artificial Intelligence
    last_name: Cohan
    name: Arman Cohan
    semantic_scholar_id: https://www.semanticscholar.org/author/Arman-Cohan/2527954
    username: ~Arman_Cohan1
  decision: toFindings
  end_page: 4583
  file: 1099.pdf
  id: 1099
  num_pages: 24
  openreview_id: biO6OcViZu
  pdf_file: 891cdb8925da7e9dea7268e183016f35c5d17581.pdf
  start_page: 4560
  title: On Evaluating the Integration of Reasoning and Action in LLM Agents with
    Database Question Answering
- abstract: 'Extracting fine-grained experimental findings from literature can provide
    dramatic utility for scientific applications. Prior work has developed annotation
    schemas and datasets for limited aspects of this problem, failing to capture the
    real-world complexity and nuance required. Focusing on biomedicine, this work
    presents CARE---a new IE dataset for the task of extracting clinical findings.
    We develop a new annotation schema capturing fine-grained findings as n-ary relations
    between entities and attributes, which unifies phenomena challenging for current
    IE systems such as discontinuous entity spans, nested relations, variable arity
    n-ary relations and numeric results in a single schema. We collect extensive annotations
    for 700 abstracts from two sources: clinical trials and case reports. We also
    demonstrate the generalizability of our schema to the computer science and materials
    science domains. We benchmark state-of-the-art IE systems on CARE, showing that
    even models such as GPT4 struggle. We release our resources to advance research
    on extracting and aggregating literature findings.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Extraction
  authors:
  - dblp_id: https://dblp.org/pid/204/7137
    emails: '****@gmail.com'
    first_name: Aakanksha
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=izvklcoAAAAJ
    homepage: http://www.cs.cmu.edu/~anaik/
    institution: Allen Institute for Artificial Intelligence and National Institutes
      of Health
    last_name: Naik
    name: Aakanksha Naik
    semantic_scholar_id: https://www.semanticscholar.org/author/Aakanksha-Naik/23175870
    username: ~Aakanksha_Naik1
  - emails: '****@allenai.org'
    first_name: Bailey
    google_scholar_id: https://scholar.google.com/citations?user=1lzjTX0AAAAJ&hl=en&oi=ao
    last_name: Kuehl
    name: Bailey Kuehl
    semantic_scholar_id: https://www.semanticscholar.org/author/Bailey-Kuehl/2003338023
    username: ~Bailey_Kuehl1
  - emails: '****@allenai.org'
    first_name: Erin
    institution: Allen Institute for Artificial Intelligence
    last_name: Bransom
    name: Erin Bransom
    semantic_scholar_id: https://www.semanticscholar.org/author/Erin-Bransom/2203427167
    username: ~Erin_Bransom1
  - dblp_id: https://dblp.org/pid/57/5363
    emails: '****@eecs.northwestern.edu'
    first_name: Doug
    google_scholar_id: https://scholar.google.com/citations?user=E8evkcQAAAAJ&hl=en
    homepage: https://www.cs.northwestern.edu/~ddowney/
    institution: Allen Institute for Artificial Intelligence and Northwestern University
    last_name: Downey
    name: Doug Downey
    semantic_scholar_id: https://www.semanticscholar.org/author/Doug-Downey/145612610
    username: ~Doug_Downey1
  - dblp_id: https://dblp.org/pid/27/5588
    emails: '****@allenai.org'
    first_name: Tom
    institution: Allen Institute for Artificial Intelligence and Hebrew University,
      Hebrew University of Jerusalem
    last_name: Hope
    name: Tom Hope
    username: ~Tom_Hope2
  decision: toFindings
  end_page: 4600
  file: 1101.pdf
  id: 1101
  num_pages: 17
  openreview_id: Ewvza86xFv
  pdf_file: d820d42a1e50a3cfd8c0c1c22d89db07ca19a010.pdf
  start_page: 4584
  title: 'CARE: Extracting Experimental Findings From Clinical Literature'
- abstract: 'In this paper, we study personalized federated learning for text classification
    with Pretrained Language Models (PLMs). We identify two challenges in efficiently
    leveraging PLMs for personalized federated learning: 1) Communication. PLMs are
    usually large in size, e.g., with hundreds of millions of parameters, inducing
    huge communication cost in a federated setting. 2) Local Training. Training with
    PLMs generally requires back-propagation, during which memory consumption can
    be several times that of the forward-propagation. This may not be affordable when
    the PLMs are trained locally on the clients that are resource constrained, e.g.,
    mobile devices with limited access to memory resources. Additionally, the proprietary
    PLMs can be provided as concealed APIs, for which the back-propagation operations
    may not be available. In solving these, we propose a training framework that includes
    an approach of discrete local search for gradient-free local training, along with
    a compression mechanism inspired from the linear word analogy that allows communicating
    with discretely indexed tokens, thus significantly reducing the communication
    cost. Experiments show that our gradient-free framework achieves superior performance
    compared with baselines.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Efficient/Low-Resource Methods for NLP
  authors:
  - emails: '****@outlook.com'
    first_name: Rui
    google_scholar_id: https://scholar.google.com/citations?user=ia-rjLUAAAAJ&hl=en
    last_name: Wang
    name: Rui Wang
    username: ~Rui_Wang25
  - dblp_id: https://dblp.org/pid/32/1593-1
    emails: '****@gmail.com'
    first_name: Tong
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=6-ARmXsAAAAJ&view_op=list_works&sortby=pubdate
    institution: Adobe Research
    last_name: Yu
    name: Tong Yu
    semantic_scholar_id: https://www.semanticscholar.org/author/Tong-Yu/1500399016
    username: ~Tong_Yu3
  - dblp_id: https://dblp.org/pid/08/7975
    emails: '****@gmail.com'
    first_name: Ruiyi
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=-seCWbAAAAAJ
    homepage: http://zhangry868.github.io/
    institution: Adobe Systems
    last_name: Zhang
    name: Ruiyi Zhang
    username: ~Ruiyi_Zhang3
  - dblp_id: https://dblp.org/pid/61/1573
    emails: '****@live.co.kr'
    first_name: Sungchul
    institution: Adobe Systems
    last_name: Kim
    name: Sungchul Kim
    username: ~Sungchul_Kim1
  - dblp_id: https://dblp.org/pid/17/5085
    emails: '****@gmail.com'
    first_name: Ryan
    google_scholar_id: https://scholar.google.com/citations?user=_Dc6lbQAAAAJ&hl=en
    homepage: http://ryanrossi.com
    institution: Adobe Research
    last_name: Rossi
    middle_name: A.
    name: Ryan A. Rossi
    semantic_scholar_id: https://www.semanticscholar.org/author/Ryan-A.-Rossi/1862090
    username: ~Ryan_A._Rossi2
  - dblp_id: https://dblp.org/pers/z/Zhao:Handong.html
    emails: '****@gmail.com'
    first_name: Handong
    google_scholar_id: https://scholar.google.com/citations?user=0f-YOFgAAAAJ&hl=en&oi=ao
    homepage: https://hdzhao.github.io/
    institution: Adobe Systems
    last_name: Zhao
    name: Handong Zhao
    semantic_scholar_id: https://www.semanticscholar.org/author/Handong-Zhao/7574699
    username: ~Handong_Zhao3
  - dblp_id: https://dblp.org/pid/295/8249
    emails: '****@ucsd.edu'
    first_name: Junda
    google_scholar_id: https://scholar.google.com/citations?user=_iKeQFwAAAAJ&hl=en
    last_name: Wu
    name: Junda Wu
    semantic_scholar_id: https://www.semanticscholar.org/author/Junda-Wu/2146666303
    username: ~Junda_Wu1
  - dblp_id: https://dblp.org/pid/64/3670
    emails: '****@adobe.com'
    first_name: Subrata
    google_scholar_id: https://scholar.google.co.in/citations?user=k41NGc8AAAAJ&hl=en
    homepage: https://sites.google.com/site/subratamitraweb/
    institution: Adobe Systems
    last_name: Mitra
    name: Subrata Mitra
    username: ~Subrata_Mitra1
  - dblp_id: https://dblp.org/pid/56/6651-1
    emails: '****@unsw.edu.au'
    first_name: Lina
    google_scholar_id: https://scholar.google.com.au/citations?user=EU3snBgAAAAJ
    homepage: https://www.linayao.com/
    institution: University of New South Wales and CSIRO's Data61
    last_name: Yao
    name: Lina Yao
    username: ~Lina_Yao2
  - dblp_id: https://dblp.org/pid/27/3207
    emails: '****@duke.edu'
    first_name: Ricardo
    google_scholar_id: https://scholar.google.com/citations?user=p_mm4-YAAAAJ
    homepage: http://rhenaog.github.io
    institution: Duke University and King Abdullah University of Science and Technology
    last_name: Henao
    name: Ricardo Henao
    orcid: https://orcid.org/0000-0003-4980-845X
    semantic_scholar_id: https://www.semanticscholar.org/author/Ricardo-Henao/145153424
    username: ~Ricardo_Henao1
  decision: toFindings
  end_page: 4616
  file: 1102.pdf
  id: 1102
  num_pages: 16
  openreview_id: ngtEGB472w
  pdf_file: 6d354f944e4464eac4e235d8ae8fd169237ed2b6.pdf
  start_page: 4601
  title: Personalized Federated Learning for Text Classification with Gradient-Free
    Prompt Tuning
- abstract: "Knowledge base question generation (KBQG) aims to generate natural language\
    \ questions from a set of triplet facts extracted from KB. Existing methods have\
    \ significantly boosted the performance of KBQG via pre-trained language models\
    \ (PLMs) thanks to the richly endowed semantic knowledge. With the advance of\
    \ pre-training techniques, large language models (LLMs) (e.g., GPT-3.5) undoubtedly\
    \ possess much more semantic knowledge. Therefore, how to effectively organize\
    \ and exploit the abundant knowledge for KBQG becomes the focus of our study.\
    \ In this work, we propose SGSH --- a simple and effective framework to Stimulate\
    \ GPT-3.5 with Skeleton Heuristics to enhance KBQG. The framework incorporates\
    \ \"skeleton heuristics'', which provides more fine-grained guidance associated\
    \ with each input to stimulate LLMs to generate optimal questions, encompassing\
    \ essential elements like the question phrase and the auxiliary verb.\nMore specifically,\
    \ we devise an automatic data construction strategy leveraging ChatGPT to construct\
    \ a skeleton training dataset, based on which we employ a soft prompting approach\
    \ to train a BART model dedicated to generating the skeleton associated with each\
    \ input.\nSubsequently, skeleton heuristics are encoded into the prompt to incentivize\
    \ GPT-3.5 to generate desired questions. \nExtensive experiments demonstrate that\
    \ SGSH derives the new state-of-the-art performance on the KBQG tasks."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Generation
  authors:
  - emails: '****@ruc.edu.cn'
    first_name: Shasha
    google_scholar_id: https://scholar.google.com/citations?user=WVmbX48AAAAJ&hl=zh-CN&oi=ao
    homepage: https://github.com/PersistenceForever
    last_name: Guo
    name: Shasha Guo
    username: ~Shasha_Guo1
  - dblp_id: https://dblp.org/pid/149/1249
    emails: '****@gmail.com'
    first_name: Lizi
    google_scholar_id: https://scholar.google.com.sg/citations?user=W2b08EUAAAAJ&hl=en
    homepage: https://liziliao.github.io/
    institution: Singapore Management University
    last_name: Liao
    name: Lizi Liao
    username: ~Lizi_Liao1
  - dblp_id: https://dblp.org/pid/05/3499-1.html
    emails: '****@ruc.edu.cn'
    first_name: Jing
    google_scholar_id: https://scholar.google.com/citations?user=T7Wa3GQAAAAJ&hl=zh-CN
    homepage: https://xiaojingzi.github.io/
    last_name: Zhang
    name: Jing Zhang
    username: ~Jing_Zhang24
  - dblp_id: https://dblp.org/pid/89/894
    emails: '****@zgclab.edu.cn'
    first_name: Yanling
    google_scholar_id: https://scholar.google.com/citations?user=aFfvdRwAAAAJ&hl=zh-CN&oi=ao
    homepage: https://www.researchgate.net/profile/Yanling-Wang-22
    institution: Zhongguancun Laboratory
    last_name: Wang
    name: Yanling Wang
    username: ~Yanling_Wang1
  - dblp_id: https://dblp.org/pid/03/6827
    emails: '****@ruc.edu.cn'
    first_name: Cuiping
    institution: Renmin University of China
    last_name: Li
    name: Cuiping Li
    username: ~Cuiping_Li1
  - dblp_id: https://dblp.org/pid/52/4150-1
    emails: '****@ruc.edu.cn'
    first_name: Hong
    institution: Renmin University of China
    last_name: Chen
    name: Hong Chen
    username: ~Hong_Chen5
  decision: toFindings
  end_page: 4629
  file: 1103.pdf
  id: 1103
  num_pages: 13
  openreview_id: fXRg8qySOW
  pdf_file: d93dd85d477529198319689b13089992a94f6de0.pdf
  start_page: 4617
  title: 'SGSH: Stimulate Large Language Models with Skeleton Heuristics for Knowledge
    Base Question Generation'
- abstract: 'Modern biomedical concept representations are mostly trained on synonymous
    concept names from a biomedical knowledge base, ignoring the inter-concept interactions
    and a concept''s local neighborhood in a knowledge base graph. In this paper,
    we introduce Biomedical Entity Representation with a Graph-Augmented Multi-Objective
    Transformer (BERGAMOT), which adopts the power of pre-trained language models
    (LMs) and graph neural networks to capture both inter-concept and intra-concept
    interactions from the multilingual UMLS graph. To obtain fine-grained graph representations,
    we introduce two additional graph-based objectives: (i) a node-level contrastive
    objective and (ii) the Deep Graph Infomax (DGI) loss, which maximizes the mutual
    information between a local subgraph and a high-level graph summary. We apply
    contrastive loss on textual and graph representations to make them less sensitive
    to surface forms and enable intermodal knowledge exchange. BERGAMOT achieves state-of-the-art
    results in zero-shot entity linking without task-specific supervision on 4 of
    5 languages of the Mantra corpus and on 8 of 10 languages of the XL-BEL benchmark.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Extraction
  authors:
  - emails: '****@gmail.com'
    first_name: Andrey
    google_scholar_id: https://scholar.google.com/citations?user=bf9-6ssAAAAJ&hl=ru&oi=ao
    institution: Kazan Federal University
    last_name: Sakhovskiy
    name: Andrey Sakhovskiy
    username: ~Andrey_Sakhovskiy1
  - emails: '****@gmail.com'
    first_name: Natalia
    last_name: Semenova
    name: Natalia Semenova
    orcid: https://orcid.org/0000-0003-4189-5739
    username: ~Natalia_Semenova1
  - dblp_id: https://dblp.org/pid/230/8564
    emails: '****@gmail.com'
    first_name: Artur
    google_scholar_id: https://scholar.google.ru/citations?user=HFLFHzUAAAAJ&hl=en
    institution: Artificial Intelligence Research Institute and Kuban State University
    last_name: Kadurin
    name: Artur Kadurin
    orcid: https://orcid.org/0000-0003-1482-9365
    username: ~Artur_Kadurin1
  - dblp_id: https://dblp.org/pid/153/5554
    emails: '****@gmail.com'
    first_name: Elena
    google_scholar_id: https://scholar.google.ru/citations?user=npM9yekAAAAJ
    institution: Kazan Federal University
    last_name: Tutubalina
    name: Elena Tutubalina
    orcid: https://orcid.org/0000-0001-7936-0284
    semantic_scholar_id: https://www.semanticscholar.org/author/E.-Tutubalina/2617496
    username: ~Elena_Tutubalina1
  decision: toFindings
  end_page: 4647
  file: 1104.pdf
  id: 1104
  num_pages: 18
  openreview_id: epbXllswem
  pdf_file: 1908275bb59fed4c7544f126c44480f19762c1bc.pdf
  start_page: 4630
  title: Biomedical Entity Representation with Graph-Augmented Multi-Objective Transformer
- abstract: Cross-Lingual Summarization (XLS) aims to summarize a document in the
    source language into a condensed version in the target language, effectively removing
    language barriers for non-native readers. Previous approaches, however, have the
    same limitation that only a single reference (gold summary) is exploited during
    model training, making the base model exposed to an underrepresented hypothesis
    space since the actual number of possible hypotheses is exponentially large. To
    alleviate this problem, we present a study adopting pseudo-labels in regularizing
    standard cross-lingual summarization training. We investigate several components
    leading to the gains in regularization training with verified experiments involving
    8 diverse languages from different families. Conclusively, we show that pseudo-labeling
    is a simple and effective approach that significantly improves over standard gold
    reference training in XLS.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Summarization
  authors:
  - dblp_id: https://dblp.org/pid/362/8561
    emails: '****@gmail.com'
    first_name: Thang
    institution: VinAI Research
    last_name: Le
    name: Thang Le
    username: ~Thang_Le2
  decision: toFindings
  end_page: 4681
  file: 1109.pdf
  id: 1109
  num_pages: 34
  openreview_id: Xwg6U7Cf3R
  pdf_file: 252fd2f05729a3ecdd627c0a756144062ba54c54.pdf
  start_page: 4648
  title: Cross-Lingual Summarization with Pseudo-Label Regularization
- abstract: Politeness is a multifaceted concept influenced by individual perceptions
    of what is considered polite or impolite. With this objective, we introduce a
    novel task - Politeness Cause Elicitation and Intensity Tagging (PCEIT). This
    task focuses on conversations and aims to identify the underlying reasons behind
    the use of politeness and gauge the degree of politeness conveyed. To address
    this objective, we create HING-POEM, a new conversational dataset in Hinglish
    (a blend of Hindi and English) for mental health and legal counseling of crime
    victims. The rationale for the domain selection lies in the paramount importance
    of politeness in mental health and legal counseling of crime victims to ensure
    a compassionate and cordial atmosphere for them. We enrich the HING-POEM dataset
    by annotating it with politeness labels, politeness causal spans, and intensity
    values at the level of individual utterances. In the context of the introduced
    PCEIT task, we present PAANTH (Politeness CAuse ElicitAion and INtensity Tagging
    in Hinglish), a comprehensive framework based on Contextual Enhanced Attentive
    Convolution Transformer. We conduct extensive quantitative and qualitative evaluations
    to establish the effectiveness of our proposed approach using the newly constructed
    dataset. Our approach is compared against state-of-the-art baselines, and these
    analyses help demonstrate the superiority of our method.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multilinguality and Language Diversity
  authors:
  - emails: '****@gmail.com'
    first_name: Priyanshu
    last_name: Priya
    name: Priyanshu Priya
    username: ~Priyanshu_Priya1
  - dblp_id: https://dblp.org/pid/258/4363.html
    emails: '****@gmail.com'
    first_name: Gopendra
    last_name: Singh
    middle_name: Vikram
    name: gopendra Vikram singh
    semantic_scholar_id: https://www.semanticscholar.org/author/Gopendra-Vikram-Singh/2109170213
    username: ~gopendra_Vikram_singh1
  - dblp_id: https://dblp.org/pid/223/8272
    emails: '****@gmail.com'
    first_name: Mauajama
    google_scholar_id: https://scholar.google.co.in/citations?user=nVmB914AAAAJ&hl=en
    last_name: Firdaus
    name: Mauajama Firdaus
    orcid: https://orcid.org/0000-0001-7485-5974
    semantic_scholar_id: https://www.semanticscholar.org/author/M.-Firdaus/40195882
    username: ~Mauajama_Firdaus1
  - emails: '****@nimhans.ac.in'
    first_name: Jyotsna
    google_scholar_id: https://scholar.google.co.in/citations?user=2IBqTIYAAAAJ&hl=en
    last_name: Agrawal
    name: Jyotsna Agrawal
    username: ~Jyotsna_Agrawal1
  - dblp_id: https://dblp.org/pid/11/3590
    emails: '****@gmail.com'
    first_name: Asif
    google_scholar_id: https://scholar.google.co.in/citations?user=IAL_F04AAAAJ&hl=en
    homepage: https://www.iitp.ac.in/~asif/
    institution: IIT Patna
    last_name: Ekbal
    name: Asif Ekbal
    orcid: https://orcid.org/0000-0003-3612-8834
    semantic_scholar_id: https://www.semanticscholar.org/author/Asif-Ekbal/1734904
    username: ~Asif_Ekbal1
  decision: toFindings
  end_page: 4700
  file: 1114.pdf
  id: 1114
  num_pages: 19
  openreview_id: ZajitrtK8z
  pdf_file: da15b9654ecc1fbb3cb4c8484a7da6c506153d0f.pdf
  start_page: 4682
  title: 'On the Way to Gentle AI Counselor: Politeness Cause Elicitation and Intensity
    Tagging in Code-mixed Hinglish Conversations for Social Good'
- abstract: Traditional approaches to dialogue segmentation perform reasonably well
    on synthetic or written dialogues but suffer when dealing with spoken, noisy dialogs.
    In addition, such methods require careful tuning of hyperparameters. We propose
    to leverage a novel approach that is based on dialogue summaries. Experiments
    on different datasets showed that the new approach outperforms popular state-of-the-art
    algorithms in unsupervised topic segmentation and requires less setup.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Dialogue and Interactive Systems
  authors:
  - emails: '****@gmail.com'
    first_name: Aleksei
    last_name: Artemiev
    name: Aleksei Artemiev
    username: ~Aleksei_Artemiev1
  - emails: '****@gmail.com'
    first_name: Daniil
    homepage: https://www.linkedin.com/in/daniil-parinov-94051115a/
    institution: Yandex
    last_name: Parinov
    name: Daniil Parinov
    username: ~Daniil_Parinov1
  - dblp_id: https://dblp.uni-trier.de/pid/329/4073.html
    emails: '****@phystech.edu'
    first_name: Alexey
    google_scholar_id: https://scholar.google.com/citations?user=LLgj-U4AAAAJ&hl=en
    homepage: https://intsystems.github.io/people/grishanov_av
    last_name: Grishanov
    name: Alexey Grishanov
    orcid: https://orcid.org/0000-0001-5085-060X
    username: ~Alexey_Grishanov1
  - emails: '****@mail.ru'
    first_name: Ivan
    homepage: https://github.com/sqz1337
    last_name: Borisov
    name: Ivan Borisov
    username: ~Ivan_Borisov2
  - emails: '****@yandex.ru'
    first_name: Alexey
    google_scholar_id: https://scholar.google.com/citations?user=4vb0JIwAAAAJ&hl=ru
    institution: Sber, AI Lab
    last_name: Vasilev
    name: Alexey Vasilev
    orcid: https://orcid.org/0009-0007-1415-2004
    username: ~Alexey_Vasilev1
  - emails: '****@phystech.edu'
    first_name: Daniil
    homepage: https://github.com/DMurawiecki
    last_name: Muravetskii
    name: Daniil Muravetskii
    username: ~Daniil_Muravetskii1
  - emails: '****@yandex.ru'
    first_name: Aleksey
    last_name: Rezvykh
    name: Aleksey Rezvykh
    username: ~Aleksey_Rezvykh1
  - emails: '****@mil-team.com'
    first_name: Aleksei
    google_scholar_id: https://scholar.google.com/citations?user=gUw78OkAAAAJ&hl=en&authuser=1
    homepage: http://mil-team.com
    institution: MIL Team
    last_name: Goncharov
    name: Aleksei Goncharov
    username: ~Aleksei_Goncharov1
  - dblp_id: https://dblp.org/pid/25/9832
    emails: '****@hse.ru'
    first_name: Andrey
    google_scholar_id: https://scholar.google.ru/citations?user=1feIO4YAAAAJ
    homepage: http://www.hse.ru/en/staff/avsavchenko
    institution: Sber AI Lab and HSE University
    last_name: Savchenko
    name: Andrey Savchenko
    orcid: https://orcid.org/0000-0001-6196-0564
    username: ~Andrey_Savchenko1
  decision: toFindings
  end_page: 4708
  file: 1122.pdf
  id: 1122
  num_pages: 8
  openreview_id: fVvLGcrawX
  pdf_file: 7155288d176ec108383b8fb19124dab045420919.pdf
  start_page: 4701
  title: Leveraging Summarization for Unsupervised Dialogue Topic Segmentation
- abstract: Recently, various studies have leveraged Large Language Models (LLMs)
    to help decision-making and planning in environments and try to align the LLMs'
    knowledge with the world conditions. Nonetheless, the capacity of LLMs to continuously
    acquire environmental knowledge and adapt in an open world remains uncertain.
    In this paper, we propose an approach to spur LLMs to explore the open world,
    gather experiences, and learn to improve their task-solving capabilities. In this
    approach, a multi-round feedback-revision mechanism is utilized to encourage LLMs
    to actively select appropriate revision actions guided by feedback information
    from the environment. This facilitates exploration and enhances the model's performance.
    Besides, we integrate sub-task relabeling to assist LLMs in maintaining consistency
    in sub-task planning and help the model learn the combinatorial nature between
    tasks, enabling it to complete a wider range of tasks through training based on
    the acquired exploration experiences. By evaluation in Minecraft, an open-ended
    sandbox world, we demonstrate that our approach LLaMA-Rider enhances the efficiency
    of the LLM in exploring the environment, and effectively improves the LLM's ability
    to accomplish more tasks through fine-tuning with merely 1.3k instances of collected
    data, showing minimal training costs compared to the baseline using reinforcement
    learning. The code is available at https://github.com/PKU-RL/LLaMA-Rider.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multimodality and Language Grounding to Vision, Robotics and Beyond
  authors:
  - dblp_id: https://dblp.org/pid/340/4016
    emails: '****@pku.edu.cn'
    first_name: Yicheng
    homepage: https://takenpeanut.github.io/
    institution: Peking University
    last_name: Feng
    name: Yicheng Feng
    semantic_scholar_id: https://www.semanticscholar.org/author/Yicheng-Feng/2115319860
    username: ~Yicheng_Feng1
  - emails: '****@stu.pku.edu.cn'
    first_name: Yuxuan
    homepage: https://z0ngqing.github.io/project/
    last_name: Wang
    name: Yuxuan Wang
    username: ~Yuxuan_Wang10
  - emails: '****@stu.pku.edu.cn'
    first_name: Jiazheng
    homepage: https://github.com/saki-37
    institution: Peking University
    last_name: Liu
    name: jiazheng liu
    username: ~jiazheng_liu2
  - dblp_id: https://dblp.org/pid/251/3691
    emails: '****@baai.ac.cn'
    first_name: Sipeng
    google_scholar_id: https://scholar.google.com/citations?user=OonuDhcAAAAJ&hl=zh-CN
    homepage: https://github.com/zhengsipeng
    institution: Beijing Academy of Artificial Intelligence
    last_name: Zheng
    name: Sipeng Zheng
    username: ~Sipeng_Zheng1
  - dblp_id: https://dblp.org/pid/99/965
    emails: '****@pku.edu.cn'
    first_name: Zongqing
    google_scholar_id: https://scholar.google.com.tw/citations?user=k3IFtTYAAAAJ
    homepage: https://z0ngqing.github.io/
    institution: Peking University
    last_name: Lu
    name: Zongqing Lu
    username: ~Zongqing_Lu1
  decision: toFindings
  end_page: 4728
  file: 1129.pdf
  id: 1129
  num_pages: 20
  openreview_id: evcfO3QnqI
  pdf_file: 7992f1f7f2dd1d99d7f1413d9156965a05139f31.pdf
  start_page: 4709
  title: 'LLaMA-Rider: Spurring Large Language Models to Explore the Open World'
- abstract: 'Recently, language models have accelerated the improvement in natural
    language processing. However, recent studies have highlighted a significant issue:
    social biases inherent in training data can lead models to learn and propagate
    these biases. In this study, we propose a contrastive learning method for bias
    mitigation, utilizing anchor points to push further negatives and pull closer
    positives within the representation space. This approach employs stereotypical
    data as negatives and stereotype-free data as positives, enhancing debiasing performance.
    Our model attained state-of-the-art performance in the ICAT score on the StereoSet,
    a benchmark for measuring bias in models. In addition, we observed that effective
    debiasing is achieved through an awareness of biases, as evidenced by improved
    hate speech detection scores. The implementation code and trained models are available
    at https://github.com/HUFS-NLP/CL_Polarizer.git.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Ethics, Bias, and Fairness
  authors:
  - emails: '****@gmail.com'
    first_name: Kyungmin
    homepage: https://www.notion.so/Resume-a7fc22c7e584427381c5ef54741c3a04
    institution: Hankuk University of Foreign Studies
    last_name: Park
    name: Kyungmin Park
    username: ~Kyungmin_Park1
  - emails: '****@naver.com'
    first_name: Sihyun
    homepage: https://github.com/osh4git
    last_name: Oh
    name: Sihyun Oh
    username: ~Sihyun_Oh1
  - emails: '****@gmail.com'
    first_name: Daehyun
    homepage: https://github.com/maokaistrong
    last_name: Kim
    name: Daehyun KIM
    username: ~Daehyun_KIM7
  - emails: '****@hufs.ac.kr'
    first_name: Juae
    google_scholar_id: https://scholar.google.com/citations?user=d0pN8j8AAAAJ&hl=ko
    institution: Hankuk University of Foreign Studies
    last_name: Kim
    name: Juae Kim
    username: ~Juae_Kim1
  decision: toFindings
  end_page: 4740
  file: 1130.pdf
  id: 1130
  num_pages: 12
  openreview_id: CgUNxTtm1e
  pdf_file: 7e829a022d1f302ec3b8abc92a51cf6dfd798df8.pdf
  start_page: 4729
  title: 'Contrastive Learning as a Polarizer: Mitigating Gender Bias by Fair and
    Biased sentences'
- abstract: "Despite tremendous advancements in large language models (LLMs) over\
    \ recent years,  a notably urgent challenge for their practical deployment is\
    \ the phenomenon of  \"$\\textit{hallucination}$'', where the model fabricates\
    \ facts and produces non-factual statements. In response, we propose $\\texttt{PoLLMgraph}$\u2014\
    a Polygraph for LLMs\u2014as an effective model-based white-box detection and\
    \ forecasting approach. $\\texttt{PoLLMgraph}$ distinctly differs from the large\
    \ body of existing research that concentrates on addressing such challenges through\
    \ black-box evaluations. In particular, we demonstrate that hallucination can\
    \ be effectively detected by analyzing the LLM's internal state transition dynamics\
    \ during generation via tractable probabilistic models. Experimental results on\
    \ various open-source LLMs confirm the efficacy of $\\texttt{PoLLMgraph}$, outperforming\
    \ state-of-the-art methods by a considerable margin, evidenced by over 20\\% improvement\
    \ in AUC-ROC on common benchmarking datasets like TruthfulQA. Our work paves a\
    \ new way for model-based white-box analysis of LLMs, motivating the research\
    \ community to further explore, understand, and refine the intricate dynamics\
    \ of LLM behaviors."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - dblp_id: https://dblp.org/pid/203/9320
    emails: '****@tum.de'
    first_name: Derui
    last_name: Zhu
    name: Derui Zhu
    username: ~Derui_Zhu2
  - dblp_id: https://dblp.org/pid/248/8198
    emails: '****@cispa.de'
    first_name: Dingfan
    google_scholar_id: https://scholar.google.com/citations?user=iARn00oAAAAJ&hl=en&authuser=1
    homepage: https://dingfanchen.github.io/homepage/
    institution: CISPA, saarland university, saarland informatics campus
    last_name: Chen
    name: Dingfan Chen
    semantic_scholar_id: https://www.semanticscholar.org/author/Dingfan-Chen/153642281
    username: ~Dingfan_Chen1
  - dblp_id: https://dblp.org/pid/181/2689-38
    emails: '****@mbzuai.ac.ae'
    first_name: Qing
    google_scholar_id: https://scholar.google.com/citations?view_op=list_works&hl=en&user=uPgix8AAAAAJ&gmla=AJsN-F5gG0-dKguEtq5LsG-BkSQvSKNuvxGtYhjqdi_UExvIcYMFNHW9EGEiF7BktFY-isRekHL8zBA8arp4lpp-BS4JG-l0IMrUBRvR5uz5k7PP779tfVQ
    institution: Mohamed bin Zayed University of Artificial Intelligence
    last_name: Li
    name: Qing Li
    orcid: https://orcid.org/0000-0002-6442-5003
    username: ~Qing_Li13
  - dblp_id: https://dblp.org/pid/256/6334.html
    emails: '****@fokus.fraunhofer.de'
    first_name: Zongxiong
    homepage: https://test.com
    institution: Fraunhofer FOKUS
    last_name: Chen
    name: Zongxiong Chen
    orcid: https://orcid.org/0000-0003-2452-0572
    username: ~Zongxiong_Chen1
  - dblp_id: https://dblp.org/pid/20/6534-3
    emails: '****@acm.org'
    first_name: Lei
    google_scholar_id: https://scholar.google.com/citations?user=xsfGc58AAAAJ&hl=en
    homepage: https://www.malei.org
    institution: The University of Tokyo and University of Alberta
    last_name: Ma
    name: Lei Ma
    username: ~Lei_Ma1
  - dblp_id: https://dblp.org/pid/28/3855
    emails: '****@in.tum.de'
    first_name: Jens
    google_scholar_id: https://scholar.google.de/citations?user=apxyerQAAAAJ
    homepage: https://www.cs.cit.tum.de/ct/
    institution: "Technische Universit\xE4t M\xFCnchen"
    last_name: Grossklags
    name: Jens Grossklags
    orcid: https://orcid.org/0000-0003-1093-1282
    username: ~Jens_Grossklags2
  - dblp_id: https://dblp.org/pers/hd/f/Fritz:Mario
    emails: '****@cispa.saarland'
    first_name: Mario
    google_scholar_id: https://scholar.google.de/citations?user=4V1nNm4AAAAJ&hl=en&oi=ao
    homepage: https://fritz.cispa.saarland
    institution: CISPA Helmholtz Center for Information Security and Saarland University
    last_name: Fritz
    name: Mario Fritz
    username: ~Mario_Fritz1
  decision: toFindings
  end_page: 4755
  file: 1133.pdf
  id: 1133
  num_pages: 15
  openreview_id: DqenvW3MNn
  pdf_file: 7e50db6ad65f88d89e89ec0e3a78aa985fefb922.pdf
  start_page: 4741
  title: 'PoLLMgraph: Unraveling Hallucinations in Large Language Models via State
    Transition Dynamics'
- abstract: In today's digital world, seeking answers to health questions on the Internet
    is a common practice. However, existing question answering (QA) systems often
    rely on using pre-selected and annotated evidence documents, thus making them
    inadequate for addressing novel questions. Our study focuses on the open-domain
    QA setting, where the key challenge is to first uncover relevant evidence in large
    knowledge bases. By utilizing the common retrieve-then-read QA pipeline and PubMed
    as a trustworthy collection of medical research documents, we answer health questions
    from three diverse datasets. We modify different retrieval settings to observe
    their influence on the QA pipeline's performance, including the number of retrieved
    documents, sentence selection process, the publication year of articles, and their
    number of citations. Our results reveal that cutting down on the amount of retrieved
    documents and favoring more recent and highly cited documents can improve the
    final macro F1 score up to 10\%. We discuss the results, highlight interesting
    examples, and outline challenges for future research, like managing evidence disagreement
    and crafting user-friendly explanations.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - emails: '****@tum.de'
    first_name: Juraj
    google_scholar_id: https://scholar.google.com/citations?user=h7dD6rsAAAAJ&hl=en
    institution: "Technische Universit\xE4t M\xFCnchen"
    last_name: Vladika
    name: Juraj Vladika
    semantic_scholar_id: https://www.semanticscholar.org/author/Juraj-Vladika/2066962303
    username: ~Juraj_Vladika1
  - dblp_id: https://dblp.org/pid/m/FlorianMatthes.html
    emails: '****@tum.de'
    first_name: Florian
    google_scholar_id: https://scholar.google.com.tw/citations?user=3zjU5eIAAAAJ
    homepage: https://wwwmatthes.in.tum.de/pages/88bkmvw6y7gx/Prof.-Dr.-Florian-Matthes/
    institution: "Technische Universit\xE4t M\xFCnchen"
    last_name: Matthes
    name: Florian Matthes
    orcid: https://orcid.org/0000-0002-6667-5452
    username: ~Florian_Matthes1
  decision: toFindings
  end_page: 4767
  file: 1143.pdf
  id: 1143
  num_pages: 12
  openreview_id: rFtQCyGf8u
  pdf_file: 918cc2160d6fa3b4d3c9c73462a0b31f7c695da8.pdf
  start_page: 4756
  title: Improving Health Question Answering with Reliable and Time-Aware Evidence
    Retrieval
- abstract: "In recent years, several interpretability methods have been proposed\
    \ to interpret the inner workings of Transformer models at different levels of\
    \ precision and complexity.\nIn this work, we propose a simple but effective technique\
    \ to analyze encoder-decoder Transformers. \nOur method, which we name DecoderLens,\
    \ allows the decoder to cross-attend representations of intermediate encoder activations\
    \ instead of using the default final encoder output.\nThe method thus maps uninterpretable\
    \ intermediate vector representations to human-interpretable sequences of words\
    \ or symbols, shedding new light on the information flow in this popular but understudied\
    \ class of models.\nWe apply DecoderLens to question answering, logical reasoning,\
    \ speech recognition and machine translation models, finding that simpler subtasks\
    \ are solved with high precision by low and intermediate encoder layers."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - emails: '****@gmail.com'
    first_name: Anna
    homepage: https://anna.computer
    institution: Utrecht University (ICS), Utrecht University and University of Amsterdam
    last_name: Langedijk
    name: Anna Langedijk
    username: ~Anna_Langedijk1
  - dblp_id: https://dblp.org/pid/289/6362
    emails: '****@tilburguniversity.edu'
    first_name: Hosein
    google_scholar_id: https://scholar.google.com/citations?user=tWOBwigAAAAJ&hl=en
    homepage: https://hmohebbi.github.io/
    last_name: Mohebbi
    name: Hosein Mohebbi
    orcid: https://orcid.org/0000-0001-8184-7825
    semantic_scholar_id: https://www.semanticscholar.org/author/Hosein-Mohebbi/2064594486
    username: ~Hosein_Mohebbi1
  - dblp_id: https://dblp.org/pid/273/4259.html
    emails: '****@gmail.com'
    first_name: Gabriele
    google_scholar_id: https://scholar.google.it/citations?user=sK0B_08AAAAJ
    homepage: https://gsarti.com
    institution: University of Groningen
    last_name: Sarti
    name: Gabriele Sarti
    orcid: https://orcid.org/0000-0001-8715-2987
    semantic_scholar_id: https://www.semanticscholar.org/author/Gabriele-Sarti/1897770594
    username: ~Gabriele_Sarti1
  - dblp_id: https://dblp.org/pid/67/1016
    emails: '****@uva.nl'
    first_name: Willem
    google_scholar_id: https://scholar.google.com/citations?user=MBkG_FYAAAAJ&hl=nl&oi=sra
    homepage: https://staff.fnwi.uva.nl/w.zuidema/
    institution: University of Amsterdam
    last_name: Zuidema
    name: Willem Zuidema
    orcid: https://orcid.org/0000-0002-2362-5447
    username: ~Willem_Zuidema1
  - dblp_id: https://dblp.org/pid/225/7711
    emails: '****@gmail.com'
    first_name: Jaap
    google_scholar_id: https://scholar.google.com/citations?user=i2wNV20AAAAJ&hl=en
    homepage: https://jumelet.ai/
    last_name: Jumelet
    name: Jaap Jumelet
    semantic_scholar_id: https://www.semanticscholar.org/author/Jaap-Jumelet/51247075
    username: ~Jaap_Jumelet1
  decision: toFindings
  end_page: 4784
  file: 1146.pdf
  id: 1146
  num_pages: 17
  openreview_id: gOfnkmD9rz
  pdf_file: 6f5d9f76e57ff43c589aaae86eb8d05c9776de2c.pdf
  start_page: 4768
  title: 'DecoderLens: Layerwise Interpretation of Encoder-Decoder Transformers'
