- abstract: 'Named entity recognition is a key component of Information Extraction
    (IE), particularly in scientific domains such as biomedicine and chemistry, where
    large language models (LLMs), e.g., ChatGPT, fall short. We investigate the applicability
    of transfer learning for enhancing a named entity recognition model trained in
    the biomedical domain (the source domain) to be used in the chemical domain (the
    target domain). A common practice for training such a model in a few-shot learning
    setting is to pretrain the model on the labeled source data, and then, to finetune
    it on a hand-full of labeled target examples. In our experiments, we observed
    that such a model is prone to mislabeling the source entities, which can often
    appear in the text, as the target entities. To alleviate this problem, we propose
    a model to transfer the knowledge from the source domain to the target domain,
    but, at the same time, to project the source entities and target entities into
    separate regions of the feature space. This diminishes the risk of mislabeling
    the source entities as the target entities. Our model consists of two stages:
    1) entity grouping in the source domain, which incorporates knowledge from annotated
    events to establish relations between entities, and 2) entity discrimination in
    the target domain, which relies on pseudo labeling and contrastive learning to
    enhance discrimination between the entities in the two domains. We conduct our
    extensive experiments across three source and three target datasets, demonstrating
    that our method outperforms the baselines by up to 5\% absolute value. Code, data,
    and resources are publicly available for research purposes: https://github.com/Lhtie/Bio-Domain-Transfer
    .'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Extraction
  authors:
  - dblp_id: https://dblp.org/pid/45/4076-8
    emails: '****@sjtu.edu.cn'
    first_name: Hongyi
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=rn18Dc4AAAAJ&view_op=list_works&gmla=AHoSzlXMvU15QFKF_6Bmu5gliexiPLU6l4BSwdvCqLTg6uHl2s6gFQUmNio0Lxezu5XFawt6tsRcYDGUxoZRYTUTcdmgOw21kehPKpp384O6HUUfMYEb7xSqiX2AJDJQIHiUi0s
    last_name: Liu
    name: Hongyi Liu
    orcid: https://orcid.org/0009-0001-0810-474X
    semantic_scholar_id: https://www.semanticscholar.org/author/Hongyi-Liu/2115669628
    username: ~Hongyi_Liu4
  - dblp_id: https://dblp.org/pid/53/3310-5
    emails: '****@illinois.edu'
    first_name: Qingyun
    google_scholar_id: https://scholar.google.com/citations?user=HQcZOHMAAAAJ&hl=en
    homepage: https://eaglew.github.io/
    institution: University of Illinois, Urbana Champaign
    last_name: Wang
    name: Qingyun Wang
    orcid: https://orcid.org/0000-0002-2659-6100
    semantic_scholar_id: https://www.semanticscholar.org/author/Qingyun-Wang/1786863
    username: ~Qingyun_Wang1
  - emails: '****@gmail.com'
    first_name: Payam
    institution: University of Illinois at Urbana-Champaign
    last_name: Karisani
    name: Payam Karisani
    semantic_scholar_id: https://www.semanticscholar.org/author/Payam-Karisani/2497878
    username: ~Payam_Karisani1
  - emails: '****@illinois.edu'
    first_name: Heng
    google_scholar_id: https://scholar.google.com/citations?user=z7GCqT4AAAAJ&hl=en
    homepage: http://blender.cs.illinois.edu/hengji.html
    institution: University of Illinois, Urbana-Champaign
    last_name: Ji
    name: Heng Ji
    username: ~Heng_Ji3
  decision: toMainConference
  end_page: 21
  file: 3.pdf
  id: 3
  num_pages: 21
  openreview_id: xoLbE6rPxW
  pdf_file: 014122511222b4a92eb2aa0defcca8c5e2052884.pdf
  start_page: 1
  title: Named Entity Recognition Under Domain Shift via Metric Learning for Life
    Sciences
- abstract: "The diffusion model, a new generative modeling paradigm, has achieved\
    \ great success in image, audio, and video generation.\nHowever, considering the\
    \ discrete categorical nature of the text, it is not trivial to extend continuous\
    \ diffusion models to natural language.  In this work, we propose SeqDiffuSeq,\
    \ a text diffusion model, to approach sequence-to-sequence text generation with\
    \ an encoder-decoder Transformer architecture.\nTo improve the generation performance,\
    \ SeqDiffuSeq is equipped with the self-conditioning technique and our newly proposed\
    \ adaptive noise schedule technique. \nSelf-conditioning enables SeqDiffuSeq to\
    \ better use the predicted sequence information during the generation process.\n\
    The adaptive noise schedule balances the difficulty of denoising across time steps\
    \ at the token level.\nExperiment results illustrate the improved performance\
    \ on five sequence-to-sequence generation tasks compared to other diffusion-based\
    \ models regarding text quality and inference time."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Generation
  authors:
  - dblp_id: https://dblp.org/pid/308/0909
    emails: '****@mails.tsinghua.edu.cn'
    first_name: Hongyi
    google_scholar_id: https://scholar.google.com/citations?user=FG3O4i8AAAAJ
    last_name: Yuan
    name: Hongyi Yuan
    semantic_scholar_id: https://www.semanticscholar.org/author/Hongyi-Yuan/2114128654
    username: ~Hongyi_Yuan1
  - dblp_id: https://dblp.org/pid/56/2877-2
    emails: '****@qq.com'
    first_name: Zheng
    google_scholar_id: https://scholar.google.com/citations?hl=zh-CN&user=kRgiVnUAAAAJ
    institution: Alibaba Group
    last_name: Yuan
    name: Zheng Yuan
    semantic_scholar_id: https://www.semanticscholar.org/author/Zheng-Yuan/2112340945
    username: ~Zheng_Yuan2
  - dblp_id: https://dblp.org/pid/148/4497
    emails: '****@alibaba-inc.com'
    first_name: Chuanqi
    google_scholar_id: https://scholar.google.com/citations?user=tOfo4ncAAAAJ&hl=en
    institution: Alibaba Group
    last_name: Tan
    name: Chuanqi Tan
    orcid: https://orcid.org/0000-0002-6676-3057
    semantic_scholar_id: https://www.semanticscholar.org/author/Chuanqi-Tan/2111727840
    username: ~Chuanqi_Tan3
  - dblp_id: https://dblp.org/pid/h/FeiHuang.html
    emails: '****@gmail.com'
    first_name: Fei
    google_scholar_id: https://scholar.google.com/citations?user=9r98PpoAAAAJ
    homepage: https://sites.google.com/view/fei-huang
    institution: Alibaba Group
    last_name: Huang
    name: Fei Huang
    username: ~Fei_Huang1
  - dblp_id: https://dblp.org/pid/05/4919
    emails: '****@alibaba-inc.com'
    first_name: Songfang
    institution: Alibaba Group
    last_name: Huang
    name: Songfang Huang
    username: ~Songfang_Huang1
  decision: toMainConference
  end_page: 39
  file: 5.pdf
  id: 5
  num_pages: 18
  openreview_id: BGNaAwT7Km
  pdf_file: 053b429172890bf751a7ea0ab249544c540880b4.pdf
  start_page: 22
  title: Text Diffusion Model with Encoder-Decoder Transformers for Sequence-to-Sequence
    Generation
- abstract: 'The recent rise of social media has led to the spread of large amounts
    of fake and biased news, content published with the intent to sway beliefs. While
    detecting and profiling the sources that spread this news is important to maintain
    a healthy society, it is challenging for automated systems.


    In this paper, we propose an interactive framework for news media profiling. It
    combines the strengths of graph based news media profiling models, Pre-trained
    Large Language Models, and human insight to characterize the social context on
    social media. Experimental results show that with as little as 5 human interactions,
    our framework can rapidly detect fake and biased news media, even in the most
    challenging settings of emerging news events, where test data is unseen.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Computational Social Science and Cultural Analytics
  authors:
  - dblp_id: https://dblp.org/pid/89/7487
    emails: '****@purdue.edu'
    first_name: Nikhil
    google_scholar_id: https://scholar.google.com/citations?user=HxebdycAAAAJ&hl=en
    homepage: https://hockeybro12.github.io
    last_name: Mehta
    name: Nikhil Mehta
    semantic_scholar_id: https://www.semanticscholar.org/author/Nikhil-Mehta/144370236
    username: ~Nikhil_Mehta2
  - dblp_id: https://dblp.org/pid/38/3382
    emails: '****@purdue.edu'
    first_name: Dan
    google_scholar_id: https://scholar.google.com.tw/citations?user=u8358QgAAAAJ
    homepage: https://www.cs.purdue.edu/homes/dgoldwas/
    institution: Purdue University, Purdue University and Purdue University
    last_name: Goldwasser
    name: Dan Goldwasser
    username: ~Dan_Goldwasser1
  decision: toMainConference
  end_page: 58
  file: 6.pdf
  id: 6
  num_pages: 19
  openreview_id: cz1ZOteGMz
  pdf_file: 7485668d94e77a22e9f44f056970447d973ce514.pdf
  start_page: 40
  title: An Interactive Framework for Profiling News Media Sources
- abstract: Large Language Models (LLMs) have shown remarkable proficiency in language
    understanding and have been successfully applied to a variety of real-world tasks
    through task-specific fine-tuning or prompt engineering.  Despite these advancements,
    it remains an open question whether LLMs are fundamentally capable of reasoning
    and planning, or if they primarily rely on recalling and synthesizing information
    from their training data.  In our research, we introduce a novel task---Minesweeper---specifically
    designed in a format unfamiliar to LLMs and absent from their training datasets.  This
    task challenges LLMs to identify the locations of mines based on numerical clues
    provided by adjacent opened cells.  Successfully completing this task requires
    an understanding of each cell's state, discerning spatial relationships between
    the clues and mines, and strategizing actions based on logical deductions drawn
    from the arrangement of the cells.  Our experiments, including trials with the
    advanced GPT-4 model, indicate that while LLMs possess the foundational abilities
    required for this task, they struggle to integrate these into a coherent, multi-step
    logical reasoning process needed to solve Minesweeper.  These findings highlight
    the need for further research to understand the nature of reasoning capabilities
    in LLMs under similar circumstances, and to explore pathways towards more sophisticated
    AI reasoning and planning models.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - dblp_id: https://dblp.org/pid/15/1534
    emails: '****@gatech.edu'
    first_name: Yinghao
    google_scholar_id: https://scholar.google.com/citations?user=2WSooDIAAAAJ
    homepage: https://yinghao-li.github.io/
    last_name: Li
    name: Yinghao Li
    orcid: https://orcid.org/0000-0002-7188-4136
    semantic_scholar_id: https://www.semanticscholar.org/author/Yinghao-Li/1527089853
    username: ~Yinghao_Li3
  - emails: '****@gatech.edu'
    first_name: Haorui
    institution: Georgia Institute of Technology
    last_name: Wang
    name: Haorui Wang
    username: ~Haorui_Wang1
  - dblp_id: https://dblp.org/pid/94/3019-14.html
    emails: '****@gatech.edu'
    first_name: Chao
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=CeEO6SIAAAAJ&view_op=list_works&sortby=pubdate
    homepage: http://chaozhang.org/
    institution: Georgia Institute of Technology
    last_name: Zhang
    name: Chao Zhang
    username: ~Chao_Zhang15
  decision: toMainConference
  end_page: 81
  file: 8.pdf
  id: 8
  num_pages: 23
  openreview_id: PY400gJGLD
  pdf_file: f454593431b946c6d6161d1264da085f5d7effed.pdf
  start_page: 59
  title: 'Assessing Logical Puzzle Solving in Large Language Models: Insights from
    a Minesweeper Case Study'
- abstract: Emotion Recognition in Conversation (ERC) plays a crucial role in enabling
    dialogue sys- tems to effectively respond to user requests. The emotions in a
    conversation can be identi- fied by the representations from various modal- ities,
    such as audio, visual, and text. How- ever, due to the weak contribution of non-verbal
    modalities to recognize emotions, multimodal ERC has always been considered a
    challenging task. In this paper, we propose Teacher-leading Multimodal fusion
    network for ERC (TelME). TelME incorporates cross-modal knowledge distillation
    to transfer information from a lan- guage model acting as the teacher to the non-
    verbal students, thereby optimizing the efficacy of the weak modalities. We then
    combine multi- modal features using a shifting fusion approach in which student
    networks support the teacher. TelME achieves state-of-the-art performance in MELD,
    a multi-speaker conversation dataset for ERC. Finally, we demonstrate the effec-
    tiveness of our components through additional experiments.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Sentiment Analysis, Stylistic Analysis, and Argument Mining
  authors:
  - emails: '****@yonsei.ac.kr'
    first_name: Taeyang
    homepage: https://yaeyang0629.tistory.com/
    last_name: Yun
    name: Taeyang Yun
    username: ~Taeyang_Yun1
  - emails: '****@yonsei.ac.kr'
    first_name: Hyunkuk
    homepage: https://www.github.com/lsh950919
    institution: Yonsei University
    last_name: Lim
    name: Hyunkuk Lim
    username: ~Hyunkuk_Lim1
  - emails: '****@yonsei.ac.kr'
    first_name: Jeonghwan
    homepage: https://github.com/jh-lee95
    last_name: Lee
    name: Jeonghwan Lee
    username: ~Jeonghwan_Lee1
  - emails: '****@yonsei.ac.kr'
    first_name: Min
    homepage: http://informatics.yonsei.ac.kr
    last_name: Song
    name: Min Song
    username: ~Min_Song4
  decision: toMainConference
  end_page: 95
  file: 9.pdf
  id: 9
  num_pages: 14
  openreview_id: dX1ORgXDLN
  pdf_file: 3a0288dc4b09ad70aa0b95c60f2b8b9738baacc3.pdf
  start_page: 82
  title: 'TelME: Teacher-leading Multimodal Fusion Network for Emotion Recognition
    in Conversation'
- abstract: 'Few-shot dialogue state tracking (DST) with Large Language Models (LLM)
    relies on an effective and efficient conversation retriever to find similar in-context
    examples for prompt learning. Previous works use raw dialogue context as search
    keys and queries, and a retriever is fine-tuned with annotated dialogues to achieve
    superior performance. However, the approach is less suited for scaling to new
    domains or new annotation languages, where fine-tuning data is unavailable. To
    address this problem, we handle the task of conversation retrieval based on text
    summaries of the conversations.

    A LLM-based conversation summarizer is adopted for query and key generation, which
    enables effective maximum inner product search. To avoid the extra inference cost
    brought by LLM-based conversation summarization, we further distill a light-weight
    conversation encoder which produces query embeddings without  decoding summaries
    for test conversations.  We validate our retrieval approach on MultiWOZ datasets
    with GPT-Neo-2.7B and LLaMA-7B/30B. The experimental results show a significant
    improvement over relevant baselines in real few-shot DST settings.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Dialogue and Interactive Systems
  authors:
  - dblp_id: https://dblp.org/pid/219/6771
    emails: '****@kaist.ac.kr'
    first_name: Seanie
    google_scholar_id: https://scholar.google.com/citations?user=zrZu6GkAAAAJ
    homepage: https://seanie12.github.io/
    institution: Korea Advanced Institute of Science & Technology
    last_name: Lee
    name: Seanie Lee
    username: ~Seanie_Lee1
  - emails: '****@gmail.com'
    first_name: Jianpeng
    google_scholar_id: https://scholar.google.com/citations?user=51FYPYsAAAAJ&hl=zh-CN
    last_name: Cheng
    name: Jianpeng Cheng
    username: ~Jianpeng_Cheng1
  - dblp_id: https://dblp.org/pid/24/7248
    emails: '****@apple.com'
    first_name: Joris
    institution: Apple
    last_name: Driesen
    name: Joris Driesen
    username: ~Joris_Driesen2
  - dblp_id: https://dblp.org/pid/270/3329.html
    emails: '****@cam.ac.uk'
    first_name: Alexandru
    last_name: Coca
    name: Alexandru Coca
    semantic_scholar_id: https://www.semanticscholar.org/author/Alexandru-Coca/1811299098
    username: ~Alexandru_Coca1
  - dblp_id: https://dblp.org/pid/94/8340
    emails: '****@johannsen.com'
    first_name: Anders
    last_name: Johannsen
    name: Anders Johannsen
    username: ~Anders_Johannsen1
  decision: toMainConference
  end_page: 111
  file: 14.pdf
  id: 14
  num_pages: 16
  openreview_id: SX6JuCZtsq
  pdf_file: b85dfe8370003158f36226b4b6e8517c651ade32.pdf
  start_page: 96
  title: Effective and Efficient Conversation Retrieval for Dialogue State Tracking
    with Implicit Text Summaries
- abstract: 'Prompt-based methods have been used extensively across NLP to build zero-
    and few-shot label predictors. Many NLP tasks are naturally structured: that is,
    their outputs consist of multiple labels which constrain each other.  Annotating
    data for such tasks can be cumbersome. Can the promise of the prompt-based paradigm
    be extended to such structured outputs? In this paper, we present a framework
    for constructing zero- and few-shot linguistic structure predictors. Our key insight
    is that we can use structural constraints---and combinatorial inference derived
    from them---to filter out inconsistent structures predicted by large language
    models.  We instantiated this framework on two structured prediction tasks, and
    five datasets. Across all cases, our results show that enforcing consistency not
    only constructs structurally valid outputs, but also improves performance over
    the unconstrained variants.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Learning for NLP
  authors:
  - dblp_id: https://dblp.org/pid/218/5511
    emails: '****@gmail.com'
    first_name: Maitrey
    google_scholar_id: https://scholar.google.com/citations?user=YJ7kyKwAAAAJ&hl=en
    homepage: http://www.cs.utah.edu/~maitrey
    institution: University of Utah
    last_name: Mehta
    name: Maitrey Mehta
    username: ~Maitrey_Mehta1
  - dblp_id: https://dblp.org/pid/204/1165
    emails: '****@gmail.com'
    first_name: Valentina
    google_scholar_id: https://scholar.google.com/citations?user=E9EgKkMAAAAJ&hl=en
    homepage: https://valentinapy.github.io
    last_name: Pyatkin
    name: Valentina Pyatkin
    semantic_scholar_id: https://www.semanticscholar.org/author/Valentina-Pyatkin/22330666
    username: ~Valentina_Pyatkin1
  - dblp_id: https://dblp.org/pid/37/44
    emails: '****@cs.utah.edu'
    first_name: Vivek
    google_scholar_id: https://scholar.google.com/citations?user=TsTUfOIAAAAJ
    homepage: https://svivek.com
    institution: University of Utah
    last_name: Srikumar
    name: Vivek Srikumar
    semantic_scholar_id: https://www.semanticscholar.org/author/Vivek-Srikumar/3052879
    username: ~Vivek_Srikumar1
  decision: toMainConference
  end_page: 130
  file: 15.pdf
  id: 15
  num_pages: 19
  openreview_id: FUL0sAdPZj
  pdf_file: db73d1f8e3524b56441f69145ed2a10b1b641a2b.pdf
  start_page: 112
  title: 'Promptly Predicting Structures: The Return of Inference'
- abstract: 'Structured data, prevalent in tables, databases, and knowledge graphs,
    poses a significant challenge in its representation. With the advent of large
    language models (LLMs), there has been a shift towards linearization-based methods,
    which process structured data as sequential token streams, diverging from approaches
    that explicitly model structure, often as a graph. Crucially, there remains a
    gap in our understanding of how these linearization-based methods handle structured
    data, which is inherently non-linear.

    This work investigates the linear handling of structured data in encoder-decoder
    language models, specifically T5. Our findings reveal the model''s ability to
    mimic human-designed processes such as schema linking and syntax prediction, indicating
    a deep, meaningful learning of structure beyond simple token sequencing. We also
    uncover insights into the model''s internal mechanisms, including the ego-centric
    nature of structure node encodings and the potential for model compression due
    to modality fusion redundancy. Overall, this work sheds light on the inner workings
    of linearization-based methods and could potentially provide guidance for future
    research.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - dblp_id: https://dblp.org/pid/210/0877
    emails: '****@gmail.com'
    first_name: Yutong
    homepage: https://sythello.github.io
    institution: University of California, San Diego
    last_name: Shao
    name: Yutong Shao
    username: ~Yutong_Shao1
  - dblp_id: https://dblp.org/pid/98/53
    emails: '****@ucsd.edu'
    first_name: Ndapa
    homepage: http://ndapa.us
    institution: University of California, San Diego
    last_name: Nakashole
    name: Ndapa Nakashole
    username: ~Ndapa_Nakashole2
  decision: toMainConference
  end_page: 156
  file: 16.pdf
  id: 16
  num_pages: 26
  openreview_id: Q7qC8CyDYl
  pdf_file: dd419579baa3283eb7b214f2e0d4e363cb9ca3ca.pdf
  start_page: 131
  title: 'On Linearizing Structured Data in Encoder-Decoder Language Models: Insights
    from Text-to-SQL'
- abstract: Standard extractive systems suffer from the lack of gold training signals
    since existing corpora solely provide document and human-written summary pairs
    while disregarding extractive labels. As a result, existing methods resort to
    imperfect pseudo-labels that are both biased and error-prone, thereby hindering
    the learning process of extractive models. In contrast, text generators which
    are commonly employed in abstractive summarization can effortlessly overcome this
    predicament on account of flexible sequence-to-sequence architectures. Motivated
    to bypass this inherent limitation, we investigate the possibility of conducting
    extractive summarization with text generators. Through extensive experiments covering
    six summarization benchmarks, we show that high-quality extractive summaries can
    be assembled via approximating the outputs (abstractive summaries) of these generators.
    Moreover, we find that the approximate summaries correlate positively with the
    auxiliary summaries (i.e. a better generator enables the production of better
    extractive summaries). Our results signify a new paradigm for training extractive
    summarizers i.e. learning with generation (abstractive) objectives rather than
    extractive schemes.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Summarization
  authors:
  - dblp_id: https://dblp.org/pid/362/8561
    emails: '****@gmail.com'
    first_name: Thang
    institution: VinAI Research
    last_name: Le
    name: Thang Le
    username: ~Thang_Le2
  - dblp_id: https://dblp.org/pid/81/8329
    emails: '****@ntu.edu.sg'
    first_name: Anh Tuan
    google_scholar_id: https://scholar.google.com.sg/citations?hl=en&user=d6ixOGYAAAAJ&view_op=list_works
    homepage: https://tuanluu.github.io/
    institution: Nanyang Technological University
    last_name: Luu
    name: Anh Tuan Luu
    semantic_scholar_id: https://www.semanticscholar.org/author/Anh-Tuan-Luu/26336902
    username: ~Anh_Tuan_Luu2
  decision: toMainConference
  end_page: 174
  file: 18.pdf
  id: 18
  num_pages: 18
  openreview_id: 5QNk5Natoy
  pdf_file: dabc625c84640f9ff6207cede6bc61172861e72e.pdf
  start_page: 157
  title: Extractive Summarization with Text Generator
- abstract: Modern Neural Machine Translation systems exhibit strong performance in
    several different languages and are constantly improving. Their ability to learn
    continuously is, however, still severely limited by the catastrophic forgetting
    issue. In this work, we leverage a key property of encoder-decoder Transformers,
    i.e. their generative ability, to propose a novel approach to continually learning
    Neural Machine Translation systems. We show how this can effectively learn on
    a stream of experiences comprising different languages, by leveraging a replay
    memory populated by using the model itself as a generator of parallel sentences.
    We empirically demonstrate that our approach can counteract catastrophic forgetting
    without requiring explicit memorization of training data. Code will be publicly
    available upon publication.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Translation
  authors:
  - emails: '****@phd.unipi.it'
    first_name: Michele
    last_name: Resta
    name: Michele Resta
    orcid: https://orcid.org/0000-0002-7811-3895
    username: ~Michele_Resta1
  - dblp_id: https://dblp.org/pid/07/6626
    emails: '****@unipi.it'
    first_name: Davide
    google_scholar_id: https://scholar.google.it/citations?user=1d5n2WkAAAAJ&hl
    homepage: http://pages.di.unipi.it/bacciu/
    institution: University of Pisa
    last_name: Bacciu
    name: Davide Bacciu
    orcid: https://orcid.org/0000-0001-5213-2468
    username: ~Davide_Bacciu1
  decision: toMainConference
  end_page: 191
  file: 19.pdf
  id: 19
  num_pages: 17
  openreview_id: YqCKig1o7E
  pdf_file: fea1b24b7ae93b34325282c7066f9310e2afa679.pdf
  start_page: 175
  title: Self-generated Replay Memories for Continual Neural Machine Translation
- abstract: Vision-language models (VLMs) have recently demonstrated strong efficacy
    as visual assistants that can parse natural queries about the visual content and
    generate human-like outputs. In this work, we explore the ability of these models
    to demonstrate human-like reasoning based on the perceived information. To address
    a crucial concern regarding the extent to which their reasoning capabilities are
    fully consistent and grounded, we also measure the reasoning consistency of these
    models. We achieve this by proposing a chain-of-thought (CoT) based consistency
    measure. However, such an evaluation requires a benchmark that encompasses both
    high-level inference and detailed reasoning chains, which is costly. We tackle
    this challenge by proposing an LLM-Human-in-the-Loop pipeline, which notably reduces
    cost while simultaneously ensuring the generation of a high-quality dataset. Based
    on this pipeline and the existing coarse-grained annotated dataset, we build the
    CURE benchmark to measure both the zero-shot reasoning performance and consistency
    of VLMs. We evaluate existing state-of-the-art VLMs, and find that even the best-performing
    model is unable to demonstrate strong visual reasoning capabilities and consistency,
    indicating that substantial efforts are required to enable VLMs to perform visual
    reasoning as systematically and consistently as humans. As an early step, we propose
    a two-stage training framework aimed at improving both the reasoning performance
    and consistency of VLMs. The first stage involves employing supervised fine-tuning
    of VLMs using step-by-step reasoning samples automatically generated by LLMs.
    In the second stage, we further augment the training process by incorporating
    feedback provided by LLMs to produce reasoning chains that are highly consistent
    and grounded. We empirically highlight the effectiveness of our framework in both
    reasoning performance and consistency.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - dblp_id: https://dblp.org/pid/05/10083
    emails: '****@gmail.com'
    first_name: Yangyi
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=5e9tBtQAAAAJ&view_op=list_works&gmla=AJsN-F6ieV5-6P_WzCdbvRYvxWSI33-VELtb0CU6B5dRbXHRE5PhOLn2bmG_5XkhAUdOEgKxiZd864yv2IVcuooJbWq6x7N7lL1nm_vxeK_QPHLncFhdjSA
    homepage: https://yangyi-chen.github.io/
    institution: School of Computer Science, University of Illinois at Urbana-Champaign
    last_name: Chen
    name: Yangyi Chen
    semantic_scholar_id: https://www.semanticscholar.org/author/Yangyi-Chen/123331686
    username: ~Yangyi_Chen1
  - dblp_id: https://dblp.org/pid/119/1499
    emails: '****@sri.com'
    first_name: Karan
    google_scholar_id: https://scholar.google.co.in/citations?user=Kn-2t0oAAAAJ&hl=en
    homepage: https://www.ksikka.com
    institution: SRI International
    last_name: Sikka
    name: Karan Sikka
    username: ~Karan_Sikka1
  - dblp_id: http://dblp.uni-trier.de/pers/hd/c/Cogswell:Michael
    emails: '****@sri.com'
    first_name: Michael
    google_scholar_id: https://scholar.google.com/citations?user=9e2wRsoAAAAJ&hl=en
    homepage: http://mcogswell.io/
    institution: SRI International
    last_name: Cogswell
    name: Michael Cogswell
    orcid: https://orcid.org/0000-0003-1647-0325
    username: ~Michael_Cogswell1
  - emails: '****@illinois.edu'
    first_name: Heng
    google_scholar_id: https://scholar.google.com/citations?user=z7GCqT4AAAAJ&hl=en
    homepage: http://blender.cs.illinois.edu/hengji.html
    institution: University of Illinois, Urbana-Champaign
    last_name: Ji
    name: Heng Ji
    username: ~Heng_Ji3
  - dblp_id: https://dblp.org/pid/73/467
    emails: '****@sri.com'
    first_name: Ajay
    google_scholar_id: https://scholar.google.com/citations?user=N-i5QKIAAAAJ&hl=en
    homepage: https://sites.google.com/view/ajaydivakaran/home
    institution: SRI International
    last_name: Divakaran
    name: Ajay Divakaran
    orcid: https://orcid.org/0000-0003-0371-5346
    semantic_scholar_id: https://www.semanticscholar.org/author/Ajay-Divakaran/47977519
    username: ~Ajay_Divakaran1
  decision: toMainConference
  end_page: 210
  file: 20.pdf
  id: 20
  num_pages: 19
  openreview_id: GWqnDYUukH
  pdf_file: ff5c2d2adb4adbe61f276cb79e82b8835fa55b3d.pdf
  start_page: 192
  title: Measuring and Improving Chain-of-Thought Reasoning in Vision-Language Models
- abstract: We characterize and study zero-shot abstractive summarization in Large
    Language Models (LLMs) by measuring position bias, which we propose as a general
    formulation of the more restrictive lead bias phenomenon studied previously in
    the literature. Position bias captures the tendency of a model unfairly prioritizing
    information from certain parts of the input text over others, leading to undesirable
    behavior. Through numerous experiments on four diverse real-world datasets, we
    study position bias in multiple LLM models such as GPT 3.5-Turbo, Llama-2, and
    Dolly-v2, as well as state-of-the-art pretrained encoder-decoder abstractive summarization
    models such as Pegasus and BART. Our findings lead to novel insights and discussion
    on performance and position bias of models for zero-shot summarization tasks.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - dblp_id: https://dblp.org/pid/199/8940.html
    emails: '****@ucdavis.edu'
    first_name: Anshuman
    google_scholar_id: https://scholar.google.co.in/citations?user=1U7Zy7sAAAAJ&hl=en
    homepage: https://anshumanc.com
    institution: University of California, Davis
    last_name: Chhabra
    name: Anshuman Chhabra
    orcid: https://orcid.org/0000-0001-9376-2896
    username: ~Anshuman_Chhabra1
  - emails: '****@ucdavis.edu'
    first_name: Hadi
    google_scholar_id: https://scholar.google.com/citations?user=27esPRsAAAAJ&hl=en&authuser=1
    last_name: Askari
    name: Hadi Askari
    username: ~Hadi_Askari1
  - emails: '****@ucdavis.edu'
    first_name: Prasant
    homepage: https://faculty.engineering.ucdavis.edu/mohapatra/
    institution: University of South Florida
    last_name: Mohapatra
    name: Prasant Mohapatra
    username: ~Prasant_Mohapatra1
  decision: toMainConference
  end_page: 221
  file: 21.pdf
  id: 21
  num_pages: 11
  openreview_id: 7ZTpp2bY0W
  pdf_file: 31416cf99a9e9d56fad31782e6ed6cf48c1e2107.pdf
  start_page: 211
  title: Revisiting Zero-Shot Abstractive Summarization in the Era of Large Language
    Models from the Perspective of Position Bias
- abstract: 'Cultural variation exists between nations (e.g., the United States vs.
    China), but also within regions (e.g., California vs. Texas, Los Angeles vs. San
    Francisco). Measuring this regional cultural variation can illuminate how and
    why people think and behave differently. Historically, it has been difficult to
    computationally model cultural variation due to a lack of training data and scalability
    constraints.  In this work, we introduce a new research problem for the NLP community:
    How do we measure variation in cultural constructs across regions using language?
    We then provide a scalable solution: building knowledge-guided lexica to model
    cultural variation, encouraging future work at the intersection of NLP and cultural
    understanding. We also highlight modern LLMs'' failure to measure cultural variation
    or generate culturally varied language.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Computational Social Science and Cultural Analytics
  authors:
  - emails: '****@seas.upenn.edu'
    first_name: Shreya
    google_scholar_id: https://scholar.google.com/citations?user=h2tzi9MAAAAJ&hl=en
    homepage: https://shreyahavaldar.com
    institution: University of Pennsylvania
    last_name: Havaldar
    name: Shreya Havaldar
    username: ~Shreya_Havaldar1
  - dblp_id: https://dblp.org/pid/184/3739
    emails: '****@sas.upenn.edu'
    first_name: Salvatore
    google_scholar_id: https://scholar.google.com/citations?user=0O7xQLkAAAAJ&hl=en&oi=ao
    homepage: http://www.salvatoregiorgi.com
    institution: University of Pennsylvania
    last_name: Giorgi
    name: Salvatore Giorgi
    orcid: https://orcid.org/0000-0001-7381-6295
    semantic_scholar_id: https://www.semanticscholar.org/author/Salvatore-Giorgi/35037954
    username: ~Salvatore_Giorgi1
  - dblp_id: https://dblp.org/pid/161/3908
    emails: '****@seas.upenn.edu'
    first_name: Sunny
    google_scholar_id: https://scholar.google.com/citations?user=C-gi0v8AAAAJ&hl=en
    homepage: https://raisunny.com/news/
    institution: School of Engineering and Applied Science, University of Pennsylvania
    last_name: Rai
    name: Sunny Rai
    orcid: https://orcid.org/0000-0002-0677-3747
    semantic_scholar_id: https://www.semanticscholar.org/author/Sunny-Rai/2773814
    username: ~Sunny_Rai1
  - emails: '****@uchicago.edu'
    first_name: Thomas
    google_scholar_id: https://scholar.google.com/citations?user=XIEKgCQAAAAJ&hl=en&oi=ao
    homepage: https://thomastalhelm.weebly.com/
    institution: University of Chicago
    last_name: Talhelm
    name: Thomas Talhelm
    orcid: https://orcid.org/0000-0002-0954-5758
    username: ~Thomas_Talhelm1
  - dblp_id: https://dblp.org/pid/132/8947
    emails: '****@gmail.com'
    first_name: Sharath Chandra
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=76_hrfUAAAAJ&view_op=list_works&sortby=pubdate
    homepage: http://chandrasg.github.io/
    institution: University of Pennsylvania
    last_name: Guntuku
    name: Sharath Chandra Guntuku
    semantic_scholar_id: https://www.semanticscholar.org/author/Sharath-Chandra-Guntuku/2731733
    username: ~Sharath_Chandra_Guntuku2
  - dblp_id: https://dblp.org/pid/u/LyleHUngar
    emails: '****@cis.upenn.edu'
    first_name: Lyle
    google_scholar_id: https://scholar.google.com.tw/citations?user=KCiDjbkAAAAJ
    homepage: http://www.cis.upenn.edu/~ungar/
    last_name: Ungar
    name: Lyle Ungar
    semantic_scholar_id: https://www.semanticscholar.org/author/Lyle-Ungar/1717822
    username: ~Lyle_Ungar1
  decision: toMainConference
  end_page: 237
  file: 22.pdf
  id: 22
  num_pages: 16
  openreview_id: geW1Jhys51
  pdf_file: ccae4d2b641cd63cc3e22d8e9545ce1c96dda9a0.pdf
  start_page: 222
  title: Building Knowledge-Guided Lexica to Model Cultural Variation
- abstract: Singular Value Decomposition (SVD) or its weighted variants has significantly
    progressed in compressing language models. Previous works assume the same importance
    for all operations and assign the same number of ranks for different layers in
    a language model. However, such a uniform rank selection is sub-optimal since
    different operations (layers) have non-uniform demand in capacity. In other words,
    a desired SVD strategy should allocate more ranks for important operations and
    vice versa. However, a globally-optimized selection of ranks for neural networks
    is still an open problem, and this is a non-trivial challenge since the selection
    is discrete. In this work, we propose a novel binary masking mechanism for optimizing
    the number of ranks in a differentiable framework. Our strategy uses a novel regularization
    to enable the masking to comply with the SVD property where the ranks have sorted
    singular values. The experiments examined both types of language models, encoder-only
    and decoder-only models, including large language models like LLaMA. Our compressed
    model achieves much better accuracy than previous SVD and their SOTA variants.
    More interestingly, our method retains significantly better accuracy with zero
    or limited fine-tuning, proving the substantial advantage of adaptive rank selection.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Efficient/Low-Resource Methods for NLP
  authors:
  - dblp_id: https://dblp.org/pid/195/2523
    emails: '****@pitt.edu'
    first_name: Shangqian
    google_scholar_id: https://scholar.google.com/citations?user=9mNI83oAAAAJ&hl=en
    last_name: Gao
    name: Shangqian Gao
    semantic_scholar_id: https://www.semanticscholar.org/author/Shangqian-Gao/9355577
    username: ~Shangqian_Gao1
  - dblp_id: https://dblp.org/pid/134/4002
    emails: '****@gmail.com'
    first_name: Ting
    google_scholar_id: https://scholar.google.com/citations?user=ynA-x2wAAAAJ&hl=en
    institution: Samsung
    last_name: Hua
    name: Ting Hua
    username: ~Ting_Hua1
  - dblp_id: https://dblp.org/pid/172/1140
    emails: '****@gmail.com'
    first_name: Yen-Chang
    google_scholar_id: https://scholar.google.com/citations?user=7QWAiigAAAAJ&hl=en
    institution: Samsung Research America
    last_name: Hsu
    name: Yen-Chang Hsu
    username: ~Yen-Chang_Hsu1
  - dblp_id: https://dblp.org/pid/30/383
    emails: '****@samsung.com'
    first_name: Yilin
    google_scholar_id: https://scholar.google.com/citations?user=9PSFMzAAAAAJ
    institution: Samsung Research America
    last_name: Shen
    name: Yilin Shen
    username: ~Yilin_Shen1
  - dblp_id: https://dblp.org/pid/55/2789
    emails: '****@samsung.com'
    first_name: Hongxia
    institution: Samsung Research America AI center
    last_name: Jin
    name: Hongxia Jin
    username: ~Hongxia_Jin1
  decision: toMainConference
  end_page: 252
  file: 23.pdf
  id: 23
  num_pages: 15
  openreview_id: GGbbTfM2VE
  pdf_file: c0a2f474e7255f5c217c75629479531340ee436c.pdf
  start_page: 238
  title: Adaptive Rank Selections for Low-Rank Approximation of Language Models
- abstract: Consistency regularization methods, such as R-Drop (Liang et al., 2021)
    and CrossConST (Gao et al., 2023), have achieved impressive supervised and zero-shot
    performance in the neural machine translation (NMT) field. Can we also boost end-to-end
    (E2E) speech-to-text translation (ST) by leveraging consistency regularization?
    In this paper, we conduct empirical studies on intra-modal and cross-modal consistency
    and propose two training strategies, SimRegCR and SimZeroCR, for E2E ST in regular
    and zero-shot scenarios. Experiments on the MuST-C benchmark show that our approaches
    achieve state-of-the-art (SOTA) performance in most translation directions. The
    analyses prove that regularization brought by the intra-modal consistency, instead
    of the modality gap, is crucial for the regular E2E ST, and the cross-modal consistency
    could close the modality gap and boost the zero-shot E2E ST performance.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Translation
  authors:
  - dblp_id: https://dblp.org/pid/157/1201
    emails: '****@gmail.com'
    first_name: Pengzhi
    google_scholar_id: https://scholar.google.com/citations?user=r-eKI0oAAAAJ&hl=en
    homepage: https://gpengzhi.github.io/
    institution: Baidu
    last_name: Gao
    name: Pengzhi Gao
    semantic_scholar_id: https://www.semanticscholar.org/author/Pengzhi-Gao/1878737
    username: ~Pengzhi_Gao1
  - dblp_id: https://dblp.org/pid/23/7727.html
    emails: '****@baidu.com'
    first_name: Ruiqing
    google_scholar_id: https://scholar.google.com/citations?user=zi380bkAAAAJ&hl=en
    last_name: Zhang
    name: Ruiqing Zhang
    semantic_scholar_id: https://www.semanticscholar.org/author/Ruiqing-Zhang/49775095
    username: ~Ruiqing_Zhang1
  - dblp_id: https://dblp.org/pid/50/1726
    emails: '****@baidu.com'
    first_name: Zhongjun
    google_scholar_id: https://scholar.google.com/citations?user=a-1wSFYAAAAJ&hl=zh-CN
    homepage: https://zhongjunhe.github.io
    institution: Baidu
    last_name: He
    name: Zhongjun He
    username: ~Zhongjun_He1
  - dblp_id: https://dblp.org/pid/27/6045-3
    emails: '****@baidu.com'
    first_name: Hua
    google_scholar_id: https://scholar.google.com/citations?user=9X2ThuAAAAAJ&hl=en
    homepage: https://wuhuanlp.github.io/
    last_name: Wu
    name: Hua Wu
    orcid: https://orcid.org/0000-0001-8254-1561
    semantic_scholar_id: https://www.semanticscholar.org/author/Hua-Wu/40354707
    username: ~Hua_Wu4
  - emails: '****@baidu.com'
    first_name: Haifeng
    homepage: http://ir.hit.edu.cn/~wanghaifeng/
    institution: Baidu
    last_name: Wang
    name: Haifeng Wang
    username: ~Haifeng_Wang3
  decision: toMainConference
  end_page: 267
  file: 25.pdf
  id: 25
  num_pages: 15
  openreview_id: 5z7tzSgAEl
  pdf_file: 8d0e998a2d7babd81ac23510e9a6ece1dfa0db01.pdf
  start_page: 253
  title: An Empirical Study of Consistency Regularization for End-to-End Speech-to-Text
    Translation
- abstract: 'Human intelligence thrives on cognitive synergy, where collaboration
    among different minds yield superior outcomes compared to isolated individuals.
    In this work, we propose Solo Performance Prompting (SPP), which transforms a
    single LLM into a cognitive synergist by engaging in multi-turn self-collaboration
    with multiple personas. A cognitive synergist is an intelligent agent that collaboratively
    combines multiple minds'' strengths and knowledge to enhance problem-solving in
    complex tasks. By dynamically identifying and simulating different personas based
    on task inputs, SPP unleashes the potential of cognitive synergy in LLMs. Our
    in-depth analysis shows that assigning multiple fine-grained personas in LLMs
    improves problem-solving abilities compared to using a single or fixed number
    of personas. We evaluate SPP on three challenging tasks: Trivia Creative Writing,
    Codenames Collaborative, and Logic Grid Puzzle, encompassing both knowledge-intensive
    and reasoning-intensive types. Unlike previous works, such as Chain-of-Thought,
    that solely enhance the reasoning abilities in LLMs, experimental results demonstrate
    that SPP effectively reduces factual hallucination, and maintains strong reasoning
    capabilities. Additionally, comparative experiments show that cognitive synergy
    only emerges in GPT-4 and does not appear in less capable models, such as GPT-3.5-turbo
    and Llama2-13b-chat, which draws an interesting analogy to human development.
    Code, data, and prompts can be found at: https://github.com/MikeWangWZHL/Solo-Performance-Prompting.git.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - dblp_id: https://dblp.org/pid/290/1319
    emails: '****@illinois.edu'
    first_name: Zhenhailong
    google_scholar_id: https://scholar.google.com/citations?user=arzvOlgAAAAJ&hl=en
    homepage: https://mikewangwzhl.github.io/
    last_name: Wang
    name: Zhenhailong Wang
    orcid: https://orcid.org/0000-0002-4704-5455
    username: ~Zhenhailong_Wang1
  - dblp_id: https://dblp.org/pid/214/0365
    emails: '****@microsoft.com'
    first_name: Shaoguang
    google_scholar_id: https://scholar.google.com/citations?user=S6XnZsQAAAAJ&hl=en
    homepage: https://www.linkedin.com/in/shaoguang-mao-929733120/
    institution: Microsoft
    last_name: Mao
    name: Shaoguang Mao
    semantic_scholar_id: https://www.semanticscholar.org/author/Shaoguang-Mao/35374367
    username: ~Shaoguang_Mao1
  - dblp_id: https://dblp.org/pid/224/7692
    emails: '****@microsoft.com'
    first_name: Wenshan
    google_scholar_id: https://scholar.google.com/citations?user=AwANOsQAAAAJ&hl=en
    institution: Microsoft
    last_name: Wu
    name: Wenshan Wu
    semantic_scholar_id: https://www.semanticscholar.org/author/Wenshan-Wu/51198241
    username: ~Wenshan_Wu2
  - dblp_id: https://dblp.org/pid/136/7923
    emails: '****@gmail.com'
    first_name: Tao
    google_scholar_id: https://scholar.google.com/citations?user=LYbs7Q8AAAAJ&hl=en
    homepage: https://getao.github.io/
    last_name: Ge
    name: Tao Ge
    semantic_scholar_id: https://www.semanticscholar.org/author/Tao-Ge/2547492
    username: ~Tao_Ge1
  - dblp_id: https://dblp.org/pid/72/5870
    emails: '****@microsoft.com'
    first_name: Furu
    google_scholar_id: https://scholar.google.com/citations?user=G-V1VpwAAAAJ&hl=en
    homepage: https://www.microsoft.com/en-us/research/people/fuwei/
    institution: Microsoft Research
    last_name: Wei
    name: Furu Wei
    username: ~Furu_Wei1
  - emails: '****@illinois.edu'
    first_name: Heng
    google_scholar_id: https://scholar.google.com/citations?user=z7GCqT4AAAAJ&hl=en
    homepage: http://blender.cs.illinois.edu/hengji.html
    institution: University of Illinois, Urbana-Champaign
    last_name: Ji
    name: Heng Ji
    username: ~Heng_Ji3
  decision: toMainConference
  end_page: 290
  file: 26.pdf
  id: 26
  num_pages: 23
  openreview_id: xugzsQ21Lb
  pdf_file: fe05d22d1032885866c41d2072a0ec6f6b5fccaf.pdf
  start_page: 268
  title: 'Unleashing the Emergent Cognitive Synergy in Large Language Models: A Task-Solving
    Agent through Multi-Persona Self-Collaboration'
- abstract: Prompt-based methods have achieved promising results in most few-shot
    text classification tasks. However, for readability assessment tasks, traditional
    prompt methods lack crucial linguistic knowledge, which has already been proven
    to be essential.Moreover, previous studies on utilizing linguistic features have
    shown non-robust performance in few-shot settings and may even impair model performance.To
    address these issues, we propose a novel prompt-based tuning framework that incorporates
    rich linguistic knowledge, called Feature Prompt Tuning (FPT). Specifically, we
    extract linguistic features from the text and embed them into trainable soft prompts.
    Further, we devise a new loss function to calibrate the similarity ranking order
    between categories. Experimental results demonstrate that our proposed method
    FTPnot only exhibits a significant performance improvement over the prior best
    prompt-based tuning approaches, but also surpasses the previous leading methods
    that incorporate linguistic features.  Also, our proposed model significantly
    outperforms the large language model gpt-3.5-turbo-16k in most cases. Our proposed
    method establishes a new architecture for prompt tuning that sheds light on how
    linguistic features can be easily adapted to linguistic-related tasks.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Retrieval and Text Mining
  authors:
  - emails: '****@stu.pku.edu.cn'
    first_name: Ziyang
    homepage: https://github.com/Wzy232303/Wzy232303.github.io
    last_name: Wang
    name: Ziyang Wang
    username: ~Ziyang_Wang2
  - emails: '****@pku.edu.cn'
    first_name: Sanwoo
    google_scholar_id: https://scholar.google.com/citations?user=-444B1UAAAAJ&hl=ko
    homepage: https://github.com/elixir2325
    institution: Peking University
    last_name: Lee
    name: Sanwoo Lee
    username: ~Sanwoo_Lee1
  - emails: '****@stu.pku.edu.cn'
    first_name: Hsiu-Yuan
    homepage: https://github.com/Celine-hxy
    last_name: Huang
    name: Hsiu-Yuan Huang
    username: ~Hsiu-Yuan_Huang1
  - dblp_id: https://dblp.org/search?q=yunfang+wu
    emails: '****@pku.edu.cn'
    first_name: Yunfang
    homepage: https://cs.pku.edu.cn/info/1083/1300.htm
    last_name: Wu
    name: Yunfang Wu
    semantic_scholar_id: https://www.semanticscholar.org/author/Yunfang-Wu/2477658
    username: ~Yunfang_Wu1
  decision: toMainConference
  end_page: 306
  file: 27.pdf
  id: 27
  num_pages: 16
  openreview_id: QB6MGIrUfa
  pdf_file: 6336af3bc61335034a531a00999e6a108efe4a62.pdf
  start_page: 291
  title: 'FPT: Feature Prompt Tuning for Few-shot Readability Assessment'
- abstract: Despite the remarkable capabilities of Large Language Models (LLMs) like
    GPT-4, producing complex, structured tabular data remains challenging. Our study
    assesses LLMs' proficiency in structuring tables and introduces a novel fine-tuning
    method, cognizant of data structures, to bolster their performance. We unveil
    Struc-Bench, a comprehensive benchmark featuring prominent LLMs (GPT-NeoX-20B,
    GPT-3.5, GPT-4, and Vicuna), which spans text tables, HTML, and LaTeX formats.
    Our proposed FormatCoT aids in crafting format-specific instructions from the
    intended outputs to populate this benchmark. Addressing the gap in task-centered
    evaluation, we propose two innovative metrics, P-Score (Prompting Score) and H-Score
    (Heuristical Score), to more accurately gauge LLM performance. Our experiments
    show that applying our structure-aware fine-tuning to LLaMA-7B leads to substantial
    performance gains, outshining its LLM counterparts across most measures. In-depth
    error analysis and creating an ability map across six dimensions, coverage, formatting,
    reasoning, comprehension, pragmatics, and hallucination, highlight areas for future
    enhancements and suggest forthcoming research trajectories. Our code and models
    can be found at https://github.com/gersteinlab/Struc-Bench.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Generation
  authors:
  - dblp_id: https://dblp.org/pid/246/8064
    emails: '****@gmail.com'
    first_name: Xiangru
    homepage: https://xiangrutang.github.io/
    institution: Yale University
    last_name: Tang
    name: Xiangru Tang
    semantic_scholar_id: https://www.semanticscholar.org/author/Xiangru-Tang/47274259
    username: ~Xiangru_Tang2
  - emails: '****@qq.com'
    first_name: Yiming
    homepage: https://github.com/BayernEmin
    last_name: Zong
    name: Yiming Zong
    username: ~Yiming_Zong1
  - dblp_id: https://dblp.org/pid/227/3174
    emails: '****@nyu.edu'
    first_name: Jason
    google_scholar_id: https://scholar.google.com/citations?user=hxbdOuoAAAAJ&hl=en
    homepage: https://jasonphang.com/
    institution: OpenAI
    last_name: Phang
    name: Jason Phang
    username: ~Jason_Phang1
  - dblp_id: https://dblp.org/pid/271/8391
    emails: '****@yale.edu'
    first_name: Yilun
    google_scholar_id: https://scholar.google.com/citations?hl=zh-CN&user=yAQAXrIAAAAJ
    homepage: https://yilunzhao.github.io/
    institution: Yale University
    last_name: Zhao
    name: Yilun Zhao
    semantic_scholar_id: https://www.semanticscholar.org/author/Yilun-Zhao/46316984
    username: ~Yilun_Zhao1
  - dblp_id: https://dblp.uni-trier.de/pers/hd/z/Zhou:Wangchunshu
    emails: '****@aiwaves.cn'
    first_name: Wangchunshu
    google_scholar_id: https://scholar.google.com/citations?user=UebIjuQAAAAJ&hl=en
    homepage: https://michaelzhouwang.github.io
    institution: AIWaves Inc.
    last_name: Zhou
    name: Wangchunshu Zhou
    semantic_scholar_id: https://www.semanticscholar.org/author/Wangchunshu-Zhou/150341221
    username: ~Wangchunshu_Zhou1
  - dblp_id: https://dblp.org/pid/160/1727
    emails: '****@yale.edu'
    first_name: Arman
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=baI7IY0AAAAJ
    homepage: http://www.armancohan.com
    institution: Yale University and Allen Institute for Artificial Intelligence
    last_name: Cohan
    name: Arman Cohan
    semantic_scholar_id: https://www.semanticscholar.org/author/Arman-Cohan/2527954
    username: ~Arman_Cohan1
  - dblp_id: https://dblp.org/pid/67/5132
    emails: '****@gersteinlab.org'
    first_name: Mark
    google_scholar_id: https://scholar.google.com/citations?user=YvjuUugAAAAJ&hl=en
    homepage: http://www.gersteinlab.org/
    institution: Yale University
    last_name: Gerstein
    name: Mark Gerstein
    orcid: https://orcid.org/0000-0002-9746-3719
    username: ~Mark_Gerstein2
  decision: toMainConference
  end_page: 329
  file: 28.pdf
  id: 28
  num_pages: 23
  openreview_id: afsxawBzzY
  pdf_file: ab8b8d898a345a34bba2799e3f5a6177ba6f4615.pdf
  start_page: 307
  title: 'Struc-Bench: Are Large Language Models Good at Generating Complex Structured
    Tabular Data?'
- abstract: "Open-Domain Question Answering (ODQA) aims to answer questions without\
    \ explicitly providing specific background documents. \nThis task becomes notably\
    \ challenging in a zero-shot setting where no data is available to train tailored\
    \ retrieval-reader models.\nWhile recent Large Language Models (LLMs) like GPT-3\
    \ have demonstrated their effectiveness in zero-shot ODQA using direct prompting\
    \ methods, these methods still fall short of fully harnessing the potential of\
    \ LLMs when implicitly invoked.\nIn this paper, we propose a Self-Prompting framework\
    \ to explicitly utilize the massive knowledge encoded in the parameters of LLMs\
    \ and their strong instruction understanding abilities. Concretely, we prompt\
    \ LLMs step by step to generate multiple pseudo QA pairs with background passages\
    \ and explanations entirely from scratch.\nThese generated elements are then utilized\
    \ for in-context learning. \nExperimental results show that our method significantly\
    \ surpasses previous state-of-the-art zero-shot methods on three widely-used ODQA\
    \ datasets and even achieves comparable performance with various customized fine-tuned\
    \ models on full training data. Our code is available at https://github.com/lockon-n/self-prompting."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Efficient/Low-Resource Methods for NLP
  authors:
  - dblp_id: https://dblp.org/pid/115/6031
    emails: '****@sjtu.edu.cn'
    first_name: Junlong
    google_scholar_id: https://scholar.google.com/citations?user=UX7TpSYAAAAJ&hl
    last_name: Li
    name: Junlong Li
    semantic_scholar_id: https://www.semanticscholar.org/author/Junlong-Li/46276940
    username: ~Junlong_Li1
  - emails: '****@sjtu.edu.cn'
    first_name: Jinyuan
    homepage: https://website.noewang.top
    last_name: Wang
    name: Jinyuan Wang
    username: ~Jinyuan_Wang1
  - dblp_id: https://dblp.org/pid/06/9708
    emails: '****@sjtu.edu.cn'
    first_name: Zhuosheng
    google_scholar_id: https://scholar.google.co.jp/citations?user=63LTQhgAAAAJ
    homepage: https://bcmi.sjtu.edu.cn/~zhangzs/
    institution: Shanghai Jiao Tong University
    last_name: Zhang
    name: Zhuosheng Zhang
    orcid: https://orcid.org/0000-0002-4183-3645
    semantic_scholar_id: https://www.semanticscholar.org/author/Zhuosheng-Zhang/3322871
    username: ~Zhuosheng_Zhang1
  - dblp_id: https://dblp.org/pid/25/1145
    emails: '****@cs.sjtu.edu.cn'
    first_name: Hai
    google_scholar_id: https://scholar.google.com.tw/citations?user=4dU5KS0AAAAJ
    homepage: http://bcmi.sjtu.edu.cn/~zhaohai/
    institution: Shanghai Jiao Tong University
    last_name: Zhao
    name: hai zhao
    username: ~hai_zhao1
  decision: toMainConference
  end_page: 344
  file: 29.pdf
  id: 29
  num_pages: 15
  openreview_id: C5MczYuzat
  pdf_file: ea4f66d763914d958bdef91a9b5de89142e7d2eb.pdf
  start_page: 330
  title: Self-Prompting Large Language Models for Zero-Shot Open-Domain QA
- abstract: 'Since the recent prosperity of Large Language Models (LLMs), there have
    been interleaved discussions regarding how to reduce hallucinations from LLM responses,
    how to increase the factuality of LLMs, and whether Knowledge Graphs (KGs), which
    store the world knowledge in a symbolic form, will be replaced with LLMs. In this
    paper, we try to answer these questions from a new angle: How knowledgeable are
    LLMs?


    To answer this question, we constructed Head-to-Tail, a benchmark that consists
    of 18K question-answer (QA) pairs regarding head, torso, and tail facts in terms
    of popularity. We designed an automated evaluation method and a set of metrics
    that closely approximate the knowledge an LLM confidently internalizes. Through
    a comprehensive evaluation of 16 publicly available LLMs, we show that existing
    LLMs are still far from being perfect in terms of their grasp of factual knowledge,
    especially for facts of torso-to-tail entities.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - dblp_id: https://dblp.org/pid/09/1171-6.html
    emails: '****@cornell.edu'
    first_name: Kai
    google_scholar_id: https://scholar.google.com/citations?user=EIctlPwAAAAJ&hl=en
    homepage: https://www.kaisun.org/
    institution: Meta
    last_name: Sun
    name: Kai Sun
    orcid: https://orcid.org/0000-0001-8262-4906
    semantic_scholar_id: https://www.semanticscholar.org/author/Kai-Sun/49871029
    username: ~Kai_Sun5
  - emails: '****@gmail.com'
    first_name: Yifan
    google_scholar_id: https://scholar.google.com/citations?user=e0KdvxUAAAAJ&hl=en
    homepage: https://scholar.google.com/citations?user=e0KdvxUAAAAJ&hl=en
    last_name: Xu
    middle_name: Ethan
    name: Yifan Ethan Xu
    orcid: https://orcid.org/0000-0002-7539-4492
    username: ~Yifan_Ethan_Xu1
  - dblp_id: https://dblp.org/pid/219/1743
    emails: '****@gmail.com'
    first_name: Hanwen
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=xjH5FBsAAAAJ
    homepage: https://zhw12.github.io/
    institution: Facebook
    last_name: Zha
    name: Hanwen Zha
    semantic_scholar_id: https://www.semanticscholar.org/author/Hanwen-Zha/47291370
    username: ~Hanwen_Zha1
  - emails: '****@fb.com'
    first_name: Yue
    google_scholar_id: https://scholar.google.com/citations?user=fAMLTWYAAAAJ&hl=en
    last_name: Liu
    name: Yue Liu
    username: ~Yue_Liu18
  - dblp_id: https://dblp.uni-trier.de/pers/hd/d/Dong_0001:Xin
    emails: '****@amazon.com'
    first_name: Xin Luna
    google_scholar_id: https://scholar.google.com/citations?user=uGsKvHoAAAAJ&hl=en&oi=ao
    homepage: http://lunadong.com
    institution: Department of Computer Science, University of Washington and Amazon
    last_name: Dong
    name: Xin Luna Dong
    username: ~Xin_Luna_Dong1
  decision: toMainConference
  end_page: 359
  file: 30.pdf
  id: 30
  num_pages: 15
  openreview_id: p8HzhcSLuE
  pdf_file: 583c62147c0e0f95900d0d546fee9a534fc085c2.pdf
  start_page: 345
  title: 'Head-to-Tail: How Knowledgeable are Large Language Models (LLMs)? A.K.A.
    Will LLMs Replace Knowledge Graphs?'
- abstract: 'Task-Oriented Parsing (TOP) enables conversational assistants to interpret
    user commands expressed in natural language, transforming them into structured
    outputs that combine elements of both natural language and intent/slot tags. Recently,
    Large Language Models (LLMs) have achieved impressive performance in synthesizing
    computer programs based on a natural-language prompt, mitigating the gap between
    natural language and structured programs. Our paper focuses on harnessing the
    capabilities of LLMs for semantic parsing tasks, addressing the following three
    key research questions: 1) How can LLMs be effectively utilized for semantic parsing
    tasks? 2) What defines an effective prompt? and 3) How can LLM overcome the length
    constraint and streamline prompt design by including all examples as prompts?
    We introduce k Nearest Neighbor In-Context Learning (kNN-ICL), which simplifies
    prompt engineering by allowing it to be built on top of any design strategy while
    providing access to all demo examples. Extensive experiments show that: 1) Simple
    ICL without kNN search can achieve a comparable performance with strong supervised
    models on the TOP tasks, and 2) kNN-ICL significantly improves the comprehension
    of complex requests by seamlessly integrating ICL with a nearest-neighbor approach.
    Notably, this enhancement is achieved without the need for additional data or
    specialized prompts.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: 'Semantics: Sentence-level Semantics, Textual Inference and Other
      areas'
  authors:
  - dblp_id: https://dblp.org/pid/41/10049-6.html
    emails: '****@gmail.com'
    first_name: Wenting
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=aySy_OMAAAAJ
    last_name: Zhao
    name: Wenting Zhao
    username: ~Wenting_Zhao4
  - dblp_id: https://dblp.org/pid/96/2615-6
    emails: '****@salesforce.com'
    first_name: Ye
    google_scholar_id: https://scholar.google.com/citations?user=QMKD6YMAAAAJ&hl=en
    institution: SalesForce.com
    last_name: Liu
    name: Ye Liu
    username: ~Ye_Liu4
  - dblp_id: https://dblp.uni-trier.de/pid/167/0275
    emails: '****@hust.edu.cn'
    first_name: Yao
    google_scholar_id: https://scholar.google.com/citations?user=c3MtqtMAAAAJ&hl=en
    homepage: http://wanyao.me
    institution: Huazhong University of Science and Technology
    last_name: Wan
    name: Yao Wan
    username: ~Yao_Wan2
  - emails: '****@uic.edu'
    first_name: Yibo
    google_scholar_id: https://scholar.google.com/citations?user=ke40h00AAAAJ&hl=en
    homepage: https://yibowang214.github.io/
    last_name: Wang
    name: Yibo Wang
    username: ~Yibo_Wang6
  - dblp_id: https://dblp.org/pid/242/8977.html
    emails: '****@columbia.edu'
    first_name: Qingyang
    google_scholar_id: https://scholar.google.ca/citations?user=HDiw-TsAAAAJ&hl=en
    homepage: https://qywu.github.io/about.html
    institution: Columbia University
    last_name: Wu
    name: Qingyang Wu
    semantic_scholar_id: https://www.semanticscholar.org/author/Qing-yang-Wu/31060482
    username: ~Qingyang_Wu1
  - dblp_id: https://dblp.org/pid/277/9318
    emails: '****@uic.edu'
    first_name: Zhongfen
    google_scholar_id: https://scholar.google.com/citations?user=Py0r7TQAAAAJ&hl=zh-CN&oi=ao
    institution: University of Illinois, Chicago
    last_name: Deng
    name: Zhongfen Deng
    semantic_scholar_id: https://www.semanticscholar.org/author/Zhongfen-Deng/2007535648
    username: ~Zhongfen_Deng1
  - dblp_id: https://dblp.org/pid/304/3530
    emails: '****@uic.edu'
    first_name: Jiangshu
    homepage: https://github.com/jiangshdd
    institution: University of Illinois at Chicago
    last_name: Du
    name: Jiangshu Du
    username: ~Jiangshu_Du1
  - dblp_id: https://dblp.org/pid/148/2749-2
    emails: '****@connect.polyu.hk'
    first_name: Shuaiqi
    google_scholar_id: https://scholar.google.com.hk/citations?user=OLgJJ2MAAAAJ&hl=en&inst=10434768688823972643&inst=17644838422235682599&inst=569367360547434339
    homepage: https://stevenlau6.github.io/
    last_name: Liu
    name: Shuaiqi LIU
    orcid: https://orcid.org/0000-0002-6754-1587
    semantic_scholar_id: https://www.semanticscholar.org/author/Shuaiqi-Liu/3344531
    username: ~Shuaiqi_LIU1
  - emails: '****@binghamton.edu'
    first_name: Yunlong
    last_name: Xu
    name: Yunlong Xu
    username: ~Yunlong_Xu2
  - dblp_id: https://dblp.org/pid/y/PhilipSYu
    emails: '****@uic.edu'
    first_name: Philip
    google_scholar_id: https://scholar.google.com/citations?user=D0lL1r0AAAAJ&hl=zh-CN
    homepage: https://cs.uic.edu/profiles/philip-yu/
    institution: University of Illinois, Chicago
    last_name: Yu
    middle_name: S.
    name: Philip S. Yu
    orcid: https://orcid.org/0000-0002-3491-5968
    username: ~Philip_S._Yu1
  decision: toMainConference
  end_page: 371
  file: 31.pdf
  id: 31
  num_pages: 12
  openreview_id: ILq4aYrmae
  pdf_file: 563831f323e1c3c25277e58c72b58c906a19b983.pdf
  start_page: 360
  title: '$k$NN-ICL: Compositional Task-Oriented Parsing Generalization with Nearest
    Neighbor In-Context Learning'
- abstract: Evaluating retrieval-augmented generation (RAG) systems traditionally
    relies on hand annotations for input queries, passages to retrieve, and responses
    to generate. We introduce ARES, an Automated RAG Evaluation System, for evaluating
    RAG systems along the dimensions of context relevance, answer faithfulness, and
    answer relevance. By creating its own synthetic training data, ARES finetunes
    lightweight LM judges to assess the quality of individual RAG components. To mitigate
    potential prediction errors, ARES utilizes a small set of human-annotated datapoints
    for prediction-powered inference (PPI). Across eight different knowledge-intensive
    tasks in KILT, SuperGLUE, and AIS, ARES accurately evaluates RAG systems while
    using only a few hundred human annotations during evaluation. Furthermore, ARES
    judges remain effective across domain shifts, proving accurate even after changing
    the type of queries and/or documents used in the evaluated RAG systems. We make
    our code and datasets publicly available on Github.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Efficient/Low-Resource Methods for NLP
  authors:
  - emails: '****@gmail.com'
    first_name: Jon
    google_scholar_id: https://scholar.google.com/citations?user=zCVmjboAAAAJ&hl=en
    homepage: https://jonsaadfalcon.com/
    institution: Computer Science Department, Stanford University
    last_name: Saad-Falcon
    name: Jon Saad-Falcon
    semantic_scholar_id: https://www.semanticscholar.org/author/Jon-Saad-Falcon/1742192721
    username: ~Jon_Saad-Falcon1
  - dblp_id: https://dblp.org/pid/129/7815
    emails: '****@stanford.edu'
    first_name: Omar
    homepage: https://scholar.google.com/citations?hl=en&user=Lwr5ozgAAAAJ
    last_name: Khattab
    name: Omar Khattab
    semantic_scholar_id: https://www.semanticscholar.org/author/O.-Khattab/144112155
    username: ~Omar_Khattab1
  - dblp_id: https://dblp.org/pid/13/2617
    emails: '****@stanford.edu'
    first_name: Christopher
    google_scholar_id: https://scholar.google.com/citations?user=3j08YoAAAAAJ&hl=en
    homepage: http://web.stanford.edu/~cgpotts/
    institution: Stanford University
    last_name: Potts
    name: Christopher Potts
    orcid: https://orcid.org/0000-0002-7978-6055
    semantic_scholar_id: https://www.semanticscholar.org/author/Christopher-Potts/144922861
    username: ~Christopher_Potts1
  - dblp_id: https://dblp.org/pid/36/2133
    emails: '****@berkeley.edu'
    first_name: Matei
    google_scholar_id: https://scholar.google.com/citations?user=I1EvjZsAAAAJ
    homepage: https://cs.stanford.edu/~matei/
    institution: University of California, Berkeley and Databricks
    last_name: Zaharia
    name: Matei Zaharia
    orcid: https://orcid.org/0000-0002-7547-7204
    username: ~Matei_Zaharia1
  decision: toMainConference
  end_page: 388
  file: 33.pdf
  id: 33
  num_pages: 17
  openreview_id: ZeUKWDDSwu
  pdf_file: ea3340fe809af735030ae5e8d5b65abc360c31e5.pdf
  start_page: 372
  title: 'ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation
    Systems'
- abstract: Image-text matching has been a long-standing problem, which seeks to connect
    vision and language through semantic understanding. Due to the capability to manage
    large-scale raw data, unsupervised hashing-based approaches have gained prominence
    recently. They typically construct a semantic similarity structure using the natural
    distance, which subsequently guides the optimization of the hashing network. However,
    the similarity structure could be biased at the boundaries of semantic distributions,
    causing error accumulation during sequential optimization. To tackle this, we
    introduce a novel hashing approach termed Distribution-based Structure Mining
    with Consistency Learning (DEMO) for efficient image-text matching. From a statistical
    view, DEMO characterizes each image using multiple augmented views, which are
    considered as samples drawn from its intrinsic semantic distribution. Then, we
    employ a non-parametric distribution divergence to ensure a robust and precise
    similarity structure. In addition, we introduce collaborative consistency learning
    which not only preserves the similarity structure in the Hamming space but also
    encourages consistency between retrieval distribution from different directions
    in a self-supervised manner. Extensive experiments on several widely used datasets
    demonstrate that DEMO achieves superior performance compared with various state-of-the-art
    methods.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Retrieval and Text Mining
  authors:
  - emails: '****@gmail.com'
    first_name: Fan
    google_scholar_id: https://scholar.google.com.hk/citations?user=KbC_-7cAAAAJ&hl=zh-CN&oi=ao
    homepage: https://zfkarl.github.io/
    institution: Georgia Institute of Technology
    last_name: Zhang
    name: Fan Zhang
    orcid: https://orcid.org/0000-0001-5250-7258
    username: ~Fan_Zhang27
  - dblp_id: https://dblp.uni-trier.de/pid/56/5807-1.html
    emails: '****@gmail.com'
    first_name: Xian-Sheng
    google_scholar_id: https://scholar.google.co.uk/citations?user=6G-l4o0AAAAJ&hl=en
    institution: Terminus Group
    last_name: Hua
    name: Xian-Sheng Hua
    username: ~Xian-Sheng_Hua1
  - dblp_id: https://dblp.org/pid/63/713-2
    emails: '****@gmail.com'
    first_name: Chong
    google_scholar_id: https://scholar.google.com/citations?view_op=list_works&hl=en&user=DyeODPUAAAAJ
    institution: Terminus Group
    last_name: Chen
    name: Chong Chen
    username: ~Chong_Chen2
  - dblp_id: https://dblp.org/pid/50/1585-1
    emails: '****@cs.ucla.edu'
    first_name: Xiao
    google_scholar_id: https://scholar.google.com.hk/citations?&user=yJgX8agAAAAJ
    homepage: http://luoxiao12.github.io
    institution: University of California, Los Angeles
    last_name: Luo
    name: Xiao Luo
    semantic_scholar_id: https://www.semanticscholar.org/author/Xiao-Luo/2115826917
    username: ~Xiao_Luo3
  decision: toMainConference
  end_page: 403
  file: 34.pdf
  id: 34
  num_pages: 15
  openreview_id: x6hhULcCAw
  pdf_file: 648fec1e7f64ba9c47e605a699c0715dfae6b6d2.pdf
  start_page: 389
  title: 'DEMO: A Statistical Perspective for Efficient Image-Text Matching'
- abstract: We present SeaEval, a benchmark for multilingual foundation models. In
    addition to characterizing how these models understand and reason with natural
    language, we also investigate how well they comprehend cultural practices, nuances,
    and values. Alongside standard accuracy metrics, we investigate the brittleness
    of foundation models in the dimensions of semantics and multilinguality. Our analyses
    span both open-sourced and closed models, leading to empirical results across
    classic NLP tasks, reasoning, and cultural comprehension. Key findings indicate
    (1) Many models exhibit varied behavior when given paraphrased instructions. (2)
    Many models still suffer from exposure bias (e.g., positional bias, majority label
    bias). (3) For questions rooted in factual, scientific, and commonsense knowledge,
    consistent responses are expected across multilingual queries that are semantically
    equivalent. Yet, most models surprisingly demonstrate inconsistent performance
    on these queries. (4) Multilingually-trained models have not attained ``balanced
    multilingual'' capabilities. Our endeavors underscore the need for more generalizable
    semantic representations and enhanced multilingual contextualization. SeaEval
    can serve as a launchpad for more thorough investigations and evaluations for
    multilingual and multicultural scenarios.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - dblp_id: https://dblp.org/pid/13/1898-40
    emails: '****@gmail.com'
    first_name: Bin
    google_scholar_id: https://scholar.google.com/citations?user=jUrRMv4AAAAJ&hl=en
    homepage: https://binwang28.github.io/
    institution: I2R, A*STAR
    last_name: Wang
    name: Bin Wang
    orcid: https://orcid.org/0000-0001-9760-8343
    semantic_scholar_id: https://www.semanticscholar.org/author/144461647
    username: ~Bin_Wang14
  - dblp_id: https://dblp.org/pid/229/9236
    emails: '****@i2r.a-star.edu.sg'
    first_name: Zhengyuan
    institution: I2R
    last_name: Liu
    name: Zhengyuan Liu
    semantic_scholar_id: https://www.semanticscholar.org/author/Zhengyuan-Liu/49293155
    username: ~Zhengyuan_Liu2
  - emails: '****@i2r.a-star.edu.sg'
    first_name: Xin
    homepage: https://github.com/hxssgaa
    last_name: Huang
    name: Xin Huang
    username: ~Xin_Huang17
  - dblp_id: https://dblp.org/pid/264/9981
    emails: '****@hotmail.com'
    first_name: Fangkai
    google_scholar_id: https://scholar.google.com/citations?user=_u8lwyIAAAAJ&hl=zh-CN
    homepage: https://sparkjiao.github.io/
    last_name: Jiao
    name: Fangkai Jiao
    orcid: https://orcid.org/0000-0002-0670-6990
    semantic_scholar_id: https://www.semanticscholar.org/author/1689176705
    username: ~Fangkai_Jiao1
  - dblp_id: https://dblp.org/pid/17/1255
    emails: '****@i2r.a-star.edu.sg'
    first_name: Yang
    institution: ', A*STAR'
    last_name: Ding
    name: Yang Ding
    username: ~Yang_Ding1
  - dblp_id: https://dblp.org/pid/40/357
    emails: '****@i2r.a-star.edu.sg'
    first_name: AiTi
    google_scholar_id: https://scholar.google.com/citations?user=kNsaEYMAAAAJ&hl=en
    institution: I2R
    last_name: Aw
    name: AiTi Aw
    semantic_scholar_id: https://www.semanticscholar.org/author/AiTi-Aw/1829583
    username: ~AiTi_Aw1
  - dblp_id: https://dblp.org/pid/84/8761
    emails: '****@i2r.a-star.edu.sg'
    first_name: Nancy
    google_scholar_id: https://scholar.google.com.sg/citations?user=K3Z9UiAAAAAJ&hl=en
    homepage: http://alum.mit.edu/www/nancychen
    last_name: Chen
    middle_name: F.
    name: Nancy F. Chen
    orcid: https://orcid.org/0000-0003-0872-5877
    semantic_scholar_id: https://www.semanticscholar.org/author/Nancy-F.-Chen/2185019
    username: ~Nancy_F._Chen1
  decision: toMainConference
  end_page: 424
  file: 35.pdf
  id: 35
  num_pages: 21
  openreview_id: YfSdRItQzE
  pdf_file: 4e8f542ae97d136135d5d4d34e846f0883e4b241.pdf
  start_page: 404
  title: 'SeaEval for Multilingual Foundation Models: From Cross-Lingual Alignment
    to Cultural Reasoning'
- abstract: Large multimodal models suffer from multimodal hallucination, where they
    provide incorrect responses misaligned with the given visual information. Recent
    works have conjectured that one of the reasons behind multimodal hallucination
    is due to the vision encoder failing to ground on the image properly. To mitigate
    this issue, we propose a novel approach that leverages self-feedback as visual
    cues. Building on this approach, we introduce Volcano, a multimodal self-feedback
    guided revision model. Volcano generates natural language feedback to its initial
    response based on the provided visual information and utilizes this feedback to
    self-revise its initial response. Volcano effectively reduces multimodal hallucination
    and achieves state-of-the-art on MMHal-Bench, POPE, and GAVIE. It also improves
    on general multimodal abilities and outperforms previous models on MM-Vet and
    MMBench. Through qualitative analysis, we show that Volcano's feedback is properly
    grounded on the image than the initial response. This indicates that Volcano can
    provide itself with richer visual information through feedback generation, leading
    to self-correct hallucinations. We publicly release our model, data, and code
    at https://github.com/kaistAI/Volcano}{github.com/kaistAI/Volcano
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multimodality and Language Grounding to Vision, Robotics and Beyond
  authors:
  - emails: '****@korea.ac.kr'
    first_name: Seongyun
    last_name: Lee
    name: Seongyun Lee
    username: ~Seongyun_Lee1
  - emails: '****@kaist.ac.kr'
    first_name: Sue
    google_scholar_id: https://scholar.google.com/citations?user=AyZdps8AAAAJ
    homepage: https://suehyunpark.github.io/
    last_name: Park
    middle_name: Hyun
    name: Sue Hyun Park
    username: ~Sue_Hyun_Park1
  - dblp_id: https://dblp.org/pid/252/6347
    emails: '****@gmail.com'
    first_name: Yongrae
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=6BtCrX4AAAAJ&authuser=1&scilu=&scisig=ACseELIAAAAAZF33lvLUcKqN_X5-rTh-BMiLbRE&gmla=AHoSzlUvsJAE5QyJueXcTw1z-zN7qVVOIOVGpkx7ZyGyIl27lmE__kWe_YlYh69MiLL1fQiYOTlS0VH7xsbsLUQsdX4vUPa0MaXsatE&sciund=2870709815918689887
    homepage: https://github.com/dreamgonfly
    last_name: Jo
    name: Yongrae Jo
    username: ~Yongrae_Jo1
  - dblp_id: https://dblp.org/pid/149/1367
    emails: '****@gmail.com'
    first_name: Minjoon
    google_scholar_id: https://scholar.google.com/citations?user=zYze5fIAAAAJ&hl=en
    homepage: https://seominjoon.github.io
    institution: Korea Advanced Institute of Science and Technology
    last_name: Seo
    name: Minjoon Seo
    semantic_scholar_id: https://www.semanticscholar.org/author/Minjoon-Seo/4418074
    username: ~Minjoon_Seo1
  decision: toMainConference
  end_page: 438
  file: 36.pdf
  id: 36
  num_pages: 14
  openreview_id: yKCwVrPTqX
  pdf_file: d8cf1d602495c7cfbb0178fb9e32aba590727a51.pdf
  start_page: 425
  title: 'Volcano: Mitigating Multimodal Hallucination through Self-Feedback Guided
    Revision'
- abstract: "In-context learning (ICL) empowers large language models (LLMs) to perform\
    \ diverse tasks in underrepresented languages using only short in-context information,\
    \ offering a crucial avenue for narrowing the gap between high-resource and low-resource\
    \ languages.\nNonetheless, there is only a handful of works explored ICL for low-resource\
    \ languages with most of them focusing on relatively high-resource languages,\
    \ such as French and Spanish. \nIn this work, we extensively study ICL and its\
    \ cross-lingual variation (X-ICL) on 25 low-resource and 7 relatively higher-resource\
    \ languages.\nOur study not only assesses the effectiveness of ICL with LLMs in\
    \ low-resource languages but also identifies the shortcomings of in-context label\
    \ alignment, and introduces a more effective alternative: query alignment. Moreover,\
    \ we provide valuable insights into various facets of ICL for low-resource languages.\n\
    Our study concludes the significance of few-shot in-context information on enhancing\
    \ the low-resource understanding quality of LLMs through semantically relevant\
    \ information by closing the language gap in the target language and aligning\
    \ the semantics between the targeted low-resource and the high-resource language\
    \ that the model is proficient in. Our work highlights the importance of advancing\
    \ ICL research, particularly for low-resource languages."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multilinguality and Language Diversity
  authors:
  - dblp_id: https://dblp.org/pid/235/2988.html
    emails: '****@connect.ust.hk'
    first_name: Samuel
    google_scholar_id: https://scholar.google.com/citations?user=w5w_WZEAAAAJ&hl=en
    homepage: https://samuelcahyawijaya.github.io/
    institution: The Hong Kong University of Science and Technology
    last_name: Cahyawijaya
    name: Samuel Cahyawijaya
    orcid: https://orcid.org/0000-0002-9891-1608
    semantic_scholar_id: https://www.semanticscholar.org/author/Samuel-Cahyawijaya/66986482
    username: ~Samuel_Cahyawijaya1
  - dblp_id: https://dblp.org/pid/243/6573
    emails: '****@gmail.com'
    first_name: Holy
    google_scholar_id: https://scholar.google.com/citations?user=bugb-lAAAAAJ&hl
    homepage: https://holylovenia.github.io/
    institution: AI Singapore
    last_name: Lovenia
    name: Holy Lovenia
    orcid: https://orcid.org/0000-0002-8995-5107
    semantic_scholar_id: https://www.semanticscholar.org/author/Holy-Lovenia/116344405
    username: ~Holy_Lovenia1
  - dblp_id: https://dblp.org/pid/29/4187
    emails: '****@ece.ust.hk'
    first_name: Pascale
    homepage: http://pascale.home.ece.ust.hk/
    institution: HKUST
    last_name: Fung
    name: Pascale Fung
    username: ~Pascale_Fung1
  decision: toMainConference
  end_page: 467
  file: 37.pdf
  id: 37
  num_pages: 29
  openreview_id: FW5Adlrrz0
  pdf_file: d5513a39ea2120ccd24b08c224e47162a7819086.pdf
  start_page: 439
  title: LLMs Are Few-Shot In-Context Low-Resource Language Learners
- abstract: "Compositional generalization, the ability to predict complex meanings\
    \ from training on simpler sentences, poses challenges for powerful pretrained\
    \ seq2seq models. \nIn this paper, we show that data augmentation methods that\
    \ sample MRs and backtranslate them can be effective for compositional generalization,\
    \ but only if we sample from the right distribution.  \nRemarkably, sampling from\
    \ a uniform distribution performs almost as well as sampling from the test distribution,\
    \ and greatly outperforms earlier methods that sampled from the training distribution.\n\
    We further conduct experiments to investigate the reason why this happens and\
    \ where the benefit of such data augmentation methods come from."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: 'Semantics: Sentence-level Semantics, Textual Inference and Other
      areas'
  authors:
  - dblp_id: https://dblp.org/pid/266/7884.html
    emails: '****@coli.uni-saarland.de'
    first_name: Yuekun
    google_scholar_id: https://scholar.google.com/citations?hl=zh-CN&user=sWCmrQEAAAAJ
    homepage: https://ykyaol7.github.io/about/
    last_name: Yao
    name: Yuekun Yao
    semantic_scholar_id: https://www.semanticscholar.org/author/Yuekun-Yao/1733485928
    username: ~Yuekun_Yao2
  - dblp_id: https://dblp.org/pid/52/3406
    emails: '****@coli.uni-saarland.de'
    first_name: Alexander
    homepage: http://www.coli.uni-saarland.de/~koller/
    institution: Saarland University
    last_name: Koller
    name: Alexander Koller
    semantic_scholar_id: https://www.semanticscholar.org/author/Alexander-Koller/145542037
    username: ~Alexander_Koller2
  decision: toMainConference
  end_page: 483
  file: 40.pdf
  id: 40
  num_pages: 16
  openreview_id: tqrCmcTWJT
  pdf_file: 2b57f7df416bab67bb3b9eaae6e54a900b671577.pdf
  start_page: 468
  title: Simple and effective data augmentation for compositional generalization
- abstract: 'Large Language Models (LLMs) have shown to be capable of various tasks,
    yet their capability in interpreting and reasoning over tabular data remains an
    underexplored area. In this context, this study investigates from three core perspectives:
    the robustness of LLMs to structural perturbations in tables, the comparative
    analysis of textual and symbolic reasoning on tables, and the potential of boosting
    model performance through the aggregation of multiple reasoning pathways. We discover
    that structural variance of tables presenting the same content reveals a notable
    performance decline, particularly in symbolic reasoning tasks. This prompts the
    proposal of a method for table structure normalization. Moreover, textual reasoning
    slightly edges out symbolic reasoning, and a detailed error analysis reveals that
    each exhibits different strengths depending on the specific tasks. Notably, the
    aggregation of textual and symbolic reasoning pathways, bolstered by a mix self-consistency
    mechanism, resulted in achieving SOTA performance, with an accuracy of 73.6% on
    WikiTableQuestions, representing a substantial advancement over previous existing
    table processing paradigms of LLMs.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Question Answering
  authors:
  - dblp_id: https://dblp.org/pid/89/1676-3.html
    emails: '****@ucsd.edu'
    first_name: Tianyang
    google_scholar_id: https://scholar.google.com/citations?user=rJAeYdwAAAAJ&hl=en
    homepage: https://leolty.github.io/
    institution: University of California, San Diego
    last_name: Liu
    name: Tianyang Liu
    orcid: https://orcid.org/0000-0001-7754-7029
    semantic_scholar_id: https://www.semanticscholar.org/author/Tianyang-Liu/2115347044
    username: ~Tianyang_Liu2
  - dblp_id: https://dblp.org/pid/52/3194-60
    emails: '****@gmail.com'
    first_name: Fei
    google_scholar_id: https://scholar.google.com/citations?user=N1O2KT8AAAAJ
    homepage: https://feiwang96.github.io/
    institution: University of Southern California
    last_name: Wang
    name: Fei Wang
    semantic_scholar_id: https://www.semanticscholar.org/author/F.-Wang/47939052
    username: ~Fei_Wang12
  - dblp_id: https://dblp.org/pid/173/2608
    emails: '****@ucdavis.edu'
    first_name: Muhao
    google_scholar_id: https://scholar.google.com/citations?user=k79yEZkAAAAJ&hl=en
    homepage: https://muhaochen.github.io/
    institution: University of California, Davis and University of Southern California
    last_name: Chen
    name: Muhao Chen
    orcid: https://orcid.org/0000-0003-0118-3147
    semantic_scholar_id: https://www.semanticscholar.org/author/Muhao-Chen/1998918
    username: ~Muhao_Chen1
  decision: toMainConference
  end_page: 516
  file: 43.pdf
  id: 43
  num_pages: 33
  openreview_id: l2UdMLphhW
  pdf_file: 491cc1e32bcda602e462c3644c24419cf6d2822d.pdf
  start_page: 484
  title: Rethinking Tabular Data Understanding with Large Language Models
- abstract: 'Language models are often at risk of diverse backdoor attacks, especially
    data poisoning. Thus, it is important to investigate defense solutions for addressing
    them. Existing backdoor defense methods mainly focus on backdoor attacks with
    explicit triggers, leaving a universal defense against various backdoor attacks
    with diverse triggers largely unexplored. In this paper, we propose an end-to-end
    ensemble-based backdoor defense framework, DPoE (Denoised Product-of-Experts),
    which is inspired by the shortcut nature of backdoor attacks, to defend various
    backdoor attacks. DPoE consists of two models: a shallow model that captures the
    backdoor shortcuts and a main model that is prevented from learning the shortcuts.
    To address the label flip caused by backdoor attackers, DPoE incorporates a denoising
    design. Experiments on three NLP tasks show that DPoE significantly improves the
    defense performance against various types of backdoor triggers including word-level,
    sentence-level, and syntactic triggers. Furthermore, DPoE is also effective under
    a more challenging but practical setting that mixes multiple types of triggers.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Ethics, Bias, and Fairness
  authors:
  - dblp_id: https://dblp.org/pid/06/2123
    emails: '****@ucdavis.edu'
    first_name: Qin
    google_scholar_id: https://scholar.google.com/citations?hl=zh-CN&user=dApKnRkAAAAJ
    homepage: https://qinliu9.github.io
    institution: University of California, Davis
    last_name: Liu
    name: Qin Liu
    semantic_scholar_id: https://www.semanticscholar.org/author/2109185819
    username: ~Qin_Liu5
  - dblp_id: https://dblp.org/pid/52/3194-60
    emails: '****@gmail.com'
    first_name: Fei
    google_scholar_id: https://scholar.google.com/citations?user=N1O2KT8AAAAJ
    homepage: https://feiwang96.github.io/
    institution: University of Southern California
    last_name: Wang
    name: Fei Wang
    semantic_scholar_id: https://www.semanticscholar.org/author/F.-Wang/47939052
    username: ~Fei_Wang12
  - dblp_id: https://dblp.org/pid/150/3317
    emails: '****@umich.edu'
    first_name: Chaowei
    google_scholar_id: https://scholar.google.com/citations?user=Juoqtj8AAAAJ&hl=en
    homepage: https://xiaocw11.github.io/
    institution: University of Wisconsin - Madison and NVIDIA
    last_name: Xiao
    name: Chaowei Xiao
    username: ~Chaowei_Xiao2
  - dblp_id: https://dblp.org/pid/173/2608
    emails: '****@ucdavis.edu'
    first_name: Muhao
    google_scholar_id: https://scholar.google.com/citations?user=k79yEZkAAAAJ&hl=en
    homepage: https://muhaochen.github.io/
    institution: University of California, Davis and University of Southern California
    last_name: Chen
    name: Muhao Chen
    orcid: https://orcid.org/0000-0003-0118-3147
    semantic_scholar_id: https://www.semanticscholar.org/author/Muhao-Chen/1998918
    username: ~Muhao_Chen1
  decision: toMainConference
  end_page: 530
  file: 44.pdf
  id: 44
  num_pages: 14
  openreview_id: x2Fl4yS5Cc
  pdf_file: 20690458ed520d3907db972d673618acbcf019a4.pdf
  start_page: 517
  title: 'From Shortcuts to Triggers: Backdoor Defense with Denoised PoE'
- abstract: 'Several large-scale datasets (e.g., WikiSQL, Spider) for developing natural
    language interfaces to databases have recently been proposed. These datasets cover
    a wide breadth of domains but fall short on some essential domains, such as finance
    and accounting. Given that accounting databases are used worldwide, particularly
    by non-technical people, there is an imminent need to develop models that could
    help extract information from accounting databases via natural language queries.
    In this resource paper, we aim to fill this gap by proposing a new large-scale
    Text-to-SQL dataset for the accounting and financial domain: BookSQL. The dataset
    consists of 100k natural language queries-SQL pairs, and accounting databases
    of 1 million records. We experiment with and analyze existing state-of-the-art
    models (including GPT-4) for the Text-to-SQL task on BookSQL. We find significant
    performance gaps, thus pointing towards developing more focused models for this
    domain.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - emails: '****@iitp.ac.in'
    first_name: Rahul
    google_scholar_id: https://scholar.google.com/citations?user=ypgG92MAAAAJ&hl=en&authuser=2
    homepage: https://www.cse.iitk.ac.in/users/rahulkumar/
    last_name: Kumar
    name: Rahul Kumar
    username: ~Rahul_Kumar5
  - emails: '****@yahoo.in'
    first_name: Amar Raja
    last_name: Dibbu
    name: Amar Raja Dibbu
    username: ~Amar_Raja_Dibbu1
  - dblp_id: https://dblp.org/pid/124/7268
    emails: '****@gmail.com'
    first_name: Shrutendra
    google_scholar_id: https://scholar.google.com/citations?user=NYcrXGsAAAAJ&hl=en
    institution: Intuit AI Bangalore India
    last_name: Harsola
    name: Shrutendra Harsola
    username: ~Shrutendra_Harsola2
  - emails: '****@intuit.com'
    first_name: Vignesh
    last_name: Subrahmaniam
    name: Vignesh Subrahmaniam
    username: ~Vignesh_Subrahmaniam1
  - dblp_id: https://dblp.org/pid/139/0873
    emails: '****@cse.iitk.ac.in'
    first_name: Ashutosh
    google_scholar_id: https://scholar.google.com/citations?user=AWu6f60AAAAJ&hl=en
    homepage: https://ashutosh-modi.github.io/
    institution: IIT Kanpur
    last_name: Modi
    name: Ashutosh Modi
    semantic_scholar_id: https://www.semanticscholar.org/author/Ashutosh-Modi/2477939
    username: ~Ashutosh_Modi1
  decision: toMainConference
  end_page: 550
  file: 45.pdf
  id: 45
  num_pages: 20
  openreview_id: GhFx1KhmSl
  pdf_file: b7a9d75208bb65f98834728eeaef886524980e05.pdf
  start_page: 531
  title: 'BookSQL: A Large Scale Text-to-SQL Dataset for Accounting Domain'
- abstract: 'Geocoding is the task of converting location mentions in text into structured
    geospatial data.

    We propose a new prompt-based paradigm for geocoding, where the machine learning
    algorithm encodes only the location mention and its context.

    We design a transformer network for predicting the country, state, and feature
    class of a location mention, and a deterministic algorithm that leverages the
    country, state, and feature class predictions as constraints in a search for compatible
    entries in the ontology.

    Our architecture, GeoPLACE, achieves new state-of-the-art performance on multiple
    datasets.

    Code and models are available at \url{https://github.com/clulab/geonorm}.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Extraction
  authors:
  - dblp_id: https://dblp.org/pid/44/8352-2.html
    emails: '****@email.arizona.edu'
    first_name: Zeyu
    google_scholar_id: https://scholar.google.com/citations?user=k_fCQhkAAAAJ&hl=en
    homepage: https://jerryzeyu.github.io/
    institution: Amazon AGI
    last_name: Zhang
    name: Zeyu Zhang
    semantic_scholar_id: https://www.semanticscholar.org/author/Zeyu-Zhang/74723434
    username: ~Zeyu_Zhang1
  - dblp_id: https://dblp.org/pid/81/8154
    emails: '****@email.arizona.edu'
    first_name: Egoitz
    google_scholar_id: https://scholar.google.com/citations?user=sCY2JkMAAAAJ
    institution: University of Arizona
    last_name: Laparra
    name: Egoitz Laparra
    orcid: https://orcid.org/0000-0002-1046-2378
    username: ~Egoitz_Laparra1
  - dblp_id: https://dblp.org/pid/52/5246
    emails: '****@arizona.edu'
    first_name: Steven
    google_scholar_id: https://scholar.google.com.tw/citations?user=sXM8J5EAAAAJ
    homepage: http://bethard.faculty.arizona.edu/
    institution: University of Arizona
    last_name: Bethard
    name: Steven Bethard
    orcid: https://orcid.org/0000-0001-9560-6491
    semantic_scholar_id: https://www.semanticscholar.org/author/Steven-Bethard/2105138
    username: ~Steven_Bethard1
  decision: toMainConference
  end_page: 560
  file: 46.pdf
  id: 46
  num_pages: 10
  openreview_id: flrJf7gRCp
  pdf_file: a9a11f15a3fc66593b14e3deee67716adeb6f348.pdf
  start_page: 551
  title: Improving Toponym Resolution by Predicting Attributes to Constrain Geographical
    Ontology Entries
- abstract: Planning is a crucial task for agents in task oriented dialogs (TODs).
    Human agents typically resolve user issues by following predefined workflows,
    decomposing workflow steps into actionable items, and performing actions by executing
    APIs in order; all of which require reasoning and planning. With the recent advances
    in LLMs, there have been increasing attempts to use them for task planning and
    API usage. However, the faithfulness of the plans to predefined workflows and
    API dependencies, is not guaranteed with LLMs. Moreover, workflows in real life
    are often custom-defined and prone to changes; hence, adaptation is desirable.
    To study this, we propose the problem of faithful planning in TODs that needs
    to resolve user intents by following predefined flows and preserving API dependencies.
    To solve this problem, we propose $\textbf{FLAP}$, a $\textbf{Fl}$ow-$\textbf{A}$dhering
    $\textbf{P}$lanning algorithm based on constrained decoding with lookahead heuristic
    for LLMs. Our algorithm alleviates the need for finetuning LLMs using domain specific
    (plan/dependency) data, enables quick adaptation to predefined flows, and outperforms
    other decoding and prompting-based baselines. Further, our algorithm empowers
    smaller LLMs ($\approx7$B) to perform at par larger LLMs ($\approx30$B-$40$B).
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Dialogue and Interactive Systems
  authors:
  - dblp_id: https://dblp.org/pid/274/6982
    emails: '****@gmail.com'
    first_name: Shamik
    google_scholar_id: https://scholar.google.com/citations?user=qbbGZ8EAAAAJ&hl=en&oi=sra
    homepage: https://www.linkedin.com/in/shamik-roy-97698288/
    institution: Amazon
    last_name: Roy
    name: Shamik Roy
    semantic_scholar_id: https://www.semanticscholar.org/author/Shamik-Roy/89998369
    username: ~Shamik_Roy1
  - dblp_id: https://dblp.org/pid/139/7992
    emails: '****@gmail.com'
    first_name: Sailik
    google_scholar_id: https://scholar.google.com/citations?user=Hlm-ti8AAAAJ&hl=en&oi=ao
    homepage: https://sailik1991.github.io/
    institution: Amazon
    last_name: Sengupta
    name: Sailik Sengupta
    username: ~Sailik_Sengupta1
  - dblp_id: https://dblp.org/pid/185/5551
    emails: '****@gmail.com'
    first_name: Daniele
    google_scholar_id: https://scholar.google.it/citations?user=a373nucAAAAJ&hl=en
    institution: Amazon
    last_name: Bonadiman
    name: Daniele Bonadiman
    semantic_scholar_id: https://www.semanticscholar.org/author/Daniele-Bonadiman/3457102
    username: ~Daniele_Bonadiman1
  - dblp_id: https://dblp.org/pid/03/8053
    emails: '****@gmail.com'
    first_name: Saab
    google_scholar_id: https://scholar.google.de/citations?user=1tCbwIQAAAAJ&hl=en
    institution: Amazon
    last_name: Mansour
    name: Saab Mansour
    semantic_scholar_id: https://www.semanticscholar.org/author/Saab-Mansour/39674628
    username: ~Saab_Mansour1
  - dblp_id: https://dblp.org/pid/238/0361
    emails: '****@gmail.com'
    first_name: Arshit
    google_scholar_id: https://scholar.google.com/citations?user=qDPYswEAAAAJ&hl=en
    institution: Amazon
    last_name: Gupta
    name: Arshit Gupta
    semantic_scholar_id: https://www.semanticscholar.org/author/Arshit-Gupta/144877669
    username: ~Arshit_Gupta1
  decision: toMainConference
  end_page: 583
  file: 47.pdf
  id: 47
  num_pages: 23
  openreview_id: bsbdy3sBSs
  pdf_file: 3c2e75bbe2057801b03e5852d45b8366f5dd601a.pdf
  start_page: 561
  title: 'FLAP: Flow-Adhering Planning with Constrained Decoding in LLMs'
- abstract: Document-level Relation Extraction (RE) aims to extract relation triples
    from documents. Existing document-RE models typically rely on supervised learning
    which requires substantial labeled data. To alleviate the amount of human supervision,
    Self-training (ST) has prospered again in language understanding by augmenting
    the fine-tuning of big pre-trained models whenever labeled data is insufficient.
    However, existing ST methods in RE fail to tackle the challenge of long-tail relations.
    In this work, we propose DuRE, a novel ST framework to tackle these problems.
    DuRE jointly models RE classification and text generation as a dual process. In
    this way, our model could construct and utilize both pseudo text generated from
    given labels and pseudo labels predicted from available unlabeled text, which
    are gradually refined during the ST phase. We proposed a contrastive loss to leverage
    the signal of the RE classifier to improve generation quality. In addition, we
    propose a self-adaptive way to sample pseudo text from different relation classes.  Experiments
    on two document-level RE tasks show that DuRE significantly boosts recall and
    F1 score with comparable precision, especially for long-tail relations against
    several strong baselines.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Extraction
  authors:
  - dblp_id: https://dblp.org/pid/215/9691
    emails: '****@cs.ubc.ca'
    first_name: Yuxi
    google_scholar_id: https://scholar.google.com/citations?user=xkVh4zoAAAAJ
    last_name: Feng
    name: Yuxi Feng
    semantic_scholar_id: https://www.semanticscholar.org/author/Yuxi-Feng/2185845790
    username: ~Yuxi_Feng1
  - dblp_id: https://dblp.org/pid/l/LVSLakshmanan
    emails: '****@cs.ubc.ca'
    first_name: Laks
    google_scholar_id: https://scholar.google.ca/citations?user=_RCsaOsAAAAJ&hl=en
    homepage: https://www.cs.ubc.ca/~laks
    institution: University of British Columbia
    last_name: Lakshmanan
    middle_name: V. S.
    name: Laks V. S. Lakshmanan
    orcid: https://orcid.org/0000-0002-9775-4241
    semantic_scholar_id: https://www.semanticscholar.org/search?author%5B0%5D=Laks%20V.%20S.%20Lakshmanan&q=Laks%20V.S.%20Lakshmanan&sort=relevance
    username: ~Laks_V._S._Lakshmanan1
  decision: toMainConference
  end_page: 599
  file: 48.pdf
  id: 48
  num_pages: 16
  openreview_id: iVPfkqGC3M
  pdf_file: 26259cc544eebb6f4f1e214b237d40d853928ce0.pdf
  start_page: 584
  title: 'DuRE: Dual Contrastive Self Training for Semi-Supervised Relation Extraction'
- abstract: "Deep neural networks for Natural Language Processing (NLP) have been\
    \ demonstrated to be vulnerable to textual adversarial examples. Existing black-box\
    \ attacks typically require thousands of queries on the target model, making them\
    \ expensive in real-world applications. \nIn this paper, we propose a new approach\
    \ that guides the word substitutions using prior knowledge from the training set\
    \ to improve the attack efficiency. Specifically, we introduce Adversarial Boosting\
    \ Preference (ABP), a metric that quantifies the importance of words and guides\
    \ adversarial word substitutions. We then propose two query-efficient attack strategies\
    \ based on ABP: query-free attack ($ABP_{free}$) and guided search attack ($ABP_{guide}$).\
    \  Extensive evaluations for text classification demonstrate that $ABP_{free}$\
    \ generates more natural adversarial examples than existing universal attacks,\
    \ $ABP_{guide}$ significantly reduces the number of queries by a factor of 10~500\
    \ while achieving comparable or even better performance than black-box attack\
    \ baselines. Furthermore, we introduce the first ensemble attack $ABP_{ens}$ in\
    \ NLP, which gains further performance improvements and achieves better transferability\
    \ and generalization by the ensemble of the ABP across different models and domains.\
    \ Code is available at https://github.com/BaiDingHub/ABP."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Learning for NLP
  authors:
  - emails: '****@hust.edu.cn'
    first_name: Zhen
    google_scholar_id: https://scholar.google.com/citations?user=Df0dSVUAAAAJ&hl=zh-CN
    institution: Huazhong University of Science and Technology
    last_name: Yu
    name: Zhen Yu
    username: ~Zhen_Yu2
  - emails: '****@hust.edu.cn'
    first_name: Zhenhua
    homepage: https://hust.edu.cn/
    last_name: Chen
    name: Zhenhua Chen
    username: ~Zhenhua_Chen2
  - dblp_id: https://dblp.org/pid/59/1028-1
    emails: '****@hust.edu.cn'
    first_name: Kun
    google_scholar_id: https://scholar.google.com/citations?user=YTQnGJsAAAAJ&hl=en
    homepage: http://faculty.hust.edu.cn/hekun/zh_CN/more/1411001/jsjjgd/index.htm
    institution: Huazhong University of Sceince and Technology
    last_name: He
    name: Kun He
    orcid: https://orcid.org/0000-0001-7627-4604
    semantic_scholar_id: https://www.semanticscholar.org/author/Kun-He/1702188
    username: ~Kun_He1
  decision: toMainConference
  end_page: 613
  file: 49.pdf
  id: 49
  num_pages: 14
  openreview_id: REyR2IKtTJ
  pdf_file: 2d2397d31ae7ba910c317b81df89d09ab8194f8f.pdf
  start_page: 600
  title: Query-Efficient Textual Adversarial Example Generation for Black-Box Attacks
- abstract: Previous research in multi-document news summarization has typically concentrated
    on collating information that all sources agree upon. However, the summarization
    of diverse information dispersed across multiple articles about an event remains
    underexplored. In this paper, we propose a new task of summarizing diverse information
    encountered in multiple news articles encompassing the same event. To facilitate
    this task, we outlined a data collection schema for identifying diverse information
    and curated a dataset named DiverseSumm. The dataset includes 245 news stories,
    with each story comprising 10 news articles and paired with a human-validated
    reference. Next, to enable consistent automatic evaluation, we conducted a comprehensive
    analysis to pinpoint the position and verbosity biases when utilizing Large Language
    Model (LLM)-based metrics for evaluating the coverage and faithfulness of summaries.
    Through correlation analyses, we outline the best practices for effectively using
    automatic LLM-based metrics on the DiverseSumm dataset. Finally, we study how
    LLMs summarize multiple news articles by analyzing which type of diverse information
    LLMs are capable of identifying. Our analyses suggest that despite the extraordinary
    capabilities of LLMs in single-document summarization, the proposed task remains
    a complex challenge for them mainly due to their limited coverage, with GPT-4
    only able to cover under 40% of the diverse information on average.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Summarization
  authors:
  - dblp_id: https://dblp.org/pid/274/7102
    emails: '****@illinois.edu'
    first_name: Kung-Hsiang
    google_scholar_id: https://scholar.google.com/citations?user=Yuk2_IMAAAAJ&hl=en
    homepage: http://khuangaf.github.io/
    institution: University of Illinois Urbana-Champaign
    last_name: Huang
    name: Kung-Hsiang Huang
    semantic_scholar_id: https://www.semanticscholar.org/author/Kung-Hsiang-Huang/1420116116
    username: ~Kung-Hsiang_Huang1
  - dblp_id: https://dblp.org/pid/220/3590
    emails: '****@berkeley.edu'
    first_name: Philippe
    google_scholar_id: https://scholar.google.com/citations?user=fR5t200AAAAJ&hl=en&oi=ao
    homepage: https://people.eecs.berkeley.edu/~phillab/
    last_name: Laban
    name: Philippe Laban
    semantic_scholar_id: https://www.semanticscholar.org/author/Philippe-Laban/46180754
    username: ~Philippe_Laban1
  - dblp_id: https://dblp.org/pid/203/8539
    emails: '****@salesforce.com'
    first_name: Alexander
    google_scholar_id: https://scholar.google.com/citations?user=GgfJdhwAAAAJ&hl=en
    homepage: https://alex-fabbri.github.io
    institution: SalesForce.com
    last_name: Fabbri
    name: Alexander Fabbri
    username: ~Alexander_Fabbri1
  - dblp_id: https://dblp.org/pid/203/8260
    emails: '****@salesforce.com'
    first_name: Prafulla Kumar
    google_scholar_id: https://scholar.google.com/citations?user=k7aMOCsAAAAJ&hl=en&oi=ao
    institution: SalesForce.com
    last_name: Choubey
    name: Prafulla Kumar Choubey
    semantic_scholar_id: https://www.semanticscholar.org/author/Prafulla-Kumar-Choubey/3466801
    username: ~Prafulla_Kumar_Choubey2
  - dblp_id: https://dblp.org/pid/62/2078
    emails: '****@salesforce.com'
    first_name: Shafiq
    google_scholar_id: https://scholar.google.com/citations?user=hR249csAAAAJ&hl=en
    homepage: https://raihanjoty.github.io/
    institution: SalesForce.com and Nanyang Technological University
    last_name: Joty
    name: Shafiq Joty
    username: ~Shafiq_Joty1
  - dblp_id: https://dblp.org/pid/80/7282
    emails: '****@gmail.com'
    first_name: Caiming
    google_scholar_id: https://scholar.google.com/citations?user=vaSdahkAAAAJ&hl=en
    homepage: http://cmxiong.com/
    institution: Salesforce Research
    last_name: Xiong
    name: Caiming Xiong
    username: ~Caiming_Xiong1
  - dblp_id: https://dblp.org/pid/180/5537
    emails: '****@gmail.com'
    first_name: Chien-Sheng
    google_scholar_id: https://scholar.google.com/citations?user=1G4GV2EAAAAJ&hl=en&oi=ao
    homepage: http://jasonwu0731.github.io
    institution: Salesforce AI
    last_name: Wu
    name: Chien-Sheng Wu
    semantic_scholar_id: https://www.semanticscholar.org/author/Chien-Sheng-Wu/30340989
    username: ~Chien-Sheng_Wu1
  decision: toMainConference
  end_page: 637
  file: 50.pdf
  id: 50
  num_pages: 24
  openreview_id: xES0DrS26L
  pdf_file: f8bf440ad1348f6bedf26648e1202215c0c85137.pdf
  start_page: 614
  title: 'Embrace Divergence for Richer Insights: A Multi-document Summarization Benchmark
    and a Case Study on Summarizing Diverse Information from News Articles'
- abstract: Ensuring factual consistency is crucial for natural language generation
    tasks, particularly in abstractive summarization, where preserving the integrity
    of information is paramount. Prior works on evaluating factual consistency of
    summarization often take the entailment-based approaches that first generate perturbed
    (factual inconsistent) summaries and then train a classifier on the generated
    data to detect the factually inconsistencies during testing time. However, previous
    approaches generating perturbed summaries are either of low coherence or lack
    error-type coverage. To address these issues, we propose AMRFact, a framework
    that generates perturbed summaries using Abstract Meaning Representations (AMRs).
    Our approach parses factually consistent summaries into AMR graphs and injects
    controlled factual inconsistencies to create negative examples, allowing for coherent
    factually inconsistent summaries to be generated with high error-type coverage.
    Additionally, we present a data selection module NegFilter based on natural language
    inference and BARTScore to ensure the quality of the generated negative samples.
    Experimental results demonstrate our approach significantly outperforms previous
    systems on the AggreFact-SOTA benchmark, showcasing its efficacy in evaluating
    factuality of abstractive summarization.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Summarization
  authors:
  - dblp_id: https://dblp.org/pid/348/5711
    emails: '****@g.ucla.edu'
    first_name: Haoyi
    google_scholar_id: https://scholar.google.com/citations?hl=en&view_op=list_works&gmla=AHoSzlXQYbhMQNJnmhICStQdFysxD0Y18_nTsnlK8KHbQyaVoXhuN_rUGhqiA4LqZ2IPLSs5d4_-hQT2pwP9Lw&user=vQd32CIAAAAJ
    homepage: https://haoyiq114.github.io/
    institution: UCLA Computer Science Department, University of California, Los Angeles
    last_name: Qiu
    name: Haoyi Qiu
    username: ~Haoyi_Qiu1
  - dblp_id: https://dblp.org/pid/274/7102
    emails: '****@illinois.edu'
    first_name: Kung-Hsiang
    google_scholar_id: https://scholar.google.com/citations?user=Yuk2_IMAAAAJ&hl=en
    homepage: http://khuangaf.github.io/
    institution: University of Illinois Urbana-Champaign
    last_name: Huang
    name: Kung-Hsiang Huang
    semantic_scholar_id: https://www.semanticscholar.org/author/Kung-Hsiang-Huang/1420116116
    username: ~Kung-Hsiang_Huang1
  - emails: '****@cs.ucla.edu'
    first_name: Jingnong
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=6Lxm9HMAAAAJ
    institution: University of California, Los Angeles
    last_name: Qu
    name: Jingnong Qu
    username: ~Jingnong_Qu1
  - dblp_id: https://dblp.org/pid/117/4036
    emails: '****@cs.ucla.edu'
    first_name: Nanyun
    google_scholar_id: https://scholar.google.com/citations?user=XxRXvX0AAAAJ&hl=en
    homepage: http://vnpeng.net/
    institution: University of California, Los Angeles
    last_name: Peng
    name: Nanyun Peng
    semantic_scholar_id: https://www.semanticscholar.org/author/Nanyun-Peng/3157053
    username: ~Nanyun_Peng1
  decision: toMainConference
  end_page: 652
  file: 51.pdf
  id: 51
  num_pages: 15
  openreview_id: 0CGs6nBgqd
  pdf_file: d1383c643bbe3ef7ea96fdbbda697062e9d8c4a0.pdf
  start_page: 638
  title: 'AMRFact: Enhancing Summarization Factuality Evaluation with AMR-Driven Negative
    Samples Generation'
- abstract: Machine learning shows promise in predicting the outcome of legal cases,
    but most research has concentrated on civil law cases rather than case law systems.
    We identified two unique challenges in making legal case outcome predictions with
    case law. First, it is crucial to identify relevant precedent cases that serve
    as fundamental evidence for judges during decision-making. Second, it is necessary
    to consider the evolution of legal principles over time, as early cases may adhere
    to different legal contexts. In this paper, we proposed a new framework named
    PILOT (PredictIng Legal case OuTcome) for case outcome prediction. It comprises
    two modules for relevant case retrieval and temporal pattern handling, respectively.
    To benchmark the performance of existing legal case outcome prediction models,
    we curated a dataset from a large-scale case law database. We demonstrate the
    importance of accurately identifying precedent cases and mitigating the temporal
    shift when making predictions for case law, as our method shows a significant
    improvement over the prior methods that focus on civil law case outcome predictions.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - dblp_id: https://dblp.org/pid/133/4286.html
    emails: '****@illinois.edu'
    first_name: Lang
    homepage: https://github.com/windszzlang
    last_name: Cao
    name: Lang Cao
    semantic_scholar_id: https://www.semanticscholar.org/author/Lang-Cao/2158463125
    username: ~Lang_Cao2
  - dblp_id: https://dblp.org/pid/43/7716-8
    emails: '****@illinois.edu'
    first_name: Zifeng
    google_scholar_id: https://scholar.google.com/citations?user=kMlWwTAAAAAJ&hl=en
    homepage: https://zifengwang.xyz
    institution: University of Illinois, Urbana Champaign
    last_name: Wang
    name: Zifeng Wang
    username: ~Zifeng_Wang3
  - dblp_id: https://dblp.org/pid/170/1833
    emails: '****@gmail.com'
    first_name: Cao
    institution: GE Healthcare
    last_name: Xiao
    name: Cao Xiao
    username: ~Cao_Xiao2
  - dblp_id: https://dblp.uni-trier.de/pid/54/4948.html
    emails: '****@gmail.com'
    first_name: Jimeng
    google_scholar_id: https://scholar.google.com/citations?user=9jmmp5sAAAAJ&hl=en
    homepage: http://sunlab.org
    institution: Georgia Tech Research Corporation, University of Illinois, Urbana
      Champaign, College of Computing and Georgia Institute of Technology
    last_name: Sun
    name: Jimeng Sun
    orcid: https://orcid.org/0000-0003-1512-6426
    username: ~Jimeng_Sun3
  decision: toMainConference
  end_page: 665
  file: 52.pdf
  id: 52
  num_pages: 13
  openreview_id: tfesSDgwIG
  pdf_file: 97620c7ac177ff6f17f9f4a1ddbe7ed0afceb8fc.pdf
  start_page: 653
  title: 'PILOT: Legal Case Outcome Prediction with Case Law'
- abstract: Parameter-efficient fine-tuning (PEFT) is widely studied for its effectiveness
    and efficiency in the era of large language models. Low-rank adaptation (LoRA)
    has demonstrated commendable performance as a popular and representative method.
    However, it is implemented with a fixed intrinsic rank that might not be the ideal
    setting for the downstream tasks. Recognizing the need for more flexible downstream
    task adaptation, we extend the methodology of LoRA to an innovative approach we
    call allocating low-rank adaptation (ALoRA) that enables dynamic adjustments to
    the intrinsic rank during the adaptation process. First, we propose a novel method,
    AB-LoRA, that can effectively estimate the importance score of each LoRA rank.
    Second, guided by AB-LoRA, we gradually prune abundant and negatively impacting
    LoRA ranks and allocate the pruned LoRA budgets to important Transformer modules
    needing higher ranks. We have conducted experiments on various tasks, and the
    experimental results demonstrate that our ALoRA method can outperform the recent
    baselines with comparable tunable parameters.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Learning for NLP
  authors:
  - emails: '****@163.com'
    first_name: Zequan
    google_scholar_id: https://scholar.google.com/citations?user=iqp4g0oAAAAJ&hl=en
    last_name: Liu
    name: Zequan Liu
    username: ~Zequan_Liu2
  - emails: '****@163.com'
    first_name: Jiawen
    google_scholar_id: https://scholar.google.com/citations?user=fXN3dl0AAAAJ&hl=en
    last_name: Lyn
    name: Jiawen Lyn
    username: ~Jiawen_Lyn1
  - dblp_id: https://dblp.org/pid/83/4805.html
    emails: '****@stu.ecnu.edu.cn'
    first_name: Wei
    google_scholar_id: https://scholar.google.com/citations?user=EF5J_BYAAAAJ&hl=en
    homepage: https://www.researchgate.net/profile/Wei-Zhu-111
    institution: East China Normal University
    last_name: Zhu
    name: Wei Zhu
    semantic_scholar_id: https://www.semanticscholar.org/author/Wei-Zhu/2152348673
    username: ~Wei_Zhu7
  - emails: '****@163.com'
    first_name: Xing
    google_scholar_id: https://scholar.google.com/citations?user=u0mIBF0AAAAJ&hl=en
    last_name: Tian
    name: Xing Tian
    username: ~Xing_Tian3
  decision: toMainConference
  end_page: 685
  file: 53.pdf
  id: 53
  num_pages: 20
  openreview_id: WnVSTvjlF9
  pdf_file: cf85fe2cbbdf38ab2a28db45910d393c819685b3.pdf
  start_page: 666
  title: 'ALoRA: Allocating Low-Rank Adaptation for Fine-tuning Large Language Models'
- abstract: This paper introduces Robust Spin (R-Spin), a data-efficient domain-specific
    self-supervision method for speaker and noise-invariant speech representations
    by learning discrete acoustic units with speaker-invariant clustering (Spin).
    R-Spin resolves Spin's issues and enhances content representations by learning
    to predict acoustic pieces. R-Spin offers a 12X reduction in computational resources
    compared to previous state-of-the-art methods while outperforming them in severely
    distorted speech scenarios. This paper provides detailed analyses to show how
    discrete units contribute to speech encoder training and improving robustness
    in diverse acoustic environments.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Speech recognition, text-to-speech and spoken language understanding
  authors:
  - dblp_id: https://dblp.org/pid/264/5138
    emails: '****@mit.edu'
    first_name: Heng-Jui
    google_scholar_id: https://scholar.google.com/citations?user=Kzn0Ks8AAAAJ
    homepage: https://people.csail.mit.edu/hengjui/
    institution: Massachusetts Institute of Technology
    last_name: Chang
    name: Heng-Jui Chang
    semantic_scholar_id: https://www.semanticscholar.org/author/Heng-Jui-Chang/2116152118
    username: ~Heng-Jui_Chang1
  - dblp_id: https://dblp.org/pid/37/6580
    emails: '****@mit.edu'
    first_name: James
    google_scholar_id: https://scholar.google.com/citations?user=pfGI-KcAAAAJ&hl=en&oi=ao
    homepage: https://www.csail.mit.edu/person/jim-glass
    last_name: Glass
    middle_name: R.
    name: James R. Glass
    orcid: https://orcid.org/0000-0002-3097-360X
    semantic_scholar_id: https://www.semanticscholar.org/author/James-R.-Glass/145898106
    username: ~James_R._Glass1
  decision: toMainConference
  end_page: 706
  file: 55.pdf
  id: 55
  num_pages: 21
  openreview_id: ezOlrCqpJl
  pdf_file: 76f7283437cb22a89ea9c28dfbb75dec278c4337.pdf
  start_page: 686
  title: 'R-Spin: Efficient Speaker and Noise-invariant Representation Learning with
    Acoustic Pieces'
- abstract: Instruction tuning effectively optimizes Large Language Models (LLMs)
    for downstream tasks. Due to the changing environment in real-life applications,
    LLMs necessitate continual task-specific adaptation without catastrophic forgetting.
    Considering the heavy computational cost, replay-based Continual Learning (CL)
    methods are the simplest and most widely used for LLMs to address the forgetting
    issue. However, traditional replay-based methods do not fully utilize instructions
    to customize the replay strategy. In this work, we propose a novel paradigm called
    Instruction-based Continual Learning (InsCL). InsCL dynamically replays previous
    data based on task similarity, calculated by Wasserstein Distance with instructions.
    Moreover, we further introduce an Instruction Information Metric (InsInfo) to
    quantify the complexity and diversity of instructions. According to InsInfo, InsCL
    guides the replay process more inclined to high-quality data. We conduct extensive
    experiments over 16 tasks with different training orders, observing consistent
    performance improvements of InsCL. When all tasks have been trained, InsCL achieves
    performance gains of 3.0 Relative Gain compared with Random Replay, and 27.96
    Relative Gain compared with No Replay.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Efficient/Low-Resource Methods for NLP
  authors:
  - emails: '****@mails.tsinghua.edu.cn'
    first_name: Yifan
    google_scholar_id: https://scholar.google.com/citations?user=plGpmz0AAAAJ&hl=zh-CN
    institution: Tsinghua University
    last_name: Wang
    name: Yifan Wang
    orcid: https://orcid.org/0009-0003-4660-3210
    username: ~Yifan_Wang22
  - emails: '****@oppo.com'
    first_name: Yafei
    google_scholar_id: https://scholar.google.com/citations?user=ctd2hiEAAAAJ&hl=zh-CN
    institution: OPPO
    last_name: Liu
    name: Yafei Liu
    semantic_scholar_id: https://www.semanticscholar.org/author/Yafei-Liu/2108146204
    username: ~Yafei_Liu2
  - emails: '****@mails.tsinghua.edu.cn'
    first_name: Chufan
    last_name: Shi
    name: Chufan Shi
    orcid: https://orcid.org/0009-0005-7889-5187
    username: ~Chufan_Shi1
  - emails: '****@mails.tsinghua.edu.cn'
    first_name: Haoling
    homepage: https://openreview.net/
    last_name: Li
    name: Haoling Li
    username: ~Haoling_Li1
  - dblp_id: https://dblp.uni-trier.de/pid/65/4423-15
    emails: '****@gmail.com'
    first_name: Chen
    google_scholar_id: https://scholar.google.com/citations?user=CANDhfAAAAAJ&hl=zh-CN
    institution: OPPO Research Institute
    last_name: Chen
    name: Chen Chen
    orcid: https://orcid.org/0000-0003-3498-2527
    username: ~Chen_Chen24
  - dblp_id: https://dblp.org/pid/129/0998
    emails: '****@oppo.com'
    first_name: Haonan
    google_scholar_id: https://scholar.google.com/citations?user=EPBgKu0AAAAJ&hl=en
    institution: OPPO Guangdong Mobile Telecommunications Co., Ltd.
    last_name: Lu
    name: Haonan Lu
    orcid: https://orcid.org/0000-0001-6332-2785
    semantic_scholar_id: https://www.semanticscholar.org/author/H.-Lu/2130373
    username: ~Haonan_Lu1
  - dblp_id: https://dblp.org/pid/30/3847
    emails: '****@sz.tsinghua.edu.cn'
    first_name: Yujiu
    google_scholar_id: https://scholar.google.com/citations?user=4gH3sxsAAAAJ&hl=zh-TW
    homepage: https://sites.google.com/view/iigroup-thu
    institution: Graduate School at Shenzhen,Tsinghua University
    last_name: Yang
    name: Yujiu Yang
    orcid: https://orcid.org/0000-0002-6427-1024
    semantic_scholar_id: https://www.semanticscholar.org/author/Yujiu-Yang/3001727
    username: ~Yujiu_Yang2
  decision: toMainConference
  end_page: 721
  file: 58.pdf
  id: 58
  num_pages: 15
  openreview_id: ncZdtxZxhY
  pdf_file: 9596bb2f716b985bfa751adf87134cbe43c2070b.pdf
  start_page: 707
  title: 'InsCL: A Data-efficient Continual Learning Paradigm for Fine-tuning Large
    Language Models with Instructions'
- abstract: 'Recently, code language models have achieved notable advancements in
    addressing a diverse array of essential code comprehension and generation tasks.
    Yet, the field lacks a comprehensive deep dive and understanding of the code embeddings
    of multilingual code models. In this paper, we present a comprehensive study on
    multilingual code embeddings, focusing on the cross-lingual capabilities of these
    embeddings across different programming languages. Through probing experiments,
    we demonstrate that code embeddings comprise two distinct components: one deeply
    tied to the nuances and syntax of a specific language, and the other remaining
    agnostic to these details, primarily focusing on semantics. Further, we show that
    when we isolate and eliminate this language-specific component, we witness significant
    improvements in downstream code retrieval tasks, leading to an absolute increase
    of up to +17 in the Mean Reciprocal Rank (MRR).'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Retrieval and Text Mining
  authors:
  - emails: '****@gmail.com'
    first_name: Saiteja
    last_name: Utpala
    name: Saiteja Utpala
    username: ~Saiteja_Utpala1
  - dblp_id: https://dblp.org/pid/285/4734
    emails: '****@mit.edu'
    first_name: Alex
    google_scholar_id: https://scholar.google.com/citations?user=jRQtBp0AAAAJ&hl=en
    homepage: https://minimario.github.io/
    institution: Massachusetts Institute of Technology
    last_name: Gu
    name: Alex Gu
    username: ~Alex_Gu1
  - dblp_id: https://dblp.org/pid/39/8969
    emails: '****@gmail.com'
    first_name: Pin-Yu
    google_scholar_id: https://scholar.google.com/citations?user=jxwlCUUAAAAJ&hl=en&oi=ao
    homepage: http://www.pinyuchen.com
    institution: International Business Machines
    last_name: Chen
    name: Pin-Yu Chen
    orcid: https://orcid.org/0000-0003-1039-8369
    semantic_scholar_id: https://www.semanticscholar.org/author/Pin-Yu-Chen/153191489
    username: ~Pin-Yu_Chen3
  decision: toMainConference
  end_page: 735
  file: 60.pdf
  id: 60
  num_pages: 14
  openreview_id: jBounmBOEV
  pdf_file: faf4e8620fbc47fb03f15709e2e6a3c0a9ab5db4.pdf
  start_page: 722
  title: Language Agnostic Code Embeddings
- abstract: In recent studies, linear recurrent neural networks (LRNNs) have achieved
    Transformer-level performance in natural language and long-range modeling, while
    offering rapid parallel training and constant inference cost. With the resurgence
    of interest in LRNNs, we study whether they can learn the hidden rules in training
    sequences, such as the grammatical structures of regular language. We theoretically
    analyze some existing LRNNs and discover their limitations in modeling regular
    language. Motivated by this analysis, we propose a new LRNN equipped with a block-diagonal
    and input-dependent transition matrix. Experiments suggest that the proposed model
    is the only LRNN capable of performing length extrapolation on regular language
    tasks such as Sum, Even Pair, and Modular Arithmetic. The code is released at
    \url{https://github.com/tinghanf/RegluarLRNN}.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Learning for NLP
  authors:
  - dblp_id: https://dblp.org/pid/213/0948
    emails: '****@alumni.princeton.edu'
    first_name: Ting-Han
    google_scholar_id: https://scholar.google.com/citations?user=1mQ3kTEAAAAJ&hl=en
    last_name: Fan
    name: Ting-Han Fan
    semantic_scholar_id: https://www.semanticscholar.org/author/Ting-Han-Fan/32037089
    username: ~Ting-Han_Fan1
  - dblp_id: https://dblp.org/pid/207/7824
    emails: '****@andrew.cmu.edu'
    first_name: Ta-Chung
    google_scholar_id: https://scholar.google.com.tw/citations?user=ZqpdQOoAAAAJ&hl=en
    institution: Carnegie Mellon University
    last_name: Chi
    name: Ta-Chung Chi
    semantic_scholar_id: https://www.semanticscholar.org/author/Ta-Chung-Chi/27531332
    username: ~Ta-Chung_Chi1
  - dblp_id: https://dblp.org/pid/29/5401
    emails: '****@cs.cmu.edu'
    first_name: Alexander
    google_scholar_id: https://scholar.google.com/citations?user=axOnEnQAAAAJ&hl=en
    homepage: http://www.cs.cmu.edu/~air/
    institution: Carnegie Mellon University and Carnegie Mellon University
    last_name: Rudnicky
    name: Alexander Rudnicky
    orcid: https://orcid.org/0000-0003-3896-9397
    semantic_scholar_id: https://www.semanticscholar.org/author/A.-Rudnicky/3156164
    username: ~Alexander_Rudnicky1
  decision: toMainConference
  end_page: 744
  file: 61.pdf
  id: 61
  num_pages: 9
  openreview_id: 7rzFuXZBde
  pdf_file: eb25cad9948d7c2ecfe9d3018281922b7c5d5f2b.pdf
  start_page: 736
  title: Advancing Regular Language Reasoning in Linear Recurrent Neural Networks
- abstract: "With the success of Large Language Models (LLMs), many Generative Vision-Language\
    \ Models (GVLMs) have been constructed via multimodal instruction tuning. However,\
    \ the performance of GVLMs in multimodal compositional reasoning remains under-explored.\
    \ In this paper, we examine both the evaluation metrics ( VisualGPTScore, etc.)\
    \ and current benchmarks for evaluating the compositionality of GVLMs. We identify\
    \ the syntactical bias in current benchmarks, which is exploited by the linguistic\
    \ capability of GVLMs. \nThe bias renders VisualGPTScore an insufficient metric\
    \ for assessing GVLMs. To combat this, we first introduce a **SyntaxBias Score**,\
    \ leveraging LLMs to quantify such bias for mitigation. A challenging new task\
    \ is subsequently added to evaluate the robustness of GVLMs against inherent inclination\
    \ toward syntactical correctness. Using the bias-mitigated datasets and the new\
    \ task, we propose a novel benchmark, namely **S**ynt**A**ctically **DE**-biased\
    \ benchmark (SADE). Our study provides an unbiased benchmark for the compositionality\
    \ of GVLMs, facilitating future research in this direction. Code and dataset are\
    \ available at https://github.com/TeleeMa/SADE."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multimodality and Language Grounding to Vision, Robotics and Beyond
  authors:
  - dblp_id: https://dblp.org/pid/276/3611
    emails: '****@gmail.com'
    first_name: Teli
    google_scholar_id: https://scholar.google.com/citations?user=tW37g0UAAAAJ&hl=en&oi=ao
    homepage: https://teleema.github.io/
    institution: Hong Kong University of Science and Technology
    last_name: Ma
    name: Teli Ma
    semantic_scholar_id: https://www.semanticscholar.org/author/Teli-Ma/1390452961
    username: ~Teli_Ma1
  - emails: '****@qq.com'
    first_name: Rong
    google_scholar_id: https://scholar.google.com/citations?user=M68wBgkAAAAJ&hl=zh-CN
    institution: HKUST(GZ)
    last_name: Li
    name: Rong Li
    username: ~Rong_Li1
  - dblp_id: https://dblp.org/pid/62/10704-1
    emails: '****@gmail.com'
    first_name: Junwei
    google_scholar_id: https://scholar.google.com/citations?user=bMedjfUAAAAJ&hl=en
    homepage: https://junweiliang.me/
    institution: Hong Kong University of Science and Technology
    last_name: Liang
    name: Junwei Liang
    orcid: https://orcid.org/0000-0003-2219-5569
    semantic_scholar_id: https://www.semanticscholar.org/author/Junwei-Liang/1915796
    username: ~Junwei_Liang1
  decision: toMainConference
  end_page: 758
  file: 65.pdf
  id: 65
  num_pages: 14
  openreview_id: zzQAV1UHSv
  pdf_file: 5ae266effb77a45604adac48e44b8166324a6a7c.pdf
  start_page: 745
  title: An Examination of the Compositionality of Large Generative Vision-Language
    Models
- abstract: Identifying linguistic differences between dialects of a language often
    requires expert knowledge and meticulous human analysis. This is largely due to
    the complexity and nuance involved in studying various dialects. We present a
    novel approach to extract distinguishing lexical features of dialects by utilizing
    interpretable dialect classifiers, even in the absence of human experts. We explore
    both post-hoc and intrinsic approaches to interpretability, conduct experiments
    on Mandarin, Italian, and Low Saxon, and experimentally demonstrate that our method
    successfully identifies key language-specific lexical features that contribute
    to dialectal variations.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multilinguality and Language Diversity
  authors:
  - emails: '****@duke.edu'
    first_name: Roy
    homepage: https://royxie.com/
    last_name: Xie
    name: Roy Xie
    username: ~Roy_Xie1
  - emails: '****@gmail.com'
    first_name: Orevaoghene
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=xoNi-cIAAAAJ&view_op=list_works&sortby=pubdate
    last_name: Ahia
    name: Orevaoghene Ahia
    username: ~Orevaoghene_Ahia1
  - dblp_id: https://dblp.org/pid/75/8157
    emails: '****@cs.washington.edu'
    first_name: Yulia
    google_scholar_id: https://scholar.google.com/citations?user=SEDPkrsAAAAJ&hl=en
    homepage: https://homes.cs.washington.edu/~yuliats/
    institution: Department of Computer Science, University of Washington
    last_name: Tsvetkov
    name: Yulia Tsvetkov
    username: ~Yulia_Tsvetkov1
  - dblp_id: https://dblp.org/pid/148/9479
    emails: '****@gmu.edu'
    first_name: Antonios
    google_scholar_id: https://scholar.google.com/citations?user=g_G_SNAAAAAJ&hl=en
    homepage: http://www.cs.gmu.edu/~antonis/
    institution: Athena Research Center and George Mason University
    last_name: Anastasopoulos
    name: Antonios Anastasopoulos
    orcid: https://orcid.org/0000-0002-8544-246X
    semantic_scholar_id: https://www.semanticscholar.org/author/Antonios-Anastasopoulos/49513989
    username: ~Antonios_Anastasopoulos1
  decision: toMainConference
  end_page: 774
  file: 66.pdf
  id: 66
  num_pages: 16
  openreview_id: 0CEcp8KAz7
  pdf_file: fbf7d4cdab8d2e04bb036ad99bb4138b50c98c58.pdf
  start_page: 759
  title: Extracting Lexical Features from Dialects via Interpretable Dialect Classifiers
- abstract: 'Data poisoning backdoor attacks can cause undesirable behaviors in large
    language models (LLMs), and defending against them is of increasing importance.
    Existing defense mechanisms often assume that only one type of trigger is adopted
    by the attacker, while defending against multiple simultaneous and independent
    trigger types necessitates general defense frameworks and is relatively unexplored.
    In this paper, we propose Nested Product of Experts (NPoE) defense framework,
    which involves a mixture of experts (MoE) as a trigger-only ensemble within the
    PoE defense framework to simultaneously defend against multiple trigger types.
    During NPoE training, the main model

    is trained in an ensemble with a mixture of smaller expert models that learn the
    features of backdoor triggers. At inference time, only the main model is used.
    Experimental results on sentiment analysis, hate speech detection, and question
    classification tasks demonstrate that NPoE effectively defends against a variety
    of triggers both separately and in trigger mixtures. Due to the versatility of
    the MoE structure in NPoE, this framework can be further expanded to defend against
    other attack settings.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Ethics, Bias, and Fairness
  authors:
  - emails: '****@gmail.com'
    first_name: Victoria
    google_scholar_id: https://scholar.google.com/citations?user=0arBo88AAAAJ&hl=en&oi=sra
    institution: University of Southern California and Princeton University
    last_name: Graf
    name: Victoria Graf
    username: ~Victoria_Graf1
  - dblp_id: https://dblp.org/pid/06/2123
    emails: '****@ucdavis.edu'
    first_name: Qin
    google_scholar_id: https://scholar.google.com/citations?hl=zh-CN&user=dApKnRkAAAAJ
    homepage: https://qinliu9.github.io
    institution: University of California, Davis
    last_name: Liu
    name: Qin Liu
    semantic_scholar_id: https://www.semanticscholar.org/author/2109185819
    username: ~Qin_Liu5
  - dblp_id: https://dblp.org/pid/173/2608
    emails: '****@ucdavis.edu'
    first_name: Muhao
    google_scholar_id: https://scholar.google.com/citations?user=k79yEZkAAAAJ&hl=en
    homepage: https://muhaochen.github.io/
    institution: University of California, Davis and University of Southern California
    last_name: Chen
    name: Muhao Chen
    orcid: https://orcid.org/0000-0003-0118-3147
    semantic_scholar_id: https://www.semanticscholar.org/author/Muhao-Chen/1998918
    username: ~Muhao_Chen1
  decision: toMainConference
  end_page: 787
  file: 68.pdf
  id: 68
  num_pages: 13
  openreview_id: kw350adZuD
  pdf_file: a34ec58c6226e583bf399352e60fa390b3a58c29.pdf
  start_page: 775
  title: 'Two Heads are Better than One: Nested PoE for Robust Defense Against Multi-Backdoors'
- abstract: 'Cross-domain few-shot Relation Extraction (RE) aims to transfer knowledge
    from a source domain to a different target domain to address low-resource problems.

    Previous work utilized label descriptions and entity information to leverage the
    knowledge of the source domain.

    However, these models are prone to confusion when directly applying this knowledge
    to a target domain with entirely new types of relations, which becomes particularly
    pronounced when facing similar relations.

    In this work, we propose a relation-aware prompt learning method with pre-training.

    Specifically, we empower the model to clear confusion by decomposing various relation
    types through an innovative label prompt, while a context prompt is employed to
    capture differences in different scenarios, enabling the model to further discern
    confusion. Two pre-training tasks are designed to leverage the prompt knowledge
    and paradigm.

    Experiments show that our method outperforms previous sota methods, yielding significantly
    better results on cross-domain few-shot RE tasks.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Efficient/Low-Resource Methods for NLP
  authors:
  - emails: '****@bupt.edu.cn'
    first_name: Ge
    last_name: Bai
    name: Ge Bai
    orcid: https://orcid.org/0009-0005-1953-3207
    username: ~Ge_Bai2
  - emails: '****@bupt.edu.cn'
    first_name: Chenji
    homepage: https://github.com/luchenji
    last_name: Lu
    name: Chenji Lu
    username: ~Chenji_Lu1
  - emails: '****@bupt.edu.cn'
    first_name: Daichi
    homepage: https://pris-nlp.github.io/author/%E9%83%AD%E5%B2%B1%E9%A9%B0/
    last_name: Guo
    name: Daichi Guo
    username: ~Daichi_Guo1
  - emails: '****@bupt.edu.cn'
    first_name: Shilong
    homepage: https://github.com/longls777
    last_name: Li
    name: Shilong Li
    username: ~Shilong_Li3
  - emails: '****@bupt.edu.cn'
    first_name: Ying
    homepage: https://blog.csdn.net/qq_43355120?spm=1000.2115.3001.5343
    last_name: Liu
    name: Ying Liu
    username: ~Ying_Liu19
  - emails: '****@bupt.edu.cn'
    first_name: Zhang
    homepage: https://2hang2hang.top/
    last_name: Zhang
    name: Zhang Zhang
    username: ~Zhang_Zhang6
  - dblp_id: https://dblp.org/pid/227/7667
    emails: '****@bupt.edu.cn'
    first_name: Guanting
    google_scholar_id: https://scholar.google.com/citations?user=amozZDkAAAAJ&hl=zh-CN&oi=ao
    homepage: https://dongguanting.github.io/
    last_name: Dong
    name: Guanting Dong
    semantic_scholar_id: https://www.semanticscholar.org/author/Guanting-Dong/51490462?sort=influence
    username: ~Guanting_Dong1
  - emails: '****@bupt.edu.cn'
    first_name: Ruifang
    institution: Beijing University of Posts and Telecommunications
    last_name: Liu
    name: Ruifang Liu
    semantic_scholar_id: https://www.semanticscholar.org/author/Ruifang-Liu/2152936487
    username: ~Ruifang_Liu1
  - emails: '****@bupt.edu.cn'
    first_name: Sun
    homepage: https://teacher.bupt.edu.cn/sunyong
    institution: Beijing University of Posts and Telecommunications
    last_name: Yong
    name: Sun Yong
    username: ~Sun_Yong1
  decision: toMainConference
  end_page: 796
  file: 69.pdf
  id: 69
  num_pages: 9
  openreview_id: Ueyc7djFhR
  pdf_file: 10711da6cc085aa20d8bb1fa4bb4c4fd2992e5f7.pdf
  start_page: 788
  title: 'Clear Up Confusion: Advancing Cross-Domain Few-Shot Relation Extraction
    through Relation-Aware Prompt Learning'
- abstract: "Text classification systems have continuously\nimproved in performance\
    \ over the years. How-\never, nearly all current SOTA classifiers have a\nsimilar\
    \ shortcoming, they process text in a hor-\nizontal manner. Vertically written\
    \ words will\nnot be recognized by a classifier. In contrast,\nhumans are easily\
    \ able to recognize and read\nwords written both horizontally and vertically.\n\
    Hence, a human adversary could write problem-\natic words vertically and the meaning\
    \ would\nstill be preserved to other humans. We simulate\nsuch an attack, VertAttack.\
    \ VertAttack identifies\nwhich words a classifier is reliant on and then\nrewrites\
    \ those words vertically. We find that\nVertAttack is able to greatly drop the\
    \ accuracy\nof 4 different transformer models on 5 datasets.\nFor example, on\
    \ the SST2 dataset, VertAttack\nis able to drop RoBERTa\u2019s accuracy from 94\
    \ to\n13%. Furthermore, since VertAttack does not\nreplace the word, meaning is\
    \ easily preserved.\nWe verify this via a human study and find that\ncrowdworkers\
    \ are able to correctly label 77%\nperturbed texts perturbed, compared to 81%\
    \ of\nthe original texts. We believe VertAttack offers\na look into how humans\
    \ might circumvent clas-\nsifiers in the future and thus inspire a look into\n\
    more robust algorithms."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Ethics, Bias, and Fairness
  authors:
  - dblp_id: https://dblp.org/pid/242/4812
    emails: '****@uiowa.edu'
    first_name: Jonathan
    homepage: https://jonathanrusert.org/
    institution: Purdue University Fort Wayne
    last_name: Rusert
    name: Jonathan Rusert
    semantic_scholar_id: https://www.semanticscholar.org/author/Jonathan-Rusert/147146848
    username: ~Jonathan_Rusert1
  decision: toMainConference
  end_page: 810
  file: 71.pdf
  id: 71
  num_pages: 14
  openreview_id: fO1ihGWWEs
  pdf_file: 17163f8f28fdf5c1c3fca99862892e822af02d38.pdf
  start_page: 797
  title: 'VertAttack: Taking Advantage of Text Classifiers'' Horizontal Vision'
- abstract: "Predicting unseen relations that cannot be observed during the training\
    \ phase is a challenging task in relation extraction. \nPrevious works have made\
    \ progress by matching the semantics between input instances and label descriptions.\
    \ However, fine-grained matching often requires laborious manual annotation, and\
    \ rich interactions between instances and label descriptions come with significant\
    \ computational overhead. \nIn this work, we propose an efficient multi-grained\
    \ matching approach that uses virtual entity matching to reduce manual annotation\
    \ cost, and fuses coarse-grained recall and fine-grained classification for rich\
    \ interactions with guaranteed inference speed.\nExperimental results show that\
    \ our approach outperforms the previous State Of The Art (SOTA) methods, and achieves\
    \ a balance between inference efficiency and prediction accuracy in zero-shot\
    \ relation extraction tasks.\nOur code is available at https://github.com/longls777/EMMA."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Efficient/Low-Resource Methods for NLP
  authors:
  - emails: '****@bupt.edu.cn'
    first_name: Shilong
    homepage: https://github.com/longls777
    last_name: Li
    name: Shilong Li
    username: ~Shilong_Li3
  - emails: '****@bupt.edu.cn'
    first_name: Ge
    last_name: Bai
    name: Ge Bai
    orcid: https://orcid.org/0009-0005-1953-3207
    username: ~Ge_Bai2
  - emails: '****@bupt.edu.cn'
    first_name: Zhang
    homepage: https://2hang2hang.top/
    last_name: Zhang
    name: Zhang Zhang
    username: ~Zhang_Zhang6
  - emails: '****@bupt.edu.cn'
    first_name: Ying
    homepage: https://blog.csdn.net/qq_43355120?spm=1000.2115.3001.5343
    last_name: Liu
    name: Ying Liu
    username: ~Ying_Liu19
  - emails: '****@bupt.edu.cn'
    first_name: Chenji
    homepage: https://github.com/luchenji
    last_name: Lu
    name: Chenji Lu
    username: ~Chenji_Lu1
  - emails: '****@bupt.edu.cn'
    first_name: Daichi
    homepage: https://pris-nlp.github.io/author/%E9%83%AD%E5%B2%B1%E9%A9%B0/
    last_name: Guo
    name: Daichi Guo
    username: ~Daichi_Guo1
  - emails: '****@bupt.edu.cn'
    first_name: Ruifang
    institution: Beijing University of Posts and Telecommunications
    last_name: Liu
    name: Ruifang Liu
    semantic_scholar_id: https://www.semanticscholar.org/author/Ruifang-Liu/2152936487
    username: ~Ruifang_Liu1
  - emails: '****@bupt.edu.cn'
    first_name: Sun
    homepage: https://teacher.bupt.edu.cn/sunyong
    institution: Beijing University of Posts and Telecommunications
    last_name: Yong
    name: Sun Yong
    username: ~Sun_Yong1
  decision: toMainConference
  end_page: 817
  file: 72.pdf
  id: 72
  num_pages: 7
  openreview_id: Zc12GqxpqD
  pdf_file: 55e08236dab356c485759e594eb7b3853e68fe7e.pdf
  start_page: 811
  title: 'Fusion Makes Perfection: An Efficient Multi-Grained Matching Approach for
    Zero-Shot Relation Extraction'
- abstract: "Users usually browse product reviews before buying products from e-commerce\
    \ websites. Lots of e-commerce websites can recommend reviews. However, existing\
    \ research on review recommendation mainly focuses on the general usefulness of\
    \ reviews and ignores personalized and implicit requirements. To address the issue,\
    \ we propose a Large language model driven Personalized Review Recommendation\
    \ model based on Implicit dimension mining (PRR-LI). The model mines implicit\
    \ dimensions from reviews and requirements, and encodes them in the form of \u201C\
    text + dimension\u201D. The experiments show that our model significantly outperforms\
    \ other state-of-the-art textual models on the Amazon-MRHP dataset, with some\
    \ of the metrics outperforming the state-of-the-art multimodal models. And we\
    \ prove that encoding  \u201Ctext + dimension\u201D is better than encoding \u201C\
    text\u201D and \u201Cdimension\u201D separately in review recommendation."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Retrieval and Text Mining
  authors:
  - dblp_id: https://dblp.org/rec/journals/concurrency/XuZ22.html
    emails: '****@njupt.edu.cn'
    first_name: Bei
    homepage: https://yjs.njupt.edu.cn/dsgl/nocontrol/college/dsfcxq.htm?dsJbxxId=9B9D05C52CD82DCFE050007F01006EFE
    last_name: Xu
    name: Bei Xu
    username: ~Bei_Xu2
  - emails: '****@petalmail.com'
    first_name: Yifan
    homepage: https://www.xuyifan.top
    last_name: xu
    name: Yifan xu
    orcid: https://orcid.org/0009-0003-7417-8434
    username: ~Yifan_xu13
  decision: toMainConference
  end_page: 823
  file: 74.pdf
  id: 74
  num_pages: 6
  openreview_id: 76fLEk0wHH
  pdf_file: a5b62b950d52cadc16db24ea02a90141179b1bc1.pdf
  start_page: 818
  title: Personalized Review Recommendation based on Implicit dimension mining
- abstract: Previous work on multimodal sentence embedding has proposed multimodal
    contrastive learning and achieved promising results. However, by taking the rest
    of the batch as negative samples without reviewing when forming contrastive pairs,
    those studies encountered many suspicious and noisy negative examples, significantly
    affecting the methods' overall performance. In this work, we propose KDMCSE (Knowledge
    Distillation Multimodal contrastive learning of Sentence Embeddings), a novel
    approach that enhances the discrimination and generalizability of multimodal representation
    and inherits the knowledge from the teacher model to learn the difference between
    positive and negative instances and via that, can detect noisy and wrong negative
    samples effectively before they are calculated in the contrastive objective. Furthermore,
    to overcome the limitation of modeling the variation within negative pairs, we
    introduce a new contrastive objective, AdapACSE (Adaptive Angular Margin Supervised
    Contrastive Learning for Multimodal sentence embeddings), that enhances the discriminative
    representation by strengthening the margin within the angular space while capturing
    varying semantics within the negative. Experimental results on widely used Semantic
    Textual Similarity (STS) benchmarks demonstrate the effectiveness of our approach.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multimodality and Language Grounding to Vision, Robotics and Beyond
  authors:
  - emails: '****@e.ntu.edu.sg'
    first_name: Cong-Duy
    homepage: https://duyngtr16061999.github.io/
    institution: School of Computer Science and  Engineering, Nanyang Technological
      University
    last_name: Nguyen
    middle_name: T
    name: Cong-Duy T Nguyen
    username: ~Cong-Duy_T_Nguyen1
  - dblp_id: https://dblp.org/pid/29/5255.html
    emails: '****@u.nus.edu'
    first_name: Thong
    google_scholar_id: https://scholar.google.com/citations?user=C2zb0lkAAAAJ&hl=en
    homepage: https://nguyentthong.github.io/
    last_name: Nguyen
    middle_name: Thanh
    name: Thong Thanh Nguyen
    semantic_scholar_id: https://www.semanticscholar.org/author/2116028119
    username: ~Thong_Thanh_Nguyen1
  - dblp_id: https://dblp.org/pid/249/8429
    emails: '****@e.ntu.edu.sg'
    first_name: Xiaobao
    google_scholar_id: https://scholar.google.com/citations?user=Y1oag4sAAAAJ
    homepage: https://bobxwu.github.io/
    institution: Nanyang Technological University
    last_name: Wu
    name: Xiaobao Wu
    username: ~Xiaobao_Wu1
  - dblp_id: https://dblp.org/pid/81/8329
    emails: '****@ntu.edu.sg'
    first_name: Anh Tuan
    google_scholar_id: https://scholar.google.com.sg/citations?hl=en&user=d6ixOGYAAAAJ&view_op=list_works
    homepage: https://tuanluu.github.io/
    institution: Nanyang Technological University
    last_name: Luu
    name: Anh Tuan Luu
    semantic_scholar_id: https://www.semanticscholar.org/author/Anh-Tuan-Luu/26336902
    username: ~Anh_Tuan_Luu2
  decision: toMainConference
  end_page: 840
  file: 75.pdf
  id: 75
  num_pages: 17
  openreview_id: bwRLUfd6Z6
  pdf_file: e467ce12cf0b09c92ae5071c0abe4e5b94455be4.pdf
  start_page: 824
  title: 'KDMCSE: Knowledge Distillation Multimodal Sentence Embeddings with Adaptive
    Angular margin Contrastive Learning'
- abstract: In this project, we demonstrate that phoneme-based models for speech processing
    can achieve strong crosslinguistic generalizability to unseen languages. We curated
    the IPAPACK, a massively multilingual speech corpora with phonemic transcriptions,
    encompassing more than 115 languages from diverse language families, selectively
    checked by linguists. Based on the IPAPACK, we propose CLAP-IPA, a multi-lingual
    phoneme-speech contrastive embedding model capable of open-vocabulary matching
    between arbitrary speech signals and phonemic sequences. The proposed model was
    tested on 95 unseen languages, showing strong generalizability across languages.
    Temporal alignments between phonemes and speech signals also emerged from contrastive
    training, enabling zeroshot forced alignment in unseen languages. We further introduced
    a neural forced aligner IPA-ALIGNER by finetuning CLAP-IPA with the Forward-Sum
    loss to learn better phone-to-audio alignment. Evaluation results suggest that
    IPA-ALIGNER can generalize to unseen languages without adaptation.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multilinguality and Language Diversity
  authors:
  - dblp_id: https://dblp.org/pid/98/960
    emails: '****@ubc.ca'
    first_name: Jian
    google_scholar_id: https://scholar.google.com/citations?user=jLtpcLgAAAAJ&hl=en
    homepage: https://lingjzhu.github.io/
    institution: University of British Columbia
    last_name: Zhu
    name: Jian Zhu
    semantic_scholar_id: https://www.semanticscholar.org/author/Jian-Zhu/144549416
    username: ~Jian_Zhu2
  - dblp_id: https://dblp.org/pid/15/10389
    emails: '****@colorado.edu'
    first_name: Changbing
    google_scholar_id: https://scholar.google.com/citations?user=tqzTAxoAAAAJ&hl=en
    institution: University of British Columbia
    last_name: Yang
    name: Changbing Yang
    semantic_scholar_id: https://www.semanticscholar.org/author/Changbing-Yang/15418153
    username: ~Changbing_Yang1
  - dblp_id: https://dblp.org/pid/284/4690
    emails: '****@mail.ubc.ca'
    first_name: Farhan
    google_scholar_id: https://scholar.google.com/citations?user=YQOEOXYAAAAJ&hl=en
    homepage: https://smfsamir.github.io/
    institution: University of British Columbia
    last_name: Samir
    name: Farhan Samir
    semantic_scholar_id: https://www.semanticscholar.org/author/Farhan-Samir/2047738160
    username: ~Farhan_Samir1
  - emails: '****@ubc.ca'
    first_name: Jahurul
    last_name: Islam
    name: Jahurul Islam
    username: ~Jahurul_Islam1
  decision: toMainConference
  end_page: 863
  file: 80.pdf
  id: 80
  num_pages: 23
  openreview_id: 6ngA19rPP0
  pdf_file: 4d50bf4f67c16c315da16dbf6f4560c55e0532ae.pdf
  start_page: 841
  title: 'The taste of IPA: Towards open-vocabulary keyword spotting and forced alignment
    in any language'
- abstract: 'Gender bias in vision-language models (VLMs) can reinforce harmful stereotypes
    and discrimination. In this paper, we focus on mitigating gender bias towards
    vision-language tasks. We identify object hallucination as the essence of gender
    bias in VLMs. Existing VLMs tend to focus on salient or familiar attributes in
    images but ignore contextualized nuances. Moreover, most VLMs rely on the co-occurrence
    between specific objects and gender attributes to infer the ignored features,
    ultimately resulting in gender bias. We propose GAMA, a task-agnostic generation
    framework to mitigate gender bias. GAMA consists of two stages: narrative generation
    and answer inference. During narrative generation, GAMA yields all-sided but gender-obfuscated
    narratives, which prevents premature concentration on localized image features,
    especially gender attributes. During answer inference, GAMA integrates the image,
    generated narrative, and a task-specific question prompt to infer answers for
    different vision-language tasks. This approach allows the model to rethink gender
    attributes and answers. We conduct extensive experiments on GAMA, demonstrating
    its debiasing and generalization ability.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Ethics, Bias, and Fairness
  authors:
  - dblp_id: https://dblp.org/pid/89/5923
    emails: '****@stu.ecnu.edu.cn'
    first_name: Yunqi
    last_name: Zhang
    name: Yunqi Zhang
    orcid: https://orcid.org/0009-0003-7112-8722
    username: ~Yunqi_Zhang3
  - dblp_id: https://dblp.org/pid/345/7796
    emails: '****@stu.ecnu.edu.cn'
    first_name: Songda
    last_name: Li
    name: Songda Li
    username: ~Songda_Li1
  - emails: '****@gatech.edu'
    first_name: Chunyuan
    google_scholar_id: https://scholar.google.com/citations?user=g7Y0RHcAAAAJ&hl=en
    homepage: https://charlesdddd.github.io/
    last_name: Deng
    name: Chunyuan Deng
    semantic_scholar_id: https://www.semanticscholar.org/author/Chunyuan-Deng/2266840268
    username: ~Chunyuan_Deng1
  - emails: '****@stu.ecnu.edu.cn'
    first_name: Luyi
    last_name: Wang
    name: Luyi Wang
    username: ~Luyi_Wang1
  - dblp_id: https://dblp.org/pid/39/6153
    emails: '****@sei.ecnu.edu.cn'
    first_name: Hui
    homepage: https://faculty.ecnu.edu.cn/_s43/zh2/main.psp
    institution: East China Normal University
    last_name: Zhao
    name: Hui Zhao
    username: ~Hui_Zhao2
  decision: toMainConference
  end_page: 882
  file: 82.pdf
  id: 82
  num_pages: 19
  openreview_id: I8ocM0Xh1n
  pdf_file: 00ffe88276673e9343976566875654639ea50fa1.pdf
  start_page: 864
  title: 'Think Before You Act: A Two-Stage Framework for Mitigating Gender Bias Towards
    Vision-Language Tasks'
- abstract: 'Sentence embeddings are crucial in measuring semantic similarity. Most
    recent studies employed large language models (LLMs) to learn sentence embeddings.
    Existing LLMs mainly adopted autoregressive architecture without explicit backward
    dependency modeling. Therefore, we examined the effects of backward dependencies
    in LLMs for semantic similarity measurements. Concretely, we propose a novel model:
    backward dependency enhanced large language model (BeLLM). It learns sentence
    embeddings via transforming specific attention layers from uni- to bi-directional.
    We extensively experiment across various semantic textual similarity (STS) tasks
    and downstream applications. BeLLM achieves state-of-the-art performance in varying
    scenarios. It shows that autoregressive LLMs benefit from backward dependencies
    for sentence embeddings.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: 'Semantics: Sentence-level Semantics, Textual Inference and Other
      areas'
  authors:
  - dblp_id: https://dblp.org/pid/175/5398.html
    emails: '****@connect.polyu.hk'
    first_name: Xianming
    google_scholar_id: https://scholar.google.com/citations?user=WwCp3OcAAAAJ&hl=zh-CN
    last_name: LI
    name: Xianming LI
    orcid: https://orcid.org/0009-0009-2610-7934
    semantic_scholar_id: https://www.semanticscholar.org/author/Xianming-Li/2108221563
    username: ~Xianming_LI1
  - dblp_id: https://dblp.org/pid/181/2820-49
    emails: '****@polyu.edu.hk'
    first_name: Jing
    google_scholar_id: https://scholar.google.com/citations?user=jvjOLx4AAAAJ&hl=en
    homepage: http://www4.comp.polyu.edu.hk/~jing1li/
    institution: The Hong Kong Polytechnic University
    last_name: Li
    name: Jing Li
    orcid: https://orcid.org/0000-0002-8044-2284
    semantic_scholar_id: https://www.semanticscholar.org/author/Jing-Li/49297986
    username: ~Jing_Li18
  decision: toMainConference
  end_page: 895
  file: 85.pdf
  id: 85
  num_pages: 13
  openreview_id: b8C5DEosw6
  pdf_file: f46dd00dd7a375fe30475b824f158f43b205067e.pdf
  start_page: 883
  title: 'BeLLM: Backward Dependency Enhanced Large Language Model for Sentence Embeddings'
- abstract: The factual knowledge of LLMs is typically evaluated using accuracy, yet
    this metric does not capture the vulnerability of LLMs to hallucination-inducing
    factors like prompt and context variability. How do we evaluate the capabilities
    of LLMs to consistently produce factually correct answers? In this paper, we propose
    MOdel kNowledge relIabiliTy scORe (MONITOR), a novel metric designed to directly
    measure LLMs' factual reliability. MONITOR is designed to compute the distance
    between the probability distributions of a valid output and its counterparts produced
    by the same LLM probing the same fact using different styles of prompts and contexts.
    Experiments on a comprehensive range of 12 LLMs demonstrate the effectiveness
    of MONITOR in evaluating the factual reliability of LLMs while maintaining a low
    computational overhead. In addition, we release the FKTC (Factual Knowledge Test
    Corpus) to foster research along this line https://github.com/Vicky-Wil/MONITOR.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - emails: '****@ed.ac.uk'
    first_name: Weixuan
    google_scholar_id: https://scholar.google.com/citations?user=qAduuoUAAAAJ&hl=en
    institution: University of Edinburgh, University of Edinburgh
    last_name: Wang
    name: Weixuan Wang
    username: ~Weixuan_Wang2
  - dblp_id: https://dblp.org/pid/12/5915
    emails: '****@ed.ac.uk'
    first_name: Barry
    google_scholar_id: https://scholar.google.com/citations?user=6NqRjRYAAAAJ&hl=en
    homepage: https://homepages.inf.ed.ac.uk/bhaddow/
    last_name: Haddow
    name: Barry Haddow
    username: ~Barry_Haddow1
  - dblp_id: https://dblp.org/pid/24/6740
    emails: '****@ed.ac.uk'
    first_name: Alexandra
    google_scholar_id: https://scholar.google.co.uk/citations?user=gZOV9kMAAAAJ&hl=en&oi=sra
    homepage: http://homepages.inf.ed.ac.uk/abmayne/
    institution: University of Edinburgh
    last_name: Birch
    name: Alexandra Birch
    semantic_scholar_id: https://www.semanticscholar.org/author/Alexandra-Birch/2539211
    username: ~Alexandra_Birch1
  - emails: '****@huawei.com'
    first_name: Wei
    homepage: https://scholars.latrobe.edu.au/w2peng/publications
    last_name: Peng
    name: Wei Peng
    username: ~Wei_Peng6
  decision: toMainConference
  end_page: 910
  file: 88.pdf
  id: 88
  num_pages: 15
  openreview_id: mCcyF32FTg
  pdf_file: 4076f99f6ef356e3039ee41b778a5c0f60c2a04c.pdf
  start_page: 896
  title: Assessing Factual Reliability of Large Language Model Knowledge
- abstract: Dialogue response selection aims to select an appropriate response from
    several candidates based on a given user and system utterance history. Most existing
    works primarily focus on post-training and fine-tuning tailored for cross-encoders.
    However, there are no post-training methods tailored for dense encoders in dialogue
    response selection. We argue that when the current language model, based on dense
    dialogue systems (such as BERT), is employed as a dense encoder, it separately
    encodes dialogue context and response, leading to a struggle to achieve the alignment
    of both representations. Thus, we propose Dial-MAE (Dialogue Contextual Masking
    Auto-Encoder), a straightforward yet effective post-training technique tailored
    for dense encoders in dialogue response selection. Dial-MAE uses an asymmetric
    encoder-decoder architecture to compress the dialogue semantics into dense vectors,
    which achieves better alignment between the features of the dialogue context and
    response. Our experiments have demonstrated that Dial-MAE is highly effective,
    achieving state-of-the-art performance on two commonly evaluated benchmarks.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Dialogue and Interactive Systems
  authors:
  - emails: '****@iie.ac.cn'
    first_name: Zhenpeng
    homepage: https://github.com/Saggressive
    last_name: Su
    name: Zhenpeng Su
    username: ~Zhenpeng_Su1
  - emails: '****@iie.ac.cn'
    first_name: Xing
    google_scholar_id: https://scholar.google.com.hk/citations?user=ZKd3UjkAAAAJ&hl=zh-CN
    homepage: https://scholar.google.com.hk/citations?user=ZKd3UjkAAAAJ&hl=zh-CN
    last_name: W
    name: Xing W
    username: ~Xing_W1
  - dblp_id: https://dblp.org/pid/69/5011-19
    emails: '****@iie.ac.cn'
    first_name: Wei
    homepage: http://people.ucas.ac.cn/~iiezhouwei
    last_name: Zhou
    name: Wei Zhou
    username: ~Wei_Zhou5
  - dblp_id: https://dblp.org/pid/289/8498.html
    emails: '****@iie.ac.cn'
    first_name: Guangyuan
    google_scholar_id: https://scholar.google.com/citations?user=GHBLzN0AAAAJ
    last_name: Ma
    name: Guangyuan Ma
    orcid: https://orcid.org/0000-0001-6916-9611
    semantic_scholar_id: https://www.semanticscholar.org/author/Guangyuan-Ma/2068996632
    username: ~Guangyuan_Ma1
  - emails: '****@iie.ac.cn'
    first_name: Songlin
    homepage: http://people.ucas.ac.cn/~0000967?language=en
    last_name: Hu
    name: Songlin Hu
    username: ~Songlin_Hu2
  decision: toMainConference
  end_page: 921
  file: 91.pdf
  id: 91
  num_pages: 11
  openreview_id: ootyMLwYPG
  pdf_file: 25ace309cc7b1e9e05527a5d42d1b090e0f1229f.pdf
  start_page: 911
  title: 'Dial-MAE: ConTextual Masked Auto-Encoder for Retrieval-based Dialogue Systems'
- abstract: Large Language Models (LLMs) have demonstrated remarkable progress in
    utilizing tools, but their closed-source nature and high inference costs pose
    limitations on their adaptability, necessitating a valid method that leverages
    smaller, open-sourced models. In this paper, we introduce Toolink, a comprehensive
    framework that performs task-solving by first creating a toolkit and then integrating
    the planning and calling of tools through a chain-of-solving (CoS) approach. We
    first validate the efficacy of Toolink in harnessing the model's creativity and
    CoS ability on ChatGPT. Subsequently, we curate CoS-GPT, a chain-of-solving dataset
    designed for tool-using, and finetune the LLaMA-7B model. It results in LLaMA-CoS,
    a powerful open-source model with advanced tool-planning and tool-calling capabilities.
    Evaluation of diverse tasks from BIG-bench demonstrates its CoS ability matches
    that of ChatGPT while its performance surpasses the chain-of-thought approach.
    Further studies highlight the generalization of LLaMA-CoS to unseen tasks and
    showcase its capability in using toolkits not explicitly tailored for the target
    task, affirming its robustness in real-world scenarios. All codes and data are
    released.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Efficient/Low-Resource Methods for NLP
  authors:
  - emails: '****@mails.tsinghua.edu.cn'
    first_name: Cheng
    google_scholar_id: https://scholar.google.com/citations?user=p2bY7oAAAAAJ&hl=en
    homepage: https://qiancheng0.github.io/
    last_name: Qian
    name: Cheng Qian
    username: ~Cheng_Qian4
  - dblp_id: https://dblp.org/pid/18/10886
    emails: '****@cs.cmu.edu'
    first_name: Chenyan
    google_scholar_id: https://scholar.google.com/citations?user=E9BaEBYAAAAJ&hl=en
    homepage: https://www.cs.cmu.edu/~cx/
    institution: School of Computer Science, Carnegie Mellon University
    last_name: Xiong
    name: Chenyan Xiong
    semantic_scholar_id: https://www.semanticscholar.org/author/Chenyan-Xiong/144628574
    username: ~Chenyan_Xiong1
  - dblp_id: https://dblp.org/pid/243/2880.html
    emails: '****@mail.neu.edu.cn'
    first_name: Zhenghao
    google_scholar_id: https://scholar.google.com/citations?user=4vrZRk0AAAAJ&hl=en
    homepage: https://edwardzh.github.io
    institution: Northeastern University
    last_name: Liu
    name: Zhenghao Liu
    username: ~Zhenghao_Liu2
  - dblp_id: https://dblp.org/pid/53/3245-1
    emails: '****@tsinghua.edu.cn'
    first_name: Zhiyuan
    google_scholar_id: https://scholar.google.com/citations?user=dT0v5u0AAAAJ&hl=en
    homepage: http://nlp.csai.tsinghua.edu.cn/~lzy
    institution: Tsinghua University
    last_name: Liu
    name: Zhiyuan Liu
    orcid: https://orcid.org/0000-0002-7709-2543
    username: ~Zhiyuan_Liu1
  decision: toMainConference
  end_page: 945
  file: 93.pdf
  id: 93
  num_pages: 24
  openreview_id: sNNgGq71MJ
  pdf_file: ac1b8b9873dcc3a47b38a1c6743cdd0cd2ec8195.pdf
  start_page: 922
  title: 'Toolink: Linking Toolkit Creation and Using through Chain-of-Solving on
    Open-Source Model'
- abstract: We propose Label Creative Generation (LCG), a new paradigm in multi-label
    data augmentation. Beyond repeating data points with fixed labels, LCG creates
    new data by exploring innovative label combinations. Within LCG, we introduce
    Tail-Driven Conditional Augmentation (TDCA), combining tail-driven label sampling
    and label-conditioned text generation for balanced, consistent data augmentation.
    Our approach has demonstrated a **100.21%** increase in PSP@1 across three datasets,
    successfully mitigating the long-tail effect in MLTC and markedly enhancing model
    performance.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Retrieval and Text Mining
  authors:
  - emails: '****@qq.com'
    first_name: Letian
    institution: Sichuan University
    last_name: Wang
    name: Letian Wang
    orcid: https://orcid.org/0000-0003-4433-2966
    username: ~Letian_Wang2
  - dblp_id: https://dblp.org/pid/150/5942
    emails: '****@qq.com'
    first_name: Xianggen
    google_scholar_id: https://scholar.google.com/citations?user=qxNzQfQAAAAJ&hl=zh-CN&oi=ao
    institution: Sichuan University
    last_name: Liu
    name: Xianggen Liu
    semantic_scholar_id: https://www.semanticscholar.org/author/Xianggen-Liu/9137409
    username: ~Xianggen_Liu1
  - dblp_id: https://dblp.org/pid/68/2367.html
    emails: '****@scu.edu.cn'
    first_name: Jiancheng
    google_scholar_id: https://scholar.google.com/citations?hl=zh-CN&user=ppaXFxIAAAAJ
    homepage: https://cs.scu.edu.cn/info/1303/13767.htm
    institution: Sichuan University
    last_name: Lv
    name: Jiancheng Lv
    username: ~Jiancheng_Lv2
  decision: toMainConference
  end_page: 960
  file: 95.pdf
  id: 95
  num_pages: 15
  openreview_id: F6P0giIE7B
  pdf_file: bbbe328525c0f28b1f378463b3f72e3fd405a323.pdf
  start_page: 946
  title: 'Create! Don''t Repeat: A Paradigm Shift in Multi-Label Augmentation through
    Label Creative Generation'
- abstract: 'This paper introduces Neurocache, an approach to extend the effective
    context size of large language models (LLMs) using an external vector cache to
    store its past states. Like recent vector retrieval approaches, Neurocache uses
    an efficient k-nearest-neighbor (kNN) algorithm to retrieve relevant past states
    and incorporate them into the attention process. Neurocache improves upon previous
    methods by (1) storing compressed states, which reduces cache size; (2) performing
    a single retrieval operation per token which increases inference speed; and (3)
    extending the retrieval window to neighboring states, which improves both language
    modeling and downstream task accuracy. Our experiments show the effectiveness
    of Neurocache both for models trained from scratch and for pre-trained models
    such as Llama2-7B and Mistral-7B when enhanced with the cache mechanism. We also
    compare Neurocache with text retrieval methods and show improvements in single-document
    question-answering and few-shot learning tasks. We made the source code available
    under: https://github.com/alisafaya/neurocache'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Efficient/Low-Resource Methods for NLP
  authors:
  - dblp_id: https://dblp.uni-trier.de/pid/245/4209
    emails: '****@ku.edu.tr'
    first_name: Ali
    google_scholar_id: https://scholar.google.com/citations?user=j5mgvE4AAAAJ&hl=en
    homepage: https://asafaya.me/
    last_name: Safaya
    name: Ali Safaya
    orcid: https://orcid.org/0000-0003-0945-8897
    semantic_scholar_id: https://www.semanticscholar.org/author/Ali-Safaya/150920423
    username: ~Ali_Safaya1
  - dblp_id: https://dblp.org/pid/84/4160
    emails: '****@ku.edu.tr'
    first_name: Deniz
    google_scholar_id: https://scholar.google.com.tw/citations?user=EJurXJ4AAAAJ
    homepage: http://www.denizyuret.com/
    institution: Koc University
    last_name: Yuret
    name: Deniz Yuret
    username: ~Deniz_Yuret1
  decision: toMainConference
  end_page: 974
  file: 96.pdf
  id: 96
  num_pages: 14
  openreview_id: gGK2UjksRf
  pdf_file: 01f159862f8cfaecaa6a8f5704419787fd6e70b0.pdf
  start_page: 961
  title: 'Neurocache: Efficient Vector Retrieval for Long-range Language Modeling'
- abstract: 'While Large Language Models (LLMs) have demonstrated exceptional multitasking
    abilities, fine-tuning these models on downstream, domain-specific datasets is
    often necessary to yield superior performance on test sets compared to their counterparts
    without fine-tuning. However, the comprehensive effects of fine-tuning on the
    LLMs'' generalization ability are not fully understood.

    This paper delves into the differences between original, unmodified LLMs and their
    fine-tuned variants. Our primary investigation centers on whether fine-tuning
    affects the generalization ability intrinsic to LLMs. To elaborate on this, we
    conduct extensive experiments across five distinct language tasks on various datasets.

    Our main findings reveal that models fine-tuned on generation and classification
    tasks exhibit dissimilar behaviors in generalizing to different domains and tasks.

    Intriguingly, we observe that integrating the in-context learning strategy during
    fine-tuning on generation tasks can enhance the model''s generalization ability.

    Through this systematic investigation, we aim to contribute valuable insights
    into the evolving landscape of fine-tuning practices for LLMs.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - dblp_id: https://dblp.uni-trier.de/pid/241/5752
    emails: '****@se.cuhk.edu.hk'
    first_name: Haoran
    google_scholar_id: https://scholar.google.com/citations?user=yq6_Dr8AAAAJ&hl=en
    last_name: Yang
    name: Haoran Yang
    semantic_scholar_id: https://www.semanticscholar.org/author/Haoran-Yang/2115538342
    username: ~Haoran_Yang2
  - emails: '****@mails.tsinghua.edu.cn'
    first_name: Yumeng
    homepage: https://perhacept.github.io/
    last_name: Zhang
    name: Yumeng Zhang
    username: ~Yumeng_Zhang3
  - dblp_id: https://dblp.org/pid/87/10695
    emails: '****@cse.cuhk.edu.hk'
    first_name: Jiaqi
    google_scholar_id: https://scholar.google.com/citations?user=oWvJUfoAAAAJ&hl=en
    homepage: https://jiaqixuac.github.io/
    institution: The Chinese University of Hong Kong
    last_name: Xu
    name: Jiaqi Xu
    orcid: https://orcid.org/0000-0003-1279-6782
    username: ~Jiaqi_Xu1
  - emails: '****@link.cuhk.edu.hk'
    first_name: Hongyuan
    last_name: Lu
    name: Hongyuan Lu
    username: ~Hongyuan_Lu2
  - dblp_id: https://dblp.org/pid/52/2889
    emails: '****@cse.cuhk.edu.hk'
    first_name: Pheng-Ann
    google_scholar_id: https://scholar.google.com/citations?sortby=pubdate&hl=en&user=OFdytjoAAAAJ&view_op=list_works
    homepage: http://www.cse.cuhk.edu.hk/~pheng
    last_name: Heng
    name: Pheng-Ann Heng
    username: ~Pheng-Ann_Heng1
  - dblp_id: https://dblp.org/pid/48/1707
    emails: '****@se.cuhk.edu.hk'
    first_name: Wai
    google_scholar_id: https://scholar.google.com/citations?user=ewA4NAcAAAAJ&hl=en
    homepage: http://www.se.cuhk.edu.hk/~textmine
    institution: The Chinese University of Hong Kong
    last_name: Lam
    name: Wai Lam
    semantic_scholar_id: https://www.semanticscholar.org/author/144594306
    username: ~Wai_Lam1
  decision: toMainConference
  end_page: 990
  file: 97.pdf
  id: 97
  num_pages: 16
  openreview_id: k6z1VuZr6O
  pdf_file: 7a48f33d7f37b29a3909a92ecd8e862268f286bd.pdf
  start_page: 975
  title: Unveiling the Generalization Power of Fine-Tuned Large Language Models
- abstract: Logical reasoning has been an ongoing pursuit in the field of AI. Despite
    significant advancements made by large language models (LLMs), they still struggle
    with complex logical reasoning problems. To enhance reasoning performance, one
    promising direction is scalable oversight, which requires LLMs to identify their
    own errors and then improve by themselves. Various self-verification methods have
    been proposed in pursuit of this goal. Nevertheless, whether existing models understand
    their own errors well is still under investigation. In this paper, we take a closer
    look at the self-verification abilities of LLMs in the context of logical reasoning,
    focusing on their ability to identify logical fallacies accurately. We introduce
    a dataset, FALLACIES, containing 232 types of reasoning fallacies categorized
    in a hierarchical taxonomy. By conducting exhaustive experiments on FALLACIES,
    we obtain comprehensive and detailed analyses of a series of models on their verification
    abilities. Our main findings suggest that existing LLMs could struggle to identify
    fallacious reasoning steps accurately and may fall short of guaranteeing the validity
    of self-verification methods. Drawing from these observations, we offer suggestions
    for future research and practical applications of self-verification methods.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - dblp_id: https://dblp.uni-trier.de/pid/316/2349.html
    emails: '****@mails.tsinghua.edu.cn'
    first_name: Ruixin
    institution: Tsinghua University, Tsinghua University
    last_name: Hong
    name: Ruixin Hong
    orcid: https://orcid.org/0000-0003-2852-3746
    semantic_scholar_id: https://www.semanticscholar.org/author/Ruixin-Hong/2151067405
    username: ~Ruixin_Hong2
  - dblp_id: https://dblp.org/pid/48/859.html
    emails: '****@cse.ust.hk'
    first_name: Hongming
    google_scholar_id: https://scholar.google.com/citations?user=i5ETuuQAAAAJ&hl=en
    homepage: http://www.cse.ust.hk/~hzhangal/
    last_name: Zhang
    name: Hongming Zhang
    username: ~Hongming_Zhang2
  - emails: '****@mails.tsinghua.edu.cn'
    first_name: Xinyu
    homepage: https://github.com/Vic-PPPang
    institution: Tsinghua University, Tsinghua University
    last_name: Pang
    name: Xinyu Pang
    username: ~Xinyu_Pang1
  - dblp_id: https://dblp.org/pid/71/4598-1
    emails: '****@ieee.org'
    first_name: Dong
    google_scholar_id: https://scholar.google.com/citations?user=tMY31_gAAAAJ
    homepage: https://sites.google.com/view/dongyu888/
    institution: Tencent AI Lab
    last_name: Yu
    name: Dong Yu
    semantic_scholar_id: https://www.semanticscholar.org/author/Dong-Yu/144580027
    username: ~Dong_Yu2
  - dblp_id: https://dblp.org/pid/z/ChangshuiZhang
    emails: '****@mail.tsinghua.edu.cn'
    first_name: Changshui
    google_scholar_id: https://scholar.google.com/citations?user=GL9M37YAAAAJ&hl=zh-CN
    homepage: http://bigeye.au.tsinghua.edu.cn/english/Introduction.html
    institution: Tsinghua University and Department of Computer Science and Technology
    last_name: Zhang
    name: Changshui Zhang
    username: ~Changshui_Zhang2
  decision: toMainConference
  end_page: 1016
  file: 98.pdf
  id: 98
  num_pages: 26
  openreview_id: VcVmlBPk5c
  pdf_file: 370f24aa7a2639a2f1f176a4da5920df5ac61062.pdf
  start_page: 991
  title: A Closer Look at the Self-Verification Abilities of Large Language Models
    in Logical Reasoning
- abstract: Traditional attempts to enhance the logical reasoning abilities of language
    models often rely on supervised fine-tuning, limiting their generalization to
    new tasks or domains. Large Language Models (LLMs), with their capacity to condense
    vast knowledge, can effectively tackle many tasks. Yet, our experiments reveal
    a gap in their performance on logical reasoning benchmarks when compared to state-of-the-art
    fine-tuning based models. To bridge this gap, we present LogicLLM, a first-of-its-kind,
    fully self-supervised framework for integrating logical reasoning capabilities
    into LLMs, and activating them via in-context learning. We apply this to two LLM
    series, FLAN-T5 and LLaMA, with parameter sizes from 3 billion to 33 billion.
    LogicLLM demonstrates its effectiveness through successful improvements on two
    logical reasoning benchmarks (ReClor and LogiQA-v2). Additionally, LogicLLM based
    on FLAN-T5-11B attains comparable results to ChatGPT, and evaluations with LLaMA-based
    models on three language understanding benchmarks (RACE, MMLU and Big-Bench-Hard)
    confirm that the improvements come without compromising the model's general language
    understanding capabilities.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Question Answering
  authors:
  - dblp_id: https://dblp.org/pid/264/9981
    emails: '****@hotmail.com'
    first_name: Fangkai
    google_scholar_id: https://scholar.google.com/citations?user=_u8lwyIAAAAJ&hl=zh-CN
    homepage: https://sparkjiao.github.io/
    last_name: Jiao
    name: Fangkai Jiao
    orcid: https://orcid.org/0000-0002-0670-6990
    semantic_scholar_id: https://www.semanticscholar.org/author/1689176705
    username: ~Fangkai_Jiao1
  - dblp_id: https://dblp.org/pid/136/8660
    emails: '****@gmail.com'
    first_name: Zhiyang
    google_scholar_id: https://scholar.google.com/citations?user=9wOJrf8AAAAJ&hl=en
    homepage: https://zeeeyang.github.io
    last_name: Teng
    name: Zhiyang Teng
    semantic_scholar_id: https://www.semanticscholar.org/author/Zhiyang-Teng/2272668
    username: ~Zhiyang_Teng1
  - dblp_id: https://dblp.org/pid/277/9378
    emails: '****@e.ntu.edu.sg'
    first_name: Bosheng
    google_scholar_id: https://scholar.google.com/citations?user=Bp8u4lgAAAAJ&hl=en
    last_name: Ding
    name: Bosheng Ding
    semantic_scholar_id: https://www.semanticscholar.org/author/Bosheng-Ding/2064493724
    username: ~Bosheng_Ding1
  - dblp_id: https://dblp.org/pid/229/9236
    emails: '****@i2r.a-star.edu.sg'
    first_name: Zhengyuan
    institution: I2R
    last_name: Liu
    name: Zhengyuan Liu
    semantic_scholar_id: https://www.semanticscholar.org/author/Zhengyuan-Liu/49293155
    username: ~Zhengyuan_Liu2
  - dblp_id: https://dblp.org/pid/84/8761
    emails: '****@i2r.a-star.edu.sg'
    first_name: Nancy
    google_scholar_id: https://scholar.google.com.sg/citations?user=K3Z9UiAAAAAJ&hl=en
    homepage: http://alum.mit.edu/www/nancychen
    last_name: Chen
    middle_name: F.
    name: Nancy F. Chen
    orcid: https://orcid.org/0000-0003-0872-5877
    semantic_scholar_id: https://www.semanticscholar.org/author/Nancy-F.-Chen/2185019
    username: ~Nancy_F._Chen1
  - dblp_id: https://dblp.org/pid/62/2078
    emails: '****@salesforce.com'
    first_name: Shafiq
    google_scholar_id: https://scholar.google.com/citations?user=hR249csAAAAJ&hl=en
    homepage: https://raihanjoty.github.io/
    institution: SalesForce.com and Nanyang Technological University
    last_name: Joty
    name: Shafiq Joty
    username: ~Shafiq_Joty1
  decision: toMainConference
  end_page: 1032
  file: 99.pdf
  id: 99
  num_pages: 16
  openreview_id: HZtXzsE0Z9
  pdf_file: 20a1887845628221908705323c0a349ebf572422.pdf
  start_page: 1017
  title: Exploring Self-supervised Logic-enhanced Training for Large Language Models
- abstract: Tool-augmented Large Language Models (TALMs) are known to enhance the
    skillset of large language models (LLMs), thereby, leading to their improved reasoning
    abilities across many tasks. While, TALMs have been successfully employed in different
    question-answering benchmarks, their efficacy on complex mathematical reasoning
    benchmarks, and the potential complementary benefits offered by tools for knowledge
    retrieval and mathematical equation solving are open research questions. In this
    work, we present MathSensei, a tool-augmented large language model for mathematical
    reasoning. We study the complementary benefits of the tools - knowledge retriever
    (Bing Web Search), program generator + executor (Python), and symbolic equation
    solver (Wolfram-Alpha API) through evaluations on mathematical reasoning datasets.
    We perform exhaustive ablations on MATH, a popular dataset for evaluating mathematical
    reasoning on diverse mathematical disciplines. We also conduct experiments involving
    well-known tool planners to study the impact of tool sequencing on the model performance.
    MathSensei achieves 13.5% better accuracy over gpt-3.5-turbo with Chain-of-Thought
    on the MATH dataset. We further observe that TALMs are not as effective for simpler
    math word problems (in GSM-8K), and the benefit increases as the complexity and
    required knowledge increases (progressively over AQuA, MMLU-Math, and higher level
    complex questions in MATH). The code and data are available at https://github.com/Debrup-61/MathSensei.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - emails: '****@gmail.com'
    first_name: Debrup
    homepage: https://www.linkedin.com/in/debrup-das-6448a41a7/
    last_name: Das
    name: DEBRUP DAS
    username: ~DEBRUP_DAS1
  - dblp_id: https://dblp.org/pid/154/8755.html
    emails: '****@gmail.com'
    first_name: Debopriyo
    google_scholar_id: https://scholar.google.com/citations?user=GOOXm9gAAAAJ&hl=en
    homepage: https://dpbanerjee.site
    institution: Rakuten Global Inc.
    last_name: Banerjee
    name: Debopriyo Banerjee
    orcid: https://orcid.org/0000-0001-9773-776X
    username: ~Debopriyo_Banerjee1
  - dblp_id: https://dblp.org/pid/165/0785
    emails: '****@asu.edu'
    first_name: Somak
    google_scholar_id: https://scholar.google.com/citations?user=2shiHpwAAAAJ&hl=en&authuser=1
    homepage: https://adityasomak.github.io/
    institution: Indian Institute of Technology Kharagpur
    last_name: Aditya
    name: Somak Aditya
    username: ~Somak_Aditya1
  - dblp_id: https://dblp.org/pid/141/2070
    emails: '****@gmail.com'
    first_name: Ashish
    institution: Rakuten
    last_name: Kulkarni
    name: Ashish Kulkarni
    username: ~Ashish_Kulkarni1
  decision: toMainConference
  end_page: 1056
  file: 100.pdf
  id: 100
  num_pages: 24
  openreview_id: xLvcuaGdEZ
  pdf_file: 86e4eaa06d62223c0677bbf428ed99a42a0b7255.pdf
  start_page: 1033
  title: 'MATHSENSEI: A Tool-Augmented Large Language Model for Mathematical Reasoning'
- abstract: "Coherence evaluation aims to assess the organization and structure of\
    \ a discourse, which remains challenging even in the era of large language models.\
    \ \nDue to the scarcity of annotated data, data augmentation is commonly used\
    \ for training coherence evaluation models. \nHowever, previous augmentations\
    \ for this task primarily rely on heuristic rules, lacking designing criteria\
    \ as guidance.\nIn this paper, we take inspiration from linguistic theory of discourse\
    \ structure, and propose a data augmentation framework named CoUDA. CoUDA breaks\
    \ down discourse coherence into global and local aspects, and designs augmentation\
    \ strategies for both aspects, respectively.\nEspecially for local coherence,\
    \ we propose a novel generative strategy for constructing augmentation samples,\
    \ which involves post-pretraining a generative model and applying two controlling\
    \ mechanisms to control the difficulty of generated samples. \nDuring inference,\
    \ CoUDA also jointly evaluates both global and local aspects  to comprehensively\
    \ assess the overall coherence of a discourse.\nExtensive experiments in coherence\
    \ evaluation show that, with only 233M parameters,  CoUDA achieves state-of-the-art\
    \ performance in both pointwise scoring and pairwise ranking tasks, even surpassing\
    \ recent GPT-3.5 and GPT-4 based metrics."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - emails: '****@pku.edu.cn'
    first_name: Dawei
    google_scholar_id: https://scholar.google.com/citations?user=oD2HPaYAAAAJ&hl=en
    institution: Peking University
    last_name: Zhu
    name: Dawei Zhu
    semantic_scholar_id: https://www.semanticscholar.org/author/Dawei-Zhu/2116276849
    username: ~Dawei_Zhu2
  - emails: '****@pku.edu.cn'
    first_name: Wenhao
    google_scholar_id: https://scholar.google.com/citations?user=LZFvCrwAAAAJ&hl=en
    last_name: Wu
    name: Wenhao Wu
    username: ~Wenhao_Wu7
  - emails: '****@outlook.com'
    first_name: Yifan
    homepage: https://yifan-song793.github.io/
    last_name: Song
    name: Yifan Song
    username: ~Yifan_Song2
  - emails: '****@foxmail.com'
    first_name: Fangwei
    last_name: Zhu
    name: Fangwei Zhu
    semantic_scholar_id: https://www.semanticscholar.org/author/Fangwei-Zhu/70585600
    username: ~Fangwei_Zhu1
  - dblp_id: https://dblp.org/pid/148/4447
    emails: '****@suda.edu.cn'
    first_name: Ziqiang
    google_scholar_id: https://scholar.google.com/citations?hl=zh-CN&user=06ITfcEAAAAJ
    last_name: Cao
    name: Ziqiang Cao
    orcid: https://orcid.org/0000-0002-1077-9033
    semantic_scholar_id: https://www.semanticscholar.org/author/Ziqiang-Cao/2314396
    username: ~Ziqiang_Cao2
  - dblp_id: https://dblp.org/pid/05/4288
    emails: '****@pku.edu.cn'
    first_name: Sujian
    google_scholar_id: https://scholar.google.com.tw/citations?user=RvBDhSwAAAAJ
    homepage: http://123.56.88.210/
    institution: Peking University
    last_name: Li
    name: Sujian Li
    username: ~Sujian_Li1
  decision: toMainConference
  end_page: 1068
  file: 103.pdf
  id: 103
  num_pages: 12
  openreview_id: vWmtDpcaDp
  pdf_file: 7669360111c70ad9a1c4ddbeb586a59057e1ccd9.pdf
  start_page: 1057
  title: 'CoUDA: Coherence Evaluation via Unified Data Augmentation'
- abstract: "We introduce mEdIT, a multi-lingual extension to CoEdIT \u2013 the recent\
    \ state-of-the-art text editing models for writing assistance. mEdIT models are\
    \ trained by fine-tuning multi-lingual large, pre-trained language models (LLMs)\
    \ via instruction tuning. They are designed to take instructions from the user\
    \ specifying the attributes of the desired text in the form of natural language\
    \ instructions, such as \"Grammatik korrigieren\" (German) or \"\u110B\u1175 \u1110\
    \u1166\u11A8\u1109\u1173 \u1110\u1173\u1105\u1173\u11AF \u1103\u1161\u11AB\u1109\
    \u116E\u11AB\u1112\u116A\" (Korean). We build mEdIT by curating data from multiple\
    \ publicly available human-annotated text editing datasets for three text editing\
    \ tasks (Grammatical Error Correction (GEC), Text Simplification, and Paraphrasing)\
    \ across diverse languages belonging to six different language families. We detail\
    \ the design and training of mEdIT models and demonstrate their strong performance\
    \ on many multi-lingual text editing benchmarks against other multilingual LLMs.\
    \ We also find that mEdIT generalizes effectively to new languages over multilingual\
    \ baselines. We publicly release our data, code, and trained models."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - dblp_id: https://dblp.org/pid/124/3068
    emails: '****@columbia.edu'
    first_name: Vipul
    google_scholar_id: https://scholar.google.com/citations?user=-Ap77UMAAAAJ&hl=en
    institution: Columbia University, Grammarly and International Institute of Information
      Technology Hyderabad
    last_name: Raheja
    name: Vipul Raheja
    semantic_scholar_id: https://www.semanticscholar.org/author/Vipul-Raheja/2831377
    username: ~Vipul_Raheja1
  - dblp_id: https://dblp.org/pid/162/7497.html
    emails: '****@grammarly.com'
    first_name: Dimitris
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=8ZsPobcAAAAJ
    homepage: http://dimitrisalikaniotis.com
    institution: Grammarly
    last_name: Alikaniotis
    name: Dimitris Alikaniotis
    semantic_scholar_id: https://www.semanticscholar.org/author/Dimitrios-Alikaniotis/1958685
    username: ~Dimitris_Alikaniotis2
  - dblp_id: https://dblp.org/pid/92/9922
    emails: '****@gmail.com'
    first_name: Vivek
    google_scholar_id: https://scholar.google.com/citations?user=bCwtPMUAAAAJ&hl=en&oi=ao
    homepage: http://VivekKulkarni.net
    last_name: Kulkarni
    name: Vivek Kulkarni
    semantic_scholar_id: https://www.semanticscholar.org/author/Vivek-Kulkarni/144592382
    username: ~Vivek_Kulkarni2
  - dblp_id: https://dblp.org/pid/234/6160.html
    emails: '****@nyu.edu'
    first_name: Bashar
    google_scholar_id: https://scholar.google.com/citations?user=DFkVNJwAAAAJ
    homepage: https://basharalhafni.com/
    institution: New York University
    last_name: Alhafni
    name: Bashar Alhafni
    semantic_scholar_id: https://www.semanticscholar.org/author/Bashar-Alhafni/66589548
    username: ~Bashar_Alhafni1
  - dblp_id: https://dblp.org/pid/159/9419-5
    emails: '****@gmail.com'
    first_name: Dhruv
    google_scholar_id: https://scholar.google.com/citations?user=IiMW328AAAAJ&hl=en&oi=ao
    homepage: https://ddhruvkr.github.io/
    last_name: Kumar
    name: Dhruv Kumar
    orcid: https://orcid.org/0000-0002-8191-0123
    semantic_scholar_id: https://www.semanticscholar.org/author/Dhruv-Kumar/50271213
    username: ~Dhruv_Kumar2
  decision: toMainConference
  end_page: 1091
  file: 106.pdf
  id: 106
  num_pages: 23
  openreview_id: k0Dvaq7wWz
  pdf_file: b9cb6a88cb7bae82156fcfeb875a3b79f96fae43.pdf
  start_page: 1069
  title: 'mEdIT: Multilingual Text Editing via Instruction Tuning'
- abstract: Federated embodied agent learning protects the data privacy of individual
    visual environments by keeping data locally at each client (the individual environment)
    during training. However, since the local data is inaccessible to the server under
    federated learning, attackers may easily poison the training data of the local
    client to build a backdoor in the agent without notice. Deploying such an agent
    raises the risk of potential harm to humans, as the attackers may easily navigate
    and control the agent as they wish via the backdoor. Towards Byzantine-robust
    federated embodied agent learning, in this paper, we study the attack and defense
    for the task of vision-and-language navigation (VLN), where the agent is required
    to follow natural language instructions to navigate indoor environments. First,
    we introduce a simple but effective attack strategy, Navigation as Wish (NAW),
    in which the malicious client manipulates local trajectory data to implant a backdoor
    into the global model. Results on two VLN datasets (R2R and RxR) show that NAW
    can easily navigate the deployed VLN agent regardless of the language instruction,
    without affecting its performance on normal test sets. Then, we propose a new
    Prompt-Based Aggregation (PBA) to defend against the NAW attack in federated VLN,
    which provides the server with a ''prompt'' of the vision-and-language alignment
    variance between the benign and malicious clients so that they can be distinguished
    during training. We validate the effectiveness of the PBA method on protecting
    the global model from the NAW attack, which outperforms other state-of-the-art
    defense methods by a large margin in the defense metrics on R2R and RxR.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multimodality and Language Grounding to Vision, Robotics and Beyond
  authors:
  - emails: '****@gmail.com'
    first_name: Yunchao
    homepage: https://yunchaozhang.netlify.app/
    institution: University of Hong Kong
    last_name: Zhang
    name: Yunchao Zhang
    username: ~Yunchao_Zhang2
  - dblp_id: https://dblp.org/pid/234/0988
    emails: '****@ucsc.edu'
    first_name: Zonglin
    homepage: https://elegantlin.github.io/
    institution: University of California, Santa Cruz
    last_name: Di
    name: Zonglin Di
    username: ~Zonglin_Di1
  - emails: '****@ucsc.edu'
    first_name: Kaiwen
    google_scholar_id: https://scholar.google.com/citations?view_op=list_works&hl=en&user=FbzTX04AAAAJ
    homepage: https://kevinz-01.github.io/
    last_name: Zhou
    name: Kaiwen Zhou
    username: ~Kaiwen_Zhou3
  - dblp_id: https://dblp.org/pid/175/3366
    emails: '****@gmail.com'
    first_name: Cihang
    google_scholar_id: https://scholar.google.com/citations?user=X3vVZPcAAAAJ&hl=en
    homepage: https://cihangxie.github.io/
    institution: University of California, Santa Cruz
    last_name: Xie
    name: Cihang Xie
    username: ~Cihang_Xie3
  - dblp_id: https://dblp.uni-trier.de/pers/hd/w/Wang_0061:Xin
    emails: '****@ucsc.edu'
    first_name: Xin
    google_scholar_id: https://scholar.google.com/citations?user=YjqluE0AAAAJ
    homepage: https://eric-xw.github.io
    institution: University of California, Santa Cruz
    last_name: Wang
    middle_name: Eric
    name: Xin Eric Wang
    orcid: https://orcid.org/0000-0003-2605-5504
    semantic_scholar_id: https://www.semanticscholar.org/author/Xin-Eric-Wang/48631993
    username: ~Xin_Eric_Wang2
  decision: toMainConference
  end_page: 1106
  file: 107.pdf
  id: 107
  num_pages: 15
  openreview_id: P3pSLBnG29
  pdf_file: 1256b02bafa52c11b3f3bfc653857d4cf1ca0448.pdf
  start_page: 1092
  title: Navigation as Attackers Wish? Towards Building Robust Embodied Agents under
    Federated Learning
- abstract: "In-context learning (ICL) has shown impressive results in few-shot learning\
    \ tasks, yet its underlying mechanism is still not fully understood. A recent\
    \ line of work suggests that ICL performs gradient descent (GD)-based optimization\
    \ implicitly. While appealing, much of the research focuses on simplified settings,\
    \ where the parameters of a shallow model are optimized. In this work, we revisit\
    \ evidence for ICL-GD correspondence on realistic NLP tasks and models. \nWe find\
    \ gaps in evaluation, both in terms of problematic metrics and insufficient baselines.\
    \ We show that surprisingly, even untrained models achieve comparable ICL-GD similarity\
    \ scores despite not exhibiting ICL.\nNext, we explore a major discrepancy in\
    \ the flow of information throughout the model between ICL and GD, which we term\
    \ \\textit{Layer Causality}. We propose a simple GD-based optimization procedure\
    \ that respects layer causality, and show it improves similarity scores significantly."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - emails: '****@gmail.com'
    first_name: Gilad
    last_name: Deutch
    name: Gilad Deutch
    username: ~Gilad_Deutch1
  - emails: '****@gmail.com'
    first_name: Nadav
    google_scholar_id: https://scholar.google.com/citations?hl=iw&user=2xIrHR8AAAAJ
    last_name: Magar
    name: Nadav Magar
    username: ~Nadav_Magar1
  - emails: '****@gmail.com'
    first_name: Tomer
    last_name: Natan
    middle_name: Bar
    name: Tomer Bar Natan
    username: ~Tomer_Bar_Natan1
  - dblp_id: https://dblp.org/pid/294/8909
    emails: '****@gmail.com'
    first_name: Guy
    google_scholar_id: https://scholar.google.com/citations?user=743l8mMAAAAJ
    institution: Tel Aviv University
    last_name: Dar
    name: Guy Dar
    semantic_scholar_id: https://www.semanticscholar.org/author/Guy-Dar/2111880293
    username: ~Guy_Dar1
  decision: toMainConference
  end_page: 1118
  file: 108.pdf
  id: 108
  num_pages: 12
  openreview_id: Zi2sAmpOXL
  pdf_file: b1e2efc8c3555b51dd71b45e3e59c8bac692d7c1.pdf
  start_page: 1107
  title: In-context Learning and Gradient Descent Revisited
- abstract: Recent trends in natural language processing research and annotation tasks
    affirm a paradigm shift from the traditional reliance on a single ground truth
    to a focus on individual perspectives, particularly in subjective tasks. In scenarios
    where annotation tasks are meant to encompass diversity, models that solely rely
    on the majority class labels may inadvertently disregard valuable minority perspectives.
    This oversight could result in the omission of crucial information and, in a broader
    context, risk disrupting the balance within larger ecosystems. As the landscape
    of annotator modeling unfolds with diverse representation techniques, it becomes
    imperative to investigate their effectiveness with the fine-grained features of
    the datasets in view. This study systematically explores various annotator modeling
    techniques and compares their performance across seven corpora. From our findings,
    we show that the commonly used user token model consistently outperforms more
    complex models. We introduce a composite embedding approach and show distinct
    differences in which model performs best as a function of the agreement with a
    given dataset. Our findings shed light on the relationship between corpus statistics
    and annotator modeling performance, which informs future work on corpus construction
    and perspectivist NLP.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Computational Social Science and Cultural Analytics
  authors:
  - emails: '****@uni-marburg.de'
    first_name: Sarumi
    homepage: https://cfwelch.com/collaborators/
    last_name: Oluyemi
    middle_name: Olufunke
    name: Sarumi Olufunke Oluyemi
    username: ~Sarumi_Olufunke_Oluyemi1
  - emails: '****@gmail.com'
    first_name: "B\xE9la"
    last_name: Neuendorf
    name: "B\xE9la Neuendorf"
    username: "~B\xE9la_Neuendorf1"
  - emails: '****@gmail.com'
    first_name: Joan
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=ZWajsOsAAAAJ
    institution: "Rheinische Friedrich-Wilhelms Universit\xE4t Bonn"
    last_name: Plepi
    name: Joan Plepi
    username: ~Joan_Plepi1
  - dblp_id: https://dblp.org/pid/143/9405
    emails: '****@bit.uni-bonn.de'
    first_name: Lucie
    google_scholar_id: https://scholar.google.com/citations?user=qZCZFp0AAAAJ
    homepage: https://caisa-lab.github.io
    institution: "Rheinische Friedrich-Wilhelms Universit\xE4t Bonn"
    last_name: Flek
    name: Lucie Flek
    orcid: https://orcid.org/0000-0002-5995-8454
    semantic_scholar_id: https://www.semanticscholar.org/author/Lucie-Flekova/2192277
    username: ~Lucie_Flek1
  - dblp_id: https://dblp.org/pid/160/1725
    emails: '****@uni-marburg.de'
    first_name: "J\xF6rg"
    google_scholar_id: https://scholar.google.com/citations?user=5A2TGRgAAAAJ
    institution: "Universit\xE4t Mannheim and Phillips-Universit\xE4t Marburg"
    last_name: "Schl\xF6tterer"
    name: "J\xF6rg Schl\xF6tterer"
    orcid: https://orcid.org/0000-0002-3678-0390
    semantic_scholar_id: "https://www.semanticscholar.org/author/J\xF6rg-Schl\xF6\
      tterer/3044872"
    username: "~J\xF6rg_Schl\xF6tterer1"
  - dblp_id: https://dblp.org/pid/132/1076
    emails: '****@umich.edu'
    first_name: Charles
    google_scholar_id: https://scholar.google.com/citations?user=yotA3JYAAAAJ
    homepage: http://cfwelch.com
    last_name: Welch
    name: Charles Welch
    semantic_scholar_id: https://www.semanticscholar.org/author/Charles-F-Welch/145645240
    username: ~Charles_Welch1
  decision: toMainConference
  end_page: 1130
  file: 109.pdf
  id: 109
  num_pages: 12
  openreview_id: lORCfuAEnh
  pdf_file: fe17483d4592127138e7de2c538da9ba5415905a.pdf
  start_page: 1119
  title: Corpus Considerations for Annotator Modeling and Scaling
- abstract: "Large language models are successful in answering factoid questions but\
    \ are also prone to hallucination.\nWe investigate the phenomenon of LLMs possessing\
    \ correct answer knowledge yet still hallucinating from the perspective of inference\
    \ dynamics, an area not previously covered in studies on hallucinations.\nWe are\
    \ able to conduct this analysis via two key ideas.\nFirst, we identify the factual\
    \ questions that query the same triplet knowledge but result in different answers.\
    \ The difference between the model behaviors on the correct and incorrect outputs\
    \ hence suggests the patterns when hallucinations happen.\nSecond, to measure\
    \ the pattern, we utilize mappings from the residual streams to vocabulary space.\n\
    We reveal the different dynamics of the output token probabilities along the depths\
    \ of layers between the correct and hallucinated cases. \nIn hallucinated cases,\
    \ the output token's information rarely demonstrates abrupt increases and consistent\
    \ superiority in the later stages of the model.\nLeveraging the dynamic curve\
    \ as a feature, we build a classifier capable of accurately detecting hallucinatory\
    \ predictions with an 88\\% success rate. \nOur study shed light on understanding\
    \ the reasons for LLMs' hallucinations on their known facts, and more importantly,\
    \ on accurately predicting when they are hallucinating."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - emails: '****@mails.tsinghua.edu.cn'
    first_name: Che
    google_scholar_id: https://scholar.google.com/citations?user=vJ8g_VAAAAAJ&hl=en&oi=sra
    last_name: Jiang
    name: Che Jiang
    username: ~Che_Jiang1
  - dblp_id: https://dblp.org/pid/233/4949.html
    emails: '****@gmail.com'
    first_name: Biqing
    institution: Tsinghua University and Harbin Institute of Technology
    last_name: Qi
    name: Biqing Qi
    semantic_scholar_id: https://www.semanticscholar.org/author/Biqing-Qi/66242399
    username: ~Biqing_Qi1
  - emails: '****@mails.tsinghua.edu.cn'
    first_name: Xiangyu
    homepage: https://lilhongxy.github.io/
    institution: Tsinghua University, Tsinghua University
    last_name: Hong
    name: Xiangyu Hong
    username: ~Xiangyu_Hong1
  - emails: '****@bupt.edu.cn'
    first_name: Dayuan
    last_name: Fu
    name: Dayuan Fu
    orcid: https://orcid.org/0000-0003-3614-6653
    username: ~Dayuan_Fu2
  - emails: '****@usc.edu'
    first_name: Yang
    last_name: Cheng
    name: Yang Cheng
    username: ~Yang_Cheng3
  - dblp_id: https://dblp.org/pid/117/4056
    emails: '****@tencent.com'
    first_name: Fandong
    google_scholar_id: https://scholar.google.com/citations?user=sA8U4S0AAAAJ&hl=en
    homepage: http://fandongmeng.github.io/
    institution: WeChat AI, Tencent Inc.
    last_name: Meng
    name: Fandong Meng
    semantic_scholar_id: https://www.semanticscholar.org/author/Fandong-Meng/33427918
    username: ~Fandong_Meng3
  - dblp_id: https://dblp.org/pid/32/7445.html
    emails: '****@gmail.com'
    first_name: Mo
    google_scholar_id: https://scholar.google.com/citations?user=vC8DssQAAAAJ&hl=en
    homepage: http://researcher.ibm.com/researcher/view.php?person=us-yum
    institution: WeChat AI, Tencent
    last_name: Yu
    name: Mo Yu
    semantic_scholar_id: https://www.semanticscholar.org/author/2482533
    username: ~Mo_Yu1
  - emails: '****@tsinghua.edu.cn'
    first_name: Bowen
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=h3Nsz6YAAAAJ
    homepage: http://web.ee.tsinghua.edu.cn/zhoubowen/zh_CN/index.htm?eqid=b894e49b0000ec7d0000000464857b51
    institution: Tsinghua University
    last_name: Zhou
    name: Bowen Zhou
    username: ~Bowen_Zhou8
  - dblp_id: https://dblp.org/pid/00/5012-16
    emails: '****@tencent.com'
    first_name: Jie
    last_name: Zhou
    name: Jie Zhou
    orcid: https://orcid.org/0000-0002-5899-5165
    semantic_scholar_id: https://www.semanticscholar.org/author/Jie-Zhou/49178343
    username: ~Jie_Zhou8
  decision: toMainConference
  end_page: 1143
  file: 114.pdf
  id: 114
  num_pages: 13
  openreview_id: 27NBD7NN0t
  pdf_file: 811dab79310ec053482c278b1cc52d3bd50e96a1.pdf
  start_page: 1131
  title: On Large Language Models' Hallucination with Regard to Known Facts
- abstract: Fairness-related assumptions about what constitute appropriate NLG system
    behaviors range from invariance, where systems are expected to behave identically
    for social groups, to adaptation, where behaviors should instead vary across them.
    To illuminate tensions around invariance and adaptation, we conduct five case
    studies, in which we perturb different types of identity-related language features
    (names, roles, locations, dialect, and style) in NLG system inputs. Through these
    cases studies, we examine people's expectations of system behaviors, and surface
    potential caveats of these contrasting yet commonly held assumptions. We find
    that motivations for adaptation include social norms, cultural differences, feature-specific
    information, and accommodation; in contrast, motivations for invariance include
    perspectives that favor prescriptivism, view adaptation as unnecessary or too
    difficult for NLG systems to do appropriately, and are wary of false assumptions.
    Our findings highlight open challenges around what constitute "fair" or "good"
    NLG system behaviors.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Ethics, Bias, and Fairness
  authors:
  - dblp_id: https://dblp.org/pid/200/8869
    emails: '****@berkeley.edu'
    first_name: Li
    google_scholar_id: https://scholar.google.com/citations?user=9IyAwc4AAAAJ&hl=en
    homepage: https://lucy3.github.io/
    institution: Allen Institute for Artificial Intelligence and University of California
      Berkeley
    last_name: Lucy
    name: Li Lucy
    semantic_scholar_id: https://www.semanticscholar.org/author/Li-Lucy/15983089
    username: ~Li_Lucy1
  - dblp_id: https://dblp.org/pid/182/2034
    emails: '****@microsoft.com'
    first_name: Su Lin
    google_scholar_id: https://scholar.google.com/citations?user=8jbAkOUAAAAJ&hl=en
    homepage: https://sblodgett.github.io/
    institution: Microsoft
    last_name: Blodgett
    name: Su Lin Blodgett
    semantic_scholar_id: https://www.semanticscholar.org/author/Su-Lin-Blodgett/3422038
    username: ~Su_Lin_Blodgett2
  - dblp_id: http://dblp.uni-trier.de/pers/hd/s/Shokouhi:Milad
    emails: '****@gmail.com'
    first_name: Milad
    google_scholar_id: https://scholar.google.com/citations?user=SbYANgwAAAAJ&hl=en
    homepage: ''
    institution: Microsoft
    last_name: Shokouhi
    name: Milad Shokouhi
    username: ~Milad_Shokouhi1
  - dblp_id: https://dblp.org/pid/84/3994
    emails: '****@dirichlet.net'
    first_name: Hanna
    homepage: http://dirichlet.net
    institution: Microsoft
    last_name: Wallach
    name: Hanna Wallach
    username: ~Hanna_Wallach1
  - dblp_id: https://dblp.org/pid/56/11270.html
    emails: '****@microsoft.com'
    first_name: Alexandra
    google_scholar_id: https://scholar.google.com/citations?user=8IjN_vgAAAAJ
    institution: Research, Microsoft
    last_name: Olteanu
    name: Alexandra Olteanu
    username: ~Alexandra_Olteanu1
  decision: toMainConference
  end_page: 1179
  file: 116.pdf
  id: 116
  num_pages: 36
  openreview_id: EVWmzN0xgX
  pdf_file: a21665760a4b94d3728eaab1f57854d12931aa83.pdf
  start_page: 1144
  title: '"One-Size-Fits-All"? Examining Expectations around What Constitute "Fair"
    or "Good" NLG System Behaviors'
- abstract: "Recent progress in natural language processing (NLP) owes much to remarkable\
    \ advances in large language models (LLMs). Nevertheless, LLMs frequently \u201C\
    hallucinate,\u201D resulting in non-factual outputs. Our carefully-designed human\
    \ evaluation substantiates the serious hallucination issue, revealing that even\
    \ GPT-3.5 produces factual outputs less than 25% of the time. This underscores\
    \ the importance of fact verifiers in order to measure and incentivize progress.\
    \ Our systematic investigation affirms that LLMs can be repurposed as effective\
    \ fact verifiers with strong correlations with human judgments. Surprisingly,\
    \ FLAN-T5-11B , the least factual generator in our study, performs the best as\
    \ a fact verifier, even outperforming more capable LLMs like GPT3.5 and ChatGPT.\
    \ Delving deeper, we analyze the reliance of these LLMs on high-quality evidence,\
    \ as well as their deficiencies in robustness and generalization ability. Our\
    \ study presents insights for developing trustworthy generation models."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - dblp_id: https://dblp.org/pid/58/2489
    emails: '****@mails.tsinghua.edu.cn'
    first_name: Jian
    google_scholar_id: https://scholar.google.com/citations?user=BWCDa8YAAAAJ&hl=zh-CN
    homepage: https://jianguanthu.github.io/
    last_name: Guan
    name: Jian Guan
    orcid: https://orcid.org/0000-0002-3597-0176
    semantic_scholar_id: https://www.semanticscholar.org/author/Jian-Guan/145902734
    username: ~Jian_Guan1
  - dblp_id: https://dblp.org/pid/49/11425
    emails: '****@gmail.com'
    first_name: Jesse
    google_scholar_id: https://scholar.google.com/citations?user=nHy_1doAAAAJ&hl=en&oi=ao
    homepage: http://www.cs.cmu.edu/~jessed/
    last_name: Dodge
    name: Jesse Dodge
    username: ~Jesse_Dodge1
  - dblp_id: https://dblp.org/pid/239/4346
    emails: '****@allenai.org'
    first_name: David
    google_scholar_id: https://scholar.google.com/citations?user=BeTUvHIAAAAJ&hl=en
    homepage: https://dwadden.github.io/
    institution: Allen Institute for Artificial Intelligence
    last_name: Wadden
    name: David Wadden
    semantic_scholar_id: https://www.semanticscholar.org/author/David-Wadden/30051202
    username: ~David_Wadden1
  - dblp_id: https://dblp.org/pid/47/6668.html
    emails: '****@tsinghua.edu.cn'
    first_name: Minlie
    google_scholar_id: https://scholar.google.com/citations?hl=zh-CN&user=P1jPSzMAAAAJ&view_op=list_works
    homepage: http://coai.cs.tsinghua.edu.cn/hml
    institution: Tsinghua University, Tsinghua University
    last_name: Huang
    name: Minlie Huang
    semantic_scholar_id: https://www.semanticscholar.org/author/Minlie-Huang/1730108
    username: ~Minlie_Huang1
  - emails: '****@illinois.edu'
    first_name: Hao
    google_scholar_id: https://scholar.google.com/citations?view_op=list_works&hl=en&user=sl4q0WkAAAAJ
    homepage: https://haopeng-nlp.github.io/
    institution: Department of Computer Science,  University of Illinois Urbana-Champaign
    last_name: Peng
    name: Hao Peng
    username: ~Hao_Peng4
  decision: toMainConference
  end_page: 1201
  file: 117.pdf
  id: 117
  num_pages: 22
  openreview_id: cSrzbYRjma
  pdf_file: e37e1dd4b364cb4d2c6c82a6f4ba79aef5c8a93c.pdf
  start_page: 1180
  title: Language Models Hallucinate, but May Excel at Fact Verification
- abstract: "Based on Pre-trained Language Models (PLMs), event coreference resolution\
    \ (ECR) systems have demonstrated outstanding performance in clustering coreferential\
    \ events across documents. However, the state-of-the-art system exhibits an excessive\
    \ reliance on the \u2018triggers lexical matching\u2019 spurious pattern in the\
    \ input mention pair text. We formalize the decision-making process of the baseline\
    \ ECR system using a Structural Causal Model (SCM), aiming to identify spurious\
    \ and causal associations (i.e., rationales) within the ECR task. Leveraging the\
    \ debiasing capability of counterfactual data augmentation, we develop a rationale-centric\
    \ counterfactual data augmentation method with LLM-in-the-loop. This method is\
    \ specialized for pairwise input in the ECR system, where we conduct direct interventions\
    \ on triggers and context to mitigate the spurious association while emphasizing\
    \ the causation. Our approach achieves state-of-the-art performance on three popular\
    \ cross-document ECR benchmarks and demonstrates robustness in out-of-domain scenarios."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Extraction
  authors:
  - emails: '****@westlake.edu.cn'
    first_name: Bowen
    institution: Westlake University
    last_name: Ding
    name: Bowen Ding
    username: ~Bowen_Ding1
  - dblp_id: https://dblp.org/pid/249/8137
    emails: '****@westlake.edu.cn'
    first_name: Qingkai
    google_scholar_id: https://scholar.google.com/citations?user=bsj7GpoAAAAJ
    last_name: Min
    name: Qingkai Min
    semantic_scholar_id: https://www.semanticscholar.org/author/Qingkai-Min/29447234
    username: ~Qingkai_Min1
  - emails: '****@bupt.edu.cn'
    first_name: Shengkun
    google_scholar_id: https://scholar.google.com.hk/citations?hl=zh-CN&user=reH5ELMAAAAJ
    institution: Beijing University of Posts and Telecommunications
    last_name: Ma
    name: Shengkun Ma
    username: ~Shengkun_Ma1
  - emails: '****@westlake.edu.cn'
    first_name: Yingjie
    google_scholar_id: https://scholar.google.com/citations?user=6OWRfPoAAAAJ&hl=en
    institution: Westlake University
    last_name: Li
    name: Yingjie Li
    orcid: https://orcid.org/0000-0003-4015-4576
    semantic_scholar_id: https://www.semanticscholar.org/author/Yingjie-Li/50820271
    username: ~Yingjie_Li2
  - dblp_id: https://dblp.org/pid/218/8007
    emails: '****@westlake.edu.cn'
    first_name: Linyi
    google_scholar_id: https://scholar.google.com/citations?user=go3sFxcAAAAJ&hl=en
    homepage: https://www.linyi-yang.me/
    institution: Westlake University
    last_name: Yang
    name: Linyi Yang
    username: ~Linyi_Yang1
  - dblp_id: https://dblp.org/pid/47/722-4.html
    emails: '****@westlake.edu.cn'
    first_name: Yue
    homepage: http://frcchang.github.io
    institution: Westlake University
    last_name: Zhang
    name: Yue Zhang
    orcid: https://orcid.org/0000-0002-5214-2268
    username: ~Yue_Zhang7
  decision: toMainConference
  end_page: 1230
  file: 121.pdf
  id: 121
  num_pages: 29
  openreview_id: JqGLJ8eFk8
  pdf_file: 04aa7d0e62763212846554b9aab4d3186942cea1.pdf
  start_page: 1202
  title: A Rationale-centric Counterfactual Data Augmentation Method for Cross-Document
    Event Coreference Resolution
- abstract: "Prompt tuning is one of the most effective solutions to adapting a fixed\
    \ pre-trained language model (PLM) for various downstream tasks, especially with\
    \ only a few input samples. However, the security issues, e.g., Trojan attacks,\
    \ of prompt tuning on a few data samples are not well-studied. \nTransferring\
    \ established data poisoning attacks directly to few-shot prompt tuning presents\
    \ multiple challenges. One significant issue is the _poisoned imbalance issue_,\
    \ where non-target class samples are added to the target class, resulting in a\
    \ greater number of target-class samples compared to non-target class. While this\
    \ issue is not critical in regular tuning, it significantly hampers the few-shot\
    \ prompt tuning, making it difficult to simultaneously achieve a high attack success\
    \ rate (ASR) and maintain clean data accuracy (CDA). Additionally, few-shot prompting\
    \ is prone to overfitting in terms of both ASR and CDA. In this paper, we introduce\
    \ _TrojFSP_, a method designed to address the challenges. To solve the poisoned\
    \ imbalance issue, we develop a _Target-Class Shrink (TC-Shrink)_ technique, which\
    \ aims to equalize the number of poisoning samples. To combat overfitting, we\
    \ employ a _Selective Token Poisoning_ technique to boost attack performance.\
    \ Furthermore, we introduce a _Trojan-Trigger Attention_ objective function to\
    \ amplify the attention of the poisoned trojan prompt on triggers. Experiments\
    \ show that our TrojFSP achieves an ASR of over 99\\% while maintaining negligible\
    \ decreases in CDA across various PLMs and datasets. The source code of TrojFSP\
    \ is available at _https://github.com/UCF-ML-Research/TrojFSP_."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Ethics, Bias, and Fairness
  authors:
  - emails: '****@iu.edu'
    first_name: Mengxin
    homepage: https://mxzheng.github.io/
    institution: University of Central Florida
    last_name: Zheng
    name: Mengxin Zheng
    username: ~Mengxin_Zheng1
  - dblp_id: https://dblp.org/pid/170/2507.html
    emails: '****@ucf.edu'
    first_name: Jiaqi
    google_scholar_id: https://scholar.google.com/citations?user=NI2jppcAAAAJ&hl=en&oi=sra
    homepage: https://jqxue1999.github.io
    last_name: Xue
    name: Jiaqi Xue
    username: ~Jiaqi_Xue1
  - dblp_id: https://dblp.org/pid/34/6795
    emails: '****@samsung.com'
    first_name: Xun
    google_scholar_id: https://scholar.google.com/citations?user=70mGgQoAAAAJ&hl=en
    institution: Samsung Research America
    last_name: Chen
    name: Xun Chen
    username: ~Xun_Chen1
  - dblp_id: https://dblp.org/pid/45/11295
    emails: '****@pitt.edu'
    first_name: Yanshan
    institution: University of Pittsburgh
    last_name: Wang
    name: Yanshan Wang
    username: ~Yanshan_Wang1
  - dblp_id: https://dblp.org/pid/207/3962.html
    emails: '****@ucf.edu'
    first_name: Qian
    google_scholar_id: https://scholar.google.com/citations?user=SBYgXLoAAAAJ&hl=en
    homepage: https://qlou.org
    institution: University of Central Florida
    last_name: Lou
    name: Qian Lou
    username: ~Qian_Lou1
  - dblp_id: https://dblp.org/pid/96/1994-1.html
    emails: '****@iu.edu'
    first_name: Lei
    google_scholar_id: https://scholar.google.com/citations?user=-1sXorAAAAAJ
    homepage: https://www.jianglei.org
    institution: Indiana University
    last_name: Jiang
    name: Lei Jiang
    username: ~Lei_Jiang1
  decision: toMainConference
  end_page: 1241
  file: 122.pdf
  id: 122
  num_pages: 11
  openreview_id: BbIiDzmlJ8
  pdf_file: 8013d169a9ebb384f6251e7e58c370dc09328f7e.pdf
  start_page: 1231
  title: 'TrojFSP: Trojan Insertion in Few-shot Prompt Tuning'
- abstract: "Large Language Models (LLMs) exhibit impressive capabilities but also\
    \ present risks such as biased content generation and privacy issues. One of the\
    \ current alignment techniques includes principle-driven integration, but it faces\
    \ challenges arising from the imprecision of manually crafted rules and inadequate\
    \ risk perception in models without safety training. To address these, we introduce\
    \ Guide-Align, a two-stage approach. \nInitially, a safety-trained model identifies\
    \ potential risks and formulates specific guidelines for various inputs, establishing\
    \ a comprehensive library of guidelines and a model for input-guidelines retrieval.\
    \ Subsequently, the retrieval model correlates new inputs with relevant guidelines,\
    \ which guide LLMs in response generation to ensure safe and high-quality outputs,\
    \ thereby aligning with human values. An additional optional stage involves fine-tuning\
    \ a model with well-aligned datasets generated through the process implemented\
    \ in the second stage.\nOur method customizes guidelines to accommodate diverse\
    \ inputs, thereby enhancing the fine-grainedness and comprehensiveness of the\
    \ guideline library. Furthermore, it incorporates safety expertise from a safety-trained\
    \ LLM through a lightweight retrieval model.\nWe evaluate our approach on three\
    \ benchmarks, demonstrating significant improvements in LLM security and quality.\
    \ Notably, our fine-tuned model, Labrador, even at 13 billion parameters, outperforms\
    \ GPT-3.5-turbo and surpasses GPT-4 in alignment capabilities."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Generation
  authors:
  - dblp_id: https://dblp.org/pid/56/4437
    emails: '****@stu.xmu.edu.cn'
    first_name: Yi
    homepage: https://github.com/YiLuo-roy
    last_name: Luo
    name: Yi Luo
    username: ~Yi_Luo7
  - dblp_id: https://dblp.uni-trier.de/pid/260/5513.html
    emails: '****@stu.xmu.edu.cn'
    first_name: Zhenghao
    last_name: Lin
    name: Zhenghao Lin
    username: ~Zhenghao_Lin1
  - emails: '****@qq.com'
    first_name: YuHao
    homepage: https://github.com/programingmonkey007/programingmonkey007.github.io
    last_name: Zhang
    name: YuHao Zhang
    username: ~YuHao_Zhang8
  - dblp_id: https://dblp.org/pid/336/2528
    emails: '****@gmail.com'
    first_name: Jiashuo
    homepage: https://github.com/gasolsun36
    last_name: Sun
    name: Jiashuo Sun
    username: ~Jiashuo_Sun1
  - dblp_id: https://dblp.org/pid/37/3102-1.html
    emails: '****@xmu.edu.cn'
    first_name: Chen
    google_scholar_id: https://scholar.google.com/citations?user=z1l2JSMAAAAJ&hl=en
    homepage: https://xmudm.github.io/publications/
    institution: Xiamen University
    last_name: Lin
    name: Chen Lin
    orcid: https://orcid.org/0000-0002-2275-997X
    username: ~Chen_Lin5
  - emails: '****@idea.edu.cn'
    first_name: Chengjin
    google_scholar_id: https://scholar.google.de/citations?user=sIts5VgAAAAJ&hl=en
    institution: International Digital Economy Academy
    last_name: Xu
    name: Chengjin Xu
    username: ~Chengjin_Xu1
  - dblp_id: https://dblp.org/pid/25/10184
    emails: '****@imu.edu.cn'
    first_name: Xiangdong
    homepage: http://ccs.imu.edu.cn/info/1153/3934.htm
    institution: Inner Mongolia University
    last_name: Su
    name: Xiangdong Su
    semantic_scholar_id: https://www.semanticscholar.org/author/Xiangdong-Su/80644791
    username: ~Xiangdong_Su2
  - emails: '****@gmail.com'
    first_name: Yelong
    google_scholar_id: https://scholar.google.com/citations?user=S6OFEFEAAAAJ&hl=en
    institution: Microsoft
    last_name: Shen
    name: yelong shen
    username: ~yelong_shen1
  - dblp_id: https://dblp.org/pid/96/2596-2
    emails: '****@idea.edu.cn'
    first_name: Jian
    homepage: https://idea.edu.cn/person/guojian/
    institution: Hong Kong University of Science and Technology
    last_name: Guo
    name: Jian Guo
    username: ~Jian_Guo2
  - emails: '****@microsoft.com'
    first_name: Yeyun
    google_scholar_id: https://scholar.google.com/citations?user=piUkwMYAAAAJ&hl=en
    last_name: Gong
    name: Yeyun Gong
    username: ~Yeyun_Gong2
  decision: toMainConference
  end_page: 1287
  file: 123.pdf
  id: 123
  num_pages: 46
  openreview_id: mWDfNlKzcW
  pdf_file: 7c4cc3dd0b70f9c5286888bad46671bf373ffd11.pdf
  start_page: 1242
  title: 'Ensuring Safe and High-Quality Outputs: A Guideline Library Approach for
    Language Models'
- abstract: Understanding when two pieces of text convey the same information is a
    goal touching many subproblems in NLP, including textual entailment and fact-checking.
    This problem becomes more complex when those two pieces of text are in different
    languages. Here, we introduce X-PARADE (Cross-lingual Paragraph-level Analysis
    of Divergences and Entailments), the first cross-lingual dataset of paragraph-level
    information divergences. Annotators label a paragraph in a target language at
    the span level and evaluate it with respect to a corresponding paragraph in a
    source language, indicating whether a given piece of information is the same,
    new, or new but can be inferred. This last notion establishes a link with cross-language
    NLI. Aligned paragraphs are sourced from Wikipedia pages in different languages,
    reflecting real information divergences observed in the wild. Armed with our dataset,
    we investigate a diverse set of approaches for this problem, including classic
    token alignment from machine translation, textual entailment methods that localize
    their decisions, and prompting LLMs. Our results show that these methods vary
    in their capability to handle inferable information, but they all fall short of
    human performance.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: 'Semantics: Sentence-level Semantics, Textual Inference and Other
      areas'
  authors:
  - dblp_id: https://dblp.org/pid/265/4547
    emails: '****@utexas.edu'
    first_name: Juan
    google_scholar_id: https://scholar.google.com/citations?user=IIwOwFUAAAAJ&hl=en&oi=sra
    homepage: https://www.juandiego-rodriguez.com/
    institution: University of Texas at Austin
    last_name: Rodriguez
    middle_name: Diego
    name: Juan Diego Rodriguez
    semantic_scholar_id: "https://www.semanticscholar.org/author/Juan-Diego-Rodr\xED\
      guez/40636184/"
    username: ~Juan_Diego_Rodriguez1
  - dblp_id: https://dblp.org/pid/23/556
    emails: '****@utexas.edu'
    first_name: Katrin
    google_scholar_id: https://scholar.google.com/citations?user=v7kFHRoAAAAJ&hl=en
    homepage: http://www.katrinerk.com/
    institution: University of Texas, Austin
    last_name: Erk
    name: Katrin Erk
    semantic_scholar_id: https://www.semanticscholar.org/author/Katrin-Erk/1708114
    username: ~Katrin_Erk1
  - dblp_id: https://dblp.org/pid/69/7968
    emails: '****@cs.utexas.edu'
    first_name: Greg
    google_scholar_id: https://scholar.google.com.tw/citations?user=EpQ_sDEAAAAJ
    homepage: http://www.cs.utexas.edu/~gdurrett/
    institution: University of Texas, Austin
    last_name: Durrett
    name: Greg Durrett
    semantic_scholar_id: https://www.semanticscholar.org/author/Greg-Durrett/1814094
    username: ~Greg_Durrett1
  decision: toMainConference
  end_page: 1312
  file: 125.pdf
  id: 125
  num_pages: 25
  openreview_id: 9I4AhGc7Wc
  pdf_file: 6b5ef526ae8f789fccb3b737f07d81b080dda17d.pdf
  start_page: 1288
  title: 'X-PARADE: Cross-Lingual Textual Entailment and Information Divergence across
    Paragraphs'
- abstract: 'Large language models (LLMs) are dramatically influencing AI research,
    spurring discussions on what has changed so far and how to shape the field''s
    future. To clarify such questions, we analyze a new dataset of 16,979 LLM-related
    arXiv papers, focusing on recent trends in 2023 vs. 2018-2022. First, we study
    disciplinary shifts: LLM research increasingly considers societal impacts, evidenced
    by 20$\times$ growth in LLM submissions to the Computers and Society sub-arXiv.
    An influx of new authors -- half of all first authors in 2023 -- are entering
    from non-NLP fields of CS, driving disciplinary expansion. Second, we study industry
    and academic publishing trends. Surprisingly, industry accounts for a smaller
    publication share in 2023, largely due to reduced output from Google and other
    Big Tech companies; universities in Asia are publishing more. Third, we study
    institutional collaboration: while industry-academic collaborations are common,
    they tend to focus on the same topics that industry focuses on rather than bridging
    differences. The most prolific institutions are all US- or China-based, but there
    is very little cross-country collaboration. We discuss implications around (1)
    how to support the influx of new authors, (2) how industry trends may affect academics,
    and (3) possible effects of (the lack of) collaboration.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Computational Social Science and Cultural Analytics
  authors:
  - dblp_id: https://dblp.org/pid/275/3732
    emails: '****@cornell.edu'
    first_name: Rajiv
    google_scholar_id: https://scholar.google.com/citations?user=OLbbUjcAAAAJ
    homepage: https://rajivmovva.com/
    institution: Cornell University
    last_name: Movva
    name: Rajiv Movva
    semantic_scholar_id: https://www.semanticscholar.org/author/1405369173
    username: ~Rajiv_Movva1
  - emails: '****@cs.cornell.edu'
    first_name: Sidhika
    homepage: https://sidhikabalachandar.github.io/
    institution: Department of Computer Science, Cornell University
    last_name: Balachandar
    name: Sidhika Balachandar
    orcid: https://orcid.org/0000-0001-6739-2027
    username: ~Sidhika_Balachandar1
  - emails: '****@cornell.edu'
    first_name: Kenny
    homepage: http://kennypeng.me
    institution: Cornell University
    last_name: Peng
    name: Kenny Peng
    username: ~Kenny_Peng1
  - emails: '****@cornell.edu'
    first_name: Gabriel
    homepage: https://gsagostini.github.io
    institution: Cornell University
    last_name: Agostini
    name: Gabriel Agostini
    username: ~Gabriel_Agostini1
  - dblp_id: https://dblp.org/pid/83/6058-1
    emails: '****@cornell.edu'
    first_name: Nikhil
    google_scholar_id: https://scholar.google.com/citations?user=8qSK3noAAAAJ&hl=en
    homepage: https://gargnikhil.com/
    institution: Cornell University
    last_name: Garg
    name: Nikhil Garg
    orcid: https://orcid.org/0000-0002-1988-792X
    username: ~Nikhil_Garg2
  - dblp_id: https://dblp.org/pid/159/0572
    emails: '****@cs.stanford.edu'
    first_name: Emma
    google_scholar_id: https://scholar.google.com/citations?user=xGORWi0AAAAJ&hl=en&oi=ao
    homepage: http://cs.stanford.edu/~emmap1
    institution: Cornell Tech
    last_name: Pierson
    name: Emma Pierson
    username: ~Emma_Pierson1
  decision: toMainConference
  end_page: 1333
  file: 126.pdf
  id: 126
  num_pages: 21
  openreview_id: pCiCKlCoA4
  pdf_file: a0f5bdfd18c7968be0e37a8707b80d974a7a9344.pdf
  start_page: 1313
  title: 'Topics, Authors, and Institutions in Large Language Model Research: Trends
    from 17K arXiv Papers'
- abstract: "Recent large language models (LLMs) have shown remarkable performance\
    \ in aligning generated text with user intentions across various tasks. \nWhen\
    \ it comes to long-form text generation, there has been a growing interest in\
    \ generation from a discourse coherence perspective.\nHowever, existing lexical\
    \ or semantic metrics such as BLEU, ROUGE, BertScore cannot effectively capture\
    \ the discourse coherence.\nThe development of discourse-specific automatic evaluation\
    \ methods for assessing the output of LLMs warrants greater focus and exploration.\
    \ \nIn this paper, we present a novel automatic metric designed to quantify the\
    \ discourse divergence between two long-form articles.\nExtensive experiments\
    \ on three datasets from representative domains demonstrate that our metric aligns\
    \ more closely with human preferences and GPT-4 coherence evaluation, outperforming\
    \ existing evaluation methods."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - dblp_id: https://dblp.org/pid/88/8538
    emails: '****@cam.ac.uk'
    first_name: Yinhong
    homepage: http://yinhongliu.com
    last_name: Liu
    name: Yinhong Liu
    semantic_scholar_id: https://www.semanticscholar.org/author/Yinhong-Liu/2108068935
    username: ~Yinhong_Liu1
  - dblp_id: https://dblp.org/pid/262/3282.html
    emails: '****@outlook.com'
    first_name: Yixuan
    google_scholar_id: https://scholar.google.com/citations?user=VuVuWEoAAAAJ&hl=zh-CN&authuser=2
    homepage: https://yxuansu.github.io/
    institution: Cohere
    last_name: Su
    name: Yixuan Su
    orcid: https://orcid.org/0000-0002-1472-7791
    semantic_scholar_id: https://www.semanticscholar.org/author/Yixuan-Su/50087162
    username: ~Yixuan_Su1
  - dblp_id: https://dblp.org/pid/09/7859
    emails: '****@monash.edu'
    first_name: Ehsan
    homepage: https://eehsan.github.io/
    institution: Monash University and University of Cambridge
    last_name: Shareghi
    name: Ehsan Shareghi
    semantic_scholar_id: https://www.semanticscholar.org/author/Ehsan-Shareghi/2888926
    username: ~Ehsan_Shareghi1
  - dblp_id: https://dblp.org/pid/90/2619
    emails: '****@cam.ac.uk'
    first_name: Nigel
    google_scholar_id: https://scholar.google.co.uk/citations?user=ZMelBa0AAAAJ&hl=en
    homepage: https://sites.google.com/site/nhcollier/
    institution: University of Cambridge
    last_name: Collier
    name: Nigel Collier
    orcid: https://orcid.org/0000-0002-7230-4164
    username: ~Nigel_Collier1
  decision: toMainConference
  end_page: 1342
  file: 127.pdf
  id: 127
  num_pages: 9
  openreview_id: uD7Fxduext
  pdf_file: b74c2c6216c5713147beb3f7568beaeb8f6464ea.pdf
  start_page: 1334
  title: 'Unlocking Structure Measuring: Introducing PDD, an Automatic Metric for
    Positional Discourse Coherence'
- abstract: Analyzing large hierarchical tables with multi-level headers presents
    challenges due to their complex structure, implicit semantics, and calculation
    relationships. While recent advancements in large language models (LLMs) have
    shown promise in flat table analysis, their application to hierarchical tables
    is constrained by the reliance on manually curated exemplars and the model's token
    capacity limitations. Addressing these challenges, we introduce a novel code-augmented
    LLM-based framework, $E^5$, for zero-shot hierarchical table question answering.
    This approach encompasses self-explaining the table's hierarchical structures,
    code generation to extract relevant information and apply operations, external
    code execution to prevent hallucinations, and leveraging LLMs' reasoning for final
    answer derivation. Empirical results indicate that our method, based on GPT-4,
    outperforms state-of-the-art fine-tuning methods with a 44.38 Exact Match improvement.
    Furthermore, we present $F^3$, an adaptive algorithm designed for token-limited
    scenarios, effectively condensing large tables while maintaining useful information.
    Our experiments prove its efficiency, enabling the processing of large tables
    even with models having limited context lengths. The code is available at https://github.com/zzh-SJTU/E5-Hierarchical-Table-Analysis.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Question Answering
  authors:
  - emails: '****@dartmouth.edu'
    first_name: Zhehao
    google_scholar_id: https://scholar.google.com/citations?user=QG-BAGwAAAAJ&hl=en
    homepage: https://zzh-sjtu.github.io/zhehaozhang.github.io/
    institution: Dartmouth College
    last_name: Zhang
    name: Zhehao Zhang
    username: ~Zhehao_Zhang1
  - emails: '****@microsoft.com'
    first_name: Yan
    google_scholar_id: https://scholar.google.com/citations?user=HaPgjdoAAAAJ&hl=zh-CN
    homepage: https://www.microsoft.com/en-us/research/people/gaoya/
    last_name: Gao
    name: Yan Gao
    username: ~Yan_Gao7
  - dblp_id: https://dblp.org/pid/37/1917
    emails: '****@microsoft.com'
    first_name: Jian-Guang
    google_scholar_id: https://scholar.google.com/citations?user=alDxINIAAAAJ&hl=en
    homepage: https://www.microsoft.com/en-us/research/people/jlou/
    institution: Microsoft
    last_name: Lou
    name: Jian-Guang Lou
    username: ~Jian-Guang_Lou1
  decision: toMainConference
  end_page: 1357
  file: 128.pdf
  id: 128
  num_pages: 15
  openreview_id: Kd6lwIlrqg
  pdf_file: 9ea83e93b5c349b54ee286166cb6eb94afa7388d.pdf
  start_page: 1343
  title: '$E^5$: Zero-shot Hierarchical Table Analysis using Augmented LLMs via Explain,
    Extract, Execute, Exhibit and Extrapolate'
- abstract: 'The rapid development of Large Language Models (LLMs) has led to great
    strides in model capabilities like long-context understanding and reasoning.

    However, as LLMs are able to process longer contexts, it becomes more challenging
    to evaluate whether they have acquired certain capabilities, since the length
    of text (e.g., 200K tokens) they can process far exceeds what humans can reliably
    assess in a reasonable duration.

    In this paper, we propose using complex synthetic tasks as a proxy evaluation
    method, and present S3Eval, a Synthetic, Scalable, Systematic evaluation suite
    for LLMs evaluation.

    The synthetic nature of S3Eval provides users full control over the dataset, allowing
    them to systematically probe LLM capabilities by scaling text length and varying
    task difficulty across diverse scenarios.

    The strong correlation between S3Eval and real-world benchmarks demonstrates the
    soundness of using S3Eval for evaluation of LLMs.

    S3Eval provides a flexible and infinite long-context data generation method. We
    have generated a comprehensive dataset called S3Eval-Standard, and experimental
    results have shown that it poses significant challenges for all existing LLMs.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - dblp_id: https://dblp.org/pid/329/5621
    emails: '****@qq.com'
    first_name: Fangyu
    google_scholar_id: https://scholar.google.com/citations?user=1WzAOSkAAAAJ&hl=zh-CN&oi=ao
    homepage: https://lfy79001.github.io
    last_name: Lei
    name: Fangyu Lei
    username: ~Fangyu_Lei1
  - dblp_id: https://dblp.org/pid/33/85
    emails: '****@sea.com'
    first_name: Qian
    google_scholar_id: https://scholar.google.com/citations?user=bcbeUo0AAAAJ&hl=en
    homepage: http://siviltaram.github.io/
    institution: Sea AI Lab
    last_name: Liu
    name: Qian Liu
    semantic_scholar_id: https://www.semanticscholar.org/author/Qian-Liu/1409707585
    username: ~Qian_Liu2
  - emails: '****@qq.com'
    first_name: Yiming
    homepage: https://github.com/yiyihum
    last_name: Huang
    name: Yiming Huang
    username: ~Yiming_Huang6
  - dblp_id: https://dblp.org/pid/136/8650
    emails: '****@nlpr.ia.ac.cn'
    first_name: Shizhu
    google_scholar_id: https://scholar.google.com/citations?user=zBPIt3QAAAAJ
    institution: Institute of automation, Chinese academy of science, Chinese Academy
      of Sciences
    last_name: He
    name: Shizhu He
    semantic_scholar_id: https://www.semanticscholar.org/author/Shizhu-He/1954845
    username: ~Shizhu_He2
  - dblp_id: https://dblp.uni-trier.de/pid/47/2026-1.html
    emails: '****@nlpr.ia.ac.cn'
    first_name: Jun
    google_scholar_id: https://scholar.google.com.hk/citations?user=HljRttwAAAAJ&hl=en
    homepage: http://nlpr-web.ia.ac.cn/cip/english/~junzhao/index.html
    last_name: Zhao
    name: Jun Zhao
    username: ~Jun_Zhao4
  - dblp_id: https://dblp.org/pers/hd/l/Liu_0001:Kang
    emails: '****@nlpr.ia.ac.cn'
    first_name: Kang
    google_scholar_id: https://scholar.google.com/citations?user=DtZCfl0AAAAJ&hl=en
    homepage: http://www.nlpr.ia.ac.cn/cip/~liukang/index.html
    institution: Institute of automation, Chinese academy of science, Chinese Academy
      of Sciences
    last_name: Liu
    name: Kang Liu
    username: ~Kang_Liu1
  decision: toMainConference
  end_page: 1385
  file: 130.pdf
  id: 130
  num_pages: 28
  openreview_id: APxjFplnUu
  pdf_file: 28233154b6749a5d4892dd925fe0fc8756ad79bd.pdf
  start_page: 1358
  title: 'S3Eval: A Synthetic, Scalable, Systematic Evaluation Suite for Large Language
    Model'
- abstract: 'With the rapid development of large language models (LLMs) and their
    integration into large multimodal models (LMMs), there has been

    impressive progress in zero-shot completion of user-oriented vision-language tasks.
    However, a gap remains in the domain of chart

    image understanding due to the distinct abstract components in charts. To address
    this, we introduce a large-scale MultiModal Chart

    Instruction (MMC-Instruction) dataset comprising 600k instances supporting diverse
    tasks and chart types. Leveraging this data, we de-

    velop MultiModal Chart Assistant (MMCA), an LMM that achieves state-of-the-art
    performance on existing chart QA benchmarks. Recognizing the need for a comprehensive
    evaluation of LMM chart understanding, we also propose a MultiModal Chart Benchmark
    (MMC-Benchmark), a comprehensive human-annotated benchmark with nine distinct
    tasks evaluating reasoning capabilities over charts.

    Extensive experiments on MMC-Benchmark reveal the limitations of existing LMMs
    on correctly interpreting charts, even for the most

    recent GPT-4V model. Our work provides an instruction-tuning methodology and benchmark
    to advance multimodal understanding of

    charts. Code and data are available at https://github.com/FuxiaoLiu/MMC.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multimodality and Language Grounding to Vision, Robotics and Beyond
  authors:
  - dblp_id: https://dblp.org/pid/276/0603
    emails: '****@umd.edu'
    first_name: Fuxiao
    google_scholar_id: https://scholar.google.ca/citations?user=e0P54E4AAAAJ&hl=en
    homepage: https://fuxiaoliu.github.io/
    last_name: Liu
    name: Fuxiao Liu
    semantic_scholar_id: https://www.semanticscholar.org/author/Fuxiao-Liu/52220309
    username: ~Fuxiao_Liu1
  - emails: '****@gmail.com'
    first_name: Xiaoyang
    google_scholar_id: https://scholar.google.com/citations?user=EeppWmkAAAAJ&hl
    institution: Tencent AI Lab
    last_name: Wang
    name: Xiaoyang Wang
    username: ~Xiaoyang_Wang1
  - dblp_id: https://dblp.org/pid/203/8711
    emails: '****@gmail.com'
    first_name: Wenlin
    google_scholar_id: https://scholar.google.com/citations?user=qwo2A24AAAAJ&hl=en
    homepage: https://wenlinyao.github.io/
    institution: Tencent AI Lab
    last_name: Yao
    name: Wenlin Yao
    username: ~Wenlin_Yao1
  - dblp_id: https://dblp.org/pid/11/3124
    emails: '****@gmail.com'
    first_name: Jianshu
    google_scholar_id: https://scholar.google.com/citations?user=jQeFWdoAAAAJ&hl=en
    homepage: https://chenjianshu.github.io/
    institution: Amazon
    last_name: Chen
    name: Jianshu Chen
    username: ~Jianshu_Chen1
  - emails: '****@gmail.com'
    first_name: Kaiqiang
    google_scholar_id: https://scholar.google.com/citations?user=PHoJwakAAAAJ&hl=en&oi=ao
    homepage: http://i2u.world/kqsong/
    institution: Tencent AI Lab
    last_name: Song
    name: Kaiqiang Song
    semantic_scholar_id: https://www.semanticscholar.org/author/Kaiqiang-Song/50982080
    username: ~Kaiqiang_Song2
  - dblp_id: https://dblp.org/pid/75/1848
    emails: '****@global.tencent.com'
    first_name: Sangwoo
    google_scholar_id: https://scholar.google.com/citations?user=T8mGzuoAAAAJ&hl=en
    homepage: https://sangwoo3.github.io
    institution: Tencent AI Lab
    last_name: Cho
    name: Sangwoo Cho
    orcid: https://orcid.org/0000-0002-4875-2565
    semantic_scholar_id: https://www.semanticscholar.org/author/Sangwoo-Cho/2173531
    username: ~Sangwoo_Cho1
  - dblp_id: https://dblp.org/pid/12/105.html
    emails: '****@umiacs.umd.edu'
    first_name: Yaser
    google_scholar_id: https://scholar.google.com/citations?user=h6t1HVEAAAAJ&hl=en
    homepage: http://users.umiacs.umd.edu/~yaser/
    institution: University of Maryland, College Park
    last_name: Yacoob
    name: Yaser Yacoob
    semantic_scholar_id: https://www.semanticscholar.org/author/Y.-Yacoob/1964574
    username: ~Yaser_Yacoob1
  - dblp_id: https://dblp.org/pid/71/4598-1
    emails: '****@ieee.org'
    first_name: Dong
    google_scholar_id: https://scholar.google.com/citations?user=tMY31_gAAAAJ
    homepage: https://sites.google.com/view/dongyu888/
    institution: Tencent AI Lab
    last_name: Yu
    name: Dong Yu
    semantic_scholar_id: https://www.semanticscholar.org/author/Dong-Yu/144580027
    username: ~Dong_Yu2
  decision: toMainConference
  end_page: 1409
  file: 131.pdf
  id: 131
  num_pages: 24
  openreview_id: JLoRCrs0GN
  pdf_file: 23ab8e018582c2f096fe58d31524c7b940ee3653.pdf
  start_page: 1386
  title: 'MMC: Advancing Multimodal Chart Understanding with Large-scale Instruction
    Tuning'
- abstract: "Modern neural language models (LMs) are powerful tools for modeling human\
    \ sentence production and comprehension, and their internal representations are\
    \ remarkably well-aligned with representations of language in the human brain.\
    \ But to achieve these results, LMs must be trained in distinctly un-human-like\
    \ ways \u2014 requiring orders of magnitude more language data than children receive\
    \ during development, and without perceptual or social context. Do models trained\
    \ more naturalistically \u2014 with grounded supervision \u2014 exhibit more humanlike\
    \ language learning? We investigate this question in the context of word learning,\
    \ a key sub-task in language acquisition. We train a diverse set of LM architectures,\
    \ with and without auxiliary visual supervision, on datasets of varying scales.\
    \ We then evaluate these models\u2019 learning of syntactic categories, lexical\
    \ relations, semantic features, word similarity, and alignment with human neural\
    \ representations. We find that visual supervision can indeed improve the efficiency\
    \ of word learning. However, these improvements are limited: they are present\
    \ almost exclusively in the low-data\nregime, and sometimes canceled out by the\
    \ inclusion of rich distributional signals from text. The information conveyed\
    \ by text and images is\nnot redundant\u2014models mainly driven by visual information\
    \ yield qualitatively different from those mainly driven by word co-occurrences.\
    \ However, our results suggest that current multimodal modeling approaches fail\
    \ to effectively leverage visual information to build human-like word representations\
    \ from human-scale data."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Linguistic theories, Cognitive Modeling and Psycholinguistics
  authors:
  - dblp_id: https://dblp.org/pid/154/6347
    emails: '****@mit.edu'
    first_name: Chengxu
    institution: Massachusetts Institute of Technology
    last_name: Zhuang
    name: Chengxu Zhuang
    username: ~Chengxu_Zhuang1
  - emails: '****@mit.edu'
    first_name: Evelina
    google_scholar_id: https://scholar.google.com/citations?user=1CgET20AAAAJ&hl=en&oi=ao
    homepage: http://evlab.mit.edu
    institution: Massachusetts Institute of Technology
    last_name: Fedorenko
    name: Evelina Fedorenko
    username: ~Evelina_Fedorenko1
  - dblp_id: https://dblp.org/pid/97/8154
    emails: '****@mit.edu'
    first_name: Jacob
    google_scholar_id: https://scholar.google.com/citations?user=dnZ8udEAAAAJ
    homepage: http://web.mit.edu/jda/www
    institution: Massachusetts Institute of Technology and Microsoft
    last_name: Andreas
    name: Jacob Andreas
    semantic_scholar_id: https://www.semanticscholar.org/author/Jacob-Andreas/2112400
    username: ~Jacob_Andreas1
  decision: toMainConference
  end_page: 1428
  file: 132.pdf
  id: 132
  num_pages: 19
  openreview_id: VnKeVItQx7
  pdf_file: feda8adb8b67093c77ab21e2060b5549fdba2d47.pdf
  start_page: 1410
  title: Visual Grounding Helps Learn Word Meanings in Low-Data Regimes
- abstract: "We propose utilizing n-best reranking to enhance Sequence-Level Knowledge\
    \ Distillation (Kim and Rush, 2016) where we extract pseudo-labels for student\
    \ model\u2019s training data from top n-best hypotheses and leverage a diverse\
    \ set of models with different inductive biases, objective functions or architectures,\
    \ including some publicly-available large language models, to pick the highest-quality\
    \ hypotheses as labels. The effectiveness of our proposal is validated through\
    \ experiments on the WMT\u201921 German \u2194 English and Chinese \u2194 English\
    \ translation tasks. Our results demonstrate that utilizing pseudo-labels generated\
    \ by our n-best reranker leads to a significantly more accurate student model.\
    \ In fact, our best student model achieves comparable accuracy to a large translation\
    \ model from (Tran et al., 2021) with 4.7 billion parameters, while having two\
    \ orders of magnitude fewer parameters."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Translation
  authors:
  - dblp_id: https://dblp.org/pid/22/5447
    emails: '****@gmail.com'
    first_name: Hendra
    last_name: Setiawan
    name: Hendra Setiawan
    username: ~Hendra_Setiawan1
  decision: toMainConference
  end_page: 1444
  file: 133.pdf
  id: 133
  num_pages: 16
  openreview_id: wJ0XR96lZd
  pdf_file: 7bb5b5d47236e09a56ab5e3603b8443b95fed1f4.pdf
  start_page: 1429
  title: Accurate Knowledge Distillation via n-best Reranking
- abstract: Recent advancements in large language models (LLMs) have shown promise
    in multi-step reasoning tasks, yet their reliance on extensive manual labeling
    to provide procedural feedback remains a significant impediment. To address this
    challenge, in this paper, we propose a novel self-supervised framework **AutoPRM**
    that efficiently enhances the fine-tuning of LLMs for intricate reasoning challenges.
    Specifically, **AutoPRM** first decomposes complex problems into more manageable
    subquestions with a controllable granularity switch, then sequentially apply reinforcement
    learning to iteratively improve the subquestion solver. Additionally, we propose
    context-guided decoding to avoid reward tampering and guide the subquestion solver
    towards the solution of the holistic problem. Extensive experiments show that
    **AutoPRM** significantly improves performance on mathematical and commonsense
    reasoning tasks over SOTA. More encouragingly, **AutoPRM** can be easily integrated
    with other orthogonal reasoning pipelines.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Question Answering
  authors:
  - emails: '****@purdue.edu'
    first_name: Zhaorun
    google_scholar_id: https://scholar.google.com/citations?user=UZg5N5UAAAAJ&hl=en
    homepage: https://billchan226.github.io/
    last_name: Chen
    name: Zhaorun Chen
    username: ~Zhaorun_Chen1
  - dblp_id: https://dblp.org/pid/348/5348
    emails: '****@uchicago.edu'
    first_name: Zhuokai
    google_scholar_id: https://scholar.google.com/citations?user=EGcdEjEAAAAJ&hl=en
    homepage: https://zhuokai-zhao.com/
    last_name: Zhao
    name: Zhuokai Zhao
    orcid: https://orcid.org/0000-0001-8201-2977
    semantic_scholar_id: https://www.semanticscholar.org/author/Zhuokai-Zhao/2238390520
    username: ~Zhuokai_Zhao1
  - dblp_id: https://dblp.org/pid/304/1173
    emails: '****@stu.pku.edu.cn'
    first_name: Zhihong
    homepage: https://github.com/Zhihong-Zhu
    last_name: Zhu
    name: Zhihong Zhu
    semantic_scholar_id: https://www.semanticscholar.org/author/Zhihong-Zhu/2190278563
    username: ~Zhihong_Zhu1
  - dblp_id: https://dblp.org/pid/144/2870
    emails: '****@berkeley.edu'
    first_name: Ruiqi
    google_scholar_id: https://scholar.google.com/citations?user=uErE2UUAAAAJ&hl=zh-CN
    homepage: https://rqzhangberkeley.github.io/
    last_name: Zhang
    name: Ruiqi Zhang
    username: ~Ruiqi_Zhang2
  - dblp_id: https://dblp.org/pid/40/1491
    emails: '****@gmail.com'
    first_name: Xiang
    google_scholar_id: https://scholar.google.com/citations?user=hGPBdf4AAAAJ&hl=en
    last_name: Li
    name: Xiang Li
    username: ~Xiang_Li35
  - dblp_id: https://dblp.org/pid/60/3996
    emails: '****@cs.cmu.edu'
    first_name: Bhiksha
    homepage: https://www.cs.cmu.edu/directory/bhikshar/
    institution: Carnegie Mellon University, Carnegie Mellon University and Mohamed
      bin Zayed University of Artificial Intelligence
    last_name: Raj
    name: Bhiksha Raj
    username: ~Bhiksha_Raj1
  - dblp_id: https://dblp.org/pid/197/1635
    emails: '****@cs.unc.edu'
    first_name: Huaxiu
    google_scholar_id: https://scholar.google.com/citations?user=A20BZnQAAAAJ&hl=en
    homepage: http://huaxiuyao.mystrikingly.com
    institution: Department of Computer Science, University of North Carolina at Chapel
      Hill
    last_name: Yao
    name: Huaxiu Yao
    semantic_scholar_id: https://www.semanticscholar.org/author/Huaxiu-Yao/18307037
    username: ~Huaxiu_Yao1
  decision: toMainConference
  end_page: 1461
  file: 134.pdf
  id: 134
  num_pages: 17
  openreview_id: jrzVslvWvg
  pdf_file: 1e933407bf9570dde5aabd569ebbc7efa849181c.pdf
  start_page: 1445
  title: 'AutoPRM: Automating Procedural Supervision for Multi-Step Reasoning via
    Controllable Question Decomposition'
- abstract: 'Recently proposed long-form question answering (QA) systems, supported
    by large language models (LLMs), have shown promising capabilities. Yet, attributing
    and verifying their generated abstractive answers can be difficult, and automatically
    evaluating their accuracy remains an ongoing challenge.

    In this work, we introduce a new QA task for answering multi-answer questions
    by summarizing multiple diverse sources in a semi-extractive fashion. Specifically,
    Semi-extractive Multi-source QA (SEMQA) requires models to output a comprehensive
    answer, while mixing factual quoted spans---copied verbatim from given input sources---and
    non-factual free-text connectors that glue these spans together into a single
    cohesive passage. This setting bridges the gap between the outputs of well-grounded
    but constrained extractive QA systems and more fluent but harder to attribute
    fully abstractive answers. Particularly, it enables a new mode for language models
    that leverages their advanced language generation capabilities, while also producing
    fine in-line attributions by-design that are easy to verify, interpret, and evaluate.
    To study this task, we create the first dataset of this kind, QuoteSum, with human-written
    semi-extractive answers to natural and generated questions, and define text-based
    evaluation metrics. Experimenting with several LLMs in various settings, we find
    this task to be surprisingly challenging, demonstrating the importance of QuoteSum
    for developing and studying such consolidation capabilities.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Question Answering
  authors:
  - dblp_id: https://dblp.org/pid/190/7491
    emails: '****@google.com'
    first_name: Tal
    google_scholar_id: https://scholar.google.com/citations?user=oo8QRmIAAAAJ
    homepage: https://people.csail.mit.edu/tals/
    institution: Google
    last_name: Schuster
    name: Tal Schuster
    semantic_scholar_id: https://www.semanticscholar.org/author/Tal-Schuster/32303439
    username: ~Tal_Schuster1
  - dblp_id: https://dblp.org/pid/147/5184
    emails: '****@google.com'
    first_name: Adam
    google_scholar_id: https://scholar.google.com/citations?user=PAAAaI4AAAAJ
    homepage: https://research.google/people/AdamLelkes/
    institution: Google
    last_name: Lelkes
    middle_name: D
    name: Adam D Lelkes
    username: ~Adam_D_Lelkes1
  - dblp_id: https://dblp.org/pid/185/6000
    emails: '****@gmail.com'
    first_name: Haitian
    google_scholar_id: https://scholar.google.com/citations?user=o7-PJu8AAAAJ&hl=en
    institution: School of Computer Science, Carnegie Mellon University
    last_name: Sun
    name: Haitian Sun
    username: ~Haitian_Sun2
  - dblp_id: https://dblp.org/pid/154/6787
    emails: '****@google.com'
    first_name: Jai
    institution: Google
    last_name: Gupta
    name: Jai Gupta
    username: ~Jai_Gupta1
  - dblp_id: https://dblp.org/pid/31/8178
    emails: '****@cs.tau.ac.il'
    first_name: Jonathan
    google_scholar_id: https://scholar.google.co.il/citations?user=xCYHonIAAAAJ&hl=en&oi=ao
    homepage: http://www.cs.tau.ac.il/~joberant/
    institution: Google and Tel Aviv University
    last_name: Berant
    name: Jonathan Berant
    semantic_scholar_id: https://www.semanticscholar.org/author/Jonathan-Berant/1750652
    username: ~Jonathan_Berant1
  - dblp_id: https://dblp.uni-trier.de/pers/hd/c/Cohen:William_W=
    emails: '****@google.com'
    first_name: William
    google_scholar_id: https://scholar.google.com/citations?user=8ys-38kAAAAJ&hl=en&oi=ao
    homepage: https://wwcohen.github.io/
    institution: Google DeepMind
    last_name: Cohen
    middle_name: W.
    name: William W. Cohen
    username: ~William_W._Cohen2
  - dblp_id: https://dblp.org/pid/95/2272
    emails: '****@google.com'
    first_name: Donald
    google_scholar_id: https://scholar.google.com/citations?user=bmXpOd8AAAAJ&hl=en
    homepage: https://research.google/people/DonaldMetzler/
    institution: Google
    last_name: Metzler
    name: Donald Metzler
    orcid: https://orcid.org/0000-0003-4276-6269
    semantic_scholar_id: https://www.semanticscholar.org/author/Donald-Metzler/1680617
    username: ~Donald_Metzler1
  decision: toMainConference
  end_page: 1480
  file: 135.pdf
  id: 135
  num_pages: 19
  openreview_id: TJVJ5uJ2ZD
  pdf_file: ba6a3cce88e8d96165e0fc7d7f48b6752aff18d3.pdf
  start_page: 1462
  title: 'SEMQA: Semi-Extractive Multi-Source Question Answering'
- abstract: 'Reinforcement learning from human feedback (RLHF) has emerged as an effective
    approach to aligning large language models (LLMs) to human preferences.

    RLHF contains three steps, i.e., human preference collecting, reward learning,
    and policy optimization, which are usually performed serially.

    Despite its popularity, however, (fixed) reward models may suffer from inaccurate
    off-distribution, since policy optimization continuously shifts LLMs'' data distribution.

    Repeatedly collecting new preference data from the latest LLMs may alleviate this
    issue, which unfortunately makes the resulting system more complicated and difficult
    to optimize.

    In this paper, we propose reward learning on policy (RLP), an unsupervised framework
    that refines a reward model using policy samples to keep it on-distribution.

    Specifically, an unsupervised multi-view learning method is introduced to learn
    robust representations of policy samples.

    Meanwhile, a synthetic preference generation approach is developed to simulate
    high-quality preference data with policy outputs.

    Extensive experiments on three benchmark datasets show that RLP consistently outperforms
    the state-of-the-art.

    Our code is available at \url{https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/rlp}.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Generation
  authors:
  - dblp_id: https://dblp.org/pid/71/6934.html
    emails: '****@ymail.com'
    first_name: Hao
    google_scholar_id: https://scholar.google.com/citations?user=0UGQL9QAAAAJ&hl=en
    homepage: https://github.com/langhaobeijing
    last_name: Lang
    name: Hao Lang
    username: ~Hao_Lang1
  - dblp_id: https://dblp.org/pid/h/FeiHuang.html
    emails: '****@gmail.com'
    first_name: Fei
    google_scholar_id: https://scholar.google.com/citations?user=9r98PpoAAAAJ
    homepage: https://sites.google.com/view/fei-huang
    institution: Alibaba Group
    last_name: Huang
    name: Fei Huang
    username: ~Fei_Huang1
  - dblp_id: https://dblp.org/pid/16/4349
    emails: '****@alibaba-inc.com'
    first_name: Yongbin
    google_scholar_id: https://scholar.google.com/citations?user=xF5VrokAAAAJ&hl=zh-CN
    homepage: https://yongbin-li.github.io/
    institution: Alibaba Group
    last_name: Li
    name: Yongbin Li
    semantic_scholar_id: https://www.semanticscholar.org/author/Yongbin-Li/1527090216
    username: ~Yongbin_Li2
  decision: toMainConference
  end_page: 1491
  file: 136.pdf
  id: 136
  num_pages: 11
  openreview_id: 1z8EsFxXqA
  pdf_file: 288e62dc4f0340fb291be49e00d36fbea41545f1.pdf
  start_page: 1481
  title: Fine-Tuning Language Models with Reward Learning on Policy
- abstract: We present a Universal Dependencies (UD) treebank for Highland Puebla
    Nahuatl. The treebank is only the second such UD corpus for a Mexican language,
    and supplements an existing treebank for another Nahuatl variant. We describe
    the process of data collection, annotation decisions and interesting syntactic
    constructions, and discuss some similarities and differences between the Highland
    Puebla Nahuatl treebank and the existing Western Sierra Puebla Nahuatl treebank.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: 'Special Theme: Languages of Latin America'
  authors:
  - emails: '****@gmail.com'
    first_name: Robert
    homepage: http://www.robertpugh.me
    last_name: Pugh
    name: Robert Pugh
    username: ~Robert_Pugh1
  - dblp_id: https://dblp.org/pid/84/8340
    emails: '****@prompsit.com'
    first_name: Francis
    google_scholar_id: https://scholar.google.com/citations?user=o5HSM6cAAAAJ&hl=ca&oi=ao
    homepage: http://xixona.dlsi.ua.es/~fran/
    institution: Indiana University, Bloomington
    last_name: Tyers
    middle_name: M.
    name: Francis M. Tyers
    orcid: https://orcid.org/0000-0001-6108-2220
    username: ~Francis_M._Tyers1
  decision: toMainConference
  end_page: 1502
  file: 140.pdf
  id: 140
  num_pages: 11
  openreview_id: Oamhp6FlZN
  pdf_file: d4fb8626cb2422f7af747c6cb49bfdaaf0a9b295.pdf
  start_page: 1492
  title: A Universal Dependencies Treebank for Highland Puebla Nahuatl
- abstract: "We present COPAL-ID, a novel, public Indonesian language common sense\
    \ reasoning dataset. Unlike the previous Indonesian COPA dataset (XCOPA-ID), COPAL-ID\
    \ incorporates Indonesian local and cultural nuances, and therefore, provides\
    \ a more natural portrayal of day-to-day causal reasoning within the Indonesian\
    \ cultural sphere. Professionally written by natives from scratch, COPAL-ID is\
    \ more fluent and free from awkward phrases, unlike the translated XCOPA-ID. In\
    \ addition, we present COPALID in both standard Indonesian and in Jakartan Indonesian\u2013\
    a dialect commonly used in daily conversation. COPAL-ID poses a greater challenge\
    \ for existing open-sourced and closed\nstate-of-the-art multilingual language\
    \ models, yet is trivially easy for humans. Our findings suggest that general\
    \ multilingual models struggle to perform well, achieving 66.91% accuracy on COPAL-ID.\
    \ South-East Asian-specific models achieve slightly better performance of 73.88%\
    \ accuracy. Yet, this number still falls short of near-perfect human performance.\
    \ This shows that these language models are still way behind in comprehending\
    \ the local nuances of Indonesian."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - emails: '****@gmail.com'
    first_name: Haryo
    google_scholar_id: https://scholar.google.com/citations?user=X5tK6xsAAAAJ
    institution: Mohamed bin Zayed University of Artificial Intelligence
    last_name: Wibowo
    middle_name: Akbarianto
    name: Haryo Akbarianto Wibowo
    username: ~Haryo_Akbarianto_Wibowo1
  - emails: '****@gmail.com'
    first_name: Erland
    last_name: Fuadi
    middle_name: Hilman
    name: Erland Hilman Fuadi
    username: ~Erland_Hilman_Fuadi1
  - emails: '****@gmail.com'
    first_name: Made
    homepage: https://scholar.google.com/citations?user=jvFdsfIAAAAJ
    last_name: Nityasya
    middle_name: Nindyatama
    name: Made Nindyatama Nityasya
    username: ~Made_Nindyatama_Nityasya1
  - dblp_id: https://dblp.org/pers/p/Prasojo:Radityo_Eko.html
    emails: '****@gmail.com'
    first_name: Radityo Eko
    google_scholar_id: https://scholar.google.com/citations?user=TmQLbmIAAAAJ&hl=en
    institution: Rukita
    last_name: Prasojo
    name: Radityo Eko Prasojo
    orcid: https://orcid.org/0000-0002-5148-7299
    semantic_scholar_id: https://www.semanticscholar.org/author/Radityo-Eko-Prasojo/2368148
    username: ~Radityo_Eko_Prasojo1
  - dblp_id: https://dblp.org/pid/188/8762
    emails: '****@gmail.com'
    first_name: Alham
    google_scholar_id: https://scholar.google.com/citations?user=0Cyfqv4AAAAJ&hl=en&oi=ao
    institution: Mohamed bin Zayed University of Artificial Intelligence and Amazon
    last_name: Aji
    middle_name: Fikri
    name: Alham Fikri Aji
    username: ~Alham_Fikri_Aji1
  decision: toMainConference
  end_page: 1521
  file: 141.pdf
  id: 141
  num_pages: 19
  openreview_id: oAqVw7Czb4
  pdf_file: c772a2ec636005a25d1c26d0d4d443c606044049.pdf
  start_page: 1503
  title: 'COPAL-ID: Indonesian Language Reasoning with Local Culture and Nuances'
- abstract: With the rapid development of large language models (LLMs), aligning LLMs
    with human values and societal norms to ensure their reliability and safety has
    become crucial. Reinforcement learning with human feedback (RLHF) and Constitutional
    AI (CAI) have been proposed for LLM alignment. However, these methods require
    either heavy human annotations or explicitly pre-defined constitutions, which
    are labor-intensive and resource-consuming. To overcome these drawbacks, we study
    constitution-based LLM alignment and propose a data-driven constitution discovery
    and self-alignment framework called IterAlign. IterAlign leverages red teaming
    to unveil the weaknesses of an LLM and automatically discovers new constitutions
    using a stronger LLM. These constitutions are then used to guide self-correction
    of the base LLM. Such a constitution discovery pipeline can be run iteratively
    and automatically to discover new constitutions that specifically target the alignment
    gaps in the current LLM. Empirical results on several safety benchmark datasets
    and multiple base LLMs show that IterAlign successfully improves truthfulness,
    helpfulness, harmlessness and honesty, improving the LLM alignment by up to $13.5\%$
    in harmlessness.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Ethics, Bias, and Fairness
  authors:
  - dblp_id: https://dblp.org/pid/210/1049
    emails: '****@cs.ucla.edu'
    first_name: Xiusi
    google_scholar_id: https://scholar.google.com/citations?user=JqGAil4AAAAJ
    homepage: https://xiusic.github.io/
    institution: University of California, Los Angeles
    last_name: Chen
    name: Xiusi Chen
    orcid: https://orcid.org/0000-0002-9713-8000
    semantic_scholar_id: https://www.semanticscholar.org/author/Xiusi-Chen/29963551
    username: ~Xiusi_Chen1
  - dblp_id: https://dblp.org/pid/179/0477
    emails: '****@msu.edu'
    first_name: Hongzhi
    homepage: https://www.cse.msu.edu/~wenhongz/
    last_name: Wen
    name: Hongzhi Wen
    orcid: https://orcid.org/0000-0003-0775-8538
    username: ~Hongzhi_Wen1
  - emails: '****@gmail.com'
    first_name: Sreyashi
    institution: Amazon
    last_name: Nag
    name: Sreyashi Nag
    username: ~Sreyashi_Nag1
  - dblp_id: https://dblp.org/pid/46/4719.html
    emails: '****@amazon.com'
    first_name: Chen
    google_scholar_id: https://scholar.google.com/citations?user=4EoNAFcAAAAJ&hl=zh-CN
    homepage: https://chen-luo.com/
    institution: Amazon
    last_name: Luo
    name: Chen Luo
    orcid: https://orcid.org/0000-0001-5339-5817
    username: ~Chen_Luo3
  - dblp_id: https://dblp.org/pid/179/2542
    emails: '****@gmail.com'
    first_name: Qingyu
    google_scholar_id: https://scholar.google.com/citations?user=P-mBKNYAAAAJ&hl=en
    institution: Amazon
    last_name: Yin
    name: Qingyu Yin
    username: ~Qingyu_Yin2
  - dblp_id: https://dblp.org/pid/12/8221-2
    emails: '****@amazon.com'
    first_name: Ruirui
    google_scholar_id: https://scholar.google.com/citations?user=gYCtd6cAAAAJ&hl=en
    homepage: https://ruiruili.mystrikingly.com/
    last_name: Li
    name: Ruirui Li
    username: ~Ruirui_Li3
  - dblp_id: https://dblp.org/pid/10/1143-18
    emails: '****@connect.ust.hk'
    first_name: Zheng
    google_scholar_id: https://scholar.google.com.hk/citations?user=P6fwn4AAAAAJ&hl=zh-CN
    homepage: https://hsqmlzno1.github.io/
    institution: Amazon
    last_name: Li
    name: Zheng Li
    username: ~Zheng_Li9
  - dblp_id: https://dblp.uni-trier.de/pid/w/WeiWang.html
    emails: '****@cs.ucla.edu'
    first_name: Wei
    google_scholar_id: https://scholar.google.com/citations?user=UedS9LQAAAAJ&hl=en
    homepage: http://www.cs.ucla.edu
    institution: University of California, Los Angeles
    last_name: Wang
    name: Wei Wang
    orcid: https://orcid.org/0000-0002-8180-2886
    username: ~Wei_Wang13
  decision: toMainConference
  end_page: 1532
  file: 142.pdf
  id: 142
  num_pages: 11
  openreview_id: hKxcqXiz97
  pdf_file: 78729b2a21e6fa6911e5ae96c5a3e197e92e47f0.pdf
  start_page: 1522
  title: 'IterAlign: Iterative Constitutional Alignment of Large Language Models'
- abstract: Large language models (LLMs) have revolutionized the landscape of Natural
    Language Processing, but are computationally expensive. To reduce the cost without
    sacrificing performance, previous studies have explored various approaches to
    harness the potential of Smaller Language Models (SLMs) as cost-effective alternatives
    to their larger counterparts. Driven by findings that SLMs and LLMs exhibit complementary
    strengths in a structured knowledge extraction task, this work presents a novel
    SLM/LLM routing framework designed to improve computational efficiency and enhance
    task performance. In dialogue state tracking tasks, the proposed routing framework
    enhances performance substantially compared to relying solely on LLMs, while reducing
    the computational costs by over 50\%.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Dialogue and Interactive Systems
  authors:
  - dblp_id: https://dblp.org/pid/170/4683-1
    emails: '****@gmail.com'
    first_name: Chia-Hsuan
    google_scholar_id: https://scholar.google.com/citations?user=Fw4h2iwAAAAJ&hl=en&oi=sra
    homepage: https://chia-hsuan-lee.github.io/
    last_name: Lee
    name: Chia-Hsuan Lee
    orcid: https://orcid.org/0000-0002-1933-5703
    semantic_scholar_id: https://www.semanticscholar.org/author/Chia-Hsuan-Lee/2115642672
    username: ~Chia-Hsuan_Lee1
  - dblp_id: https://dblp.org/pid/09/5158-2
    emails: '****@gmail.com'
    first_name: Hao
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=d9s3sbQAAAAJ&view_op=list_works&sortby=pubdate
    homepage: https://sites.google.com/site/hcheng2site/Home
    institution: Microsoft Research
    last_name: Cheng
    name: Hao Cheng
    orcid: https://orcid.org/0000-0001-7988-3149
    semantic_scholar_id: https://www.semanticscholar.org/author/Hao-Cheng/47413820
    username: ~Hao_Cheng4
  - dblp_id: https://dblp.org/pid/85/2189
    emails: '****@uw.edu'
    first_name: Mari
    google_scholar_id: https://scholar.google.com/citations?user=exS-GecAAAAJ&hl=en
    homepage: https://people.ece.uw.edu/ostendorf/
    institution: University of Washington
    last_name: Ostendorf
    name: Mari Ostendorf
    orcid: https://orcid.org/0000-0001-9385-9655
    semantic_scholar_id: https://www.semanticscholar.org/author/Mari-Ostendorf/144339506
    username: ~Mari_Ostendorf1
  decision: toMainConference
  end_page: 1544
  file: 143.pdf
  id: 143
  num_pages: 12
  openreview_id: CFMO1jaU6A
  pdf_file: bdd8741eec1be65c09041179c85c63cf581ab29e.pdf
  start_page: 1533
  title: 'OrchestraLLM: Efficient Orchestration of Language Models for Dialogue State
    Tracking'
- abstract: 'This paper investigates the possibility of approximating multiple mathematical
    operations in latent space for expression derivation. To this end, we introduce
    different multi-operational representation paradigms, modelling mathematical operations
    as explicit geometric transformations. By leveraging a symbolic engine, we construct
    a large-scale dataset comprising 1.7M derivation steps stemming from 61K premises
    and 6 operators, analysing the properties of each paradigm when instantiated with
    state-of-the-art neural encoders.

    Specifically, we investigate how different encoding mechanisms can approximate
    expression manipulation in latent space, exploring the trade-off between learning
    different operators and specialising within single operations, as well as the
    ability to support multi-step derivations and out-of-distribution generalisation.
    Our empirical analysis reveals that the multi-operational paradigm is crucial
    for disentangling different operators, while discriminating the conclusions for
    a single operation is achievable in the original expression encoder. Moreover,
    we show that architectural choices can heavily affect the training dynamics, structural
    organisation, and generalisation of the latent space, resulting in significant
    variations across paradigms and classes of encoders.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: 'Semantics: Sentence-level Semantics, Textual Inference and Other
      areas'
  authors:
  - dblp_id: https://dblp.org/pid/212/3533
    emails: '****@idiap.ch'
    first_name: Marco
    google_scholar_id: https://scholar.google.com/citations?user=nnaBYcIAAAAJ
    homepage: https://www.marcovalentino.net/
    last_name: Valentino
    name: Marco Valentino
    semantic_scholar_id: https://www.semanticscholar.org/author/Marco-Valentino/34102057
    username: ~Marco_Valentino1
  - emails: '****@hotmail.com'
    first_name: Jordan
    google_scholar_id: https://scholar.google.com/citations?hl=en&pli=1&user=RHcNhzMAAAAJ
    last_name: Meadows
    name: Jordan Meadows
    username: ~Jordan_Meadows1
  - emails: '****@gmail.com'
    first_name: Lan
    google_scholar_id: https://scholar.google.com/citations?&user=mlqD29MAAAAJ
    homepage: https://lanzhang128.github.io/
    institution: University of Manchester
    last_name: Zhang
    name: Lan Zhang
    orcid: https://orcid.org/0009-0007-4678-0419
    semantic_scholar_id: https://www.semanticscholar.org/author/Lan-Zhang/2109006173
    username: ~Lan_Zhang2
  - emails: '****@manchester.ac.uk'
    first_name: Andre
    homepage: http://andrefreitas.org
    institution: University of Manchester
    last_name: Freitas
    name: Andre Freitas
    username: ~Andre_Freitas1
  decision: toMainConference
  end_page: 1557
  file: 144.pdf
  id: 144
  num_pages: 13
  openreview_id: konePIXP1e
  pdf_file: 8f2e416afbe67e6af07a36c37cfff12a5e9d9c59.pdf
  start_page: 1545
  title: Multi-Operational Mathematical Derivations in Latent Space
- abstract: "Large Language Models (LLMs) are increasingly used for accessing information\
    \ on the web. Their truthfulness and factuality are thus of great interest. To\
    \ help users make the right decisions about the information they get, LLMs should\
    \ not only provide information but also help users fact-check it. We conduct human\
    \ experiments with 80 crowdworkers to compare language models with search engines\
    \ (information retrieval systems) at facilitating fact-checking. We prompt LLMs\
    \ to validate a given claim and provide corresponding explanations. Users reading\
    \ LLM explanations are significantly more efficient than those using search engines\
    \ while achieving similar accuracy. However, they over-rely on the LLMs when the\
    \ explanation is wrong. To reduce over-reliance on LLMs, we ask LLMs to provide\
    \ contrastive information\u2014explain both why the claim is true and false, and\
    \ then we present both sides of the explanation to users. This contrastive explanation\
    \ mitigates users\u2019 over-reliance on LLMs, but cannot significantly outperform\
    \ search engines. Further, showing both search engine results and LLM explanations\
    \ offers no complementary benefits compared to search engines alone. Taken together,\
    \ our study highlights that natural language explanations by LLMs may not be a\
    \ reliable replacement for reading the retrieved passages, especially in high-stakes\
    \ settings where over-relying on wrong AI explanations could lead to critical\
    \ consequences."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - dblp_id: https://dblp.org/pid/251/8778
    emails: '****@gmail.com'
    first_name: Chenglei
    google_scholar_id: https://scholar.google.com.sg/citations?user=CyKr1q8AAAAJ&hl=en
    homepage: https://noviscl.github.io/
    institution: Stanford University
    last_name: Si
    name: Chenglei Si
    semantic_scholar_id: https://www.semanticscholar.org/author/Chenglei-Si/152358188
    username: ~Chenglei_Si1
  - dblp_id: https://dblp.org/pid/277/1584
    emails: '****@umd.edu'
    first_name: Navita
    google_scholar_id: https://scholar.google.com/citations?user=YrvZ2E0AAAAJ&hl=en
    homepage: https://navitagoyal.github.io/
    institution: University of Maryland, College Park
    last_name: Goyal
    name: Navita Goyal
    username: ~Navita_Goyal1
  - dblp_id: https://dblp.org/pid/179/3791
    emails: '****@cs.cmu.edu'
    first_name: Tongshuang
    google_scholar_id: https://scholar.google.com/citations?user=CeQd_DsAAAAJ
    homepage: http://cs.cmu.edu/~sherryw
    institution: School of Computer Science, Carnegie Mellon University
    last_name: Wu
    name: Tongshuang Wu
    semantic_scholar_id: https://www.semanticscholar.org/author/Tongshuang-Sherry-Wu/35232494
    username: ~Tongshuang_Wu1
  - dblp_id: https://dblp.org/pid/81/3-9
    emails: '****@gmail.com'
    first_name: Chen
    google_scholar_id: https://scholar.google.com/citations?user=zehsvT8AAAAJ&hl=en
    homepage: http://umiacs.umd.edu/~chenz/
    last_name: Zhao
    name: Chen Zhao
    semantic_scholar_id: https://www.semanticscholar.org/author/Chen-Zhao/145756130
    username: ~Chen_Zhao2
  - dblp_id: https://dblp.org/pid/97/1374.html
    emails: '****@nyu.edu'
    first_name: Shi
    google_scholar_id: https://scholar.google.com/citations?user=d0npq2oAAAAJ&hl=en
    homepage: https://ihsgnef.github.io/
    last_name: Feng
    name: Shi Feng
    semantic_scholar_id: https://www.semanticscholar.org/author/144588144
    username: ~Shi_Feng1
  - dblp_id: https://dblp.org/pid/77/2856.html
    emails: '****@umiacs.umd.edu'
    first_name: Hal
    google_scholar_id: https://scholar.google.com/citations?user=PbEw81gAAAAJ&hl=en&oi=ao
    homepage: http://hal3.name
    institution: University of Maryland - College Park, University of Maryland, College
      Park and Microsoft
    last_name: "Daum\xE9 Iii"
    name: "Hal Daum\xE9 III"
    semantic_scholar_id: https://www.semanticscholar.org/author/Hal-Daum%C3%A9/1722360
    username: "~Hal_Daum\xE9_III1"
  - dblp_id: https://dblp.org/pid/57/5950
    emails: '****@umiacs.umd.edu'
    first_name: Jordan
    google_scholar_id: https://scholar.google.com/citations?user=BT4XTP4AAAAJ
    homepage: http://boydgraber.org
    institution: University of Maryland, College Park
    last_name: Boyd-Graber
    middle_name: Lee
    name: Jordan Lee Boyd-Graber
    orcid: https://orcid.org/0000-0002-7770-4431
    semantic_scholar_id: https://www.semanticscholar.org/author/Jordan-L.-Boyd-Graber/1389036863
    username: ~Jordan_Lee_Boyd-Graber1
  decision: toMainConference
  end_page: 1573
  file: 146.pdf
  id: 146
  num_pages: 16
  openreview_id: RoY0RAX9hi
  pdf_file: e97a5093d49697d31012b4d0426c7365fdb9b08e.pdf
  start_page: 1558
  title: "Large Language Models Help Humans Verify Truthfulness \u2013 Except When\
    \ They Are Convincingly Wrong"
- abstract: In this paper, we introduce a benchmark for evaluating the overall quality
    of emergent languages using data-driven methods. Specifically, we interpret the
    notion of the "quality" of an emergent language as its similarity to human language
    within a deep learning framework. We measure this by using the emergent language
    as pretraining data for a downstream NLP tasks in human language---the better
    the downstream performance, the better the emergent language. We implement this
    benchmark as an easy-to-use Python package that only requires a text file of utterances
    from the emergent language to be evaluated. Finally, we empirically test the benchmark's
    validity using human, synthetic, and emergent language baselines.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - dblp_id: https://dblp.org/pid/207/4805
    emails: '****@cs.cmu.edu'
    first_name: Brendon
    google_scholar_id: https://scholar.google.com/citations?user=QEXlK3AAAAAJ&hl=en
    homepage: http://brendonjboldt.xyz/
    institution: School of Computer Science, Carnegie Mellon University
    last_name: Boldt
    name: Brendon Boldt
    orcid: https://orcid.org/0000-0002-5599-5581
    username: ~Brendon_Boldt1
  - dblp_id: https://dblp.org/pid/180/5443
    emails: '****@cs.cmu.edu'
    first_name: David
    google_scholar_id: https://scholar.google.com/citations?authuser=1&user=2iS5aeoAAAAJ
    homepage: http://www.cs.cmu.edu/~dmortens/
    institution: Carnegie Mellon University
    last_name: Mortensen
    middle_name: R
    name: David R Mortensen
    semantic_scholar_id: https://www.semanticscholar.org/author/David-R.-Mortensen/3407646
    username: ~David_R_Mortensen1
  decision: toMainConference
  end_page: 1588
  file: 147.pdf
  id: 147
  num_pages: 15
  openreview_id: yBvTYAK2c7
  pdf_file: 5e473b592edd8a56756e7cbefd89ddaa2cff34c9.pdf
  start_page: 1574
  title: 'XferBench: a Data-Driven Benchmark for Emergent Language'
- abstract: 'Synthetic users are cost-effective proxies for real users in the evaluation
    of conversational recommender systems. Large language models show promise in simulating
    human-like behavior, raising the question of their ability to represent a diverse
    population of users. We introduce a new protocol to measure the degree to which
    language models can accurately emulate human behavior in conversational recommendation.
    This protocol is comprised of five tasks, each designed to evaluate a key property
    that a synthetic user should exhibit: choosing which items to talk about, expressing
    binary preferences, expressing open-ended preferences, requesting recommendations,
    and giving feedback. Through evaluation of baseline simulators, we demonstrate
    these tasks effectively reveal deviations of language models from human behavior,
    and offer insights on how to reduce the deviations with model selection and prompting
    strategies.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - emails: '****@gmail.com'
    first_name: Se-eun
    homepage: https://seeuny.com
    last_name: Yoon
    name: Se-eun Yoon
    username: ~Se-eun_Yoon1
  - dblp_id: https://dblp.org/pid/222/1220
    emails: '****@eng.ucsd.edu'
    first_name: Zhankui
    google_scholar_id: https://scholar.google.com/citations?user=EK5xD8IAAAAJ&hl=en
    homepage: https://aaronheee.github.io
    institution: University of California, San Diego, University of California, San
      Diego
    last_name: He
    name: Zhankui He
    username: ~Zhankui_He1
  - dblp_id: https://dblp.org/pid/227/5351.html
    emails: '****@ucsd.edu'
    first_name: Jessica
    institution: University of California, San Diego
    last_name: Echterhoff
    middle_name: Maria
    name: Jessica Maria Echterhoff
    username: ~Jessica_Maria_Echterhoff1
  - dblp_id: https://dblp.org/pid/29/3483
    emails: '****@cs.ucsd.edu'
    first_name: Julian
    google_scholar_id: https://scholar.google.com/citations?user=icbo4M0AAAAJ&hl=en
    homepage: http://cseweb.ucsd.edu/~jmcauley/
    institution: University of California, San Diego, University of California, San
      Diego
    last_name: McAuley
    name: Julian McAuley
    username: ~Julian_McAuley1
  decision: toMainConference
  end_page: 1603
  file: 150.pdf
  id: 150
  num_pages: 15
  openreview_id: 0cETSWCY9X
  pdf_file: bd5b15eb69598db474afc00394b0e337aa706b51.pdf
  start_page: 1589
  title: Evaluating Large Language Models as Generative User Simulators for Conversational
    Recommendation
- abstract: This paper proposes a methodology for generating and perturbing detailed
    derivations of equations at scale, aided by a symbolic engine, to evaluate the
    generalisability of Transformers to out-of-distribution mathematical reasoning
    problems. Instantiating the framework in the context of sequence classification
    tasks, we compare the capabilities of GPT-4, GPT-3.5, and a canon of fine-tuned
    BERT models, exploring the relationship between specific operators and generalisation
    failure via the perturbation of reasoning aspects such as symmetry and variable
    surface forms. Surprisingly, our empirical evaluation reveals that the average
    in-distribution performance of fine-tuned models surpasses GPT-3.5, and rivals
    GPT-4. However, perturbations to input reasoning can reduce their performance
    by up to 80 F1 points. Overall, the results suggest that the in-distribution performance
    of smaller open-source models may potentially rival GPT by incorporating appropriately
    structured derivation dependencies during training, and highlight a shared weakness
    between BERT and GPT involving a relative inability to decode indirect references
    to mathematical entities. We release the full codebase, constructed datasets,
    and fine-tuned models to encourage future progress in the field.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - emails: '****@hotmail.com'
    first_name: Jordan
    google_scholar_id: https://scholar.google.com/citations?hl=en&pli=1&user=RHcNhzMAAAAJ
    last_name: Meadows
    name: Jordan Meadows
    username: ~Jordan_Meadows1
  - dblp_id: https://dblp.org/pid/212/3533
    emails: '****@idiap.ch'
    first_name: Marco
    google_scholar_id: https://scholar.google.com/citations?user=nnaBYcIAAAAJ
    homepage: https://www.marcovalentino.net/
    last_name: Valentino
    name: Marco Valentino
    semantic_scholar_id: https://www.semanticscholar.org/author/Marco-Valentino/34102057
    username: ~Marco_Valentino1
  - dblp_id: https://dblp.org/pid/62/10068
    emails: '****@idiap.ch'
    first_name: Damien
    google_scholar_id: https://scholar.google.com.au/citations?user=iS_jP_3dpD8J
    homepage: https://www.damienteney.info
    institution: Idiap Research Institute
    last_name: Teney
    name: Damien Teney
    username: ~Damien_Teney1
  - emails: '****@manchester.ac.uk'
    first_name: Andre
    homepage: http://andrefreitas.org
    institution: University of Manchester
    last_name: Freitas
    name: Andre Freitas
    username: ~Andre_Freitas1
  decision: toMainConference
  end_page: 1622
  file: 151.pdf
  id: 151
  num_pages: 19
  openreview_id: XU9fQxDYyX
  pdf_file: fd1cf2dc6bb4fd30e0bfc94e4411eceb7e6fedfe.pdf
  start_page: 1604
  title: A Symbolic Framework for Evaluating Mathematical Reasoning and Generalisation
    with Transformers
- abstract: Transformer language models (LMs) have been shown to represent concepts
    as directions in the latent space of hidden activations. However, for any human-interpretable
    concept, how can we find its direction in the latent space? We present a technique
    called linear relational concepts (LRC) for finding concept directions corresponding
    to human-interpretable concepts by first modeling the relation between subject
    and object as a linear relational embedding (LRE). We find that inverting the
    LRE and using earlier object layers results in a powerful technique for finding
    concept directions that outperforms standard black-box probing classifiers. We
    evaluate LRCs on their performance as concept classifiers as well as their ability
    to causally change model output.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - emails: '****@ucl.ac.uk'
    first_name: David
    google_scholar_id: https://scholar.google.co.uk/citations?user=xnGP_IcAAAAJ
    institution: University College London, University of London
    last_name: Chanin
    name: David Chanin
    username: ~David_Chanin1
  - dblp_id: https://dblp.org/pid/23/4530.html
    emails: '****@ucl.ac.uk'
    first_name: Anthony
    google_scholar_id: https://scholar.google.com.tw/citations?user=N7ETYzcAAAAJ
    homepage: http://www0.cs.ucl.ac.uk/staff/a.hunter/
    last_name: Hunter
    name: Anthony Hunter
    orcid: https://orcid.org/0000-0001-5602-7446
    username: ~Anthony_Hunter1
  - emails: '****@gmail.com'
    first_name: Oana-Maria
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=yIg7b2gAAAAJ&scilu=&scisig=AMD79ooAAAAAXU8WpSFFaircqxhFSwdG4P4zZqnzvq8F&gmla=AJsN-F7gyatcPPnxZUwg8lUiYJ6YNs_fAzCTBPl0FYDilQlKzgAFRmOf6oRORkqaYr5KnUeFjhor-hpwxv-8H2IDiq0Nvaz5egNzaC5Ig3ig_zyF6BEWMnY&sciund=6548680421784953114
    homepage: https://www.cs.ox.ac.uk/people/oana-maria.camburu/
    institution: Department of Computer Science, University College London, University
      of London
    last_name: Camburu
    name: Oana-Maria Camburu
    username: ~Oana-Maria_Camburu2
  decision: toMainConference
  end_page: 1634
  file: 156.pdf
  id: 156
  num_pages: 12
  openreview_id: tYa1hkRvz9
  pdf_file: 459cdf869c6409a32a3ffb5d7db8ba8c92993bde.pdf
  start_page: 1623
  title: Identifying Linear Relational Concepts in Large Language Models
- abstract: "In this paper we present an exploratory research on quantifying the impact\
    \ that data distribution has on the performance and evaluation of NLP models.\
    \ We propose an automated framework that measures the data point distribution\
    \ across 6 different dimensions: ambiguity, difficulty, discriminability, length,\
    \ noise, and perplexity.\n\nWe use disproportional stratified sampling to measure\
    \ how much the data distribution affects absolute (Acc/F1) and relative (Rank)\
    \ model performance. \nWe experiment on 2 different datasets (SQUAD and MNLI)\
    \ and test a total of 135 different models (125 on SQUAD and 10 on MNLI). \nWe\
    \ demonstrate that without explicit control of the data distribution, standard\
    \ evaluation frameworks are inconsistent and unreliable. We find that the impact\
    \ of the data is statistically significant and is often larger than the impact\
    \ of changing the metric. \n\nIn a second set of experiments, we demonstrate that\
    \ the impact of data on evaluation is not just observable, but also predictable.\
    \ We propose to use benchmark transparency as a method for comparing datasets\
    \ and quantifying the similarity between them. We find that the ``dataset similarity\
    \ vector'' can be used to predict how well a model generalizes out of distribution."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - dblp_id: https://dblp.org/pid/185/4325
    emails: '****@bham.ac.uk'
    first_name: Venelin
    google_scholar_id: https://scholar.google.es/citations?user=aJt3MMYAAAAJ&hl=en
    homepage: http://vkovatchev.com
    institution: University of Birmingham
    last_name: Kovatchev
    name: Venelin Kovatchev
    semantic_scholar_id: https://www.semanticscholar.org/author/Venelin-Kovatchev/3455255
    username: ~Venelin_Kovatchev1
  - dblp_id: https://dblp.uni-trier.de/pid/29/239
    emails: '****@utexas.edu'
    first_name: Matthew
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=qgmiQ5IAAAAJ
    homepage: https://www.ischool.utexas.edu/~ml/
    institution: University of Texas at Austin, Amazon and University of Texas at
      Austin
    last_name: Lease
    name: Matthew Lease
    orcid: https://orcid.org/0000-0002-0056-2834
    semantic_scholar_id: https://www.semanticscholar.org/author/Matthew-Lease/1747771
    username: ~Matthew_Lease1
  decision: toMainConference
  end_page: 1650
  file: 158.pdf
  id: 158
  num_pages: 16
  openreview_id: RyifLOUPkS
  pdf_file: 2527dc646ebaa0495b0b16c3f5c53add3dc78f55.pdf
  start_page: 1635
  title: 'Benchmark Transparency: Measuring the Impact of Data on Evaluation'
- abstract: "The permanence of online content combined with the enhanced authorship\
    \ identification techniques calls for stronger computational methods to protect\
    \ the identity and privacy of online authorship when needed, e.g., blind reviews\
    \ for scientific papers, anonymous online reviews, or anonymous interactions in\
    \ the mental health forums. In this paper, we propose an unsupervised inference-time\
    \ approach to authorship obfuscation to address the unique challenges of authorship\
    \ obfuscation: lack of supervision data for diverse authorship and domains, and\
    \ the need for a sufficient level of revision beyond simple paraphrasing to obfuscate\
    \ the authorship, all the while preserving the original content and fluency.\n\
    \nWe introduce JAMDEC, a user-controlled, inference-time algorithm for authorship\
    \ obfuscation that can be in principle applied to any text and authorship. Our\
    \ approach builds on small language models such as GPT2-XL in order to help avoid\
    \ disclosing the original content to proprietary LLM\u2019s APIs, while also reducing\
    \ the performance gap between small and large language models via algorithmic\
    \ enhancement. The key idea behind our approach is to boost the creative power\
    \ of smaller language models through constrained decoding, while also allowing\
    \ for user-specified controls and flexibility. Experimental results demonstrate\
    \ that our approach based on GPT2-XL outperforms previous state-of-the-art methods\
    \ based on comparably small models, while performing competitively against GPT3.5\
    \ 175B, a propriety model that is two orders of magnitudes larger."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Generation
  authors:
  - dblp_id: https://dblp.org/pid/336/3238
    emails: '****@uw.edu'
    first_name: Jillian
    google_scholar_id: https://scholar.google.com/citations?user=Gnk0E_QAAAAJ&hl=en
    homepage: https://jillianfisher.owlstown.net
    institution: University of Washington
    last_name: Fisher
    name: Jillian Fisher
    semantic_scholar_id: https://www.semanticscholar.org/author/Jillian-R.-Fisher/33772445
    username: ~Jillian_Fisher1
  - dblp_id: https://dblp.org/pid/24/10879
    emails: '****@cs.washington.edu'
    first_name: Ximing
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=Hb6WyyoAAAAJ
    institution: Department of Computer Science, University of Washington
    last_name: Lu
    name: Ximing Lu
    username: ~Ximing_Lu1
  - dblp_id: https://dblp.org/pid/192/7707
    emails: '****@cs.washington.edu'
    first_name: Jaehun
    google_scholar_id: https://scholar.google.com/citations?user=_bXzUGEAAAAJ&hl=en
    homepage: https://jaehunjung.com
    institution: University of Washington
    last_name: Jung
    name: Jaehun Jung
    orcid: https://orcid.org/0000-0002-0292-3074
    username: ~Jaehun_Jung1
  - emails: '****@cs.washington.edu'
    first_name: Liwei
    google_scholar_id: https://scholar.google.com/citations?user=lcPsDgUAAAAJ&hl=en
    homepage: https://liweijiang.me
    last_name: Jiang
    name: Liwei Jiang
    username: ~Liwei_Jiang2
  - dblp_id: http://dblp.uni-trier.de/pers/hd/h/Harchaoui:Za=iuml=d
    emails: '****@uw.edu'
    first_name: Zaid
    google_scholar_id: https://scholar.google.fr/citations?user=yCyR-TsAAAAJ&hl=en
    homepage: http://faculty.washington.edu/zaid/index.html
    last_name: Harchaoui
    name: Zaid Harchaoui
    semantic_scholar_id: https://www.semanticscholar.org/author/Za%C3%AFd-Harchaoui/1753355
    username: ~Zaid_Harchaoui1
  - dblp_id: https://dblp.org/pid/89/579
    emails: '****@cs.washington.edu'
    first_name: Yejin
    google_scholar_id: https://scholar.google.com/citations?user=vhP-tlcAAAAJ&hl=en
    homepage: https://homes.cs.washington.edu/~yejin/
    institution: Department of Computer Science, University of Washington
    last_name: Choi
    name: Yejin Choi
    semantic_scholar_id: https://www.semanticscholar.org/author/Yejin-Choi/1699545?sort=year
    username: ~Yejin_Choi1
  decision: toMainConference
  end_page: 1680
  file: 159.pdf
  id: 159
  num_pages: 30
  openreview_id: fxNHeSENVv
  pdf_file: b4c91d1bdf43263cc9bb310218a26add56c76576.pdf
  start_page: 1651
  title: 'JAMDEC: Unsupervised Authorship Obfuscation using Constrained Decoding over
    Small Language Models'
- abstract: Human writers often *bookend* their writing with ending sentences that
    relate back to the beginning sentences in order to compose a satisfying narrative
    that "closes the loop." Motivated by this observation, we propose RENarGen, a
    controllable story-generation paradigm that generates narratives by ensuring the
    first and last sentences are related and then infilling the middle sentences.
    Our contributions include an initial exploration of how various methods of bookending
    from Narratology affect language modeling for stories. Automatic and human evaluations
    indicate RENarGen produces better stories with more narrative closure than current
    autoregressive models.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Discourse and Pragmatics
  authors:
  - emails: '****@cs.unc.edu'
    first_name: Anneliese
    homepage: https://adbrei.com/
    institution: University of North Carolina at Chapel Hill
    last_name: Brei
    name: Anneliese Brei
    username: ~Anneliese_Brei1
  - dblp_id: https://dblp.org/pid/53/8131.html
    emails: '****@gmail.com'
    first_name: Chao
    homepage: https://zhaochaocs.github.io/
    institution: Bloomberg
    last_name: Zhao
    name: Chao Zhao
    username: ~Chao_Zhao3
  - dblp_id: https://dblp.org/pid/77/8700
    emails: '****@cs.unc.edu'
    first_name: Snigdha
    google_scholar_id: https://scholar.google.com/citations?user=gZD3EesAAAAJ&hl=en
    homepage: https://sites.google.com/site/snigdhac/
    institution: Department of Computer Science, University of North Carolina, Chapel
      Hill
    last_name: Chaturvedi
    name: Snigdha Chaturvedi
    semantic_scholar_id: https://www.semanticscholar.org/author/Snigdha-Chaturvedi/37202877
    username: ~Snigdha_Chaturvedi2
  decision: toMainConference
  end_page: 1692
  file: 160.pdf
  id: 160
  num_pages: 12
  openreview_id: pfChGTyp65
  pdf_file: 77407f7fd6d3ee681135559d690f8c72a956290e.pdf
  start_page: 1681
  title: 'Returning to the Start: Generating Narratives with Related Endpoints'
- abstract: We introduce Retrieval-Based Speculative Decoding (REST), a novel algorithm
    designed to speed up language model generation. The key insight driving the development
    of REST is the observation that the process of text generation often includes
    certain common phases and patterns. Unlike previous methods that rely on a draft
    language model for speculative decoding, REST harnesses the power of retrieval
    to generate draft tokens. This method draws from the reservoir of existing knowledge,
    retrieving and employing relevant tokens based on the current context. Its plug-and-play
    nature allows for seamless integration and acceleration of any language model,
    all without necessitating additional training. When benchmarked on 7B and 13B
    language models in a single-batch setting, REST achieves a significant speedup
    of $1.62 \times$ to $2.36 \times$ on code or text generation. The source code
    of REST is available at https://github.com/FasterDecoding/REST.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Efficient/Low-Resource Methods for NLP
  authors:
  - dblp_id: https://dblp.org/pid/355/4626
    emails: '****@outlook.com'
    first_name: Zhenyu
    google_scholar_id: https://scholar.google.co.jp/citations?user=bKwkUO4AAAAJ&hl=zh-CN&oi=sra
    homepage: https://zhenyuhe00.github.io/
    last_name: He
    name: Zhenyu He
    semantic_scholar_id: https://www.semanticscholar.org/author/Zhenyu-He/2152990262
    username: ~Zhenyu_He3
  - dblp_id: https://dblp.org/pid/218/7257
    emails: '****@princeton.edu'
    first_name: Zexuan
    homepage: https://www.cs.princeton.edu/~zzhong/
    last_name: Zhong
    name: Zexuan Zhong
    username: ~Zexuan_Zhong1
  - dblp_id: https://dblp.org/pid/241/9458
    emails: '****@princeton.edu'
    first_name: Tianle
    google_scholar_id: https://scholar.google.com/citations?user=CvwLRSMAAAAJ&hl=zh-CN
    homepage: https://tianle.website
    last_name: Cai
    name: Tianle Cai
    username: ~Tianle_Cai1
  - dblp_id: https://dblp.org/pid/88/3262
    emails: '****@gmail.com'
    first_name: Jason
    google_scholar_id: https://scholar.google.com/citations?user=GR_DsT0AAAAJ&hl=en
    homepage: https://jasondlee88.github.io/
    institution: Princeton University
    last_name: Lee
    middle_name: D.
    name: Jason D. Lee
    username: ~Jason_D._Lee1
  - dblp_id: https://dblp.org/pid/74/184
    emails: '****@pku.edu.cn'
    first_name: Di
    google_scholar_id: https://scholar.google.co.jp/citations?user=orVoz4IAAAAJ
    homepage: https://dihe-pku.github.io/
    institution: Peking University and Microsoft
    last_name: He
    name: Di He
    username: ~Di_He1
  decision: toMainConference
  end_page: 1706
  file: 161.pdf
  id: 161
  num_pages: 14
  openreview_id: 0CKxJZZPfm
  pdf_file: 1dd4f173f79abbeebd2fdeab36aab6dc7f04bb8f.pdf
  start_page: 1693
  title: 'REST: Retrieval-Based Speculative Decoding'
- abstract: We introduce sub-sentence encoder, a contrastively-learned contextual
    embedding model for fine-grained semantic representation of text. In contrast
    to the standard practice with sentence embeddings, where the meaning of an entire
    sequence of text is encoded into a fixed-length vector, the sub-sentence encoder
    learns to produce distinct contextual embeddings corresponding to different atomic
    propositions, i.e. atomic units of meaning expressed within a text sequence. The
    sub-sentence embeddings are contrastively learned to recognize (inferred) semantic
    equivalence between propositions across different text sequences. Our experiments
    show the effectiveness of sub-sentence encoders in applications, such as retrieving
    supporting facts for fine-grained text attribution or recognizing the conditional
    semantic similarity between texts. In practice, we demonstrate that sub-sentence
    encoders keep the same level of inference cost and space complexity compared to
    sentence encoders.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: 'Semantics: Sentence-level Semantics, Textual Inference and Other
      areas'
  authors:
  - dblp_id: https://dblp.org/pid/153/0087
    emails: '****@cis.upenn.edu'
    first_name: Sihao
    google_scholar_id: https://scholar.google.com/citations?user=PQ9dRCgAAAAJ
    homepage: https://www.seas.upenn.edu/~sihaoc/
    last_name: Chen
    name: Sihao Chen
    semantic_scholar_id: https://www.semanticscholar.org/author/Sihao-Chen/2087205
    username: ~Sihao_Chen1
  - dblp_id: https://dblp.org/pid/48/859.html
    emails: '****@cse.ust.hk'
    first_name: Hongming
    google_scholar_id: https://scholar.google.com/citations?user=i5ETuuQAAAAJ&hl=en
    homepage: http://www.cse.ust.hk/~hzhangal/
    last_name: Zhang
    name: Hongming Zhang
    username: ~Hongming_Zhang2
  - emails: '****@cs.washington.edu'
    first_name: Tong
    google_scholar_id: https://scholar.google.com/citations?user=fOcXofAAAAAJ&hl=en
    last_name: Chen
    name: Tong Chen
    username: ~Tong_Chen3
  - dblp_id: https://dblp.org/pid/219/5276
    emails: '****@seas.upenn.edu'
    first_name: Ben
    google_scholar_id: https://scholar.google.com/citations?user=0Cb4mtIAAAAJ
    homepage: http://xuanyu.me
    institution: University of Pennsylvania
    last_name: Zhou
    name: Ben Zhou
    semantic_scholar_id: https://www.semanticscholar.org/author/Ben-Zhou/145360756
    username: ~Ben_Zhou1
  - dblp_id: https://dblp.org/pid/159/8117-2.html
    emails: '****@global.tencent.com'
    first_name: Wenhao
    google_scholar_id: https://scholar.google.com/scholar?hl=en&as_sdt=0%2C15&q=wenhao+yu+notre+dame&btnG=&oq=w
    homepage: https://wyu97.github.io/
    institution: Tencent AI Lab
    last_name: Yu
    name: Wenhao Yu
    username: ~Wenhao_Yu2
  - dblp_id: https://dblp.org/pid/136/8648
    emails: '****@gmail.com'
    first_name: Dian
    google_scholar_id: https://scholar.google.com/citations?user=ERdzqyYAAAAJ&hl=en
    homepage: https://sites.google.com/site/yudiandoris/
    institution: Tencent AI Lab
    last_name: Yu
    name: Dian Yu
    semantic_scholar_id: https://www.semanticscholar.org/author/Dian-Yu/41190054
    username: ~Dian_Yu3
  - dblp_id: https://dblp.org/pid/144/2759
    emails: '****@gmail.com'
    first_name: Baolin
    google_scholar_id: https://scholar.google.com/citations?user=u1CNjgwAAAAJ&hl=zh-CN
    last_name: Peng
    name: Baolin Peng
    username: ~Baolin_Peng2
  - dblp_id: https://dblp.org/pers/hd/w/Wang_0004:Hongwei
    emails: '****@gmail.com'
    first_name: Hongwei
    google_scholar_id: https://scholar.google.com/citations?user=3C__4wsAAAAJ&hl=en
    homepage: https://hongweiw.net
    institution: Tencent AI Lab
    last_name: Wang
    name: Hongwei Wang
    orcid: https://orcid.org/0000-0001-7474-8271
    username: ~Hongwei_Wang1
  - dblp_id: https://dblp.org/pid/r/DanRoth
    emails: '****@seas.upenn.edu'
    first_name: Dan
    google_scholar_id: https://scholar.google.com/citations?user=E-bpPWgAAAAJ&hl=en
    homepage: https://www.cis.upenn.edu/~danroth/
    institution: Amazon and University of Pennsylvania
    last_name: Roth
    name: Dan Roth
    semantic_scholar_id: https://www.semanticscholar.org/author/D.-Roth/144590225
    username: ~Dan_Roth3
  - dblp_id: https://dblp.org/pid/71/4598-1
    emails: '****@ieee.org'
    first_name: Dong
    google_scholar_id: https://scholar.google.com/citations?user=tMY31_gAAAAJ
    homepage: https://sites.google.com/view/dongyu888/
    institution: Tencent AI Lab
    last_name: Yu
    name: Dong Yu
    semantic_scholar_id: https://www.semanticscholar.org/author/Dong-Yu/144580027
    username: ~Dong_Yu2
  decision: toMainConference
  end_page: 1720
  file: 162.pdf
  id: 162
  num_pages: 14
  openreview_id: 3JdxfL4hMp
  pdf_file: 7b20539ab1b2413d84480e0bb4ff57f6dcb999fa.pdf
  start_page: 1707
  title: 'Sub-Sentence Encoder: Contrastive Learning of Propositional Semantic Representations'
- abstract: The task of scientific Natural Language Inference (NLI) involves predicting
    the semantic relation between two sentences extracted from research articles.
    This task was recently proposed along with a new dataset called SciNLI derived
    from papers published in the computational linguistics domain. In this paper,
    we aim to introduce diversity in the scientific NLI task and present MSciNLI,
    a dataset containing $132,320$ sentence pairs extracted from five new scientific
    domains. The availability of multiple domains makes it possible to study domain
    shift for scientific NLI. We establish strong baselines on MSciNLI by fine-tuning
    Pre-trained Language Models (PLMs) and prompting Large Language Models (LLMs).
    The highest Macro F1 scores of PLM and LLM baselines are $77.21$% and $51.77$%,
    respectively, illustrating that MSciNLI is challenging for both types of models.
    Furthermore, we show that domain shift degrades the performance of scientific
    NLI models which demonstrates the diverse characteristics of different domains
    in our dataset. Finally, we use both scientific NLI datasets in an intermediate
    task transfer learning setting and show that they can improve the performance
    of downstream tasks in the scientific domain. We make our dataset and code available
    on Github.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: 'Semantics: Sentence-level Semantics, Textual Inference and Other
      areas'
  authors:
  - emails: '****@uic.edu'
    first_name: Mobashir
    last_name: Sadat
    name: Mobashir Sadat
    username: ~Mobashir_Sadat1
  - dblp_id: https://dblp.uni-trier.de/pid/69/6680
    emails: '****@uic.edu'
    first_name: Cornelia
    homepage: https://www.cs.uic.edu/~cornelia/
    institution: University of Illinois, Chicago
    last_name: Caragea
    name: Cornelia Caragea
    username: ~Cornelia_Caragea2
  decision: toMainConference
  end_page: 1740
  file: 163.pdf
  id: 163
  num_pages: 20
  openreview_id: COSAK1ItoC
  pdf_file: 3c1fd49f82d5ceb6b84b4ea3e4c1b438fafe6893.pdf
  start_page: 1721
  title: 'MSciNLI: A Diverse Benchmark for Scientific Natural Language Inference'
- abstract: 'In this paper, we examine the collaborative dynamics between humans

    and language models (LMs), where the interactions typically involve

    LMs proposing text segments and humans editing or responding to these

    proposals. Productive engagement with LMs in such scenarios necessitates that
    humans discern effective text-based interaction strategies, such as editing and
    response styles, from historical human-LM interactions. This objective is inherently
    causal, driven by the counterfactual `what-if'' question: how would the outcome
    of collaboration change if humans employed a different text editing/refinement
    strategy? A key challenge in answering this causal inference question is formulating
    an appropriate causal estimand: the conventional average treatment effect (ATE)
    estimand is inapplicable to text-based treatments due to their high dimensionality.
    To address this concern, we introduce a new causal estimand-- *Incremental Stylistic
    Effect (ISE)*, which characterizes the average impact of infinitesimally shifting
    a text towards a specific style, such as increasing formality. We establish the
    conditions for the non-parametric identification of ISE. Building on this, we
    develop *CausalCollab*, an algorithm designed to estimate the ISE of various interaction
    strategies in dynamic human-LM collaborations. Our empirical investigations across
    three distinct human-LM collaboration scenarios reveal that *CausalCollab* effectively
    reduces confounding and significantly improves counterfactual estimation over
    a set of competitive baselines.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Learning for NLP
  authors:
  - emails: '****@umich.edu'
    first_name: Bohan
    institution: University of Michigan - Ann Arbor
    last_name: Zhang
    name: Bohan Zhang
    username: ~Bohan_Zhang2
  - dblp_id: https://dblp.org/pid/32/6839
    emails: '****@columbia.edu'
    first_name: Yixin
    google_scholar_id: https://scholar.google.com/citations?user=gFLW9qcAAAAJ&hl=en
    institution: University of Michigan - Ann Arbor
    last_name: Wang
    name: Yixin Wang
    orcid: https://orcid.org/0000-0002-6617-4842
    username: ~Yixin_Wang1
  - dblp_id: https://dblp.org/pid/35/3993
    emails: '****@umich.edu'
    first_name: Paramveer
    google_scholar_id: https://scholar.google.com/citations?user=vu5Mw_0AAAAJ&hl=en
    homepage: http://pdhillon.com
    institution: University of Michigan
    last_name: Dhillon
    name: Paramveer Dhillon
    semantic_scholar_id: https://www.semanticscholar.org/author/Paramveer-S.-Dhillon/1939367
    username: ~Paramveer_Dhillon1
  decision: toMainConference
  end_page: 1758
  file: 164.pdf
  id: 164
  num_pages: 18
  openreview_id: RJfe9mFkNr
  pdf_file: 4866c437685f569b521b070942a55187b4f90abe.pdf
  start_page: 1741
  title: Causal Inference for Human-Language Model Collaboration
- abstract: 'With the increasing risk posed by jailbreak attacks, recent studies have
    investigated various methods to improve the safety of large language models (LLMs),
    mainly falling into two strategies: safety training and safeguards. Safety training
    involves fine-tuning the LLM with adversarial samples, which activate the LLM''s
    capabilities against jailbreak. However, it is not always effective in countering
    new attacks and often leads to potential performance degradation. Safeguards,
    on the other hand, are methods using additional models to filter harmful content
    from the LLM''s response. Nevertheless, they can only reduce a limited amount
    of harmful output and introduce extra computational costs. Given the distinct
    strengths and weaknesses of both, we combine them to balance out their flaws and
    propose a more effective method called Self-Guard.

    Specifically, we train the LLM to review its responses for any harmful content
    and append a [harmful] or [harmless] tag to the end of the response. In this way,
    Self-Guard possesses the advantages of safety training, leveraging the powerful
    capabilities of the LLMs themselves to detect harmfulness. Besides that, it gains
    flexibility like safeguards, making the safety check target the output side, which
    makes the system less vulnerable to attack updates. Experimental results indicate
    that our Self-Guard can effectively defend against jailbreak attacks and will
    not cause LLMs'' performance degradation.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - dblp_id: https://dblp.org/pid/217/9660.html
    emails: '****@se.cuhk.edu.hk'
    first_name: Zezhong
    google_scholar_id: https://scholar.google.com/citations?user=xfl6gcgAAAAJ
    last_name: Wang
    name: Zezhong WANG
    orcid: https://orcid.org/0000-0003-4079-0097
    username: ~Zezhong_WANG1
  - emails: '****@microsoft.com'
    first_name: Fangkai
    google_scholar_id: https://scholar.google.se/citations?user=g4MrE6QAAAAJ&hl=en
    homepage: https://www.microsoft.com/en-us/research/people/fangkaiyang/
    institution: Microsoft
    last_name: Yang
    name: Fangkai Yang
    orcid: https://orcid.org/0000-0002-3089-0345
    username: ~Fangkai_Yang3
  - emails: '****@microsoft.com'
    first_name: Lu
    google_scholar_id: https://scholar.google.com/citations?user=hqlU92YAAAAJ&hl=en
    homepage: https://scholar.google.com/citations?user=hqlU92YAAAAJ&hl=en
    last_name: Wang
    name: Lu Wang
    username: ~Lu_Wang11
  - emails: '****@microsoft.com'
    first_name: Pu
    google_scholar_id: https://scholar.google.com/citations?user=G3kyd-MAAAAJ&hl=zh-CN
    last_name: Zhao
    name: Pu Zhao
    username: ~Pu_Zhao3
  - dblp_id: https://dblp.org/pid/72/1462-3.html
    emails: '****@se.cuhk.edu.hk'
    first_name: Hongru
    google_scholar_id: https://scholar.google.com/citations?user=s6UtVYUAAAAJ&hl=en
    homepage: https://rulegreen.github.io/
    last_name: Wang
    name: Hongru WANG
    orcid: https://orcid.org/0000-0001-5027-0138
    semantic_scholar_id: https://www.semanticscholar.org/author/Hongru-Wang/22642319
    username: ~Hongru_WANG1
  - emails: '****@se.cuhk.edu.hk'
    first_name: Liang
    google_scholar_id: https://scholar.google.com/citations?user=0iatxnIAAAAJ&hl=en
    institution: Chinese University of Hong Kong, The Chinese University of Hong Kong
    last_name: Chen
    name: Liang CHEN
    username: ~Liang_CHEN15
  - dblp_id: https://dblp.org/pid/120/0743
    emails: '****@microsoft.com'
    first_name: Qingwei
    google_scholar_id: https://scholar.google.co.jp/citations?hl=zh-CN&user=W9fdsxMAAAAJ
    homepage: https://www.microsoft.com/en-us/research/people/qlin/
    institution: Microsoft Research
    last_name: Lin
    name: Qingwei Lin
    orcid: https://orcid.org/0000-0003-2559-2383
    semantic_scholar_id: https://www.semanticscholar.org/author/Qingwei-Lin/2793487
    username: ~Qingwei_Lin1
  - dblp_id: https://dblp.org/pid/w/KamFaiWong
    emails: '****@se.cuhk.edu.hk'
    first_name: Kam-Fai
    homepage: http://www.se.cuhk.edu.hk/~kfwong
    institution: The Chinese University of Hong Kong
    last_name: Wong
    name: Kam-Fai Wong
    orcid: https://orcid.org/0000-0002-9427-5659
    username: ~Kam-Fai_Wong2
  decision: toMainConference
  end_page: 1779
  file: 165.pdf
  id: 165
  num_pages: 21
  openreview_id: NZBRXEhTEV
  pdf_file: a7e9ed473d118175bd25a5cf51eab94daaba3b5a.pdf
  start_page: 1759
  title: 'SELF-GUARD: Empower the LLM to Safeguard Itself'
- abstract: Knowledge graph completion (KGC) aims to infer missing facts based on
    existing facts within a KG. Recently, research on generative models (GMs) has
    addressed the limitations of embedding methods in terms of generality and scalability.
    However, GM-based methods are sensitive to contextual facts on KG, so the contextual
    facts of poor quality can cause GMs to generate erroneous results. To improve
    the performance of GM-based methods for various KGC tasks, we propose a COntextual
    FactS GuIded GeneratioN (COSIGN) model. First, to enhance the inference ability
    of the generative model, we designed a contextual facts collector to achieve human-like
    retrieval behavior. Second, a contextual facts organizer is proposed to learn
    the organized capabilities of LLMs through knowledge distillation. Finally, the
    organized contextual facts as the input of the inference generator to generate
    missing facts. Experimental results demonstrate that COSIGN outperforms state-of-the-art
    baseline techniques in terms of performance.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Extraction
  authors:
  - emails: '****@shu.edu.cn'
    first_name: Jinpeng
    homepage: https://www.shu.edu.cn/
    last_name: Li
    name: Jinpeng Li
    username: ~Jinpeng_Li4
  - dblp_id: https://dblp.org/pid/74/2568-6
    emails: '****@shu.edu.cn'
    first_name: Hang
    google_scholar_id: https://scholar.google.com.au/citations?user=3BLeGSoAAAAJ&hl=zh-CN
    institution: Shanghai University
    last_name: Yu
    name: Hang Yu
    orcid: https://orcid.org/0000-0003-3444-9992
    username: ~Hang_Yu9
  - emails: '****@shu.edu.cn'
    first_name: Xiangfeng
    homepage: https://cs.shu.edu.cn/szdw/jsxx.htm
    last_name: Luo
    name: Xiangfeng Luo
    username: ~Xiangfeng_Luo1
  - dblp_id: https://dblp.org/pid/33/85-12
    emails: '****@auckland.ac.nz'
    first_name: Qian
    google_scholar_id: https://scholar.google.com/citations?user=HgV7-XQAAAAJ&hl
    institution: University of Auckland
    last_name: Liu
    name: Qian Liu
    orcid: https://orcid.org/0000-0002-3162-935X
    username: ~Qian_Liu11
  decision: toMainConference
  end_page: 1793
  file: 166.pdf
  id: 166
  num_pages: 14
  openreview_id: 2dHw8oeKZO
  pdf_file: 66dac48bd6924cb882e574aa0433ac2e80ba6b01.pdf
  start_page: 1780
  title: 'COSIGN: Contextual Facts Guided Generation for Knowledge Graph Completion'
- abstract: 'Recent advancement in large language models (LLMs) has offered a strong
    potential for natural language systems to process informal language. A representative
    form of informal language is slang, used commonly in daily conversations and online
    social media. To date, slang has not been comprehensively evaluated in LLMs   due
    partly to the absence of a carefully designed and publicly accessible benchmark.
    Using movie subtitles, we construct a  dataset that supports evaluation on a diverse
    set of tasks pertaining to automatic processing of slang. For both evaluation
    and finetuning, we show the effectiveness of our dataset on two core applications:
    1) slang detection, and 2) identification of regional and historical sources of
    slang from natural sentences. We also show how our dataset can be used to probe
    the output distributions of LLMs for interpretive insights. We find that while
    LLMs such as GPT-4 achieve good performance in a zero-shot setting, smaller BERT-like
    models finetuned on our dataset achieve comparable performance. Furthermore, we
    show that our dataset enables finetuning of LLMs such as GPT-3.5 that achieve
    substantially better performance than strong zero-shot baselines. Our work offers
    a comprehensive evaluation and a high-quality benchmark on English slang based
    on the OpenSubtitles corpus, serving both as a publicly accessible resource and
    a platform for applying tools for informal language processing.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - dblp_id: https://dblp.org/pid/249/6968
    emails: '****@cs.toronto.edu'
    first_name: Zhewei
    google_scholar_id: https://scholar.google.ca/citations?hl=en&user=Z5Dc28EAAAAJ
    homepage: http://www.cs.toronto.edu/~zheweisun/
    institution: Department of Computer Science, University of Toronto
    last_name: Sun
    name: Zhewei Sun
    semantic_scholar_id: https://www.semanticscholar.org/author/Zhewei-Sun/2118237659
    username: ~Zhewei_Sun1
  - emails: '****@amazon.com'
    first_name: Qian
    google_scholar_id: https://scholar.google.com/citations?user=CLleKDAAAAAJ&hl=en
    institution: Amazon
    last_name: Hu
    name: Qian Hu
    username: ~Qian_Hu4
  - emails: '****@amazon.com'
    first_name: Rahul
    google_scholar_id: https://scholar.google.com/citations?user=1CFrm2YAAAAJ&hl=en
    last_name: Gupta
    name: Rahul Gupta
    username: ~Rahul_Gupta3
  - dblp_id: https://dblp.org/pid/16/6366
    emails: '****@cs.toronto.edu'
    first_name: Richard
    google_scholar_id: https://scholar.google.ca/citations?user=iBeDoRAAAAAJ&hl=en
    homepage: http://www.cs.columbia.edu/~zemel
    institution: Department of Computer Science, Columbia University and Department
      of Computer Science, University of Toronto
    last_name: Zemel
    name: Richard Zemel
    username: ~Richard_Zemel1
  - emails: '****@cs.toronto.edu'
    first_name: Yang
    google_scholar_id: https://scholar.google.com/citations?user=bbfGi6sAAAAJ&hl=en&oi=ao
    homepage: http://www.cs.toronto.edu/~yangxu/index.html
    institution: Department of Computer Science, University of Toronto
    last_name: Xu
    name: Yang Xu
    username: ~Yang_Xu7
  decision: toMainConference
  end_page: 1812
  file: 167.pdf
  id: 167
  num_pages: 19
  openreview_id: YtZlHE4Kqm
  pdf_file: b2e425f6b16551502cbe66068d0fe19299e84be0.pdf
  start_page: 1794
  title: 'Toward Informal Language Processing: Knowledge of Slang in Large Language
    Models'
- abstract: 'We introduce Ghostbuster, a state-of-the-art system for detecting AI-generated
    text.

    Our method works by passing documents through a series of weaker language models,
    running a structured search over possible combinations of their features, and
    then training a classifier on the selected features to predict whether documents
    are AI-generated.

    Crucially, Ghostbuster does not require access to token probabilities from the
    target model, making it useful for detecting text generated by black-box or unknown
    models.

    In conjunction with our model, we release three new datasets of human- and AI-generated
    text as detection benchmarks in the domains of student essays, creative writing,
    and news articles. We compare Ghostbuster to several existing detectors, including
    DetectGPT and GPTZero, as well as a new RoBERTa baseline. Ghostbuster achieves
    99.0 F1 when evaluated across domains, which is 5.9 F1 higher than the best preexisting
    model. It also outperforms all previous approaches in generalization across writing
    domains (+7.5 F1), prompting strategies (+2.1 F1), and language models (+4.4 F1).
    We also analyze our system''s robustness to a variety of perturbations and paraphrasing
    attacks, and evaluate its performance on documents by non-native English speakers.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Learning for NLP
  authors:
  - emails: '****@berkeley.edu'
    first_name: Vivek
    google_scholar_id: https://scholar.google.com/citations?user=D40otBIAAAAJ&hl=en&authuser=1
    homepage: https://vivek.lol
    last_name: Verma
    name: Vivek Verma
    username: ~Vivek_Verma2
  - emails: '****@berkeley.edu'
    first_name: Eve
    google_scholar_id: https://scholar.google.com/citations?user=NHlxXzwAAAAJ&hl=en&oi=ao
    homepage: https://www.efleisig.com
    last_name: Fleisig
    name: Eve Fleisig
    username: ~Eve_Fleisig1
  - emails: '****@berkeley.edu'
    first_name: Nicholas
    google_scholar_id: https://scholar.google.com/citations?user=zV5vhUcAAAAJ&hl=en
    homepage: https://people.eecs.berkeley.edu/~nicholas_tomlin/
    institution: University of California Berkeley
    last_name: Tomlin
    name: Nicholas Tomlin
    username: ~Nicholas_Tomlin1
  - dblp_id: https://dblp.org/pid/22/1139
    emails: '****@cs.berkeley.edu'
    first_name: Dan
    homepage: http://people.eecs.berkeley.edu/~klein/
    institution: University of California, Berkeley
    last_name: Klein
    name: Dan Klein
    username: ~Dan_Klein1
  decision: toMainConference
  end_page: 1828
  file: 168.pdf
  id: 168
  num_pages: 16
  openreview_id: 6F4RvoZLL4
  pdf_file: 58666a89cafa12a709e1dfe6dfb9f497b9236e14.pdf
  start_page: 1813
  title: 'Ghostbuster: Detecting Text Ghostwritten by Large Language Models'
- abstract: Multi-hop question answering (QA) involves finding multiple relevant passages
    and step-by-step reasoning to answer complex questions, indicating a retrieve-and-read
    paradigm. However, previous retrievers were customized for two-hop questions,
    and most of them were trained separately across different hops, resulting in a
    lack of supervision over the entire multi-hop retrieval process and leading to
    poor performance in complicated scenarios beyond two hops. In this work, we introduce
    Beam Retrieval, an end-to-end beam retrieval framework for multi-hop QA. This
    approach models the multi-hop retrieval process in an end-to-end manner by jointly
    optimizing an encoder and two classification heads across all hops. Moreover,
    Beam Retrieval maintains multiple partial hypotheses of relevant passages at each
    step, expanding the search space and reducing the risk of missing relevant passages.
    To establish a complete QA system, we incorporate a supervised reader or a large
    language model (LLM).  Experimental results demonstrate that Beam Retrieval achieves
    a nearly 50\% improvement compared with baselines on challenging MuSiQue-Ans,
    and it also surpasses all previous retrievers on HotpotQA and achieves 99.9\%
    precision on 2WikiMultiHopQA. Providing high-quality context, Beam Retrieval helps
    our supervised reader achieve new state-of-the-art performance and substantially
    improves the few-shot QA performance of LLMs.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Question Answering
  authors:
  - emails: '****@bupt.edu.cn'
    first_name: Jiahao
    homepage: https://github.com/canghongjian
    last_name: Zhang
    name: Jiahao Zhang
    username: ~Jiahao_Zhang7
  - dblp_id: https://dblp.org/pid/51/222
    emails: '****@bupt.edu.cn'
    first_name: Haiyang
    homepage: http://www.bupt.edu.cn
    last_name: Zhang
    name: Haiyang Zhang
    semantic_scholar_id: https://www.semanticscholar.org/author/H.-Zhang/3228711
    username: ~Haiyang_Zhang2
  - emails: '****@bupt.edu.cn'
    first_name: Dongmei
    homepage: https://teacher.bupt.edu.cn/zhangdongmei
    last_name: Zhang
    name: Dongmei Zhang
    username: ~Dongmei_Zhang5
  - emails: '****@tencent.com'
    first_name: Liu
    homepage: https://github.com/yjcelly
    last_name: Yong
    name: Liu Yong
    username: ~Liu_Yong1
  - dblp_id: https://dblp.org/pid/90/839.html
    emails: '****@126.com'
    first_name: Shen
    last_name: Huang
    name: Shen Huang
    username: ~Shen_Huang3
  decision: toMainConference
  end_page: 1842
  file: 170.pdf
  id: 170
  num_pages: 14
  openreview_id: au5aGogJQX
  pdf_file: b6122d600f6b71e3c80ad8299af6e6dc922ae7fd.pdf
  start_page: 1829
  title: End-to-End Beam Retrieval for Multi-Hop Question Answering
- abstract: Despite remarkable strides made in the development of entity linking systems
    in recent years, a comprehensive comparative analysis of these systems using a
    unified framework is notably absent. This paper addresses this oversight by introducing
    a new black-box benchmark and conducting a comprehensive evaluation of all state-of-the-art
    entity linking methods. We use an ablation study to investigate the impact of
    candidate sets on the performance of entity linking. Our findings uncover exactly
    how much such entity linking systems depend on candidate sets, and how much this
    limits the general applicability of each system.  We present an alternative approach
    to candidate sets, demonstrating that leveraging the entire in-domain candidate
    set can serve as a viable substitute for certain models. We show the trade-off
    between less restrictive candidate sets, increased inference time and memory footprint
    for some models.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - emails: '****@gmail.com'
    first_name: Nicolas
    homepage: https://nicolasong.com
    last_name: Ong
    name: Nicolas Ong
    username: ~Nicolas_Ong1
  - dblp_id: https://dblp.org/pid/222/9910
    emails: '****@sfu.ca'
    first_name: Hassan
    google_scholar_id: https://scholar.google.ca/citations?user=AhEoNRoAAAAJ&hl=en
    homepage: https://shavarani.github.io/
    institution: Simon Fraser University
    last_name: Shavarani
    name: Hassan Shavarani
    semantic_scholar_id: https://www.semanticscholar.org/author/Hassan-Shavarani/51121270
    username: ~Hassan_Shavarani1
  - dblp_id: https://dblp.org/pid/s/AnoopSarkar
    emails: '****@gmail.com'
    first_name: Anoop
    google_scholar_id: https://scholar.google.ca/citations?user=KhJJchQAAAAJ
    homepage: http://anoopsarkar.github.io/
    institution: Simon Fraser University
    last_name: Sarkar
    name: Anoop Sarkar
    orcid: https://orcid.org/0000-0002-4795-9361
    semantic_scholar_id: https://www.semanticscholar.org/author/Anoop-Sarkar/3028658
    username: ~Anoop_Sarkar1
  decision: toMainConference
  end_page: 1853
  file: 171.pdf
  id: 171
  num_pages: 11
  openreview_id: p8U0sZWOrt
  pdf_file: 017b7fd2581b8c75ec94dca3d99b76b7aaf9391f.pdf
  start_page: 1843
  title: Unified Examination of Entity Linking in Absence of Candidate Sets
- abstract: Multimodal sarcasm detection aims to identify sarcasm in the given image-text
    pairs and has wide applications in the multimodal domains. Previous works primarily
    design complex network structures to fuse the image-text modality features for
    classification. However, such complicated structures may risk overfitting on in-domain
    data, reducing the performance in out-of-distribution (OOD) scenarios. Additionally,
    existing methods typically do not fully utilize cross-modal features, limiting
    their performance on in-domain datasets. Therefore, to build a more reliable multimodal
    sarcasm detection model, we propose a generative multimodal sarcasm model consisting
    of a designed instruction template and a demonstration retrieval module based
    on the large language model. Moreover, to assess the generalization of current
    methods, we introduce an OOD test set, RedEval. Experimental results demonstrate
    that our method is effective and achieves state-of-the-art (SOTA) performance
    on the in-domain MMSD2.0 and OOD RedEval datasets.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multimodality and Language Grounding to Vision, Robotics and Beyond
  authors:
  - emails: '****@bupt.edu.cn'
    first_name: Binghao
    homepage: https://github.com/TangBinghao
    last_name: Tang
    name: Binghao Tang
    username: ~Binghao_Tang1
  - dblp_id: https://dblp.org/pid/305/9830
    emails: '****@bupt.edu.cn'
    first_name: Boda
    google_scholar_id: https://scholar.google.com.hk/citations?user=OTbLS30AAAAJ&hl=zh-CN
    homepage: https://github.com/TimeLessLing
    last_name: Lin
    name: Boda Lin
    orcid: https://orcid.org/0000-0002-6627-7200
    semantic_scholar_id: https://www.semanticscholar.org/author/Boda-Lin/10144804
    username: ~Boda_Lin1
  - emails: '****@bupt.edu.cn'
    first_name: Haolong
    homepage: https://github.com/summoneryhl
    last_name: Yan
    name: Haolong Yan
    username: ~Haolong_Yan1
  - dblp_id: https://dblp.org/pid/54/6603-1
    emails: '****@bupt.edu.cn'
    first_name: Si
    homepage: http://www.pris.net.cn/introduction/teacher/lisi
    institution: Beijing University of Posts and Telecommunications
    last_name: Li
    name: Si Li
    semantic_scholar_id: https://www.semanticscholar.org/author/Si-Li/2118153856
    username: ~Si_Li5
  decision: toMainConference
  end_page: 1864
  file: 172.pdf
  id: 172
  num_pages: 11
  openreview_id: WXID2cOCEF
  pdf_file: 84f68e5beaab19807fb0b8ff82f94bf8e093065e.pdf
  start_page: 1854
  title: Leveraging Generative Large Language Models with Visual Instruction and Demonstration
    Retrieval for Multimodal Sarcasm Detection
- abstract: Black-box few-shot text classification handles text classification in
    limited data without accessing the parameters and gradients of language models
    (LMs). Existing black-box optimization methods have demonstrated strong few-shot
    learning capabilities. However, they still require numerous LMs' calls to search
    optimal prompts, thus resulting in overfitting performance and increasing computational
    cost. To address this issue, we present MuSKPrompt (Multi-scale Knowledge Prompt
    for Memory Model), an efficient multi-scale knowledge prompt-based memory model
    in black-box few-shot text classification task. MuSKPrompt extracts instance-level
    and class-level knowledge at different scales and stores them in memory banks
    during training. Then, it references multi-scale memory banks to perform quick
    inference on new samples via a novel scoring module. MuSKPrompt achieves competitive
    performance in limited data through multi-scale instance-level and class-level
    knowledge. Moreover, it realizes gradient-free optimization with zero training
    parameters in the black-box scenario. Experiments on different benchmarks and
    parameter analysis demonstrate the effectiveness and efficiency of MuSKPrompt
    in black-box few-shot text classification tasks.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Efficient/Low-Resource Methods for NLP
  authors:
  - emails: '****@mail.scut.edu.cn'
    first_name: Xiaojun
    homepage: https://github.com/cuteyuqing
    last_name: Kuang
    name: Xiaojun Kuang
    username: ~Xiaojun_Kuang1
  - dblp_id: https://dblp.uni-trier.de/pid/48/4856.html
    emails: '****@scut.edu.cn'
    first_name: C. L. Philip
    google_scholar_id: https://scholar.google.com/citations?hl=zh-CN&user=Q5248zwAAAAJ
    homepage: http://www2.scut.edu.cn/cs/2019/0724/c22284a328910/page.htm
    institution: South China University of Technology
    last_name: Chen
    name: C. L. Philip Chen
    username: ~C._L._Philip_Chen1
  - emails: '****@mail.scut.edu.cn'
    first_name: Shuzhen
    last_name: Li
    name: Shuzhen Li
    orcid: https://orcid.org/0000-0001-6847-6740
    username: ~Shuzhen_Li2
  - dblp_id: https://dblp.uni-trier.de/pid/07/4227-15.html
    emails: '****@scut.edu.cn'
    first_name: Tong
    google_scholar_id: https://scholar.google.com/citations?hl=zh-CN&user=-neWbpUAAAAJ
    homepage: http://www2.scut.edu.cn/cs/2021/0317/c22284a422099/page.htm
    institution: South China University of Technology
    last_name: Zhang
    name: Tong Zhang
    username: ~Tong_Zhang14
  decision: toMainConference
  end_page: 1879
  file: 173.pdf
  id: 173
  num_pages: 15
  openreview_id: xuso2rMR9t
  pdf_file: 3ddc6d08847356f05330fe87878ba71352649739.pdf
  start_page: 1865
  title: Multi-Scale Prompt Memory-Augmented Model for Black-Box Scenarios
- abstract: In the era of large language models (LLMs), in-context learning (ICL)
    stands out as an effective prompting strategy that explores LLMs' potency across
    various tasks. However, applying LLMs to grammatical error correction (GEC) is
    still a challenging task. In this paper, we propose a novel ungrammatical-syntax-based
    in-context example selection strategy for GEC. Specifically, we measure similarity
    of sentences based on their syntactic structures with diverse algorithms, and
    identify optimal ICL examples sharing the most similar ill-formed syntax to the
    test input. Additionally, we carry out a two-stage process to further improve
    the quality of selection results. On benchmark English GEC datasets, empirical
    results show that our proposed ungrammatical-syntax-based strategies outperform
    commonly-used word-matching or semantics-based methods with multiple LLMs. This
    indicates that for a syntax-oriented task like GEC, paying more attention to syntactic
    information can effectively boost LLMs' performance. Our code is available at
    https://github.com/JamyDon/SynICL4GEC.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - dblp_id: https://dblp.org/pid/348/6015
    emails: '****@stu.pku.edu.cn'
    first_name: Chenming
    google_scholar_id: https://scholar.google.com/citations?user=qQ6PBaMAAAAJ&hl=en
    last_name: Tang
    name: Chenming Tang
    semantic_scholar_id: https://www.semanticscholar.org/author/Chenming-Tang/120527265
    username: ~Chenming_Tang1
  - emails: '****@pku.edu.cn'
    first_name: Fanyi
    last_name: Qu
    name: Fanyi Qu
    semantic_scholar_id: https://www.semanticscholar.org/author/Fanyi-Qu/2127464628
    username: ~Fanyi_Qu1
  - dblp_id: https://dblp.org/search?q=yunfang+wu
    emails: '****@pku.edu.cn'
    first_name: Yunfang
    homepage: https://cs.pku.edu.cn/info/1083/1300.htm
    last_name: Wu
    name: Yunfang Wu
    semantic_scholar_id: https://www.semanticscholar.org/author/Yunfang-Wu/2477658
    username: ~Yunfang_Wu1
  decision: toMainConference
  end_page: 1892
  file: 174.pdf
  id: 174
  num_pages: 13
  openreview_id: mFZw6eoUJ1
  pdf_file: 78525e11d706ef42048d22706da0250c2738bed3.pdf
  start_page: 1880
  title: Ungrammatical-syntax-based In-context Example Selection for Grammatical Error
    Correction
- abstract: Despite remarkable advancements in few-shot generalization in natural
    language processing, most models are developed and evaluated primarily in English.
    To establish a rigorous and equitable evaluation framework for few-shot cross-lingual
    transfer, we introduce a new benchmark, called BUFFET, which unifies 15 diverse
    tasks across 54 languages in a sequence-to-sequence format and provides a fixed
    set of few-shot examples and instructions.  Using BUFFET, we perform thorough
    evaluations of ten state-of-the-art multilingual large language models with different
    transfer methods, namely in-context learning and fine-tuning. Our findings reveal
    significant room for improvement in few-shot in-context cross-lingual transfer.
    Strong multilingual pre-trained or instruction-tuned models such as BLOOM or ChatGPT
    often lag behind much smaller mT5-base models given the same number of few-shot
    samples, particularly in low-resource languages.  Our analysis suggests avenues
    for future research in few-shot cross-lingual transfer.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multilinguality and Language Diversity
  authors:
  - emails: '****@cs.washington.edu'
    first_name: Akari
    google_scholar_id: https://scholar.google.com/citations?user=gqB4u_wAAAAJ&hl=en&oi=ao
    homepage: https://akariasai.github.io/
    institution: Paul G. Allen School of Computer Science & Engineering, University
      of Washington
    last_name: Asai
    name: Akari Asai
    username: ~Akari_Asai2
  - emails: '****@google.com'
    first_name: Sneha
    google_scholar_id: https://scholar.google.com/citations?user=LeEwxtgAAAAJ&hl=en
    institution: Department of Computer Science
    last_name: Kudugunta
    name: Sneha Kudugunta
    username: ~Sneha_Kudugunta1
  - dblp_id: https://dblp.org/pid/165/9117-1
    emails: '****@usc.edu'
    first_name: Xinyan
    google_scholar_id: https://scholar.google.com/citations?user=PoZv5KkAAAAJ&hl=en
    homepage: https://velocitycavalry.github.io
    institution: University of Southern California
    last_name: Yu
    middle_name: Velocity
    name: Xinyan Velocity Yu
    semantic_scholar_id: https://www.semanticscholar.org/author/Xinyan-Velocity-Yu/2118211280
    username: ~Xinyan_Velocity_Yu1
  - dblp_id: https://dblp.org/pid/184/3734
    emails: '****@cs.washington.edu'
    first_name: Terra
    homepage: https://blvns.github.io
    institution: University of Washington
    last_name: Blevins
    name: Terra Blevins
    semantic_scholar_id: https://www.semanticscholar.org/author/Terra-Blevins/3443287
    username: ~Terra_Blevins1
  - dblp_id: https://dblp.org/pid/167/5312
    emails: '****@gmail.com'
    first_name: Hila
    google_scholar_id: https://scholar.google.com/citations?user=URThmtMAAAAJ&hl=en
    homepage: https://gonenhila.github.io/
    institution: Facebook
    last_name: Gonen
    name: Hila Gonen
    semantic_scholar_id: https://www.semanticscholar.org/author/Hila-Gonen/1821892
    username: ~Hila_Gonen1
  - dblp_id: https://dblp.org/pid/260/6668
    emails: '****@gmail.com'
    first_name: Machel
    google_scholar_id: https://scholar.google.com/citations?user=N8ctPiIAAAAJ&hl=en
    homepage: https://machelreid.github.io/
    institution: Google
    last_name: Reid
    name: Machel Reid
    username: ~Machel_Reid1
  - dblp_id: https://dblp.org/pid/75/8157
    emails: '****@cs.washington.edu'
    first_name: Yulia
    google_scholar_id: https://scholar.google.com/citations?user=SEDPkrsAAAAJ&hl=en
    homepage: https://homes.cs.washington.edu/~yuliats/
    institution: Department of Computer Science, University of Washington
    last_name: Tsvetkov
    name: Yulia Tsvetkov
    username: ~Yulia_Tsvetkov1
  - dblp_id: https://dblp.org/pid/186/7066.html
    emails: '****@google.com'
    first_name: Sebastian
    google_scholar_id: https://scholar.google.de/citations?user=8ONXPV8AAAAJ&hl=de
    homepage: http://sebastianruder.com/
    institution: Google
    last_name: Ruder
    name: Sebastian Ruder
    semantic_scholar_id: https://www.semanticscholar.org/author/Sebastian-Ruder/2884561
    username: ~Sebastian_Ruder2
  - dblp_id: https://dblp.org/pid/52/1296
    emails: '****@washington.edu'
    first_name: Hannaneh
    google_scholar_id: https://scholar.google.com/citations?user=LOV6_WIAAAAJ&hl=en
    homepage: https://homes.cs.washington.edu/~hannaneh/
    institution: University of Washington, University of Washington, Allen Institute
      for Artificial Intelligence and University of Washington, Seattle
    last_name: Hajishirzi
    name: Hannaneh Hajishirzi
    semantic_scholar_id: https://www.semanticscholar.org/author/Hannaneh-Hajishirzi/2548384
    username: ~Hannaneh_Hajishirzi1
  decision: toMainConference
  end_page: 1922
  file: 179.pdf
  id: 179
  num_pages: 30
  openreview_id: SgRl7Jm3M9
  pdf_file: a2d533401b82c28edb0699ef77addfc4c7d86379.pdf
  start_page: 1893
  title: 'BUFFET: Benchmarking Large Language Models for Few-shot Cross-lingual Transfer'
- abstract: 'In-context learning enhances the reasoning capabilities of LLMs by providing
    several examples. A direct yet effective approach to obtain in-context example
    is to select the top-k examples based on their semantic similarity to the test
    input. However, when applied to event argument extraction (EAE), this approach
    exhibits two shortcomings: 1) It may select almost identical examples, thus failing
    to provide additional event information, and 2) It overlooks event attributes,
    leading to the selected examples being unrelated to the test event type. In this
    paper, we introduce three necessary requirements when selecting an in-context
    example for EAE task: semantic similarity, example diversity and event correlation.
    And we further propose TISE, which scores examples from these three perspectives
    and integrates them using Determinantal Point Processes to directly select a set
    of examples as context. Experimental results on the ACE05 dataset demonstrate
    the effectiveness of TISE and the necessity of three requirements. Furthermore,
    we surprisingly observe that TISE can achieve superior performance with fewer
    examples and can even exceed some supervised methods.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Extraction
  authors:
  - emails: '****@gmail.com'
    first_name: Yanhe
    last_name: Fu
    name: Yanhe Fu
    username: ~Yanhe_Fu1
  - dblp_id: https://dblp.org/pid/97/5152
    emails: '****@iie.ac.cn'
    first_name: Yanan
    last_name: Cao
    name: Yanan Cao
    username: ~Yanan_Cao1
  - emails: '****@iie.ac.cn'
    first_name: Qingyue
    google_scholar_id: https://scholar.google.com/citations?hl=zh-CN&view_op=list_works&gmla=AJsN-F4pOi4sJwVrpGhUFRoQwg8MI_Lx38iS44LpPTaIkkoitukbxUeHqevGgaDODO5CQ2dQk57KOa6nlNT_UMKJQIgYAbh8cm-YkKc8WRcPu63ib_GRzrQWPKz0IPn-9fyK-B0R9xUs1fDXOi4K1RFsohqW6VzrmQ&user=4DFgv64AAAAJ
    homepage: https://qingyue2014.github.io/
    last_name: Wang
    name: Qingyue Wang
    semantic_scholar_id: https://www.semanticscholar.org/author/Qingyue-Wang/2115979374
    username: ~Qingyue_Wang1
  - emails: '****@gmail.com'
    first_name: Yi
    google_scholar_id: https://scholar.google.com.hk/citations?user=WkIhq68AAAAJ&hl=zh-CN
    last_name: Liu
    name: Yi Liu
    username: ~Yi_Liu38
  decision: toMainConference
  end_page: 1940
  file: 180.pdf
  id: 180
  num_pages: 18
  openreview_id: bwx9X4MXLx
  pdf_file: 275ec64a4b273556deca1b46a948d0564a7937bd.pdf
  start_page: 1923
  title: 'TISE: A Tripartite In-context Selection Method for Event Argument Extraction'
- abstract: "The impressive performance of recent language models across a wide range\
    \ of tasks suggests that they possess a degree of abstract reasoning skills. Are\
    \ these skills general and transferable, or specialized to specific tasks seen\
    \ during pretraining? To disentangle these effects, we propose an evaluation framework\
    \ based on \u201Ccounterfactual\u201D task variants that deviate from the default\
    \ assumptions underlying standard tasks. Across a suite of 11 tasks, we observe\
    \ nontrivial performance on the counterfactual variants, but nevertheless find\
    \ that performance substantially and consistently degrades compared to the default\
    \ conditions. This suggests that while current LMs may possess abstract task-solving\
    \ skills to an extent, they often also rely on narrow, non-transferable procedures\
    \ for task-solving. These results motivate a more careful interpretation of language\
    \ model performance that teases apart these aspects."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - dblp_id: https://dblp.org/pid/168/7994.html
    emails: '****@csail.mit.edu'
    first_name: Zhaofeng
    google_scholar_id: https://scholar.google.com/citations?user=53baCywAAAAJ&hl=en
    homepage: https://zhaofengwu.github.io/
    institution: Massachusetts Institute of Technology
    last_name: Wu
    name: Zhaofeng Wu
    semantic_scholar_id: https://www.semanticscholar.org/author/Zhaofeng-Wu/47039405
    username: ~Zhaofeng_Wu1
  - dblp_id: https://dblp.org/pid/267/2348
    emails: '****@mit.edu'
    first_name: Linlu
    google_scholar_id: https://scholar.google.com/citations?user=D1uOAWcAAAAJ
    homepage: https://linlu-qiu.github.io/
    institution: Massachusetts Institute of Technology
    last_name: Qiu
    name: Linlu Qiu
    semantic_scholar_id: https://www.semanticscholar.org/author/1742081895
    username: ~Linlu_Qiu1
  - dblp_id: https://dblp.org/pid/239/8686.html
    emails: '****@gmail.com'
    first_name: Alexis
    homepage: https://alexisjihyeross.github.io/
    institution: Massachusetts Institute of Technology and Allen Institute for Artificial
      Intelligence
    last_name: Ross
    name: Alexis Ross
    semantic_scholar_id: https://www.semanticscholar.org/author/Alexis-Ross/32739287
    username: ~Alexis_Ross1
  - dblp_id: https://dblp.org/pid/216/3446
    emails: '****@mit.edu'
    first_name: Ekin
    google_scholar_id: https://scholar.google.com/citations?user=FQHeASwAAAAJ&hl=tr
    homepage: https://www.ekinakyurek.me/
    last_name: "Aky\xFCrek"
    name: "Ekin Aky\xFCrek"
    orcid: https://orcid.org/0000-0002-5166-4689
    semantic_scholar_id: "https://www.semanticscholar.org/author/Ekin-Aky%C3%BCrek/1992708068\xE5"
    username: "~Ekin_Aky\xFCrek1"
  - dblp_id: https://dblp.org/pid/193/7174
    emails: '****@mit.edu'
    first_name: Boyuan
    google_scholar_id: https://scholar.google.com/citations?user=rEL4-fgAAAAJ&hl=en
    homepage: https://boyuan.space/
    last_name: Chen
    name: Boyuan Chen
    username: ~Boyuan_Chen2
  - dblp_id: https://dblp.org/pid/218/7334
    emails: '****@mit.edu'
    first_name: Bailin
    homepage: https://berlino.github.io/
    last_name: Wang
    name: Bailin Wang
    semantic_scholar_id: https://www.semanticscholar.org/author/Bailin-Wang/5535859
    username: ~Bailin_Wang3
  - dblp_id: https://dblp.org/pid/194/1249
    emails: '****@gmail.com'
    first_name: Najoung
    homepage: https://najoungkim.github.io
    institution: Boston University and Google
    last_name: Kim
    name: Najoung Kim
    username: ~Najoung_Kim1
  - dblp_id: https://dblp.org/pid/97/8154
    emails: '****@mit.edu'
    first_name: Jacob
    google_scholar_id: https://scholar.google.com/citations?user=dnZ8udEAAAAJ
    homepage: http://web.mit.edu/jda/www
    institution: Massachusetts Institute of Technology and Microsoft
    last_name: Andreas
    name: Jacob Andreas
    semantic_scholar_id: https://www.semanticscholar.org/author/Jacob-Andreas/2112400
    username: ~Jacob_Andreas1
  - dblp_id: https://dblp.org/pid/07/1501
    emails: '****@mit.edu'
    first_name: Yoon
    google_scholar_id: https://scholar.google.com/citations?user=n_ts4eYAAAAJ
    homepage: https://people.csail.mit.edu/yoonkim/
    institution: Massachusetts Institute of Technology
    last_name: Kim
    name: Yoon Kim
    semantic_scholar_id: https://www.semanticscholar.org/author/Yoon-Kim/38367242
    username: ~Yoon_Kim1
  decision: toMainConference
  end_page: 1984
  file: 183.pdf
  id: 183
  num_pages: 44
  openreview_id: jOt5Pp0dxh
  pdf_file: 2779549b3b7d172ee022fa2930bc892bc95e70c0.pdf
  start_page: 1941
  title: Reasoning or Reciting? Exploring the Capabilities and Limitations of Language
    Models Through Counterfactual Tasks
- abstract: "Information extraction (IE) encounters challenges due to the variety\
    \ of schemas and objectives that differ across tasks. Recent advancements hint\
    \ at the potential for universal approaches to model such tasks, referred to as\
    \ Universal Information Extraction (UIE). While handling diverse tasks in one\
    \ model, their generalization is limited since they are actually learning task-specific\
    \ knowledge.\nIn this study, we introduce an innovative paradigm known as TRUE-UIE,\
    \ wherein all IE tasks are aligned to learn the same goals: extracting mention\
    \ spans and two universal relations named $\\mathtt{NEXT}$ and $\\mathtt{IS}$.\
    \ \nDuring the decoding process, the $\\mathtt{NEXT}$ relation is utilized to\
    \ group related elements, while the $\\mathtt{IS}$ relation, in conjunction with\
    \ structured language prompts, undertakes the role of type recognition. \nAdditionally,\
    \ we consider the sequential dependency of tokens during span extraction, an aspect\
    \ often overlooked in prevalent models.\nOur empirical experiments indicate that\
    \ TRUE-UIE achieves state-of-the-art performance on established benchmarks encompassing\
    \ 16 datasets, spanning 7 diverse IE tasks. \nFurther evaluations reveal that\
    \ our approach effectively share knowledge between different IE tasks, showcasing\
    \ significant transferability in zero-shot and few-shot scenarios."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Extraction
  authors:
  - dblp_id: https://dblp.org/pid/66/4822
    emails: '****@gmail.com'
    first_name: Yucheng
    google_scholar_id: https://scholar.google.com/citations?user=NUnlU8oAAAAJ&hl=en
    institution: Shanghai Artificial Intelligence Laboratory
    last_name: Wang
    name: Yucheng Wang
    username: ~Yucheng_Wang3
  - dblp_id: https://dblp.org/pid/95/10266-2.html
    emails: '****@gmail.com'
    first_name: Bowen
    google_scholar_id: https://scholar.google.com/citations?user=oHoEp34AAAAJ&hl=zh-CN
    homepage: https://yubowen-ph.github.io/
    institution: Alibaba Group
    last_name: Yu
    name: Bowen Yu
    orcid: https://orcid.org/0000-0002-6804-1859
    semantic_scholar_id: https://www.semanticscholar.org/author/Yu-Bowen/48613402
    username: ~Bowen_Yu3
  - emails: '****@usc.edu'
    first_name: Yilin
    homepage: https://github.com/YiandLi
    last_name: Liu
    name: Yilin Liu
    username: ~Yilin_Liu4
  - emails: '****@bupt.edu.cn'
    first_name: Shudong
    last_name: Lu
    name: Shudong Lu
    orcid: https://orcid.org/0000-0002-4214-186X
    username: ~Shudong_Lu1
  decision: toMainConference
  end_page: 1998
  file: 185.pdf
  id: 185
  num_pages: 14
  openreview_id: RNira4lDUh
  pdf_file: 3897680feb895f4812f11a62d1458fef15571d75.pdf
  start_page: 1985
  title: 'TRUE-UIE: Two Universal Relations Unify Information Extraction Tasks'
- abstract: Modeling evolving knowledge over temporal knowledge graphs (TKGs) has
    become a heated topic. Various methods have been proposed to forecast links on
    TKGs. Most of them are embedding-based, where hidden representations are learned
    to represent knowledge graph (KG) entities and relations based on the observed
    graph contexts. Although these methods show strong performance on traditional
    TKG forecasting (TKGF) benchmarks, they face a strong challenge in modeling the
    unseen zero-shot relations that have no prior graph context. In this paper, we
    try to mitigate this problem as follows. We first input the text descriptions
    of KG relations into large language models (LLMs) for generating relation representations,
    and then introduce them into embedding-based TKGF methods. LLM-empowered representations
    can capture the semantic information in the relation descriptions. This makes
    the relations, whether seen or unseen, with similar semantic meanings stay close
    in the embedding space, enabling TKGF models to recognize zero-shot relations
    even without any observed graph context. Experimental results show that our approach
    helps TKGF models to achieve much better performance in forecasting the facts
    with previously unseen relations, while still maintaining their ability in link
    forecasting regarding seen relations.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Learning for NLP
  authors:
  - dblp_id: https://dblp.org/pid/283/5849
    emails: '****@gmail.com'
    first_name: Zifeng
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=8RapuD4AAAAJ&view_op=list_works&gmla=AJ1KiT0y9wtX2xPVm-HxvvQumlW86cUyTOLc1wdVlnMtumSHsgGVLeOykON441-dgmo_Qt0TnhvE7FfeRczbeDUiBe6dAhGGQVvOukz5q_1aYQTJ9OP8gqmCIdQ8B5phqCJz2zi5PtVXkRuAX7oQkqdP21vQpIjlww2k6cc_-A6vJ-s&sciund=2013598837562029624
    institution: LMU Munich
    last_name: Ding
    name: Zifeng Ding
    semantic_scholar_id: https://www.semanticscholar.org/author/Zifeng-Ding/2046761003
    username: ~Zifeng_Ding1
  - emails: '****@campus.lmu.de'
    first_name: Heling
    institution: "Ludwig-Maximilians-Universit\xE4t M\xFCnchen"
    last_name: Cai
    name: Heling Cai
    username: ~Heling_Cai1
  - emails: '****@dbs.ifi.lmu.de'
    first_name: Jingpei
    institution: ', Institute of Computer Science'
    last_name: Wu
    name: Jingpei Wu
    username: ~Jingpei_Wu1
  - dblp_id: https://dblp.org/pid/199/8143.html
    emails: '****@gmail.com'
    first_name: Yunpu
    google_scholar_id: https://scholar.google.com/citations?user=fj5DzgcAAAAJ&hl=en
    homepage: https://dblp.org/pid/199/8143.html
    institution: Siemens Corporate Research
    last_name: Ma
    name: Yunpu Ma
    username: ~Yunpu_Ma1
  - emails: '****@outlook.com'
    first_name: Ruotong
    last_name: Liao
    name: Ruotong Liao
    username: ~Ruotong_Liao1
  - dblp_id: https://dblp.org/pid/64/4642
    emails: '****@ki.uni-stuttgart.de'
    first_name: Bo
    google_scholar_id: https://scholar.google.com/citations?user=lmBXicIAAAAJ&hl=en
    institution: University of Stuttgart
    last_name: Xiong
    name: Bo Xiong
    username: ~Bo_Xiong3
  - dblp_id: http://dblp.uni-trier.de/pers/hd/t/Tresp:Volker
    emails: '****@gmail.com'
    first_name: Volker
    google_scholar_id: https://scholar.google.com/citations?user=xIJHTUwAAAAJ&hl=en
    homepage: http://www.tresp.org
    institution: Ludwig Maximilian University of Munich and Siemens Corporate Research
    last_name: Tresp
    name: Volker Tresp
    orcid: https://orcid.org/0000-0001-9428-3686
    semantic_scholar_id: https://www.semanticscholar.org/author/Volker-Tresp/1700754
    username: ~Volker_Tresp1
  decision: toMainConference
  end_page: 2017
  file: 186.pdf
  id: 186
  num_pages: 19
  openreview_id: yED6gORgbm
  pdf_file: 0d9c5b413a93fd438545fbd1d434897ee61009b4.pdf
  start_page: 1999
  title: 'zrLLM: Zero-Shot Relational Learning on Temporal Knowledge Graphs with Large
    Language Models'
- abstract: "Large Language models (LLMs) have shown remarkable success in assisting\
    \ robot learning tasks, i.e., complex household planning.\nHowever, the performance\
    \ of pretrained LLMs heavily relies on domain-specific templated text data, which\
    \ may be infeasible in real-world robot learning tasks with image-based observations.\
    \  Moreover, existing LLMs with text inputs lack the capability to evolve with\
    \ non-expert interactions with environments.\nIn this work, we introduce a novel\
    \ learning paradigm that generates robots\u2019 executable actions in the form\
    \ of text, derived solely from visual observations. Our proposed paradigm stands\
    \ apart from previous works, which utilized either language instructions or a\
    \ combination of language and visual data as inputs. \nWe demonstrate that our\
    \ proposed method can employ two fine-tuning strategies, including imitation learning\
    \ and reinforcement learning approaches, to adapt to the target test tasks effectively.\n\
    We conduct extensive experiments involving various model selections, environments,\
    \ and tasks across 7 house layouts in the VirtualHome environment. Our experimental\
    \ results demonstrate that our method surpasses existing baselines, confirming\
    \ the effectiveness of this novel learning paradigm."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multimodality and Language Grounding to Vision, Robotics and Beyond
  authors:
  - dblp_id: https://dblp.org/pid/313/2707
    emails: '****@andrew.cmu.edu'
    first_name: Jielin
    google_scholar_id: https://scholar.google.com/citations?user=2khNwjoAAAAJ&hl=en
    homepage: https://www.cs.cmu.edu/~jielinq/
    institution: School of Computer Science, Carnegie Mellon University
    last_name: Qiu
    name: Jielin Qiu
    semantic_scholar_id: https://www.semanticscholar.org/author/Jielin-Qiu/51459833
    username: ~Jielin_Qiu2
  - dblp_id: https://dblp.org/pid/87/8693
    emails: '****@andrew.cmu.edu'
    first_name: Mengdi
    google_scholar_id: https://scholar.google.com/citations?hl=zh-CN&user=A1yMyYEAAAAJ
    homepage: https://mxu34.github.io/
    institution: Carnegie Mellon University
    last_name: Xu
    name: Mengdi Xu
    username: ~Mengdi_Xu3
  - dblp_id: https://dblp.org/pid/326/6668
    emails: '****@outlook.com'
    first_name: William
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=YcoJfMMAAAAJ
    homepage: https://willxxy.github.io/
    last_name: Han
    name: William Han
    username: ~William_Han1
  - dblp_id: https://dblp.org/pid/120/4131
    emails: '****@shanemoon.com'
    first_name: Seungwhan
    google_scholar_id: https://scholar.google.com/citations?user=HJTLcDsAAAAJ
    homepage: https://shanemoon.com
    institution: Facebook
    last_name: Moon
    name: Seungwhan Moon
    semantic_scholar_id: https://www.semanticscholar.org/author/Seungwhan-Moon/29072828
    username: ~Seungwhan_Moon1
  - emails: '****@cmu.edu'
    first_name: Ding
    google_scholar_id: https://scholar.google.com/citations?user=z7tPc9IAAAAJ&hl=en
    homepage: https://safeai-lab.github.io
    institution: Carnegie Mellon University
    last_name: Zhao
    name: Ding Zhao
    username: ~Ding_Zhao1
  decision: toMainConference
  end_page: 2035
  file: 187.pdf
  id: 187
  num_pages: 18
  openreview_id: ymnVoH1cgV
  pdf_file: 515497e92b70cba7f35ed0c2fd54dfb4c849091d.pdf
  start_page: 2018
  title: Embodied Executable Policy Learning with Language-based Scene Summarization
- abstract: 'In Large Language Models (LLMs), there have been consistent advancements
    in task-specific performance, largely influenced by effective prompt design. Recent
    advancements in prompting have enhanced reasoning in logic-intensive tasks for
    LLMs, yet the nuanced understanding abilities of these models, crucial for processing
    and interpreting complex information, remain underexplored. In this study, we
    introduce Metacognitive Prompting (MP), a strategy inspired by human introspective
    reasoning processes. Using MP, LLMs undergo a systematic series of structured,
    self-aware evaluations, drawing on both their vast inherent knowledge and new
    insights. We conduct extensive experiments on four prevalent LLMs: Llama2, PaLM2,
    GPT-3.5, and GPT-4, across ten natural language understanding (NLU) datasets from
    GLUE, SuperGLUE, BLUE, and LexGLUE benchmarks. Additionally, we compare our method
    with chain-of-thought prompting and its advanced versions. The results show that
    GPT-4 consistently excels across all tasks, while other models have shown significant
    progress in some tasks when used in conjunction with MP. Furthermore, MP consistently
    outperforms existing prompting methods in both general and domain-specific NLU
    tasks. This study underscores the potential to amplify the understanding abilities
    of LLMs and highlights the benefits of mirroring human introspective reasoning
    in NLU tasks.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Learning for NLP
  authors:
  - emails: '****@ucsb.edu'
    first_name: Yuqing
    google_scholar_id: https://scholar.google.com/citations?user=DHImZjIAAAAJ&hl=en&authuser=2
    homepage: https://yuqingwangcs.github.io/
    institution: Stanford University
    last_name: Wang
    name: Yuqing Wang
    semantic_scholar_id: https://www.semanticscholar.org/author/Yuqing-Wang/2108035197
    username: ~Yuqing_Wang5
  - dblp_id: https://dblp.org/pid/42/2862-1
    emails: '****@meta.com'
    first_name: Yun
    google_scholar_id: https://scholar.google.com/citations?user=30s9RtsAAAAJ&hl=en
    homepage: https://yunzhaocs.github.io/
    last_name: Zhao
    name: Yun Zhao
    orcid: https://orcid.org/0000-0002-5544-8983
    semantic_scholar_id: https://www.semanticscholar.org/author/Yun-Zhao/46317573
    username: ~Yun_Zhao1
  decision: toMainConference
  end_page: 2048
  file: 188.pdf
  id: 188
  num_pages: 13
  openreview_id: EG9Leo5JO1
  pdf_file: 195b9eb94b5f8095f20786d46f1ca9997055e5eb.pdf
  start_page: 2036
  title: Metacognitive Prompting Improves Understanding in Large Language Models
- abstract: "Red-teaming is a common practice for mitigating unsafe behaviors in Large\
    \ Language Models (LLMs), which involves thoroughly assessing LLMs to identify\
    \ potential flaws and addressing them with responsible and accurate responses.\n\
    While effective, manual red-teaming is costly, and existing automatic red-teaming\
    \ typically discovers safety risks without addressing them.\nIn this paper, we\
    \ propose a Multi-round Automatic Red-Teaming (MART) method, which incorporates\
    \ both automatic adversarial prompt writing and safe response generation, significantly\
    \ increasing red-teaming scalability and the safety of the target LLM.\nSpecifically,\
    \ an adversarial LLM and a target LLM interplay with each other in an iterative\
    \ manner, where the adversarial LLM aims to generate challenging prompts that\
    \ elicit unsafe responses from the target LLM, while the target LLM is fine-tuned\
    \ with safety aligned data on these adversarial prompts. \nIn each round, the\
    \ adversarial LLM crafts better attacks on the updated target LLM, while the target\
    \ LLM also improves itself through safety fine-tuning.\nOn adversarial prompt\
    \ benchmarks, the violation rate of an LLM with limited safety alignment reduces\
    \ up to 84.7% after 4 rounds of MART, achieving comparable performance to LLMs\
    \ with extensive adversarial prompt writing. \nNotably, model helpfulness on non-adversarial\
    \ prompts remains stable throughout iterations, indicating the target LLM maintains\
    \ strong performance on instruction following."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Ethics, Bias, and Fairness
  authors:
  - dblp_id: https://dblp.org/pid/242/4765
    emails: '****@illinois.edu'
    first_name: Suyu
    google_scholar_id: https://scholar.google.com/citations?user=WuCV72gAAAAJ&hl=en
    homepage: https://gesy17.github.io/
    institution: Meta
    last_name: Ge
    name: Suyu Ge
    username: ~Suyu_Ge1
  - dblp_id: https://dblp.org/pid/161/2679
    emails: '****@gmail.com'
    first_name: Chunting
    google_scholar_id: https://scholar.google.com/citations?user=mR5W7EgAAAAJ&hl=en
    homepage: https://violet-zct.github.io/
    institution: Meta AI
    last_name: Zhou
    name: Chunting Zhou
    username: ~Chunting_Zhou1
  - emails: '****@meta.com'
    first_name: Rui
    institution: 'Meta Inc. '
    last_name: Hou
    name: Rui Hou
    username: ~Rui_Hou3
  - dblp_id: https://dblp.org/pid/87/11087
    emails: '****@madiankhabsa.com'
    first_name: Madian
    google_scholar_id: https://scholar.google.com/citations?user=V9JYPP0AAAAJ&hl=en
    homepage: https://www.madiankhabsa.com
    institution: Facebook
    last_name: Khabsa
    name: Madian Khabsa
    username: ~Madian_Khabsa1
  - emails: '****@fb.com'
    first_name: Yi-Chia
    google_scholar_id: https://scholar.google.com/citations?user=9gMgFPQAAAAJ&hl=en
    institution: Meta
    last_name: Wang
    name: Yi-Chia Wang
    username: ~Yi-Chia_Wang2
  - dblp_id: https://dblp.org/pid/33/8610
    emails: '****@fb.com'
    first_name: Qifan
    google_scholar_id: https://scholar.google.com/citations?user=LrSyLosAAAAJ&hl=en
    homepage: https://wqfcr.github.io/
    institution: Meta AI
    last_name: Wang
    name: Qifan Wang
    username: ~Qifan_Wang1
  - dblp_id: https://dblp.org/pid/h/JiaweiHan.html
    emails: '****@cs.uiuc.edu'
    first_name: Jiawei
    google_scholar_id: https://scholar.google.com.tw/citations?user=Kv9AbjMAAAAJ
    homepage: http://hanj.cs.illinois.edu/
    last_name: Han
    name: Jiawei Han
    semantic_scholar_id: https://www.semanticscholar.org/author/Jiawei-Han/145325584
    username: ~Jiawei_Han1
  - dblp_id: https://dblp.org/pid/178/3692
    emails: '****@gmail.com'
    first_name: Yuning
    google_scholar_id: https://scholar.google.com/citations?user=steJe6IAAAAJ
    homepage: https://morningmoni.github.io/
    institution: Meta
    last_name: Mao
    name: Yuning Mao
    semantic_scholar_id: https://www.semanticscholar.org/author/Yuning-Mao/3375249
    username: ~Yuning_Mao1
  decision: toMainConference
  end_page: 2059
  file: 191.pdf
  id: 191
  num_pages: 11
  openreview_id: zwI7AiHm62
  pdf_file: be5051e4b5e018d8383086a91a6742ed252363e5.pdf
  start_page: 2049
  title: 'MART: Improving LLM Safety with Multi-round Automatic Red-Teaming'
- abstract: "As sharing images in an instant message is a crucial factor, there has\
    \ been active research on learning an image-text multi-modal dialogue models.\n\
    However, training a well-generalized multi-modal dialogue model remains challenging\
    \ due to the low quality and limited diversity of images per dialogue in existing\
    \ multi-modal dialogue datasets.\nIn this paper, we propose an automated pipeline\
    \ to construct a multi-modal dialogue dataset, ensuring both dialogue quality\
    \ and image diversity without requiring minimum human effort. \nIn our pipeline,\
    \ to guarantee the coherence between images and dialogue, we prompt GPT-4 to infer\
    \ potential image-sharing moments - specifically, the utterance, speaker, rationale,\
    \ and image description. \nFurthermore, we leverage CLIP similarity to maintain\
    \ consistency between aligned multiple images to the utterance.\nThrough this\
    \ pipeline, we introduce DialogCC, a high-quality and diverse multi-modal dialogue\
    \ dataset that surpasses existing datasets in terms of quality and diversity in\
    \ human evaluation.\nOur comprehensive experiments highlight that when multi-modal\
    \ dialogue models are trained using our dataset, their generalization performance\
    \ on unseen dialogue datasets is significantly enhanced. We make our source code\
    \ and dataset publicly available (https://dialogcc.github.io/)."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Dialogue and Interactive Systems
  authors:
  - dblp_id: https://dblp.org/pid/52/5671
    emails: '****@gmail.com'
    first_name: Young-Jun
    google_scholar_id: https://scholar.google.com/citations?user=8EgjKPUAAAAJ&hl=ko
    homepage: https://sites.google.com/view/passing2961/home
    institution: Korea Advanced Institute of Science & Technology
    last_name: Lee
    name: Young-Jun Lee
    semantic_scholar_id: https://www.semanticscholar.org/author/Young-Jun-Lee/2119566888
    username: ~Young-Jun_Lee1
  - emails: '****@gmail.com'
    first_name: Byungsoo
    google_scholar_id: https://scholar.google.co.kr/citations?user=verZ0N4AAAAJ&hl=en
    institution: NAVER
    last_name: Ko
    name: Byungsoo Ko
    username: ~Byungsoo_Ko1
  - dblp_id: https://dblp.org/pid/123/2302
    emails: '****@navercorp.com'
    first_name: Han-Gyu
    google_scholar_id: https://scholar.google.com/citations?user=5iakAv0AAAAJ&hl=ko&oi=ao
    last_name: Kim
    name: Han-Gyu Kim
    orcid: https://orcid.org/0000-0003-2684-1409
    username: ~Han-Gyu_Kim1
  - emails: '****@kaist.ac.kr'
    first_name: Jonghwan
    homepage: https://jonghwan.com
    institution: KAIST
    last_name: Hyeon
    name: Jonghwan Hyeon
    username: ~Jonghwan_Hyeon1
  - dblp_id: https://dblp.org/pid/60/1227
    emails: '****@kaist.ac.kr'
    first_name: Ho-Jin
    google_scholar_id: https://scholar.google.co.kr/citations?hl=en&user=fwAarOoAAAAJ
    institution: Korea Advanced Institute of Science & Technology
    last_name: Choi
    name: Ho-Jin Choi
    semantic_scholar_id: https://www.semanticscholar.org/author/Ho-Jin-Choi/145530103
    username: ~Ho-Jin_Choi1
  decision: toMainConference
  end_page: 2085
  file: 192.pdf
  id: 192
  num_pages: 26
  openreview_id: s6EFN6SWwP
  pdf_file: 1676a0e6f908d9555c4cd9dcadc640f3f1fa895e.pdf
  start_page: 2060
  title: 'DialogCC: An Automated Pipeline for Creating High-Quality Multi-Modal Dialogue
    Dataset'
- abstract: The complementary potential of Large Language Models (LLM) assumes off-the-shelf
    LLMs have heterogeneous expertise in a wide range of domains and tasks so that
    an ensemble of LLMs can achieve consistently better performance. Existing ensemble
    methods for LLMs mainly focus on reward model ranking of outputs, leading to significant
    computation overhead. To combat this issue, we revisit the complementary potential
    of LLMs and further elaborate on it by mining latent expertise with off-the-shelf
    reward models. We propose ZOOTER, a reward-guided routing method distilling rewards
    on training queries to train a routing function, which can precisely distribute
    each query to the LLM with expertise about it. We also integrate a tag-based label
    enhancement to mitigate noise from uncertainty when using rewards as silver supervision.
    ZOOTER shows computation efficiency in inference as it only introduces minor computation
    overhead of a routing function compared with reward model ranking methods. We
    evaluate ZOOTER on a comprehensive benchmark collection with 26 subsets in different
    domains and tasks. ZOOTER outperforms the best single model on average and ranks
    first on 44% of tasks, even surpassing multiple reward model ranking methods.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Dialogue and Interactive Systems
  authors:
  - dblp_id: https://dblp.org/pid/65/6898.html
    emails: '****@usc.edu'
    first_name: Keming
    google_scholar_id: https://scholar.google.com/citations?user=WuD2op4AAAAJ&hl=en&oi=ao
    last_name: Lu
    name: Keming Lu
    semantic_scholar_id: https://www.semanticscholar.org/author/K.-Lu/1515662094
    username: ~Keming_Lu1
  - dblp_id: https://dblp.org/pid/308/0909
    emails: '****@mails.tsinghua.edu.cn'
    first_name: Hongyi
    google_scholar_id: https://scholar.google.com/citations?user=FG3O4i8AAAAJ
    last_name: Yuan
    name: Hongyi Yuan
    semantic_scholar_id: https://www.semanticscholar.org/author/Hongyi-Yuan/2114128654
    username: ~Hongyi_Yuan1
  - emails: '****@ia.ac.cn'
    first_name: Runji
    homepage: https://linprophet.github.io/
    last_name: Lin
    name: Runji Lin
    username: ~Runji_Lin1
  - dblp_id: https://dblp.org/pid/215/3823
    emails: '****@alibaba-inc.com'
    first_name: Junyang
    google_scholar_id: https://scholar.google.com/citations?user=qp6IwtgAAAAJ&hl=zh-CN
    last_name: Lin
    name: Junyang Lin
    username: ~Junyang_Lin1
  - dblp_id: https://dblp.org/pid/56/2877-2
    emails: '****@qq.com'
    first_name: Zheng
    google_scholar_id: https://scholar.google.com/citations?hl=zh-CN&user=kRgiVnUAAAAJ
    institution: Alibaba Group
    last_name: Yuan
    name: Zheng Yuan
    semantic_scholar_id: https://www.semanticscholar.org/author/Zheng-Yuan/2112340945
    username: ~Zheng_Yuan2
  - emails: '****@alibaba-inc.com'
    first_name: Chang
    google_scholar_id: https://scholar.google.com/citations?user=QeSoG3sAAAAJ&hl=zh-CN
    last_name: Zhou
    name: Chang Zhou
    username: ~Chang_Zhou2
  - dblp_id: https://dblp.org/pid/84/2644
    emails: '****@alibaba-inc.com'
    first_name: Jingren
    institution: Alibaba Group
    last_name: Zhou
    name: Jingren Zhou
    username: ~Jingren_Zhou1
  decision: toMainConference
  end_page: 2096
  file: 194.pdf
  id: 194
  num_pages: 11
  openreview_id: bl06WerPni
  pdf_file: 4dff299e9899ed4c5578f557ab2464f2f5b39477.pdf
  start_page: 2086
  title: 'Routing to the Expert: Efficient Reward-guided Ensemble of Large Language
    Models'
- abstract: In an era of model and data proliferation in machine learning/AI especially
    marked by the rapid advancement of open-sourced technologies, there arises a critical
    need for standardized consistent documentation. Our work addresses the information
    incompleteness in current human-written model and data cards. We propose an automated
    generation approach using Large Language Models (LLMs). Our key contributions
    include the establishment of CardBench, a comprehensive dataset aggregated from
    over 4.8k model cards and 1.4k data cards, coupled with the development of the
    CardGen pipeline comprising a two-step retrieval process. Our approach exhibits
    enhanced completeness, objectivity, and faithfulness in generated model and data
    cards, a significant step in responsible AI documentation practices ensuring better
    accountability and traceability.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Ethics, Bias, and Fairness
  authors:
  - dblp_id: https://dblp.org/pid/134/1248-4.html
    emails: '****@cs.cmu.edu'
    first_name: Jiarui
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=4y_7ImsAAAAJ
    homepage: https://jiarui-liu.github.io/
    last_name: Liu
    name: Jiarui Liu
    semantic_scholar_id: https://www.semanticscholar.org/author/Jiarui-Liu/146961917
    username: ~Jiarui_Liu1
  - emails: '****@andrew.cmu.edu'
    first_name: Wenkai
    homepage: https://wenkaili.com
    last_name: Li
    name: Wenkai Li
    username: ~Wenkai_Li2
  - dblp_id: https://dblp.org/pid/229/4267
    emails: '****@umich.edu'
    first_name: Zhijing
    google_scholar_id: https://scholar.google.com/citations?user=RkI8h-wAAAAJ
    homepage: https://zhijing-jin.com
    last_name: Jin
    name: Zhijing Jin
    orcid: https://orcid.org/0000-0003-0238-9024
    semantic_scholar_id: https://www.semanticscholar.org/author/Zhijing-Jin/8752221
    username: ~Zhijing_Jin1
  - dblp_id: https://dblp.org/pid/15/4305
    emails: '****@gmail.com'
    first_name: Mona
    google_scholar_id: https://scholar.google.com.tw/citations?user=-y6SIhQAAAAJ
    homepage: https://www.seas.gwu.edu/~mtdiab/
    institution: Carnegie Mellon University and George Washington University
    last_name: Diab
    middle_name: T.
    name: Mona T. Diab
    username: ~Mona_T._Diab1
  decision: toMainConference
  end_page: 2119
  file: 196.pdf
  id: 196
  num_pages: 23
  openreview_id: FXXMsaOaP0
  pdf_file: 2f8bc314c936c858c7989ec356342af6fedf91c0.pdf
  start_page: 2097
  title: 'Automatic Generation of Model and Data Cards: A Step Towards Responsible
    AI'
- abstract: Standard fine-tuning of language models typically performs well on $\textit{in-distribution
    data}$, but suffers with generalization to $\textit{distribution shifts}$. In
    this work, we aim to improve the generalization of adapter-based cross-lingual
    task transfer where such cross-language distribution shifts are imminent. We investigate
    scheduled unfreezing algorithms --originally proposed to mitigate catastrophic
    forgetting in transfer learning -- for fine-tuning task adapters. Our experiments
    show that scheduled unfreezing methods close the gap to full fine-tuning and achieve
    stronger cross-lingual transfer performance, suggesting that these methods can
    go beyond just mitigating catastrophic forgetting. Next, aiming to understand
    these empirical findings, we investigate the learning dynamics of scheduled unfreezing
    using Fisher Information. Our experiments reveal that scheduled unfreezing induces
    different learning dynamics compared to standard fine-tuning, and provide evidence
    that the dynamics of Fisher Information during training correlate with cross-lingual
    generalization performance. We additionally propose a general scheduled unfreezing
    algorithm that achieves an average of 2 points improvement over four datasets
    compared to standard fine-tuning and provides empirical evidence for a theory-based
    justification of the heuristic unfreezing schedule for task adapter training.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multilinguality and Language Diversity
  authors:
  - dblp_id: https://dblp.org/pid/337/9716
    emails: '****@gmail.com'
    first_name: Chen
    google_scholar_id: https://scholar.google.ca/citations?user=eNFB3fgAAAAJ&hl=en
    homepage: https://ccliu2.github.io/
    last_name: Liu
    middle_name: Cecilia
    name: Chen Cecilia Liu
    semantic_scholar_id: https://www.semanticscholar.org/author/Chen-Liu/143876027
    username: ~Chen_Cecilia_Liu1
  - emails: '****@gmail.com'
    first_name: Jonas
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=gGB0L4kAAAAJ
    homepage: https://pfeiffer.ai
    institution: Google
    last_name: Pfeiffer
    name: Jonas Pfeiffer
    semantic_scholar_id: https://www.semanticscholar.org/author/Jonas-Pfeiffer/153733568
    username: ~Jonas_Pfeiffer1
  - dblp_id: https://dblp.org/pid/77/9768
    emails: '****@cam.ac.uk'
    first_name: Ivan
    google_scholar_id: https://scholar.google.com/citations?user=ZX8js60AAAAJ&hl=en
    homepage: https://sites.google.com/site/ivanvulic/
    institution: University of Cambridge and PolyAI Limited
    last_name: "Vuli\u0107"
    name: "Ivan Vuli\u0107"
    semantic_scholar_id: https://www.semanticscholar.org/author/Ivan-Vulic/1747849
    username: "~Ivan_Vuli\u01071"
  - dblp_id: https://dblp.org/pid/85/6201
    emails: '****@tu-darmstadt.de'
    first_name: Iryna
    google_scholar_id: https://scholar.google.com.tw/citations?user=t3A39e8AAAAJ
    homepage: https://www.informatik.tu-darmstadt.de/ukp/ukp_home/staff_ukp/prof_dr_iryna_gurevych/index.en.jsp
    institution: Mohamed bin Zayed University of Artificial Intelligence and Technical
      University of Darmstadt
    last_name: Gurevych
    name: Iryna Gurevych
    username: ~Iryna_Gurevych1
  decision: toMainConference
  end_page: 2137
  file: 197.pdf
  id: 197
  num_pages: 18
  openreview_id: uwSPnQ9Vxc
  pdf_file: c950adb2e648855de3ee0035763b96b64bbf73f9.pdf
  start_page: 2120
  title: 'FUN with Fisher: Improving Generalization of Adapter-Based Cross-lingual
    Transfer with Scheduled Unfreezing'
- abstract: 'Large language models (LLMs) are highly adept at question answering and
    reasoning tasks, but when reasoning in a situational context, human expectations
    vary depending on the relevant cultural common ground. As languages are associated
    with diverse cultures, LLMs should also be culturally-diverse reasoners. In this
    paper, we study the ability of a wide range of state-of-the-art multilingual LLMs
    (mLLMs) to reason with proverbs and sayings in a conversational context. Our experiments
    reveal that: (1) mLLMs "know" limited proverbs and memorizing proverbs does not
    mean understanding them within a conversational context; (2) mLLMs struggle to
    reason with figurative proverbs and sayings, and when asked to select the wrong
    answer (instead of asking it to select the correct answer); and (3) there is a
    "culture gap" in mLLMs when reasoning about proverbs and sayings translated from
    other languages. We construct and release our evaluation dataset MAPS (MulticulturAl
    Proverbs and Sayings) for proverb understanding with conversational context for
    six different languages.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multilinguality and Language Diversity
  authors:
  - dblp_id: https://dblp.org/pid/337/9716
    emails: '****@gmail.com'
    first_name: Chen
    google_scholar_id: https://scholar.google.ca/citations?user=eNFB3fgAAAAJ&hl=en
    homepage: https://ccliu2.github.io/
    last_name: Liu
    middle_name: Cecilia
    name: Chen Cecilia Liu
    semantic_scholar_id: https://www.semanticscholar.org/author/Chen-Liu/143876027
    username: ~Chen_Cecilia_Liu1
  - dblp_id: https://dblp.org/pid/160/0019
    emails: '****@mbzuai.ac.ae'
    first_name: Fajri
    google_scholar_id: https://scholar.google.com/citations?user=RA9l3s4AAAAJ
    homepage: http://fajrikoto.com/
    institution: Mohamed bin Zayed University of Artificial Intelligence
    last_name: Koto
    name: Fajri Koto
    semantic_scholar_id: https://www.semanticscholar.org/author/Fajri-Koto/2789148
    username: ~Fajri_Koto1
  - dblp_id: https://dblp.org/pid/65/4863
    emails: '****@ldwin.net'
    first_name: Timothy
    google_scholar_id: https://scholar.google.com/citations?user=wjBD1dkAAAAJ&hl=en
    institution: Mohamed bin Zayed University of Artificial Intelligence and The University
      of Melbourne
    last_name: Baldwin
    name: Timothy Baldwin
    username: ~Timothy_Baldwin1
  - dblp_id: https://dblp.org/pid/85/6201
    emails: '****@tu-darmstadt.de'
    first_name: Iryna
    google_scholar_id: https://scholar.google.com.tw/citations?user=t3A39e8AAAAJ
    homepage: https://www.informatik.tu-darmstadt.de/ukp/ukp_home/staff_ukp/prof_dr_iryna_gurevych/index.en.jsp
    institution: Mohamed bin Zayed University of Artificial Intelligence and Technical
      University of Darmstadt
    last_name: Gurevych
    name: Iryna Gurevych
    username: ~Iryna_Gurevych1
  decision: toMainConference
  end_page: 2161
  file: 198.pdf
  id: 198
  num_pages: 24
  openreview_id: L1axwfC9ko
  pdf_file: 9b404a785a5c0776a7922b51f898721e32af27df.pdf
  start_page: 2138
  title: Are Multilingual LLMs Culturally-Diverse Reasoners? An Investigation into
    Multicultural Proverbs and Sayings
- abstract: "Queer youth face increased mental health risks, such as depression, anxiety,\
    \ and suicidal ideation. Hindered by negative stigma, they often avoid seeking\
    \ help and rely on online resources, which may provide incompatible information.\
    \ Although access to a supportive environment and reliable information is invaluable,\
    \ many queer youth worldwide have no access to such support. However, this could\
    \ soon change due to the rapid adoption of Large Language Models (LLMs) such as\
    \ ChatGPT. This paper aims to comprehensively explore the potential of LLMs to\
    \ revolutionize emotional support for queers. \nTo this end, we conduct a qualitative\
    \ and quantitative analysis of LLM's interactions with queer-related content.\
    \ To evaluate response quality, we develop a novel ten-question scale that is\
    \ inspired by psychological standards and expert input. We apply this scale to\
    \ score several LLMs and human comments to posts where queer youth seek advice\
    \ and share experiences. \nWe find that LLM responses are supportive and inclusive,\
    \ outscoring humans. However, they tend to be generic, not empathetic enough,\
    \ and lack personalization, resulting in nonreliable and potentially harmful advice.\
    \ \nWe discuss these challenges, demonstrate that a dedicated prompt can improve\
    \ the performance, and propose a blueprint of an LLM-supporter that actively (but\
    \ sensitively) seeks user context to provide personalized, empathetic, and reliable\
    \ responses. \nOur annotated dataset is available for further research.\n\n*https://github.com/nitaytech/LGBTeenDataset"
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Computational Social Science and Cultural Analytics
  authors:
  - emails: '****@gmail.com'
    first_name: Shir
    homepage: https://www.linkedin.com/in/shir-lissak-012174166
    institution: Technion - Israel Institute of Technology, Technion
    last_name: Lissak
    name: Shir Lissak
    username: ~Shir_Lissak1
  - dblp_id: https://dblp.org/pid/305/0373
    emails: '****@gmail.com'
    first_name: Nitay
    google_scholar_id: https://scholar.google.com/citations?user=_1qnAh4AAAAJ&hl=iw
    homepage: https://nitaytech.github.io/
    institution: Technion - Israel Institute of Technology
    last_name: Calderon
    name: Nitay Calderon
    semantic_scholar_id: https://www.semanticscholar.org/author/2135910736
    username: ~Nitay_Calderon1
  - emails: '****@runi.ac.il'
    first_name: Geva
    google_scholar_id: https://scholar.google.com/citations?hl=iw&user=_t1b1swAAAAJ
    homepage: https://www.runi.ac.il/en/faculty/gshenkman/
    institution: 'Reichman University '
    last_name: Shenkman
    name: Geva Shenkman
    username: ~Geva_Shenkman1
  - emails: '****@gmail.com'
    first_name: Yaakov
    homepage: http://www.yophir.com
    last_name: Ophir
    name: Yaakov Ophir
    username: ~Yaakov_Ophir1
  - emails: '****@gmail.com'
    first_name: Eyal
    homepage: https://www.rambam.org.il/en/leadership-team/?itemid=%7B08c489ec-ba3b-4a6b-91bb-b5c9de6dee3d%7D
    institution: Technion - Israel Institute of Technology, Technion
    last_name: Fruchter
    name: Eyal Fruchter
    username: ~Eyal_Fruchter1
  - emails: '****@gmail.com'
    first_name: Anat
    homepage: https://www.runi.ac.il/en/faculty/bkanat/
    institution: Reichman
    last_name: Brunstein Klomek
    name: Anat Brunstein Klomek
    username: ~Anat_Brunstein_Klomek1
  - dblp_id: https://dblp.org/pid/96/5429
    emails: '****@gmail.com'
    first_name: Roi
    google_scholar_id: https://scholar.google.co.il/citations?user=xXJIsh4AAAAJ&hl=iw
    homepage: https://ie.technion.ac.il/~roiri/
    institution: Technion, Israel Institute of Technology
    last_name: Reichart
    name: Roi Reichart
    semantic_scholar_id: https://www.semanticscholar.org/search?q=roi%20reichart&sort=relevance
    username: ~Roi_Reichart1
  decision: toMainConference
  end_page: 2201
  file: 200.pdf
  id: 200
  num_pages: 40
  openreview_id: affbXCEM9b
  pdf_file: 407e7273cc108ec4169d8c7f6a8170519013ba9a.pdf
  start_page: 2162
  title: 'The Colorful Future of LLMs: Evaluating and Improving LLMs as Emotional
    Supporters for Queer Youth'
- abstract: Relational triple extraction is a fundamental task in the field of information
    extraction, and a promising framework based on table filling has recently gained
    attention as a potential baseline for entity relation extraction. However, inherent
    shortcomings such as redundant information and incomplete triple recognition remain
    problematic. To address these challenges, we propose an Implicit Perspective for
    relational triple Extraction based on Diffusion model (IPED), an innovative approach
    for extracting relational triples. Our classifier-free solution adopts an implicit
    strategy using block coverage to complete the tables, avoiding the limitations
    of explicit tagging methods. Additionally, we introduce a generative model structure,
    the block-denoising diffusion model, to collaborate with our implicit perspective
    and effectively circumvent redundant information disruptions. Experimental results
    on two popular datasets demonstrate that IPED achieves state-of-the-art performance
    while gaining superior inference speed and low computational complexity. To support
    future research, we have made our source code publicly available online.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Extraction
  authors:
  - emails: '****@mail.sdu.edu.cn'
    first_name: Jianli
    homepage: https://github.com/girlsuuu
    last_name: Zhao
    name: Jianli Zhao
    username: ~Jianli_Zhao1
  - emails: '****@gmail.com'
    first_name: Changhao
    homepage: https://github.com/Changhao-X
    last_name: Xu
    name: Changhao Xu
    username: ~Changhao_Xu2
  - emails: '****@sdu.edu.cn'
    first_name: Bin.
    homepage: https://faculty.sdu.edu.cn/jiangbin/en/index.htm
    last_name: Jiang
    name: Bin. Jiang
    orcid: https://orcid.org/0000-0002-2897-5745
    username: ~Bin._Jiang1
  decision: toMainConference
  end_page: 2214
  file: 201.pdf
  id: 201
  num_pages: 13
  openreview_id: vFTGVh9W5a
  pdf_file: 18863c61a8ddb0afbaef74cf6d78a038c9366b02.pdf
  start_page: 2202
  title: 'IPED: An Implicit Perspective for Relational Triple Extraction based on
    Diffusion Model'
- abstract: 'Quantitative evaluation metrics have been pivotal in gauging the advancements
    of AI systems like large language models (LLMs).

    However, due to the intricate nature of real-world tasks, a single scalar to quantify
    and compare performance trivializes the fine-grained nuances of model behavior.
    Additionally, metrics do not yield actionable diagnostics for model improvement,
    thus requiring extensive manual efforts of scientists, involving sifting through
    vast datasets and attempting hit-or-miss adjustments to training data or setups.
    In this work, we address the shortcomings of quantitative metrics by proposing
    QualEval, which uses automated qualitative evaluation as a vehicle for model improvement.
    QualEval uses a powerful LLM reasoner and our novel flexible linear programming
    solver to generate human-readable insights that when applied, accelerate model
    improvement. The insights are supported by a dashboard report with fine-grained
    visualizations and human-interpretable analyses. We corroborate the faithfulness
    of QualEval by demonstrating that leveraging its insights, for example, improves
    the absolute performance of the Llama 2 model by up to 15\% points relative on
    a challenging dialogue task (DialogSum) when compared to baselines. QualEval successfully
    increases the pace and quality of model development by eliminating the need of
    arduous manual analysis, thus serving as a data-scientist-in-a-box.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - emails: '****@princeton.edu'
    first_name: Vishvak
    google_scholar_id: https://scholar.google.com/citations?user=Y_NYX7MAAAAJ&hl=en&oi=sra
    homepage: https://vishvakmurahari.com/
    institution: Princeton University
    last_name: Murahari
    name: Vishvak Murahari
    username: ~Vishvak_Murahari1
  - dblp_id: https://dblp.org/pid/220/4337
    emails: '****@princeton.edu'
    first_name: Ameet
    google_scholar_id: https://scholar.google.com/citations?user=332L1coAAAAJ&hl=en
    homepage: https://ameet-1997.github.io
    last_name: Deshpande
    name: Ameet Deshpande
    semantic_scholar_id: https://www.semanticscholar.org/author/Ameet-Deshpande/33341943
    username: ~Ameet_Deshpande1
  - dblp_id: https://dblp.org/pid/34/1184
    emails: '****@allenai.org'
    first_name: Peter
    institution: Allen Institute for Artificial Intelligence
    last_name: Clark
    name: Peter Clark
    username: ~Peter_Clark1
  - emails: '****@gmail.com'
    first_name: Tanmay
    google_scholar_id: https://scholar.google.com/citations?user=B4NztA8AAAAJ&hl=en
    last_name: Rajpurohit
    name: Tanmay Rajpurohit
    username: ~Tanmay_Rajpurohit1
  - dblp_id: http://dblp.uni-trier.de/pers/hd/s/Sabharwal:Ashish
    emails: '****@allenai.org'
    first_name: Ashish
    google_scholar_id: https://scholar.google.com/citations?user=7VspfeAAAAAJ&hl=en
    homepage: http://allenai.org/team/ashishs
    institution: Allen Institute for Artificial Intelligence
    last_name: Sabharwal
    name: Ashish Sabharwal
    semantic_scholar_id: https://www.semanticscholar.org/author/Ashish-Sabharwal/48229640
    username: ~Ashish_Sabharwal1
  - dblp_id: https://dblp.org/pid/147/0322
    emails: '****@princeton.edu'
    first_name: Karthik
    google_scholar_id: https://scholar.google.com/citations?user=euc0GX4AAAAJ&hl=en
    homepage: http://www.karthiknarasimhan.com
    institution: Princeton University
    last_name: Narasimhan
    middle_name: R
    name: Karthik R Narasimhan
    semantic_scholar_id: https://www.semanticscholar.org/author/Karthik-Narasimhan/144958935
    username: ~Karthik_R_Narasimhan1
  - dblp_id: https://dblp.org/pid/173/5217
    emails: '****@gmail.com'
    first_name: Ashwin
    google_scholar_id: https://scholar.google.com/citations?user=KYHL9aIAAAAJ&hl=en
    homepage: http://ashwinkalyan.com/
    institution: Allen Institute for Artificial Intelligence
    last_name: Kalyan
    name: Ashwin Kalyan
    username: ~Ashwin_Kalyan6
  decision: toMainConference
  end_page: 2233
  file: 202.pdf
  id: 202
  num_pages: 19
  openreview_id: nZng5xeb9s
  pdf_file: 5f5e707b5e0c6c954d506d2668a7a367140776bd.pdf
  start_page: 2215
  title: 'QualEval: Qualitative Evaluation for Model Improvement'
- abstract: Quantum-inspired models have demonstrated superior performance in many
    downstream language tasks, such as question answering and sentiment analysis.
    However, recent models primarily focus on embedding and measurement operations,
    overlooking the significance of the quantum evolution process. In this work, we
    present a novel quantum-inspired neural network, LI-QiLM, which integrates the
    Lindblad Master Equation (LME) to model the evolution process and the interferometry
    to the measurement process, providing more physical meaning to strengthen the
    interpretability. We conduct comprehensive experiments on six sentiment analysis
    datasets. Compared to the traditional neural networks, transformer-based pre-trained
    models and quantum-inspired models, such as CICWE-QNN and ComplexQNN, the proposed
    method demonstrates superior performance in accuracy and F1-score on six commonly
    used datasets for sentiment analysis. Additional ablation tests verify the effectiveness
    of LME and interferometry.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Sentiment Analysis, Stylistic Analysis, and Argument Mining
  authors:
  - emails: '****@qq.com'
    first_name: Kehuan
    homepage: https://gitee.com/yan-kefan
    last_name: Yan
    name: Kehuan Yan
    username: ~Kehuan_Yan1
  - emails: '****@qq.com'
    first_name: Peichao
    homepage: https://creatorsn.com/team
    last_name: Lai
    name: Peichao Lai
    orcid: https://orcid.org/0000-0002-6936-5687
    semantic_scholar_id: https://www.semanticscholar.org/author/Peichao-Lai/2187454266
    username: ~Peichao_Lai1
  - emails: '****@fzu.edu.cn'
    first_name: Yilei
    last_name: Wang
    name: yilei wang
    orcid: https://orcid.org/0000-0003-1826-6707
    username: ~yilei_wang1
  decision: toMainConference
  end_page: 2243
  file: 203.pdf
  id: 203
  num_pages: 10
  openreview_id: LweKSk6ENe
  pdf_file: cd780a196461dac5dc45483f243ca69ab1798119.pdf
  start_page: 2234
  title: Quantum-inspired Language Model with Lindblad Master Equation and Interference
    Measurement for Sentiment Analysis
- abstract: "Text detoxification is a textual style transfer (TST) task where a text\
    \ is paraphrased from a toxic surface form, e.g. featuring rude words, to the\
    \ neutral register. Recently, text detoxification methods found their applications\
    \ in various task such as detoxification of Large Language Models (LLMs) (Leong\
    \ et al., 2023; He et al., 2024; Tang et al., 2023) and toxic speech combating\
    \ in social networks (Deng et al., 2023; Mun et al., 2023; Agarwal et al., 2023).\
    \ All these applications are extremely important to ensure safe communication\
    \ in modern digital worlds. However, the previous approaches for parallel text\
    \ detoxification corpora collection\u2014ParaDetox (Logacheva et al., 2022) and\
    \ APPADIA (Atwell et al., 2022)\u2014were explored only in monolingual setup.\
    \ In this work, we aim to extend ParaDetox pipeline to multiple languages presenting\
    \ MultiParaDetox to automate parallel detoxification corpus collection for potentially\
    \ any language. Then, we experiment with different text detoxification models\u2014\
    from unsupervised baselines to LLMs and fine-tuned models on the presented parallel\
    \ corpora\u2014showing the great benefit of parallel corpus presence to obtain\
    \ state-of-the-art text detoxification models for any language."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - dblp_id: https://dblp.org/pid/279/2369
    emails: '****@tum.de'
    first_name: Daryna
    google_scholar_id: https://scholar.google.com/citations?user=mLX6olgAAAAJ&hl=ru&oi=ao
    homepage: https://dardem.github.io/
    last_name: Dementieva
    name: Daryna Dementieva
    orcid: https://orcid.org/0000-0003-0929-4140
    semantic_scholar_id: https://www.semanticscholar.org/author/D.-Dementieva/2027664710
    username: ~Daryna_Dementieva1
  - emails: '****@gmail.com'
    first_name: Nikolay
    google_scholar_id: https://scholar.google.com/citations?user=y9Ju0gcAAAAJ&hl=en
    institution: Univesity of Santiago de Compostela
    last_name: Babakov
    name: Nikolay Babakov
    orcid: https://orcid.org/0000-0002-2568-6702
    username: ~Nikolay_Babakov1
  - dblp_id: https://dblp.org/pid/60/9162
    emails: '****@gmail.com'
    first_name: Alexander
    google_scholar_id: https://scholar.google.com/citations?user=BYba9hcAAAAJ&hl=en&oi=ao
    homepage: https://faculty.skoltech.ru/people/alexanderpanchenko
    institution: Skoltech
    last_name: Panchenko
    name: Alexander Panchenko
    username: ~Alexander_Panchenko1
  decision: toMainConference
  end_page: 2260
  file: 204.pdf
  id: 204
  num_pages: 17
  openreview_id: jZ7tq2GQqk
  pdf_file: cd9974d4301b87f336cdec8019e07f1b7803bb60.pdf
  start_page: 2244
  title: 'MultiParaDetox: Extending Text Detoxification with Parallel Data to New
    Languages'
- abstract: This paper presents VisLingInstruct, a novel approach to advancing Multi-Modal
    Language Models (MMLMs) in zero-shot learning. Current MMLMs show impressive zero-shot
    abilities in multi-modal tasks, but their performance depends heavily on the quality
    of instructions. VisLingInstruct tackles this by autonomously evaluating and optimizing
    instructional texts through In-Context Learning, improving the synergy between
    visual perception and linguistic expression in MMLMs. Alongside this instructional
    advancement, we have also optimized the visual feature extraction modules in MMLMs,
    further augmenting their responsiveness to textual content. Our comprehensive
    experiments on MMLMs, based on FlanT5 and Vicuna, show that VisLingInstruct significantly
    improves zero-shot performance in visual multi-modal tasks. Notably, it achieves
    a 13.1\% and 9\% increase in accuracy over the prior state-of-the-art on the TextVQA
    and HatefulMemes datasets. Our main code  is available at https://github.com/Zhudongsheng75/VisLingInstruct
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multimodality and Language Grounding to Vision, Robotics and Beyond
  authors:
  - emails: '****@fudan.edu.cn'
    first_name: Dongsheng
    google_scholar_id: https://scholar.google.com/citations?user=_GyR7gQAAAAJ&hl=zh-CN
    institution: Baidu
    last_name: Zhu
    name: Dongsheng Zhu
    username: ~Dongsheng_Zhu1
  - dblp_id: 'https://dblp.org/search/publ?q=author:Xunzhu_Tang:'
    emails: '****@uni.lu'
    first_name: Daniel
    google_scholar_id: https://scholar.google.com/citations?user=MxpAtQoAAAAJ&hl=zh-CN
    homepage: https://www.linkedin.com/in/tangxz/
    last_name: Tang
    name: Daniel Tang
    orcid: https://orcid.org/0000-0003-2831-6472
    username: ~Daniel_Tang1
  - dblp_id: https://dblp.org/pid/37/7130
    emails: '****@fudan.edu.cn'
    first_name: Weidong
    google_scholar_id: https://scholar.google.com/citations?user=CsElzloAAAAJ&hl=en
    last_name: Han
    name: Weidong Han
    semantic_scholar_id: https://www.semanticscholar.org/author/Weidong-Han/2198367430
    username: ~Weidong_Han2
  - dblp_id: https://dblp.org/pid/14/983
    emails: '****@gmail.com'
    first_name: Jinghui
    google_scholar_id: https://scholar.google.com/citations?user=ZzK_UdYAAAAJ&hl=en&oi=ao
    institution: ByteDance Inc.
    last_name: Lu
    name: Jinghui Lu
    orcid: https://orcid.org/0000-0001-7149-6961
    semantic_scholar_id: https://www.semanticscholar.org/author/Jinghui-Lu/2115404023
    username: ~Jinghui_Lu2
  - dblp_id: https://dblp.org/pid/158/3615
    emails: '****@gmail.com'
    first_name: Yukun
    google_scholar_id: https://scholar.google.com/citations?user=7EI-gJAAAAAJ&hl=zh-CN
    last_name: Zhao
    name: Yukun Zhao
    username: ~Yukun_Zhao1
  - emails: '****@gmail.com'
    first_name: Guoliang
    last_name: Xing
    name: Guoliang Xing
    username: ~Guoliang_Xing3
  - dblp_id: https://dblp.org/pid/15/885
    emails: '****@baidu.com'
    first_name: Junfeng
    last_name: Wang
    name: Junfeng Wang
    username: ~Junfeng_Wang1
  - dblp_id: https://dblp.org/pid/91/4572
    emails: '****@acm.org'
    first_name: Dawei
    google_scholar_id: https://scholar.google.com/citations?user=GuQ9bpAAAAAJ&hl=zh-CN
    institution: Baidu
    last_name: Yin
    name: Dawei Yin
    username: ~Dawei_Yin1
  decision: toMainConference
  end_page: 2274
  file: 205.pdf
  id: 205
  num_pages: 14
  openreview_id: ROddJfsmai
  pdf_file: 30f59168487d5a3afa49b4960db999dbc8878244.pdf
  start_page: 2261
  title: 'VisLingInstruct: Elevating Zero-Shot Learning in Multi-Modal Language Models
    with Autonomous Instruction Optimization'
- abstract: 'Large Language Models (LLMs), such as ChatGPT and GPT-4, are designed
    to provide useful and safe responses. However, adversarial prompts known as ''jailbreaks''
    can circumvent safeguards, leading LLMs to generate potentially harmful content.
    Exploring jailbreak prompts can help to better reveal the weaknesses of LLMs and
    further steer us to secure them. Unfortunately, existing jailbreak methods either
    suffer from intricate manual design or require optimization on other white-box
    models, which compromises either generalization or efficiency. In this paper,
    we generalize jailbreak prompt attacks into two aspects: (1) Prompt Rewriting
    and (2) Scenario Nesting. Based on this, we propose ReNeLLM, an automatic framework
    that leverages LLMs themselves to generate effective jailbreak prompts. Extensive
    experiments demonstrate that ReNeLLM significantly improves the attack success
    rate while greatly reducing the time cost compared to existing baselines. Our
    study also reveals the inadequacy of current defense methods in safeguarding LLMs.
    Finally, we analyze the failure of LLMs defense from the perspective of prompt
    execution priority, and propose corresponding defense strategies. We hope that
    our research can catalyze both the academic community and LLMs developers towards
    the provision of safer and more regulated LLMs. The code is available at https://github.com/NJUNLP/ReNeLLM.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Ethics, Bias, and Fairness
  authors:
  - dblp_id: https://dblp.org/pid/27/5296
    emails: '****@smail.nju.edu.cn'
    first_name: Peng
    google_scholar_id: https://scholar.google.com.hk/citations?user=4Hph0s8AAAAJ&hl=zh-CN
    institution: nanjing university
    last_name: Ding
    name: Peng Ding
    orcid: https://orcid.org/0000-0001-7814-6606
    semantic_scholar_id: https://www.semanticscholar.org/author/Peng-Ding/2065858902
    username: ~Peng_Ding3
  - dblp_id: https://dblp.org/pid/89/10104.html
    emails: '****@163.com'
    first_name: Jun
    google_scholar_id: https://scholar.google.com/citations?user=1Uc7VjwAAAAJ&hl=zh-CN
    institution: Meituan
    last_name: Kuang
    name: Jun Kuang
    orcid: https://orcid.org/0000-0001-8189-9449
    username: ~Jun_Kuang2
  - emails: '****@gmail.com'
    first_name: Dan
    google_scholar_id: https://scholar.google.com/citations?user=wr7A5I8AAAAJ&hl=zh-CN&authuser=2
    homepage: https://scholar.google.com/citations?user=wr7A5I8AAAAJ&hl=zh-CN&authuser=2
    last_name: Ma
    name: Dan Ma
    username: ~Dan_Ma1
  - dblp_id: https://dblp.org/pid/49/11206
    emails: '****@gmail.com'
    first_name: Xuezhi
    last_name: Cao
    name: Xuezhi Cao
    username: ~Xuezhi_Cao1
  - emails: '****@meituan.com'
    first_name: Yunsen
    homepage: http://faculty.dlut.edu.cn/GuoHe/en/xsxx/791119/content/132173.htm
    last_name: Xian
    name: Yunsen Xian
    username: ~Yunsen_Xian1
  - emails: '****@nju.edu.cn'
    first_name: Jiajun
    google_scholar_id: https://scholar.google.com.tw/citations?user=WIF7VaoAAAAJ
    homepage: https://cs.nju.edu.cn/chenjiajun/index_en.htm
    institution: Nanjing University
    last_name: Chen
    name: Jiajun Chen
    username: ~Jiajun_Chen1
  - dblp_id: https://dblp.org/pid/57/8451
    emails: '****@nju.edu.cn'
    first_name: Shujian
    google_scholar_id: https://scholar.google.com/citations?user=HF3-E9kAAAAJ&hl=en
    homepage: http://nlp.nju.edu.cn/huangsj/
    institution: Nanjing University
    last_name: Huang
    name: Shujian Huang
    username: ~Shujian_Huang1
  decision: toMainConference
  end_page: 2292
  file: 206.pdf
  id: 206
  num_pages: 18
  openreview_id: slaXo2mqw3
  pdf_file: ee4e48f88e3b4748307e7beaefb51c1b71e11cdb.pdf
  start_page: 2275
  title: 'A Wolf in Sheep''s Clothing: Generalized Nested Jailbreak Prompts can Fool
    Large Language Models Easily'
- abstract: In this work, we take a first step towards designing summarization systems
    that are faithful to the author's intent, not only the semantic content of the
    article. Focusing on a case study of preserving political perspectives in news
    summarization, we find that existing approaches alter the political opinions and
    stances of news articles in more than 50% of summaries, misrepresenting the intent
    and perspectives of the news authors. We thus propose P$^3$Sum, a diffusion model-based
    summarization approach controlled by political perspective classifiers. In P$^3$Sum,
    the political leaning of a generated summary is iteratively evaluated at each
    decoding step, and any drift from the article's original stance incurs a loss
    back-propagated to the embedding layers, steering the political stance of the
    summary at inference time. Extensive experiments on three news summarization datasets
    demonstrate that P$^3$Sum outperforms state-of-the-art summarization systems and
    large language models by up to 13.7% in terms of the success rate of stance preservation,
    with competitive performance on standard metrics of summarization quality. Our
    findings present a first analysis of preservation of pragmatic features in summarization,
    highlight the lacunae in existing summarization models---that even state-of-the-art
    models often struggle to preserve author's intents---and develop new summarization
    systems that are more faithful to author's perspectives.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Summarization
  authors:
  - emails: '****@stu.xjtu.edu.cn'
    first_name: Yuhan
    google_scholar_id: https://scholar.google.com/citations?user=or-2JE8AAAAJ&hl=en
    homepage: https://www.yhliu-nlp.info/
    last_name: Liu
    name: Yuhan Liu
    semantic_scholar_id: https://www.semanticscholar.org/author/Yuhan-Liu/2169159066
    username: ~Yuhan_Liu9
  - dblp_id: https://dblp.org/pid/295/9571
    emails: '****@cs.washington.edu'
    first_name: Shangbin
    google_scholar_id: https://scholar.google.com/citations?user=Y3rLP9UAAAAJ&hl=en&oi=ao
    homepage: https://bunsenfeng.github.io/
    institution: Paul G. Allen School of Computer Science and Engineering, University
      of Washington
    last_name: Feng
    name: Shangbin Feng
    semantic_scholar_id: https://www.semanticscholar.org/author/2114887261
    username: ~Shangbin_Feng1
  - dblp_id: https://dblp.org/pid/216/6755
    emails: '****@gmail.com'
    first_name: Xiaochuang
    google_scholar_id: https://scholar.google.com/citations?user=GamSVF0AAAAJ&hl=en
    homepage: https://xhan77.github.io/
    institution: Department of Computer Science, University of Washington
    last_name: Han
    name: Xiaochuang Han
    semantic_scholar_id: https://www.semanticscholar.org/author/Xiaochuang-Han/40500540
    username: ~Xiaochuang_Han1
  - dblp_id: https://dblp.org/pid/234/4867
    emails: '****@microsoft.com'
    first_name: Vidhisha
    google_scholar_id: https://scholar.google.com/citations?user=LgitgaIAAAAJ&hl=en
    homepage: https://vidhishanair.github.io/
    institution: Carnegie Mellon University
    last_name: Balachandran
    name: Vidhisha Balachandran
    semantic_scholar_id: https://www.semanticscholar.org/author/Vidhisha-Balachandran/143820870
    username: ~Vidhisha_Balachandran1
  - emails: '****@andrew.cmu.edu'
    first_name: Chan Young
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=yjzpOQoAAAAJ
    homepage: https://chan0park.github.io
    last_name: Park
    name: Chan Young Park
    username: ~Chan_Young_Park1
  - dblp_id: https://dblp.org/pid/31/4484
    emails: '****@gmail.com'
    first_name: Sachin
    google_scholar_id: https://scholar.google.com/citations?user=qO38fRIAAAAJ&hl=en
    homepage: https://shocheen.com
    institution: Ohio State University, Columbus
    last_name: Kumar
    name: Sachin Kumar
    semantic_scholar_id: https://www.semanticscholar.org/author/Sachin-Kumar/51467955
    username: ~Sachin_Kumar1
  - dblp_id: https://dblp.org/pid/75/8157
    emails: '****@cs.washington.edu'
    first_name: Yulia
    google_scholar_id: https://scholar.google.com/citations?user=SEDPkrsAAAAJ&hl=en
    homepage: https://homes.cs.washington.edu/~yuliats/
    institution: Department of Computer Science, University of Washington
    last_name: Tsvetkov
    name: Yulia Tsvetkov
    username: ~Yulia_Tsvetkov1
  decision: toMainConference
  end_page: 2312
  file: 207.pdf
  id: 207
  num_pages: 20
  openreview_id: 47U0l0uTQV
  pdf_file: 603f9ea6e5675745890631164330c2f72d2f8852.pdf
  start_page: 2293
  title: 'P$^3$Sum: Preserving Author''s Perspective in News Summarization with Diffusion
    Language Models'
- abstract: 'Scaling high-quality tutoring remains a major challenge in education.
    Due to growing demand, many platforms employ novice tutors who, unlike experienced
    educators, struggle to address student mistakes and thus fail to seize prime learning
    opportunities. Our work explores the potential of large language models (LLMs)
    to close the novice-expert knowledge gap in remediating math mistakes. We contribute
    Bridge, a method that uses cognitive task analysis to translate an expert''s latent
    thought process into a decision-making model for remediation. This involves an
    expert identifying (A) the student''s error, (B) a remediation strategy, and (C)
    their intention before generating a response. We construct a dataset of 700 real
    tutoring conversations, annotated by experts with their decisions. We evaluate
    state-of-the-art LLMs on our dataset and find that the expert''s decision-making
    model is critical for LLMs to close the gap: responses from GPT4 with expert decisions
    (e.g., ``simplify the problem'''') are +76% more preferred than without. Additionally,
    context-sensitive decisions are critical to closing pedagogical gaps: random decisions
    decrease GPT4''s response quality by -97% than expert decisions. Our work shows
    the potential of embedding expert thought processes in LLM generations to enhance
    their capability to bridge novice-expert knowledge gaps. Our dataset and code
    can be found at: https://github.com/rosewang2008/bridge.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Dialogue and Interactive Systems
  authors:
  - dblp_id: https://dblp.org/pid/259/1500
    emails: '****@stanford.edu'
    first_name: Rose
    google_scholar_id: https://scholar.google.com/citations?user=V-dlwF4AAAAJ&hl=en
    homepage: https://cs.stanford.edu/~rewang
    institution: Stanford University
    last_name: Wang
    middle_name: E
    name: Rose E Wang
    username: ~Rose_E_Wang1
  - emails: '****@stanford.edu'
    first_name: Qingyang
    last_name: Zhang
    name: Qingyang Zhang
    username: ~Qingyang_Zhang3
  - emails: '****@stanford.edu'
    first_name: Carly
    google_scholar_id: https://scholar.google.com/citations?user=6M_APl8AAAAJ&hl=en
    homepage: https://www.carlydrobinson.com/
    institution: Stanford University
    last_name: Robinson
    middle_name: D
    name: Carly D Robinson
    orcid: https://orcid.org/0000-0002-0663-1589
    username: ~Carly_D_Robinson1
  - emails: '****@stanford.edu'
    first_name: Susanna
    google_scholar_id: https://scholar.google.com/citations?user=RUtLWRcAAAAJ&hl=en
    homepage: https://ed.stanford.edu/faculty/sloeb
    last_name: Loeb
    name: Susanna Loeb
    orcid: https://orcid.org/0000-0003-1854-8843
    username: ~Susanna_Loeb2
  - emails: '****@stanford.edu'
    first_name: Dorottya
    google_scholar_id: https://scholar.google.com/citations?user=WtVqgE8AAAAJ&hl=en&authuser=1
    homepage: https://www.dorademszky.com/
    institution: Stanford University
    last_name: Demszky
    name: Dorottya Demszky
    orcid: https://orcid.org/0000-0002-6759-9367
    username: ~Dorottya_Demszky1
  decision: toMainConference
  end_page: 2338
  file: 208.pdf
  id: 208
  num_pages: 26
  openreview_id: Z22Aa4Td42
  pdf_file: 0f615529f5c666cf016ba8e5bb666cc908473256.pdf
  start_page: 2313
  title: 'Bridging the Novice-Expert Gap via Models of Decision-Making: A Case Study
    on Remediating Math Mistakes'
- abstract: "Contrastive learning, which utilizes positive pairs and in-batch negatives\
    \ to optimize the loss objective, has been proven to be an effective method for\
    \ learning sentence embeddings. However, we argue that the previous methods of\
    \ constructing positive pairs only through dropout perturbation or entailment\
    \ relation are limited. Since there is more sentence knowable information (SKI)\
    \ to be mined, such as sentence external knowledge, semantic analysis, and grammatical\
    \ description. In this work, we first hand-craft a simple and effective prompt\
    \ template that is able to obtain the knowable information of input sentences\
    \ from LLMs (e.g., LLaMA). Then we combine the original sentence and its knowable\
    \ information to form a positive pair for contrastive learning. We evaluate our\
    \ method on standard semantic textual similarity (STS) tasks. Experimental results\
    \ show that our unsupervised and supervised models using $\\text{BERT}_\\text{base}$\
    \ achieve an average of 78.65% and 82.45% Spearman\u2019s correlation respectively,\
    \ a 2.40% and 0.88% improvement compared to SimCSE. Our model outperforms the\
    \ previous state-of-the-art model PromptBERT in both unsupervised and supervised\
    \ settings and specifically yields a new state-of-the-art performance in supervised\
    \ setting."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: 'Semantics: Sentence-level Semantics, Textual Inference and Other
      areas'
  authors:
  - emails: '****@bjtu.edu.cn'
    first_name: Fangwei
    last_name: Ou
    name: Fangwei Ou
    username: ~Fangwei_Ou1
  - dblp_id: https://dblp.org/pid/67/3124
    emails: '****@bjtu.edu.cn'
    first_name: Jinan
    homepage: http://faculty.bjtu.edu.cn/8300/
    institution: Beijing Jiaotong University
    last_name: Xu
    name: Jinan Xu
    semantic_scholar_id: https://www.semanticscholar.org/author/Jinan-Xu/2310092
    username: ~Jinan_Xu1
  decision: toMainConference
  end_page: 2344
  file: 210.pdf
  id: 210
  num_pages: 6
  openreview_id: SITB75hnQT
  pdf_file: 94801bcaccc33eda2caf867fb57ed818ca8405e8.pdf
  start_page: 2339
  title: 'SKICSE: Sentence Knowable Information Prompted by LLMs Improves Contrastive
    Sentence Embeddings'
- abstract: For long document summarization, discourse structure is important to discern
    the key content of the text and the differences in importance level between sentences.
    Unfortunately, the integration of rhetorical structure theory (RST) into parameter-efficient
    fine-tuning strategies for long document summarization remains unexplored. Therefore,
    this paper introduces RST-LoRA and proposes four RST-aware variants to explicitly
    incorporate RST into the LoRA model. Our empirical evaluation demonstrates that
    incorporating the type and uncertainty of rhetorical relations can complementarily
    enhance the performance of LoRA in summarization tasks. Furthermore, the best-performing
    variant we introduced outperforms the vanilla LoRA and full-parameter fine-tuning
    models, as confirmed by multiple automatic and human evaluations, and even surpasses
    previous state-of-the-art methods.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Summarization
  authors:
  - dblp_id: https://dblp.org/pid/321/5271
    emails: '****@gmail.com'
    first_name: Dongqi
    google_scholar_id: https://scholar.google.com/citations?user=KrSiNLwAAAAJ&hl=en
    homepage: https://dongqi.me/
    institution: "Universit\xE4t des Saarlandes"
    last_name: Pu
    name: Dongqi Pu
    orcid: https://orcid.org/0000-0002-0778-6427
    semantic_scholar_id: https://www.semanticscholar.org/author/Dongqi-Pu/2167123387
    username: ~Dongqi_Pu1
  - dblp_id: https://dblp.org/pid/80/3304
    emails: '****@coli.uni-saarland.de'
    first_name: Vera
    google_scholar_id: https://scholar.google.com/citations?user=l2CFSAMAAAAJ&hl=de
    homepage: https://www.uni-saarland.de/lehrstuhl/demberg/members/team.html
    institution: "Universit\xE4t des Saarlandes"
    last_name: Demberg
    name: Vera Demberg
    orcid: https://orcid.org/0000-0002-8834-0020
    username: ~Vera_Demberg2
  decision: toMainConference
  end_page: 2365
  file: 211.pdf
  id: 211
  num_pages: 21
  openreview_id: wrbxvXQ3AL
  pdf_file: b24f7abf3aca261e74dd74d5d048d102dcdc047a.pdf
  start_page: 2345
  title: 'RST-LoRA: A Discourse-Aware Low-Rank Adaptation for Long Document Abstractive
    Summarization'
- abstract: Recent prompt optimisation approaches use the generative nature of language
    models to produce prompts -- even rivaling the performance of human-curated prompts.
    In this paper, we demonstrate that randomly sampling tokens from the model vocabulary
    as ``separators'' can be as effective as language models for prompt-style text
    classification. Our experiments show that random separators are competitive baselines,
    having less than a 1\% difference compared to previous self-optimisation methods
    and showing a 12\% average relative improvement over strong human baselines across
    nine text classification tasks and eight language models. We further analyse this
    phenomenon in detail using three different random generation strategies, establishing
    that the language space is rich with potentially good separators, with a greater
    than 40\% average chance that a randomly drawn separator performs better than
    human-curated separators. These observations challenge the common assumption that
    an effective prompt should be human readable or task relevant and establish a
    strong baseline for prompt optimisation research.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - emails: '****@ucl.ac.uk'
    first_name: Yao
    google_scholar_id: https://scholar.google.ca/citations?user=Dir3ds8AAAAJ
    homepage: https://yaolu.github.io
    last_name: Lu
    name: Yao Lu
    semantic_scholar_id: https://www.semanticscholar.org/author/Yao-Lu/40282678
    username: ~Yao_Lu5
  - emails: '****@gmail.com'
    first_name: Jiayi
    google_scholar_id: https://scholar.google.com/citations?user=OGTpIWYAAAAJ&hl=en
    last_name: Wang
    name: Jiayi Wang
    username: ~Jiayi_Wang11
  - dblp_id: https://dblp.org/pid/207/7684
    emails: '****@uwaterloo.ca'
    first_name: Raphael
    google_scholar_id: https://scholar.google.ca/citations?user=3icAgxMAAAAJ&hl=en
    homepage: http://ralphtang.com
    institution: Comcast
    last_name: Tang
    name: Raphael Tang
    username: ~Raphael_Tang1
  - dblp_id: https://dblp.org/pid/18/3348
    emails: '****@gmail.com'
    first_name: Sebastian
    google_scholar_id: https://scholar.google.com.tw/citations?user=AcCtcrsAAAAJ
    homepage: http://www.cs.ucl.ac.uk/people/S.Riedel.html/
    institution: Facebook and University College London
    last_name: Riedel
    name: Sebastian Riedel
    username: ~Sebastian_Riedel1
  - emails: '****@stenetorp.se'
    first_name: Pontus
    homepage: https://pontus.stenetorp.se
    institution: University College London
    last_name: Stenetorp
    name: Pontus Stenetorp
    semantic_scholar_id: https://www.semanticscholar.org/author/Pontus-Stenetorp/1918552
    username: ~Pontus_Stenetorp1
  decision: toMainConference
  end_page: 2376
  file: 214.pdf
  id: 214
  num_pages: 11
  openreview_id: HIYPKZeTGc
  pdf_file: 50b956765a73728b1b4031326f65390aa076742b.pdf
  start_page: 2366
  title: 'Strings from the Library of Babel: Random Sampling as a Strong Baseline
    for Prompt Optimisation'
- abstract: "Current logical reasoning evaluations of Large Language Models (LLMs)\
    \ primarily focus on single-turn and static environments, such as arithmetic problems.\
    \ The crucial problem of multi-turn, strategic reasoning is under-explored. In\
    \ this work, we analyze the multi-turn strategic reasoning of LLMs through text-driven\
    \ complete- and incomplete-information gaming, e.g., board games (Tic-Tac-Toe,\
    \ Connect-4) and poker games (Texas Hold\u2019em Poker). Specifically, we consider\
    \ two distinct scenarios: 1) Online Racing, featuring multiple LLMs/agents to\
    \ facilitate direct competition and comparison; 2) Offline Probing, constructing\
    \ targeted questions with verified ground truth to evaluate LLMs\u2019 strategic\
    \ behaviors. Experimental results demonstrate that existing state-of-the-art LLMs\
    \ and reasoning schemes are largely ineffective for strategic reasoning tasks.\
    \ To mitigate these limitations, we propose a simple yet effective Recursively\
    \ Thinking-Ahead (ReTA) agent, incorporating a recursive prompting mechanism that\
    \ automatically analyzes the opponents\u2019 future moves/actions and assigns\
    \ reward signals for these situations, to strengthen the strategic reasoning of\
    \ LLMs. We hope our work could spur further research and exploration in the multi-turn\
    \ strategic reasoning of LLMs. The code is available at https://github.com/jinhaoduan/ReTA."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - dblp_id: https://dblp.org/pid/282/2912
    emails: '****@drexel.edu'
    first_name: Jinhao
    google_scholar_id: https://scholar.google.com/citations?user=aWeTAXYAAAAJ&hl=en
    homepage: https://jinhaoduan.github.io
    institution: Drexel University
    last_name: Duan
    name: Jinhao Duan
    username: ~Jinhao_Duan1
  - dblp_id: https://dblp.org/pid/58/9145-2
    emails: '****@amazon.com'
    first_name: Shiqi
    google_scholar_id: https://scholar.google.com/citations?user=u_MzXeMAAAAJ&hl=en
    homepage: https://shiqi-wang.github.io
    institution: Amazon
    last_name: Wang
    name: Shiqi Wang
    orcid: https://orcid.org/0000-0002-6338-1432
    semantic_scholar_id: https://www.semanticscholar.org/author/Shiqi-Wang/51257183
    username: ~Shiqi_Wang2
  - dblp_id: https://dblp.org/pid/188/4110
    emails: '****@llnl.gov'
    first_name: James
    google_scholar_id: https://scholar.google.com/citations?user=nRr24_QAAAAJ&hl=en
    institution: Lawrence Livermore National Labs
    last_name: Diffenderfer
    name: James Diffenderfer
    username: ~James_Diffenderfer1
  - dblp_id: https://dblp.org/pid/121/0780-1.html
    emails: '****@gmail.com'
    first_name: Lichao
    google_scholar_id: https://scholar.google.com/citations?user=WhGUE7AAAAAJ&hl=en
    homepage: https://lichao-sun.github.io/
    institution: Lehigh University
    last_name: Sun
    name: Lichao Sun
    username: ~Lichao_Sun1
  - emails: '****@cs.unc.edu'
    first_name: Tianlong
    google_scholar_id: https://scholar.google.com/citations?user=LE3ctn0AAAAJ&hl=en
    homepage: https://tianlong-chen.github.io
    last_name: Chen
    name: Tianlong Chen
    username: ~Tianlong_Chen1
  - dblp_id: https://dblp.org/pid/132/8938
    emails: '****@llnl.gov'
    first_name: Bhavya
    google_scholar_id: https://scholar.google.com/citations?user=SQpJmOgAAAAJ&hl=en
    homepage: https://people.llnl.gov/kailkhura1
    institution: Lawrence Livermore National Laboratory
    last_name: Kailkhura
    name: Bhavya Kailkhura
    username: ~Bhavya_Kailkhura1
  - dblp_id: https://dblp.org/pid/195/8175
    emails: '****@northeastern.edu'
    first_name: Kaidi
    google_scholar_id: https://scholar.google.com/citations?user=lYK0wlsAAAAJ&hl=en
    homepage: https://kaidixu.com/
    institution: Drexel University
    last_name: Xu
    name: Kaidi Xu
    username: ~Kaidi_Xu1
  decision: toMainConference
  end_page: 2391
  file: 215.pdf
  id: 215
  num_pages: 15
  openreview_id: ocxx6rBYQn
  pdf_file: 01c99db7d147ecd0554b6d2ba20694ba6a9071a7.pdf
  start_page: 2377
  title: 'ReTA: Recursively Thinking Ahead to Improve the Strategic Reasoning of Large
    Language Models'
- abstract: Evaluating the veracity of everyday claims is time consuming and in some
    cases requires domain expertise. We empirically demonstrate that the commonly
    used fact checking pipeline, known as the retriever-reader, suffers from performance
    deterioration when it is trained on the labeled data from one domain and used
    in another domain. Afterwards, we delve into each component of the pipeline and
    propose novel algorithms to address this problem. We propose an adversarial algorithm
    to make the retriever component robust against distribution shift. Our core idea
    is to initially train a bi-encoder on the labeled source data, and then, to adversarially
    train two separate document and claim encoders using unlabeled target data. We
    then focus on the reader component and propose to train it such that it is insensitive
    towards the order of claims and evidence documents. Our empirical evaluations
    support the hypothesis that such a reader shows a higher robustness against distribution
    shift. To our knowledge, there is no publicly available multi-topic fact checking
    dataset. Thus, we propose a simple automatic method to re-purpose two well-known
    fact checking datasets. We then construct eight fact checking scenarios from these
    datasets, and compare our model to a set of strong baseline models, including
    recent domain adaptation models that use GPT4 for generating synthetic data.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Learning for NLP
  authors:
  - emails: '****@gmail.com'
    first_name: Payam
    institution: University of Illinois at Urbana-Champaign
    last_name: Karisani
    name: Payam Karisani
    semantic_scholar_id: https://www.semanticscholar.org/author/Payam-Karisani/2497878
    username: ~Payam_Karisani1
  - emails: '****@illinois.edu'
    first_name: Heng
    google_scholar_id: https://scholar.google.com/citations?user=z7GCqT4AAAAJ&hl=en
    homepage: http://blender.cs.illinois.edu/hengji.html
    institution: University of Illinois, Urbana-Champaign
    last_name: Ji
    name: Heng Ji
    username: ~Heng_Ji3
  decision: toMainConference
  end_page: 2406
  file: 217.pdf
  id: 217
  num_pages: 15
  openreview_id: QjoTzjD3Op
  pdf_file: 4cd21315e64d636df70de59b7adda27cc4f2fc0f.pdf
  start_page: 2392
  title: Fact Checking Beyond Training Set
- abstract: "Prior work shows that program-aided reasoning, in which large language\
    \ models (LLMs) are combined with programs written in programming languages such\
    \ as Python, can significantly improve accuracy on various reasoning tasks. However,\
    \ while accuracy is essential, it is also important for such reasoners to \u201C\
    know what they know\u201D, which can be quantified through the calibration of\
    \ the model. In this paper, we compare the calibration of Program Aided Language\
    \ Models (PAL) and text-based Chain-of-thought (COT) prompting techniques over\
    \ 5 datasets and 2 model types - LLaMA models and OpenAI models. Our results indicate\
    \ that PAL leads to improved calibration in 75% of the instances. Our analysis\
    \ uncovers that prompting styles that produce lesser diversity in generations\
    \ also have more calibrated results, and thus we also experiment with inducing\
    \ lower generation diversity using temperature scaling and find that for certain\
    \ temperatures, PAL is not only more accurate but is also more calibrated than\
    \ COT. Overall, we demonstrate that, in the majority of cases, program-aided reasoners\
    \ better know what they know than text-based counterparts."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Question Answering
  authors:
  - dblp_id: https://dblp.org/pid/266/2823
    emails: '****@andrew.cmu.edu'
    first_name: Anubha
    google_scholar_id: https://scholar.google.com/citations?user=bMie1tIAAAAJ&hl=en&authuser=1
    homepage: https://sites.google.com/view/anubha-kabra
    institution: Bloomberg
    last_name: Kabra
    name: Anubha Kabra
    semantic_scholar_id: https://www.semanticscholar.org/author/Anubha-Kabra/1735001746
    username: ~Anubha_Kabra1
  - emails: '****@andrew.cmu.edu'
    first_name: Sanketh
    last_name: Rangreji
    name: Sanketh Rangreji
    username: ~Sanketh_Rangreji1
  - emails: '****@cs.cmu.edu'
    first_name: Yash
    last_name: Mathur
    name: Yash Mathur
    username: ~Yash_Mathur1
  - dblp_id: https://dblp.org/pid/138/1043
    emails: '****@cs.cmu.edu'
    first_name: Aman
    google_scholar_id: https://scholar.google.com/citations?user=jW9ts2cAAAAJ&hl=en&oi=ao
    homepage: https://madaan.github.io
    institution: Carnegie Mellon University
    last_name: Madaan
    name: Aman Madaan
    semantic_scholar_id: https://www.semanticscholar.org/author/Aman-Madaan/21626987
    username: ~Aman_Madaan1
  - dblp_id: https://dblp.org/pid/249/6997
    emails: '****@andrew.cmu.edu'
    first_name: Emmy
    homepage: https://nightingal3.github.io/
    institution: School of Computer Science, Carnegie Mellon University
    last_name: Liu
    name: Emmy Liu
    username: ~Emmy_Liu1
  - dblp_id: https://dblp.org/pid/03/8155
    emails: '****@cs.cmu.edu'
    first_name: Graham
    google_scholar_id: https://scholar.google.com/citations?user=wlosgkoAAAAJ
    homepage: http://phontron.com
    institution: Carnegie Mellon University
    last_name: Neubig
    name: Graham Neubig
    semantic_scholar_id: https://www.semanticscholar.org/author/Graham-Neubig/1700325
    username: ~Graham_Neubig1
  decision: toMainConference
  end_page: 2423
  file: 218.pdf
  id: 218
  num_pages: 17
  openreview_id: Asja5O0Aej
  pdf_file: ac755bed29b707a2526331c8fc3f75fe4014351c.pdf
  start_page: 2407
  title: Program-Aided Reasoners (Better) Know What They Know
- abstract: Longstanding data labeling practices in machine learning involve collecting
    and aggregating labels from multiple annotators. But what should we do when annotators
    disagree? Though annotator disagreement has long been seen as a problem to minimize,
    new perspectivist approaches challenge this assumption by treating disagreement
    as a valuable source of information. In this position paper, we examine practices
    and assumptions surrounding the causes of disagreement--some challenged by perspectivist
    approaches, and some that remain to be addressed--as well as practical and normative
    challenges for work operating under these assumptions. We conclude with recommendations
    for the data labeling pipeline and avenues for future research engaging with subjectivity
    and disagreement.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Ethics, Bias, and Fairness
  authors:
  - emails: '****@berkeley.edu'
    first_name: Eve
    google_scholar_id: https://scholar.google.com/citations?user=NHlxXzwAAAAJ&hl=en&oi=ao
    homepage: https://www.efleisig.com
    last_name: Fleisig
    name: Eve Fleisig
    username: ~Eve_Fleisig1
  - dblp_id: https://dblp.org/pid/182/2034
    emails: '****@microsoft.com'
    first_name: Su Lin
    google_scholar_id: https://scholar.google.com/citations?user=8jbAkOUAAAAJ&hl=en
    homepage: https://sblodgett.github.io/
    institution: Microsoft
    last_name: Blodgett
    name: Su Lin Blodgett
    semantic_scholar_id: https://www.semanticscholar.org/author/Su-Lin-Blodgett/3422038
    username: ~Su_Lin_Blodgett2
  - dblp_id: https://dblp.org/pid/22/1139
    emails: '****@cs.berkeley.edu'
    first_name: Dan
    homepage: http://people.eecs.berkeley.edu/~klein/
    institution: University of California, Berkeley
    last_name: Klein
    name: Dan Klein
    username: ~Dan_Klein1
  - dblp_id: https://dblp.org/pid/185/5571
    emails: '****@zeerak.org'
    first_name: Zeerak
    google_scholar_id: https://scholar.google.com/citations?user=3M3WdvkAAAAJ
    homepage: https://zeerak.org
    institution: Mohamed bin Zayed University of Artificial Intelligence
    last_name: Talat
    name: Zeerak Talat
    semantic_scholar_id: https://www.semanticscholar.org/author/Zeerak-Waseem/3456512
    username: ~Zeerak_Talat1
  decision: toMainConference
  end_page: 2437
  file: 219.pdf
  id: 219
  num_pages: 14
  openreview_id: 1XszMVtADN
  pdf_file: 05c2afdf26f44f1841694e85ef89d602bd3a932b.pdf
  start_page: 2424
  title: 'The Perspectivist Paradigm Shift: Assumptions and Challenges of Capturing
    Human Labels'
- abstract: Counter narratives - informed responses to hate speech contexts designed
    to refute hateful claims and de-escalate encounters - have emerged as an effective
    hate speech intervention strategy. While previous work has proposed automatic
    counter narrative generation methods to aid manual interventions, the evaluation
    of these approaches remains underdeveloped. Previous automatic metrics for counter
    narrative evaluation lack alignment with human judgment as they rely on superficial
    reference comparisons instead of incorporating key aspects of counter narrative
    quality as evaluation criteria. To address prior evaluation limitations, we propose
    a novel evaluation framework prompting LLMs to provide scores and feedback for
    generated counter narrative candidates using 5 defined aspects derived from guidelines
    from counter narrative specialized NGOs. We found that LLM evaluators achieve
    strong alignment to human-annotated scores and feedback and outperform alternative
    metrics, indicating their potential as multi-aspect, reference-free and interpretable
    evaluators for counter narrative evaluation.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - emails: '****@osu.edu'
    first_name: Jaylen
    institution: Ohio State University, Columbus
    last_name: Jones
    name: Jaylen Jones
    username: ~Jaylen_Jones1
  - dblp_id: https://dblp.org/pid/241/5516
    emails: '****@osu.edu'
    first_name: Lingbo
    homepage: https://molingbo.github.io/
    last_name: Mo
    name: Lingbo Mo
    semantic_scholar_id: https://www.semanticscholar.org/author/Lingbo-Mo/2135579717
    username: ~Lingbo_Mo1
  - dblp_id: https://dblp.org/pid/80/6326
    emails: '****@cse.ohio-state.edu'
    first_name: Eric
    google_scholar_id: https://scholar.google.com.tw/citations?user=AlsMV98AAAAJ
    homepage: http://web.cse.ohio-state.edu/~fosler/
    institution: Ohio State University
    last_name: Fosler-Lussier
    name: Eric Fosler-Lussier
    orcid: https://orcid.org/0000-0001-8004-5169
    semantic_scholar_id: https://www.semanticscholar.org/author/E.-Fosler-Lussier/1398481836
    username: ~Eric_Fosler-Lussier1
  - dblp_id: https://dblp.org/pid/33/2952
    emails: '****@osu.edu'
    first_name: Huan
    google_scholar_id: https://scholar.google.com/citations?user=wIFkulcAAAAJ&hl=en
    homepage: http://web.cse.ohio-state.edu/~huansun/
    institution: Ohio State University, Columbus
    last_name: Sun
    name: Huan Sun
    semantic_scholar_id: https://www.semanticscholar.org/author/Huan-Sun/11121990
    username: ~Huan_Sun1
  decision: toMainConference
  end_page: 2459
  file: 220.pdf
  id: 220
  num_pages: 22
  openreview_id: l3KoMef1KA
  pdf_file: 8bd228c5b5dad4098e04082fefb219bf4c52fe1a.pdf
  start_page: 2438
  title: A Multi-Aspect Framework for Counter Narrative Evaluation using Large Language
    Models
- abstract: The NLP community typically relies on performance of a model on a held-out
    test set to assess  generalization. Performance drops observed in datasets outside
    of official test sets are generally  attributed to "out-of-distribution" effects.  Here,
    we explore the foundations of generalizability and study the  factors that affect  it,  articulating  lessons
    from clinical studies. In clinical research,  generalizability is an act of reasoning
    that depends on  (a) *internal validity* of experiments to ensure controlled measurement
    of cause and effect, and (b) *external validity* or transportability of the results
    to the wider population. We demonstrate how learning spurious correlations, such
    as the distance between entities in  relation extraction tasks, can affect a model's
    internal validity and in turn adversely impact  generalization.  We, therefore,
    present the need to ensure internal validity when building machine learning models
    in  NLP. Our recommendations also apply to generative large language models, as
    they are  known to be sensitive   to even minor semantic preserving alterations.
    We also propose adapting the idea of *matching* in randomized controlled trials
    and observational studies to  NLP evaluation to measure causation.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - dblp_id: https://dblp.org/pid/232/4874
    emails: '****@student.unimelb.edu.au'
    first_name: Aparna
    google_scholar_id: https://scholar.google.com/citations?user=eaow7uAAAAAJ&hl=en
    institution: Amazon
    last_name: Elangovan
    name: Aparna Elangovan
    semantic_scholar_id: https://www.semanticscholar.org/author/Aparna-Elangovan/26940961
    username: ~Aparna_Elangovan1
  - dblp_id: https://dblp.org/pid/119/5963-2
    emails: '****@rmit.edu.au'
    first_name: Jiayuan
    google_scholar_id: https://scholar.google.com/citations?user=Lwso-psAAAAJ&hl=en&oi=ao
    institution: Royal Melbourne Institute of Technology and The University of Melbourne
    last_name: He
    name: Jiayuan He
    username: ~Jiayuan_He1
  - dblp_id: https://dblp.uni-trier.de/pers/hd/l/Li_0012:Yuan
    emails: '****@gmail.com'
    first_name: Yuan
    google_scholar_id: https://scholar.google.com/citations?user=qDDC09EAAAAJ
    last_name: Li
    name: Yuan Li
    username: ~Yuan_Li5
  - dblp_id: https://dblp.org/pid/07/6465
    emails: '****@rmit.edu.au'
    first_name: Karin
    google_scholar_id: https://scholar.google.com/citations?user=dUxHnbcAAAAJ&hl=en
    institution: Royal Melbourne Institute of Technology
    last_name: Verspoor
    name: Karin Verspoor
    orcid: https://orcid.org/0000-0002-8661-1544
    semantic_scholar_id: https://www.semanticscholar.org/author/Karin-M.-Verspoor/144765178
    username: ~Karin_Verspoor1
  decision: toMainConference
  end_page: 2476
  file: 221.pdf
  id: 221
  num_pages: 17
  openreview_id: frHNHTEHHq
  pdf_file: d7981871682ff27e7516f3ecb04191db791f361f.pdf
  start_page: 2460
  title: Principles from Clinical Research for NLP Model Generalization
- abstract: Many NLP researchers are experiencing an existential crisis triggered
    by the astonishing success of ChatGPT and other systems based on large language
    models (LLMs). After such a disruptive change to our understanding of the field,
    what is left to do? Taking a historical lens, we look for guidance from the first
    era of LLMs, which began in 2005 with large $n$-gram models for machine translation
    (MT). We identify durable lessons from the first era, and more importantly, we
    identify evergreen problems where NLP researchers can continue to make meaningful
    contributions in areas where LLMs are ascendant. We argue that disparities in
    scale are transient and researchers can work to reduce them; that data, rather
    than hardware, is still a bottleneck for many applications; that meaningful realistic
    evaluation is still an open problem; and that there is still room for speculative
    approaches.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - dblp_id: https://dblp.org/pid/131/6883
    emails: '****@gmail.com'
    first_name: Naomi
    google_scholar_id: https://scholar.google.com/citations?user=TPhVfX8AAAAJ&hl=en
    homepage: http://nsaphra.github.io/
    institution: Harvard University
    last_name: Saphra
    name: Naomi Saphra
    username: ~Naomi_Saphra1
  - emails: '****@berkeley.edu'
    first_name: Eve
    google_scholar_id: https://scholar.google.com/citations?user=NHlxXzwAAAAJ&hl=en&oi=ao
    homepage: https://www.efleisig.com
    last_name: Fleisig
    name: Eve Fleisig
    username: ~Eve_Fleisig1
  - dblp_id: https://dblp.org/pid/41/9736
    emails: '****@nyu.edu'
    first_name: Kyunghyun
    google_scholar_id: https://scholar.google.fi/citations?user=0RAmmIAAAAAJ&hl=en
    homepage: http://kyunghyuncho.me
    institution: Genentech and New York University
    last_name: Cho
    name: Kyunghyun Cho
    semantic_scholar_id: https://www.semanticscholar.org/author/Kyunghyun-Cho/1979489
    username: ~Kyunghyun_Cho1
  - dblp_id: https://dblp.org/pid/65/5274
    emails: '****@inf.ed.ac.uk'
    first_name: Adam
    google_scholar_id: https://scholar.google.co.uk/citations?user=u4sxKZwAAAAJ&hl=en
    homepage: https://alopez.github.io/
    institution: University of Edinburgh
    last_name: Lopez
    name: Adam Lopez
    orcid: https://orcid.org/0000-0002-1533-9424
    semantic_scholar_id: https://www.semanticscholar.org/author/Adam-Lopez/144871732
    username: ~Adam_Lopez1
  decision: toMainConference
  end_page: 2493
  file: 223.pdf
  id: 223
  num_pages: 17
  openreview_id: lbZWtkUcTi
  pdf_file: e8ebeafe095b1f36896543fae11125d8a8b45ee8.pdf
  start_page: 2477
  title: 'First Tragedy, then Parse: History Repeats Itself in the New Era of Large
    Language Models'
- abstract: 'Large language models (LLMs) exhibit positional bias in how they use
    context, which especially affects listwise ranking. To address this, we propose
    permutation self-consistency, a form of self-consistency over the ranking list
    outputs of black-box LLMs. Our key idea is to marginalize out different list orders
    in the prompt to produce an order-independent ranking with less positional bias.
    First, given some input prompt, we repeatedly shuffle the list in the prompt and
    pass it through the LLM while holding the instructions the same. Next, we aggregate
    the resulting sample of rankings by computing the central ranking closest in distance
    to all of them, marginalizing out prompt order biases in the process. Theoretically,
    we prove the robustness of our method, showing convergence to the true ranking
    under random perturbations.

    Empirically, on five datasets in sorting and passage reranking, our approach improves
    scores from conventional inference by up to 34-52% for Mistral, 7-18% for GPT-3.5,
    8-16% for LLaMA v2 (70B). Our code is at https://github.com/castorini/perm-sc.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Learning for NLP
  authors:
  - dblp_id: https://dblp.org/pid/207/7684
    emails: '****@uwaterloo.ca'
    first_name: Raphael
    google_scholar_id: https://scholar.google.ca/citations?user=3icAgxMAAAAJ&hl=en
    homepage: http://ralphtang.com
    institution: Comcast
    last_name: Tang
    name: Raphael Tang
    username: ~Raphael_Tang1
  - dblp_id: https://dblp.org/pid/58/4582
    emails: '****@uwaterloo.ca'
    first_name: Xinyu
    google_scholar_id: https://scholar.google.com/citations?user=XyRHz8cAAAAJ&hl=en
    homepage: https://crystina-z.github.io/
    institution: University of Waterloo
    last_name: Zhang
    name: Xinyu Zhang
    orcid: https://orcid.org/0009-0009-0756-8110
    semantic_scholar_id: https://www.semanticscholar.org/author/Xinyu-Crystina-Zhang/2118895402
    username: ~Xinyu_Zhang30
  - emails: '****@uwaterloo.ca'
    first_name: Xueguang
    google_scholar_id: https://scholar.google.com/citations?user=4kvcmkQAAAAJ&hl=en
    last_name: Ma
    name: Xueguang Ma
    username: ~Xueguang_Ma1
  - dblp_id: https://dblp.org/pid/00/7739
    emails: '****@uwaterloo.ca'
    first_name: Jimmy
    homepage: https://cs.uwaterloo.ca/~jimmylin/
    institution: University of Waterloo
    last_name: Lin
    name: Jimmy Lin
    username: ~Jimmy_Lin2
  - emails: '****@comcast.com'
    first_name: Ferhan
    google_scholar_id: https://scholar.google.com/citations?user=flWTTygAAAAJ&hl=en
    homepage: https://ferhanture.com/
    last_name: Ture
    name: Ferhan Ture
    semantic_scholar_id: https://www.semanticscholar.org/author/Ferhan-Ture/2851411
    username: ~Ferhan_Ture1
  decision: toMainConference
  end_page: 2507
  file: 224.pdf
  id: 224
  num_pages: 14
  openreview_id: qdsvbFGwUK
  pdf_file: 7a130355ca6205d3ebf07ac85186b0f537f91177.pdf
  start_page: 2494
  title: 'Found in the Middle: Permutation Self-Consistency Improves Listwise Ranking
    in Large Language Models'
- abstract: 'Large Language Models (LLMs) have achieved remarkable success, where
    instruction tuning is the critical step in aligning LLMs with user intentions.
    In this work, we investigate how the instruction tuning adjusts pre-trained models
    with a focus on intrinsic changes. Specifically, we first develop several local
    and global explanation methods, including a gradient-based method for input-output
    attribution, and techniques for interpreting patterns and concepts in self-attention
    and feed-forward layers. The impact of instruction tuning is then studied by comparing
    the explanations derived from the pre-trained and instruction-tuned models. This
    approach provides an internal perspective of the model shifts on a human-comprehensible
    level. Our findings reveal three significant impacts of instruction tuning: 1)
    It empowers LLMs to recognize the instruction parts of user prompts, and promotes
    the response generation constantly conditioned on the instructions. 2) It encourages
    the self-attention heads to capture more word-word relationships about instruction
    verbs. 3) It encourages the feed-forward networks to rotate their pre-trained
    knowledge toward user-oriented tasks. These insights contribute to a more comprehensive
    understanding of instruction tuning and lay the groundwork for future work that
    aims at explaining and optimizing LLMs for various applications. Our code and
    data are publicly available at https://github.com/JacksonWuxs/Interpret_Instruction_Tuning_LLMs.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - dblp_id: https://dblp.org/pid/304/1261
    emails: '****@163.com'
    first_name: Xuansheng
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=dqS36BQAAAAJ
    homepage: https://github.com/JacksonWuxs
    last_name: Wu
    name: Xuansheng Wu
    username: ~Xuansheng_Wu1
  - dblp_id: https://dblp.org/pid/203/8711
    emails: '****@gmail.com'
    first_name: Wenlin
    google_scholar_id: https://scholar.google.com/citations?user=qwo2A24AAAAJ&hl=en
    homepage: https://wenlinyao.github.io/
    institution: Tencent AI Lab
    last_name: Yao
    name: Wenlin Yao
    username: ~Wenlin_Yao1
  - dblp_id: https://dblp.org/pid/11/3124
    emails: '****@gmail.com'
    first_name: Jianshu
    google_scholar_id: https://scholar.google.com/citations?user=jQeFWdoAAAAJ&hl=en
    homepage: https://chenjianshu.github.io/
    institution: Amazon
    last_name: Chen
    name: Jianshu Chen
    username: ~Jianshu_Chen1
  - dblp_id: https://dblp.org/pid/148/9210
    emails: '****@tencent.com'
    first_name: Xiaoman
    google_scholar_id: https://scholar.google.com/citations?user=tRPF03IAAAAJ&hl=en&oi=ao
    homepage: https://panx27.github.io/homepage/
    institution: Tencent AI Lab
    last_name: Pan
    name: Xiaoman Pan
    username: ~Xiaoman_Pan2
  - emails: '****@gmail.com'
    first_name: Xiaoyang
    google_scholar_id: https://scholar.google.com/citations?user=EeppWmkAAAAJ&hl
    institution: Tencent AI Lab
    last_name: Wang
    name: Xiaoyang Wang
    username: ~Xiaoyang_Wang1
  - dblp_id: https://dblp.org/pid/145/4489
    emails: '****@uga.edu'
    first_name: Ninghao
    google_scholar_id: https://scholar.google.com/citations?user=VagMMYcAAAAJ&hl=en
    homepage: http://people.tamu.edu/~nhliu43/
    institution: University of Georgia
    last_name: Liu
    name: Ninghao Liu
    username: ~Ninghao_Liu2
  - dblp_id: https://dblp.org/pid/71/4598-1
    emails: '****@ieee.org'
    first_name: Dong
    google_scholar_id: https://scholar.google.com/citations?user=tMY31_gAAAAJ
    homepage: https://sites.google.com/view/dongyu888/
    institution: Tencent AI Lab
    last_name: Yu
    name: Dong Yu
    semantic_scholar_id: https://www.semanticscholar.org/author/Dong-Yu/144580027
    username: ~Dong_Yu2
  decision: toMainConference
  end_page: 2536
  file: 225.pdf
  id: 225
  num_pages: 29
  openreview_id: PLBpTyu2Z2
  pdf_file: 8d545aa57746a81bd1dc7fecb046769672f482c0.pdf
  start_page: 2508
  title: 'From Language Modeling to Instruction Following: Understanding the Behavior
    Shift in LLMs after Instruction Tuning'
- abstract: 'Scientific information extraction (SciIE), which aims to automatically
    extract information from scientific literature, is becoming more important than
    ever. However, there are no existing SciIE datasets for polymer materials, which
    is an important class of materials used ubiquitously in our daily lives. To bridge
    this gap, we introduce POLYIE, a new SciIE dataset for polymer materials. POLYIE
    is curated from 146 full-length polymer scholarly articles, which are annotated
    with different named entities (i.e., materials, properties, values, conditions)
    as well as their N-ary relations by domain experts. POLYIE presents several unique
    challenges due to diverse lexical formats of entities, ambiguity between entities,
    and variable-length relations. We evaluate state-of-the-art named entity extraction
    and relation extraction models on POLYIE, analyze their strengths and weaknesses,
    and highlight some difficult cases for these models. To the best of our knowledge,
    POLYIE is the first SciIE benchmark for polymer materials, and we hope it will
    lead to more research efforts from the community on this challenging task. Our
    code and data are available on: https://github.com/jerry3027/PolyIE.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - emails: '****@gmail.com'
    first_name: Jerry
    last_name: Cheung
    middle_name: Junyang
    name: Jerry Junyang Cheung
    username: ~Jerry_Junyang_Cheung1
  - dblp_id: https://dblp.org/pid/191/5231.html
    emails: '****@gatech.edu'
    first_name: Yuchen
    google_scholar_id: https://scholar.google.com/citations?user=T-f6XlEAAAAJ&hl=en
    homepage: https://night-chen.github.io/
    institution: Georgia Institute of Technology
    last_name: Zhuang
    name: Yuchen Zhuang
    username: ~Yuchen_Zhuang1
  - dblp_id: https://dblp.org/pid/15/1534
    emails: '****@gatech.edu'
    first_name: Yinghao
    google_scholar_id: https://scholar.google.com/citations?user=2WSooDIAAAAJ
    homepage: https://yinghao-li.github.io/
    last_name: Li
    name: Yinghao Li
    orcid: https://orcid.org/0000-0002-7188-4136
    semantic_scholar_id: https://www.semanticscholar.org/author/Yinghao-Li/1527089853
    username: ~Yinghao_Li3
  - dblp_id: https://dblp.org/pid/293/8265
    emails: '****@gmail.com'
    first_name: Pranav
    google_scholar_id: https://scholar.google.com/citations?user=i3Z-9lQAAAAJ&hl=en
    homepage: https://www.pranavshetty.com
    institution: J.P. Morgan Chase
    last_name: Shetty
    name: Pranav Shetty
    orcid: https://orcid.org/0000-0003-2015-9556
    semantic_scholar_id: https://www.semanticscholar.org/author/Pranav-Shetty/47092843
    username: ~Pranav_Shetty1
  - emails: '****@gatech.edu'
    first_name: Wantian
    homepage: http://wantianz.linkedin
    institution: Georgia Institute of Technology
    last_name: Zhao
    name: Wantian Zhao
    username: ~Wantian_Zhao1
  - emails: '****@gatech.edu'
    first_name: Sanjeev
    google_scholar_id: https://scholar.google.com/citations?user=UfqG2roAAAAJ&hl=en
    homepage: https://sanjeevg15.github.io/
    last_name: Grampurohit
    name: Sanjeev Grampurohit
    orcid: https://orcid.org/0000-0002-7798-9350
    semantic_scholar_id: https://www.semanticscholar.org/author/Sanjeev-Grampurohit/2044444769
    username: ~Sanjeev_Grampurohit1
  - emails: '****@gatech.edu'
    first_name: Rampi
    homepage: https://ramprasad.mse.gatech.edu
    institution: Georgia Institute of Technology
    last_name: Ramprasad
    name: Rampi Ramprasad
    username: ~Rampi_Ramprasad1
  - dblp_id: https://dblp.org/pid/94/3019-14.html
    emails: '****@gatech.edu'
    first_name: Chao
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=CeEO6SIAAAAJ&view_op=list_works&sortby=pubdate
    homepage: http://chaozhang.org/
    institution: Georgia Institute of Technology
    last_name: Zhang
    name: Chao Zhang
    username: ~Chao_Zhang15
  decision: toMainConference
  end_page: 2552
  file: 230.pdf
  id: 230
  num_pages: 16
  openreview_id: TcvwZPEpGR
  pdf_file: b8cc069780657a7b66c52fbdd4827dbb67026c3c.pdf
  start_page: 2537
  title: 'POLYIE: A Dataset of Information Extraction from Polymer Material Scientific
    Literature'
- abstract: Large Language Models (LLMs), such as GPT3.5, have exhibited remarkable
    proficiency in comprehending and generating natural language. On the other hand,
    medical assistants hold the potential to offer substantial benefits for individuals.
    However, the exploration of LLM-based personalized medical assistant remains relatively
    scarce.  Typically, patients converse differently based on their background and
    preferences which necessitates the task of enhancing user-oriented medical assistant.
    While one can fully train an LLM for this objective, the resource consumption
    is unaffordable. Prior research has explored memory-based methods to enhance the
    response with aware of previous mistakes for new queries during a dialogue session.
    We contend that a mere memory module is inadequate and fully training an LLM can
    be excessively costly. In this study, we propose a novel computational bionic
    memory mechanism, equipped with a parameter-efficient fine-tuning (PEFT) schema,
    to personalize medical assistants. To encourage further research into this area,
    we are releasing a new conversation dataset generated based on an open-source
    medical corpus and our implementation.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Dialogue and Interactive Systems
  authors:
  - emails: '****@wpi.edu'
    first_name: Kai
    google_scholar_id: https://scholar.google.com/citations?user=jL6dEN4AAAAJ&hl=zh-CN
    homepage: https://users.wpi.edu/~kzhang8/
    last_name: Zhang
    name: Kai Zhang
    orcid: https://orcid.org/0000-0003-4783-6705
    semantic_scholar_id: https://www.semanticscholar.org/author/Kai-Zhang/2152981199
    username: ~Kai_Zhang18
  - dblp_id: https://dblp.org/pid/162/0109
    emails: '****@alibaba-inc.com'
    first_name: Yangyang
    google_scholar_id: https://scholar.google.com/citations?hl=zh-CN&user=EuJk27UAAAAJ&view_op=list_works&sortby=pubdate
    institution: Alibaba Group
    last_name: Kang
    name: Yangyang Kang
    username: ~Yangyang_Kang1
  - dblp_id: https://dblp.uni-trier.de/pid/249/5765.html
    emails: '****@alibaba-inc.com'
    first_name: Fubang
    institution: Alibaba Group
    last_name: Zhao
    name: Fubang Zhao
    username: ~Fubang_Zhao3
  - dblp_id: https://dblp.org/pid/11/6389.html
    emails: '****@wpi.edu'
    first_name: Xiaozhong
    google_scholar_id: https://scholar.google.com/citations?user=1BUByMcAAAAJ&hl=en
    homepage: https://www.wpi.edu/people/faculty/xliu14
    institution: Worcester Polytechnic Institute
    last_name: Liu
    name: Xiaozhong Liu
    username: ~Xiaozhong_Liu2
  decision: toMainConference
  end_page: 2565
  file: 231.pdf
  id: 231
  num_pages: 13
  openreview_id: BWqqGFykB2
  pdf_file: 898725be7a9dc4369f6310a40869fc8019aacc58.pdf
  start_page: 2553
  title: LLM-based Medical Assistant Personalization with Short- and Long-Term Memory
    Coordination
- abstract: Cross-lingual summarization (XLS) generates summaries in a language different
    from that of the input documents (e.g., English to Spanish), allowing speakers
    of the target language to gain a concise view of their content. In the present
    day, the predominant approach to this task is to take a performing, pretrained
    multilingual language model (LM) and fine-tune it for XLS on the language pairs
    of interest. However, the scarcity of fine-tuning samples makes this approach
    challenging in some cases. For this reason, in this paper we propose revisiting
    the summarize-and-translate pipeline, where the summarization and translation
    tasks are performed in a sequence. This approach allows reusing the many, publicly-available
    resources for monolingual summarization and translation, obtaining a very competitive
    zero-shot performance. In addition, the proposed pipeline is completely differentiable
    end-to-end, allowing it to take advantage of few-shot fine-tuning, where available.
    Experiments over two contemporary and widely adopted XLS datasets (CrossSum and
    WikiLingua) have shown the remarkable zero-shot performance of the proposed approach,
    and also its strong few-shot performance compared to an equivalent multilingual
    LM baseline, that the proposed approach has been able to outperform in many languages
    with only 10% of the fine-tuning samples.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Summarization
  authors:
  - dblp_id: https://dblp.org/pid/159/7013
    emails: '****@rozettatechnology.com'
    first_name: Jacob
    google_scholar_id: https://scholar.google.com.au/citations?user=m8xgDd4AAAAJ&hl=en&authuser=1
    last_name: Parnell
    name: Jacob Parnell
    orcid: https://orcid.org/0009-0001-1624-550X
    semantic_scholar_id: https://www.semanticscholar.org/author/Jacob-Parnell/2093095932
    username: ~Jacob_Parnell1
  - dblp_id: https://dblp.org/pid/202/2331
    emails: '****@gmail.com'
    first_name: Inigo
    google_scholar_id: https://scholar.google.com.au/citations?user=yBWhSgIAAAAJ&hl=en&oi=ao
    institution: University of Technology Sydney and Rozetta Technology
    last_name: Jauregi Unanue
    name: Inigo Jauregi Unanue
    orcid: https://orcid.org/0000-0001-6223-9584
    semantic_scholar_id: https://www.semanticscholar.org/author/Inigo-Jauregi-Unanue/19229462
    username: ~Inigo_Jauregi_Unanue2
  - dblp_id: https://dblp.org/pid/p/MassimoPiccardi
    emails: '****@uts.edu.au'
    first_name: Massimo
    google_scholar_id: https://scholar.google.com.au/citations?user=9OvNWLUAAAAJ&hl=en
    homepage: https://profiles.uts.edu.au/Massimo.Piccardi
    institution: University of Technology Sydney (UTS)
    last_name: Piccardi
    name: Massimo Piccardi
    orcid: https://orcid.org/0000-0001-9250-6604
    semantic_scholar_id: https://www.semanticscholar.org/author/M.-Piccardi/35153150
    username: ~Massimo_Piccardi1
  decision: toMainConference
  end_page: 2582
  file: 232.pdf
  id: 232
  num_pages: 17
  openreview_id: cwBWY7R9VL
  pdf_file: 16ec5576d639b910d490b63cdfc48b3c78697d71.pdf
  start_page: 2566
  title: 'SumTra: A Differentiable Pipeline for Few-Shot Cross-Lingual Summarization'
- abstract: Large language models (LLM) have recently shown the extraordinary ability
    to perform unseen tasks based on few-shot examples provided as text, also known
    as in-context learning (ICL). While recent works have attempted to understand
    the mechanisms driving ICL, few have explored training strategies that incentivize
    these models to generalize to multiple tasks. Multi-task learning (MTL) for generalist
    models is a promising direction that offers transfer learning potential, enabling
    large parameterized models to be trained from simpler, related tasks. In this
    work, we investigate the combination of MTL with ICL to build models that efficiently
    learn tasks while being robust to out-of-distribution examples. We propose several
    effective curriculum learning strategies that allow ICL models to achieve higher
    data efficiency and more stable convergence. Our experiments reveal that ICL models
    can effectively learn difficult tasks by training on progressively harder tasks
    while mixing in prior tasks, denoted as mixed curriculum in this work.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - emails: '****@wisc.edu'
    first_name: Harmon
    google_scholar_id: https://scholar.google.com/citations?user=60B9CO0AAAAJ&hl=en&oi=ao
    homepage: https://www.harmonbhasin.com/
    last_name: Bhasin
    name: Harmon Bhasin
    orcid: https://orcid.org/0009-0003-1569-5749
    username: ~Harmon_Bhasin1
  - emails: '****@wisc.edu'
    first_name: Timothy
    last_name: Ossowski
    name: Timothy Ossowski
    semantic_scholar_id: https://www.semanticscholar.org/author/Timothy-Ossowski/2166137162
    username: ~Timothy_Ossowski1
  - dblp_id: https://dblp.org/pid/140/7265
    emails: '****@wisc.edu'
    first_name: Yiqiao
    homepage: https://pages.stat.wisc.edu/~zhong35/
    institution: University of Wisconsin - Madison
    last_name: Zhong
    name: Yiqiao Zhong
    username: ~Yiqiao_Zhong1
  - dblp_id: https://dblp.org/pid/123/0773-1.html
    emails: '****@wisc.edu'
    first_name: Junjie
    google_scholar_id: https://scholar.google.com/citations?user=j-42gHYAAAAJ
    homepage: https://junjiehu.github.io/
    institution: University of Wisconsin, Madison
    last_name: Hu
    name: Junjie Hu
    username: ~Junjie_Hu2
  decision: toMainConference
  end_page: 2601
  file: 233.pdf
  id: 233
  num_pages: 19
  openreview_id: ZJ91L3XvqF
  pdf_file: 96d18fa7e4ce1025a1b2d8a4c87c4b6e7648eb43.pdf
  start_page: 2583
  title: How does Multi-Task Training Affect Transformer In-Context Capabilities?
    Investigations with Function Classes
- abstract: 'We introduce a new problem KTRL+F, a knowledge-augmented in-document
    search that necessitates real-time identification of all semantic targets within
    a document with the awareness of external sources through a single natural query.
    KTRL+F addresses following unique challenges for in-document search: 1) utilizing
    knowledge outside the document for extended use of additional information about
    targets, and 2) balancing between real-time applicability with the performance.

    We analyze various baselines in KTRL+F and find limitations of existing models,
    such as hallucinations, high latency, or difficulties in leveraging external knowledge.
    Therefore, we propose a Knowledge-Augmented Phrase Retrieval model that shows
    a promising balance between speed and performance by simply augmenting external
    knowledge in phrase embedding. We also conduct a user study to verify whether
    solving KTRL+F can enhance search experience for users. It demonstrates that even
    with our simple model, users can reduce the time for searching with less queries
    and reduced extra visits to other sources for collecting evidence. We encourage
    the research community to work on KTRL+F to enhance more efficient in-document
    information access.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - dblp_id: https://dblp.org/pid/304/2544
    emails: '****@gmail.com'
    first_name: Hanseok
    homepage: https://hanseokoh.github.io/
    last_name: Oh
    name: Hanseok Oh
    semantic_scholar_id: https://www.semanticscholar.org/author/Hanseok-Oh/2150052936
    username: ~Hanseok_Oh1
  - emails: '****@gmail.com'
    first_name: Haebin
    institution: Samsung
    last_name: Shin
    name: Haebin Shin
    username: ~Haebin_Shin1
  - dblp_id: https://dblp.org/pid/204/2947
    emails: '****@kaist.ac.kr'
    first_name: Miyoung
    institution: Korea Advanced Institute of Science and Technology
    last_name: Ko
    name: Miyoung Ko
    semantic_scholar_id: https://www.semanticscholar.org/author/Miyoung-Ko/22670284
    username: ~Miyoung_Ko1
  - emails: '****@gmail.com'
    first_name: Hyunji
    google_scholar_id: https://scholar.google.com/citations?user=LQ-52vsAAAAJ&hl=en
    homepage: https://amy-hyunji.github.io/
    institution: Korea Advanced Institute of Science & Technology
    last_name: Lee
    name: Hyunji Lee
    semantic_scholar_id: https://www.semanticscholar.org/author/Hyunji-Lee/2140191673
    username: ~Hyunji_Lee1
  - dblp_id: https://dblp.org/pid/149/1367
    emails: '****@gmail.com'
    first_name: Minjoon
    google_scholar_id: https://scholar.google.com/citations?user=zYze5fIAAAAJ&hl=en
    homepage: https://seominjoon.github.io
    institution: Korea Advanced Institute of Science and Technology
    last_name: Seo
    name: Minjoon Seo
    semantic_scholar_id: https://www.semanticscholar.org/author/Minjoon-Seo/4418074
    username: ~Minjoon_Seo1
  decision: toMainConference
  end_page: 2622
  file: 234.pdf
  id: 234
  num_pages: 21
  openreview_id: DqxENWotdN
  pdf_file: d8fb8fab8d1a8ba8e4c5b65d056d51a674910efd.pdf
  start_page: 2602
  title: 'KTRL+F: Knowledge-Augmented In-Document Search'
- abstract: 'To reduce issues like hallucinations and lack of control in Large Language
    Models (LLMs), a common method is to generate responses by grounding on external
    contexts given as input, known as knowledge-augmented models. However, previous
    research often narrowly defines "grounding" as just having the correct answer,
    which does not ensure the reliability of the entire response. To overcome this,
    we propose a stricter definition of grounding: a model is truly grounded if it
    (1) fully utilizes the necessary knowledge from the provided context, and (2)
    stays within the limits of that knowledge. We introduce a new dataset and a grounding
    metric to evaluate model capability under the definition. We perform experiments
    across 25 LLMs of different sizes and training methods and provide insights into
    factors that influence grounding performance. Our findings contribute to a better
    understanding of how to improve grounding capabilities and suggest an area of
    improvement toward more reliable and controllable LLM applications.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - emails: '****@gmail.com'
    first_name: Hyunji
    google_scholar_id: https://scholar.google.com/citations?user=LQ-52vsAAAAJ&hl=en
    homepage: https://amy-hyunji.github.io/
    institution: Korea Advanced Institute of Science & Technology
    last_name: Lee
    name: Hyunji Lee
    semantic_scholar_id: https://www.semanticscholar.org/author/Hyunji-Lee/2140191673
    username: ~Hyunji_Lee1
  - dblp_id: https://dblp.org/pid/329/2497
    emails: '****@naver.com'
    first_name: Se June
    google_scholar_id: https://scholar.google.com/
    homepage: https://joocjun.github.io/
    institution: Korea Advanced Institute of Science & Technology
    last_name: Joo
    name: Se June Joo
    semantic_scholar_id: https://www.semanticscholar.org/author/S.-Joo/102540266
    username: ~Se_June_Joo1
  - emails: '****@gmail.com'
    first_name: Chaeeun
    homepage: https://www.linkedin.com/in/chaeeun-kim-a68025251/
    last_name: Kim
    name: Chaeeun Kim
    username: ~Chaeeun_Kim2
  - emails: '****@gmail.com'
    first_name: Joel
    google_scholar_id: https://scholar.google.com/citations?user=xL-7eFEAAAAJ
    homepage: https://joeljang.github.io/
    last_name: Jang
    name: Joel Jang
    username: ~Joel_Jang1
  - dblp_id: https://dblp.org/pid/42/3152
    emails: '****@kaist.ac.kr'
    first_name: Doyoung
    last_name: Kim
    name: Doyoung Kim
    semantic_scholar_id: https://www.semanticscholar.org/author/Doyoung-Kim/2180527259
    username: ~Doyoung_Kim3
  - dblp_id: https://dblp.org/pid/175/0873
    emails: '****@kakaobrain.com'
    first_name: Kyoung-Woon
    last_name: 'On'
    name: Kyoung-Woon On
    username: ~Kyoung-Woon_On1
  - dblp_id: https://dblp.org/pid/149/1367
    emails: '****@gmail.com'
    first_name: Minjoon
    google_scholar_id: https://scholar.google.com/citations?user=zYze5fIAAAAJ&hl=en
    homepage: https://seominjoon.github.io
    institution: Korea Advanced Institute of Science and Technology
    last_name: Seo
    name: Minjoon Seo
    semantic_scholar_id: https://www.semanticscholar.org/author/Minjoon-Seo/4418074
    username: ~Minjoon_Seo1
  decision: toMainConference
  end_page: 2651
  file: 235.pdf
  id: 235
  num_pages: 29
  openreview_id: tvElb6CC6z
  pdf_file: 983c6e963b4f3f047b80a515da13e0f8062f62b6.pdf
  start_page: 2623
  title: How Well Do Large Language Models Truly Ground?
- abstract: In text ranking, it is generally believed that the cross-encoders already
    gather sufficient token interaction information via the attention mechanism in
    the hidden layers. However, our results show that the cross-encoders can consistently
    benefit from additional token interaction in the similarity computation at the
    last layer. We introduce CELI (Cross-Encoder with Late Interaction), which incorporates
    a late interaction layer into the current cross-encoder models. This simple method
    brings 5% improvement on BEIR without compromising in-domain effectiveness or
    search latency. Extensive experiments show that this finding is consistent across
    different sizes of the cross-encoder models and the first-stage retrievers. Our
    findings suggest that boiling all information into the [CLS] token is a suboptimal
    use for cross-encoders, and advocate further studies to investigate its relevance
    score mechanism.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Retrieval and Text Mining
  authors:
  - dblp_id: https://dblp.org/pid/58/4582
    emails: '****@uwaterloo.ca'
    first_name: Xinyu
    google_scholar_id: https://scholar.google.com/citations?user=XyRHz8cAAAAJ&hl=en
    homepage: https://crystina-z.github.io/
    institution: University of Waterloo
    last_name: Zhang
    name: Xinyu Zhang
    orcid: https://orcid.org/0009-0009-0756-8110
    semantic_scholar_id: https://www.semanticscholar.org/author/Xinyu-Crystina-Zhang/2118895402
    username: ~Xinyu_Zhang30
  - emails: '****@uwaterloo.ca'
    first_name: Minghan
    homepage: https://alexlimh.github.io/
    last_name: Li
    name: Minghan Li
    orcid: https://orcid.org/0009-0007-8972-7714
    semantic_scholar_id: https://www.semanticscholar.org/author/Minghan-Li/2135230554
    username: ~Minghan_Li4
  - dblp_id: https://dblp.org/pid/00/7739
    emails: '****@uwaterloo.ca'
    first_name: Jimmy
    homepage: https://cs.uwaterloo.ca/~jimmylin/
    institution: University of Waterloo
    last_name: Lin
    name: Jimmy Lin
    username: ~Jimmy_Lin2
  decision: toMainConference
  end_page: 2660
  file: 236.pdf
  id: 236
  num_pages: 9
  openreview_id: VOxzbmWavw
  pdf_file: 85fc803e21c3a0765258a3b5e0123d04bf7f8dc5.pdf
  start_page: 2652
  title: 'CELI: Simple yet Effective Approach to Enhance Out-of-Domain Generalization
    of Cross-Encoders.'
- abstract: "Mental health issues differ widely among individuals, with varied signs\
    \ and symptoms. Recently, language-based assessments have\nshown promise in capturing\
    \ this diversity, but they require a substantial sample of words per person for\
    \ accuracy. This work introduces\nthe task of Adaptive Language-Based Assessment\
    \ (ALBA), which involves adaptively ordering questions while also scoring an individual\u2019\
    s latent psychological trait using limited language responses to previous questions.\
    \ To this end, we develop adaptive testing methods under two psychometric measurement\
    \ theories: Classical Test Theory and Item Response Theory.\nWe empirically evaluate\
    \ ordering and scoring strategies, organizing into two new methods: a semi-supervised\
    \ item response theory-based\nmethod (ALIRT) and a supervised Actor-Critic model.\
    \ While we found both methods to improve over non-adaptive baselines, We found\n\
    ALIRT to be the most accurate and scalable, achieving the highest accuracy with\
    \ fewer questions (e.g., Pearson r \u2248 0.93 after only 3 questions as compared\
    \ to typically needing at least 7 questions). In general, adaptive language-based\
    \ assessments of depression and anxiety were able to utilize a smaller sample\
    \ of language without compromising validity or large computational costs."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - emails: '****@cs.stonybrook.edu'
    first_name: Vasudha
    homepage: https://vasevarad.github.io/
    last_name: Varadarajan
    name: Vasudha Varadarajan
    username: ~Vasudha_Varadarajan1
  - emails: '****@psy.lu.se'
    first_name: Sverker
    last_name: "Sikstr\xF6m"
    name: "Sverker Sikstr\xF6m"
    username: "~Sverker_Sikstr\xF6m1"
  - dblp_id: https://dblp.org/pid/360/6523.html
    emails: '****@psy.lu.se'
    first_name: Oscar
    google_scholar_id: https://scholar.google.com/citations?user=cgEziNIAAAAJ&hl=sv&oi=ao
    homepage: https://www.oscarkjell.se
    last_name: Kjell
    name: Oscar Kjell
    orcid: https://orcid.org/0000-0002-2728-6278
    semantic_scholar_id: https://www.semanticscholar.org/author/O.-Kjell/5033059
    username: ~Oscar_Kjell1
  - dblp_id: https://dblp.org/pid/46/3430
    emails: '****@cs.stonybrook.edu'
    first_name: H.
    google_scholar_id: https://scholar.google.com.tw/citations?user=Na16PsUAAAAJ
    homepage: http://www3.cs.stonybrook.edu/~has/
    institution: Stony Brook University (SUNY)
    last_name: Schwartz
    name: H. Schwartz
    orcid: https://orcid.org/0000-0002-6383-3339
    semantic_scholar_id: https://www.semanticscholar.org/author/H.-A.-Schwartz/145035129
    username: ~H._Schwartz1
  decision: toMainConference
  end_page: 2673
  file: 238.pdf
  id: 238
  num_pages: 13
  openreview_id: 1yZyvEFKjD
  pdf_file: 31a9b313cd4100a63e6867a3b6400fb21ddf6612.pdf
  start_page: 2661
  title: 'ALBA: Adaptive Language-Based Assessments for Mental Health'
- abstract: Table Question Answering (TQA) aims at composing an answer to a question
    based on tabular data. While prior research has shown that TQA models lack robustness,
    understanding the underlying cause and nature of this issue remains predominantly
    unclear, posing a significant obstacle to the development of robust TQA systems.
    In this paper, we formalize three major desiderata for a fine-grained evaluation
    of robustness of TQA systems. They should (i) answer questions regardless of alterations
    in table structure, (ii) base their responses on the content of relevant cells
    rather than on biases, and (iii) demonstrate robust numerical reasoning capabilities.
    To investigate these aspects, we create and publish a novel TQA evaluation benchmark
    in English. Our extensive experimental analysis reveals that none of the examined
    state-of-the-art TQA systems consistently excels in these three aspects. Our benchmark
    is a crucial instrument for monitoring the behavior of TQA systems and paves the
    way for the development of robust TQA systems. We release our benchmark publicly.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - emails: '****@de.bosch.com'
    first_name: Wei
    institution: Robert Bosch GmbH, Bosch
    last_name: Zhou
    name: Wei Zhou
    username: ~Wei_Zhou13
  - dblp_id: https://dblp.org/pid/140/3476
    emails: '****@gmail.com'
    first_name: Mohsen
    institution: Bosch
    last_name: Mesgar
    name: Mohsen Mesgar
    username: ~Mohsen_Mesgar1
  - dblp_id: https://dblp.org/pid/132/6980
    emails: '****@gmail.com'
    first_name: Heike
    google_scholar_id: https://scholar.google.de/citations?user=Fejbq9kAAAAJ
    homepage: https://sites.google.com/view/heikeadel
    institution: Hochschule der Medien (University of Applied Sciences)
    last_name: Adel
    name: Heike Adel
    semantic_scholar_id: https://www.semanticscholar.org/author/Heike-Adel/145793834
    username: ~Heike_Adel1
  - dblp_id: https://dblp.org/pid/126/8745
    emails: '****@uni-a.de'
    first_name: Annemarie
    google_scholar_id: https://scholar.google.de/citations?user=8CVIK-UAAAAJ&hl=de&oi=ao
    homepage: https://annefried.github.io
    institution: University of Augsburg
    last_name: Friedrich
    name: Annemarie Friedrich
    semantic_scholar_id: https://www.semanticscholar.org/author/Annemarie-Friedrich/33985877
    username: ~Annemarie_Friedrich1
  decision: toMainConference
  end_page: 2692
  file: 239.pdf
  id: 239
  num_pages: 19
  openreview_id: KiLALnXKIh
  pdf_file: 467e5b28d2fa1ea25f31c78cf4cdc136c4ade4a5.pdf
  start_page: 2674
  title: 'FREB-TQA: A Fine-Grained Robustness Evaluation Benchmark for Table Question
    Answering'
- abstract: Query expansion, pivotal in search engines, enhances the representation
    of user information needs with additional terms. While existing methods expand
    queries using retrieved or generated contextual documents, each approach has notable
    limitations. Retrieval-based methods often fail to accurately capture search intent,
    particularly with brief or ambiguous queries. Generation-based methods, utilizing
    large language models (LLMs), generally lack corpus-specific knowledge and entail
    high fine-tuning costs. To address these gaps, we propose a novel zero-shot query
    expansion framework utilizing LLMs for mutual verification. Specifically, we first
    design a query-query-document generation method, leveraging LLMs' zero-shot reasoning
    ability to produce diverse sub-queries and corresponding documents. Then, a mutual
    verification process synergizes generated and retrieved documents for optimal
    expansion. Our proposed method is fully zero-shot, and extensive experiments on
    three public benchmark datasets are conducted to demonstrate its effectiveness
    over existing methods. Our code is available online at https://github.com/Applied-Machine-Learning-Lab/MILL
    to ease reproduction.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Retrieval and Text Mining
  authors:
  - emails: '****@zju.edu.cn'
    first_name: Pengyue
    institution: City University of Hong Kong
    last_name: Jia
    name: Pengyue Jia
    username: ~Pengyue_Jia1
  - dblp_id: https://dblp.org/pid/155/1107
    emails: '****@gmail.com'
    first_name: Yiding
    google_scholar_id: https://scholar.google.com/citations?user=c7oiMdIAAAAJ&hl=en
    homepage: https://liuyiding.net
    institution: Baidu
    last_name: Liu
    name: Yiding Liu
    username: ~Yiding_Liu1
  - dblp_id: https://dblp.org/pid/08/890
    emails: '****@cityu.edu.hk'
    first_name: Xiangyu
    homepage: https://zhaoxyai.github.io/
    institution: City University of Hong Kong
    last_name: Zhao
    name: Xiangyu Zhao
    username: ~Xiangyu_Zhao1
  - emails: '****@my.cityu.edu.hk'
    first_name: Xiaopeng
    google_scholar_id: https://scholar.google.com/citations?user=hCCqgQQAAAAJ&hl=en
    homepage: https://xiaopengli1.github.io/
    last_name: LI
    name: Xiaopeng LI
    orcid: https://orcid.org/0009-0008-6162-8500
    username: ~Xiaopeng_LI7
  - dblp_id: https://dblp.org/pid/260/2038.html
    emails: '****@gmail.com'
    first_name: Changying
    last_name: Hao
    name: Changying Hao
    username: ~Changying_Hao1
  - dblp_id: https://dblp.org/pid/16/1524
    emails: '****@gmail.com'
    first_name: Shuaiqiang
    google_scholar_id: https://scholar.google.com.hk/citations?user=8SbYYcIAAAAJ&hl=en
    homepage: http://wangshuaiqiang.net/
    institution: Baidu Inc.
    last_name: Wang
    name: Shuaiqiang Wang
    semantic_scholar_id: https://www.semanticscholar.org/author/Shuaiqiang-Wang/2386396
    username: ~Shuaiqiang_Wang2
  - dblp_id: https://dblp.org/pid/91/4572
    emails: '****@acm.org'
    first_name: Dawei
    google_scholar_id: https://scholar.google.com/citations?user=GuQ9bpAAAAAJ&hl=zh-CN
    institution: Baidu
    last_name: Yin
    name: Dawei Yin
    username: ~Dawei_Yin1
  decision: toMainConference
  end_page: 2713
  file: 240.pdf
  id: 240
  num_pages: 21
  openreview_id: FPQ3KiLWgF
  pdf_file: 9235d1f1557543d2db8f5d8a19ff21f6461707ae.pdf
  start_page: 2693
  title: 'MILL: Mutual Verification with Large Language Models for Zero-Shot Query
    Expansion'
- abstract: "The increasing versatility of language models (LMs) has given rise to\
    \ a new class of benchmarks that comprehensively assess a broad range of capabilities.\
    \ \nSuch benchmarks are associated with massive computational costs, extending\
    \ to thousands of GPU hours per model. However, the efficiency aspect of these\
    \ evaluation efforts had raised little discussion in the literature.\nIn this\
    \ work, we present the problem of Efficient Benchmarking, namely, intelligently\
    \ reducing the computation costs of LM evaluation without compromising reliability.\
    \ \nUsing the HELM benchmark as a test case, we investigate how different benchmark\
    \ design choices affect the computation-reliability trade-off. \nWe propose to\
    \ evaluate the reliability of such decisions, by using a new measure -- Decision\
    \ Impact on Reliability,  DIoR for short.\nWe find, for example, that a benchmark\
    \ leader may change by merely removing a low-ranked model from the benchmark,\
    \ and observe that a correct benchmark ranking can be obtained by considering\
    \ only a fraction of the evaluation examples.\nBased on our findings, we outline\
    \ a set of concrete recommendations for efficient benchmark design and utilization\
    \ practices. To take a step further, we use our findings to propose an evaluation\
    \ algorithm, that, when applied to the HELM benchmark, leads to dramatic cost\
    \ savings with minimal loss of benchmark reliability, often reducing computation\
    \ by x100 or more."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Efficient/Low-Resource Methods for NLP
  authors:
  - dblp_id: https://dblp.org/pid/298/0781
    emails: '****@gmail.com'
    first_name: Yotam
    google_scholar_id: https://scholar.google.com/citations?user=n9iywfAAAAAJ&hl=en
    institution: International Business Machines
    last_name: Perlitz
    name: Yotam Perlitz
    semantic_scholar_id: https://www.semanticscholar.org/author/Yotam-Perlitz/102376484
    username: ~Yotam_Perlitz1
  - emails: '****@gmail.com'
    first_name: Elron
    homepage: https://linkedin.com/in/elron
    institution: International Business Machines
    last_name: Bandel
    name: Elron Bandel
    username: ~Elron_Bandel1
  - dblp_id: https://dblp.org/pid/245/8586
    emails: '****@ibm.com'
    first_name: Ariel
    google_scholar_id: https://scholar.google.co.il/citations?user=ESCkne8AAAAJ&hl=en
    institution: International Business Machines
    last_name: Gera
    name: Ariel Gera
    semantic_scholar_id: https://www.semanticscholar.org/author/Ariel-Gera/48835746
    username: ~Ariel_Gera1
  - emails: '****@mail.huji.ac.il'
    first_name: Ofir
    institution: Hebrew University of Jerusalem and Computer Science Departmen, Technion-Israel
      Institute of Technology
    last_name: Arviv
    name: Ofir Arviv
    username: ~Ofir_Arviv1
  - dblp_id: https://dblp.org/pid/78/3923.html
    emails: '****@gmail.com'
    first_name: Liat
    google_scholar_id: https://scholar.google.com/citations?user=V_IZ86YAAAAJ&hl=en
    homepage: https://researcher.watson.ibm.com/researcher/view.php?person=il-LIATE
    last_name: Ein-Dor
    name: Liat Ein-Dor
    semantic_scholar_id: https://www.semanticscholar.org/author/L.-Ein-Dor/1402680837
    username: ~Liat_Ein-Dor2
  - dblp_id: https://dblp.org/pid/67/2631
    emails: '****@il.ibm.com'
    first_name: Eyal
    google_scholar_id: https://scholar.google.co.il/citations?user=UHLsHeMAAAAJ&hl=en
    homepage: https://researcher.watson.ibm.com/researcher/view.php?person=il-EYALS
    last_name: Shnarch
    name: Eyal Shnarch
    username: ~Eyal_Shnarch1
  - dblp_id: https://dblp.org/pid/62/7001
    emails: '****@il.ibm.com'
    first_name: Noam
    google_scholar_id: https://scholar.google.co.il/citations?user=KjvrNGMAAAAJ&hl=iw
    homepage: https://researcher.watson.ibm.com/researcher/view.php?person=il-NOAMS
    institution: International Business Machines
    last_name: Slonim
    name: Noam Slonim
    semantic_scholar_id: https://www.semanticscholar.org/author/N.-Slonim/1766595
    username: ~Noam_Slonim1
  - dblp_id: https://dblp.org/pid/s/MShmueliS.html
    emails: '****@il.ibm.com'
    first_name: Michal
    google_scholar_id: https://scholar.google.com/citations?user=reNMHusAAAAJ&hl=en
    last_name: Shmueli-Scheuer
    name: Michal Shmueli-Scheuer
    username: ~Michal_Shmueli-Scheuer2
  - dblp_id: https://dblp.org/pid/218/5237
    emails: '****@mail.huji.ac.il'
    first_name: Leshem
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=8b8IhUYAAAAJ
    homepage: https://ktilana.wixsite.com/leshem-choshen
    institution: International Business Machines
    last_name: Choshen
    name: Leshem Choshen
    semantic_scholar_id: https://www.semanticscholar.org/author/Leshem-Choshen/41019330?sort=total-citations
    username: ~Leshem_Choshen1
  decision: toMainConference
  end_page: 2731
  file: 242.pdf
  id: 242
  num_pages: 18
  openreview_id: AB17LN1uL2
  pdf_file: d36a71b15b70443b89e0dc46b7f832550cadbf89.pdf
  start_page: 2714
  title: Efficient Benchmarking (of Language Models)
- abstract: "Our world is marked by unprecedented technological, global, and socio-political\
    \ transformations, posing a significant challenge to textto-image generative models.\
    \ \nThese models encode factual associations within their parameters that can\
    \ quickly become outdated, diminishing their utility for end-users. \nTo that\
    \ end, we introduce ReFACT, a novel approach for editing factual associations\
    \ in text-to-image models without relaying on explicit input from end-users or\
    \ costly re-training. \nReFACT updates the weights of a specific layer in the\
    \ text encoder, modifying only a tiny portion of the model\u2019s parameters and\
    \ leaving the rest of the model unaffected.\nWe empirically evaluate ReFACT on\
    \ an existing benchmark, alongside a newly curated dataset.\nCompared to other\
    \ methods, ReFACT achieves superior performance in both generalization to related\
    \ concepts and preservation of unrelated concepts.\nFurthermore, ReFACT maintains\
    \ image generation quality, making it a practical tool for updating and correcting\
    \ factual information in text-to-image models."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multimodality and Language Grounding to Vision, Robotics and Beyond
  authors:
  - emails: '****@campus.technion.ac.il'
    first_name: Dana
    institution: Computer Science Departmen, Technion-Israel Institute of Technology
    last_name: Arad
    name: Dana Arad
    username: ~Dana_Arad1
  - emails: '****@cs.technion.ac.il'
    first_name: Hadas
    institution: Computer Science Departmen, Technion-Israel Institute of Technology
      and Technion - Israel Institute of Technology, Technion - Israel Institute of
      Technology
    last_name: Orgad
    name: Hadas Orgad
    username: ~Hadas_Orgad1
  - dblp_id: https://dblp.org/pid/136/8705
    emails: '****@technion.ac.il'
    first_name: Yonatan
    google_scholar_id: https://scholar.google.com/citations?authorid=K-6ujU4AAAAJ&user=K-6ujU4AAAAJ
    homepage: https://www.belinkov.com
    institution: Technion, Technion
    last_name: Belinkov
    name: Yonatan Belinkov
    semantic_scholar_id: https://www.semanticscholar.org/search?q=%22Yonatan%20Belinkov%22&sort=relevance
    username: ~Yonatan_Belinkov1
  decision: toMainConference
  end_page: 2753
  file: 243.pdf
  id: 243
  num_pages: 22
  openreview_id: Jvv7mwSKtQ
  pdf_file: 4ab5915382889f7571c7f062a25fd1766c4e0008.pdf
  start_page: 2732
  title: 'ReFACT: Updating Text-to-Image Models by Editing the Text Encoder'
- abstract: Multilingual pretrained language models (mPLMs) have been widely adopted
    in cross-lingual transfer, and code-mixing has demonstrated effectiveness across
    various tasks in the absence of target language data. Our contribution involves
    an in-depth investigation into the counterproductive nature of training mPLMs
    on code-mixed data for information retrieval (IR). Our finding is that while code-mixing
    demonstrates a positive effect in aligning representations across languages, it
    hampers the IR-specific objective of matching representations between queries
    and relevant passages. To balance between positive and negative effects, we introduce
    ContrastiveMix, which disentangles contrastive loss between these conflicting
    objectives, thereby enhancing zero-shot IR performance. Specifically, we leverage
    both English and code-mixed data and employ two contrastive loss functions, by
    adding an additional contrastive loss that aligns embeddings of English data with
    their code-mixed counterparts in the query encoder. Our proposed ContrastiveMix
    exhibits statistically significant outperformance compared to mDPR, particularly
    in scenarios involving lower linguistic similarity, where the conflict between
    goals is more pronounced.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Efficient/Low-Resource Methods for NLP
  authors:
  - emails: '****@snu.ac.kr'
    first_name: Junggeun
    institution: Seoul National University
    last_name: Do
    name: Junggeun Do
    username: ~Junggeun_Do1
  - dblp_id: https://dblp.org/pid/141/9456-2
    emails: '****@snu.ac.kr'
    first_name: Jaeseong
    institution: Seoul National University
    last_name: Lee
    name: Jaeseong Lee
    semantic_scholar_id: https://www.semanticscholar.org/author/Jaeseong-Lee/2125053163
    username: ~Jaeseong_Lee1
  - dblp_id: https://dblp.org/pid/h/SeungwonHwang
    emails: '****@snu.ac.kr'
    first_name: Seung-won
    google_scholar_id: https://scholar.google.com/citations?user=63bBmc3mYrAC&hl=ko
    homepage: http://seungwonh.github.io
    institution: Seoul National University
    last_name: Hwang
    name: seung-won hwang
    semantic_scholar_id: https://www.semanticscholar.org/author/Seung-won-Hwang/1716415
    username: ~seung-won_hwang2
  decision: toMainConference
  end_page: 2761
  file: 244.pdf
  id: 244
  num_pages: 8
  openreview_id: OQ9mfH0y8W
  pdf_file: 38bd411c42ffe839b8c477aedfac667a8f902760.pdf
  start_page: 2754
  title: 'ContrastiveMix: Overcoming Code-Mixing Dilemma in Cross-Lingual Transfer
    for Information Retrieval'
- abstract: Lexical resemblances among a group of languages indicate that the languages
    could be genetically related, i.e., they could have descended from a common ancestral
    language.  However, such resemblances can arise by chance and, hence, need not
    always imply an underlying genetic relationship. Many tests of significance based
    on permutation of wordlists and word similarity measures appeared in the past
    to determine the statistical significance of such relationships. We demonstrate
    that although existing tests may work well for bilateral comparisons, i.e., on
    pairs of languages, they are either infeasible by design or are prone to yield
    false positives when applied to groups of languages or language families. To this
    end, inspired by molecular phylogenetics, we propose a likelihood ratio test to
    determine if given languages are related based on the proportion of invariant
    character sites in the aligned wordlists applied during tree inference. Further,
    we evaluate some language families and show that the proposed test solves the
    problem of false positives. Finally, we demonstrate that the test supports the
    existence of macro language families such as Nostratic and Macro-Mayan.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Phonology, Morphology and Word Segmentation
  authors:
  - emails: '****@iitk.ac.in'
    first_name: V.S.D.S.Mahesh
    google_scholar_id: https://scholar.google.com/citations?user=6KmJhd0AAAAJ&hl=en&oi=ao
    homepage: https://www.cse.iitk.ac.in/users/maheshak/
    institution: IIT Kanpur, IIT Kanpur
    last_name: Akavarapu
    name: V.S.D.S.Mahesh Akavarapu
    semantic_scholar_id: https://www.semanticscholar.org/author/V.S.D.S.Mahesh-Akavarapu/2257042393
    username: ~V.S.D.S.Mahesh_Akavarapu1
  - dblp_id: https://dblp.org/pid/48/2626-1
    emails: '****@cse.iitk.ac.in'
    first_name: Arnab
    google_scholar_id: https://scholar.google.co.in/citations?user=Sk-JV9QAAAAJ&hl=en&oi=ao
    homepage: https://www.cse.iitk.ac.in/users/arnabb/
    institution: IIT Kanpur
    last_name: Bhattacharya
    name: Arnab Bhattacharya
    orcid: https://orcid.org/0000-0001-7331-0788
    semantic_scholar_id: https://www.semanticscholar.org/author/Arnab-Bhattacharya/145660316
    username: ~Arnab_Bhattacharya1
  decision: toMainConference
  end_page: 2773
  file: 245.pdf
  id: 245
  num_pages: 12
  openreview_id: pfgXM3ynIe
  pdf_file: d2d9de2639c1ef21c1d027fcdc05f1a98cda1811.pdf
  start_page: 2762
  title: A Likelihood Ratio Test of Genetic Relationship among Languages
- abstract: "While large language models (LLMs) excel in various natural language\
    \ processing tasks, their huge size and the inaccessibility of parameters present\
    \ challenges for practical deployment. Previous studies try to distill task-specific\
    \ ability from LLMs to smaller models, using data synthesis and chain-of-thought\
    \ (CoT) fine-tuning. However, synthetic CoT data often contains faulty reasoning,\
    \ which deteriorates the quality of distillation, especially in reasoning capabilities.\
    \ In this work, we propose Program-aided Distillation (PaD), which introduces\
    \ reasoning programs to suppress the errors in distilled data, and thus achieves\
    \ better distillation quality for reasoning tasks. In PaD, we utilize the reasoning\
    \ program to substitute the CoT, allowing automated error checking of synthetic\
    \ data. Further, through error injecting and further training, the small distilling\
    \ model could iteratively self-refine the reasoning. \nMoreover, we conduct a\
    \ step-wise beam search by step-by-step verifying to acquire more exact reasoning\
    \ chains. \nWe evaluate PaD on arithmetic reasoning, symbolic reasoning, and general\
    \ ability.\nExperimental results demonstrate that smaller models using PaD can\
    \ not only outperform certain LLMs~(e.g., LLaMA-1 13B) but also achieve strong\
    \ improvement over \nbaselines with a significantly smaller scale of parameters\
    \ and data.  The source code is publicly available at\nhttps://github.com/Xuekai-Zhu/pad."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Question Answering
  authors:
  - dblp_id: https://dblp.org/pid/327/9656
    emails: '****@gmail.com'
    first_name: Xuekai
    google_scholar_id: https://scholar.google.com/citations?user=plXXtQkAAAAJ&hl=en
    institution: Shanghai Jiaotong University
    last_name: Zhu
    name: Xuekai Zhu
    username: ~Xuekai_Zhu1
  - dblp_id: https://dblp.org/pid/233/4949.html
    emails: '****@gmail.com'
    first_name: Biqing
    institution: Tsinghua University and Harbin Institute of Technology
    last_name: Qi
    name: Biqing Qi
    semantic_scholar_id: https://www.semanticscholar.org/author/Biqing-Qi/66242399
    username: ~Biqing_Qi1
  - dblp_id: https://dblp.uni-trier.de/pid/294/8737
    emails: '****@mails.tsinghua.edu.cn'
    first_name: Kaiyan
    institution: Electronic Engineering, Tsinghua University
    last_name: Zhang
    name: Kaiyan Zhang
    semantic_scholar_id: https://www.semanticscholar.org/author/Kaiyan-Zhang/2153281320
    username: ~Kaiyan_Zhang1
  - dblp_id: https://dblp.uni-trier.de/pid/278/2245
    emails: '****@mails.tsinghua.edu.cn'
    first_name: Xinwei
    google_scholar_id: https://scholar.google.cz/citations?hl=cs&user=gSA_egQAAAAJ
    last_name: Long
    name: Xinwei Long
    username: ~Xinwei_Long1
  - dblp_id: http://dblp.uni-trier.de/pers/hd/l/Lin:Zhouhan
    emails: '****@gmail.com'
    first_name: Zhouhan
    google_scholar_id: https://scholar.google.ca/citations?user=LNZ4efwAAAAJ&hl=en
    homepage: https://hantek.github.io
    institution: Shanghai Jiao Tong University
    last_name: Lin
    name: Zhouhan Lin
    username: ~Zhouhan_Lin1
  - emails: '****@tsinghua.edu.cn'
    first_name: Bowen
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=h3Nsz6YAAAAJ
    homepage: http://web.ee.tsinghua.edu.cn/zhoubowen/zh_CN/index.htm?eqid=b894e49b0000ec7d0000000464857b51
    institution: Tsinghua University
    last_name: Zhou
    name: Bowen Zhou
    username: ~Bowen_Zhou8
  decision: toMainConference
  end_page: 2800
  file: 248.pdf
  id: 248
  num_pages: 27
  openreview_id: uUFECJzXAs
  pdf_file: 7c050734db16f24886292722a2ce225851d97604.pdf
  start_page: 2774
  title: 'PaD: Program-aided Distillation Can Teach Small Models Reasoning Better
    than Chain-of-thought Fine-tuning'
- abstract: There has been a surge in LLM evaluation research to understand LLM capabilities
    and limitations. However, much of this research has been confined to English,
    leaving LLM building and evaluation for non-English languages relatively unexplored.
    Several new LLMs have been introduced recently, necessitating their evaluation
    on non-English languages. This study aims to perform a thorough evaluation of
    the non-English capabilities of SoTA LLMs (GPT-3.5-Turbo, GPT-4, PaLM2, Gemini-Pro,
    Mistral, Llama2, and Gemma) by comparing them on the same set of multilingual
    datasets. Our benchmark comprises 22 datasets covering 83 languages, including
    low-resource African languages. We also include two multimodal datasets in the
    benchmark and compare the performance of LLaVA models, GPT-4-Vision and Gemini-Pro-Vision.
    Our experiments show that larger models such as GPT-4, Gemini-Pro and PaLM2 outperform
    smaller models on various tasks, notably on low-resource languages, with GPT-4
    outperforming PaLM2 and Gemini-Pro on more datasets. We also perform a study on
    data contamination and find that several models are likely to be contaminated
    with multilingual evaluation benchmarks, necessitating approaches to detect and
    handle contamination while assessing the multilingual performance of LLMs.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multilinguality and Language Diversity
  authors:
  - dblp_id: https://dblp.org/pid/287/2655
    emails: '****@gmail.com'
    first_name: Sanchit
    homepage: https://sanchit-ahuja.github.io/
    institution: Research, Microsoft
    last_name: Ahuja
    name: Sanchit Ahuja
    semantic_scholar_id: https://www.semanticscholar.org/author/Sanchit-Ahuja/2266387781
    username: ~Sanchit_Ahuja1
  - dblp_id: https://dblp.org/pid/299/2130
    emails: '****@gmail.com'
    first_name: Divyanshu
    google_scholar_id: https://scholar.google.com/citations?hl=en&view_op=list_works&gmla=AJsN-F6R3Gq9iCJvhWCsxWClU5PgCmWnVRhIlofSfVcUDmfrnteszRFKWP4NTuuT7PTFW-cFZxbRQbD_RbCd-LGSGuvVjF6pMra4jzwkmhiCWdFi8jjxJPw&user=WuYKZLAAAAAJ
    homepage: https://divyanshuaggarwal.github.io/
    last_name: Aggarwal
    name: Divyanshu Aggarwal
    orcid: https://orcid.org/0000-0002-4064-6864
    semantic_scholar_id: https://www.semanticscholar.org/author/Divyanshu-Aggarwal/27543974
    username: ~Divyanshu_Aggarwal1
  - dblp_id: https://dblp.org/pid/306/1527
    emails: '****@gmail.com'
    first_name: Varun
    google_scholar_id: https://scholar.google.com/citations?user=tqDhGbwAAAAJ&hl=en
    homepage: https://varungumma.github.io
    institution: Microsoft
    last_name: Gumma
    name: Varun Gumma
    orcid: https://orcid.org/0009-0002-5746-3017
    semantic_scholar_id: https://www.semanticscholar.org/author/Varun-Gumma/2140408530
    username: ~Varun_Gumma1
  - emails: '****@gmail.com'
    first_name: Ishaan
    google_scholar_id: https://scholar.google.com/citations?user=UOCR2HsAAAAJ&hl=en
    homepage: https://wattsishaan.github.io
    institution: Research, Microsoft
    last_name: Watts
    name: Ishaan Watts
    username: ~Ishaan_Watts1
  - dblp_id: https://dblp.org/pid/332/0994.html
    emails: '****@gmail.com'
    first_name: Ashutosh
    google_scholar_id: https://scholar.google.com/citations?user=f3T-T-AAAAAJ&hl=en
    homepage: https://ashutoshbsathe.github.io
    last_name: Sathe
    name: Ashutosh Sathe
    username: ~Ashutosh_Sathe1
  - emails: '****@gmail.com'
    first_name: Millicent
    homepage: https://millicentochieng.github.io/
    institution: Microsoft
    last_name: Ochieng
    name: Millicent Ochieng
    username: ~Millicent_Ochieng1
  - emails: '****@gmail.com'
    first_name: Rishav
    google_scholar_id: https://scholar.google.com/citations?user=ctKGG_YAAAAJ&hl=en&oi=ao
    homepage: https://sites.google.com/view/rishavhada
    institution: Microsoft Research India
    last_name: Hada
    name: Rishav Hada
    username: ~Rishav_Hada1
  - dblp_id: https://dblp.org/pid/38/4450-1
    emails: '****@gmail.com'
    first_name: Prachi
    google_scholar_id: https://scholar.google.com/citations?user=C_2h8iMAAAAJ&hl=en
    homepage: http://www.cse.iitd.ernet.in/~prachij/
    institution: Microsoft
    last_name: Jain
    name: Prachi Jain
    semantic_scholar_id: https://www.semanticscholar.org/author/Prachi-Jain/3094662
    username: ~Prachi_Jain3
  - dblp_id: https://dblp.org/pid/49/4653-1
    emails: '****@microsoft.com'
    first_name: Mohamed
    google_scholar_id: https://scholar.google.com/citations?user=_any3jgAAAAJ&hl=en
    institution: Research, Microsoft
    last_name: Ahmed
    name: Mohamed Ahmed
    username: ~Mohamed_Ahmed1
  - dblp_id: https://dblp.org/pid/19/5717
    emails: '****@microsoft.com'
    first_name: Kalika
    google_scholar_id: https://scholar.google.com/citations?user=HSIGxEgAAAAJ&hl=en
    homepage: https://www.microsoft.com/en-us/research/people/kalikab/
    institution: Microsoft Research Labs
    last_name: Bali
    name: Kalika Bali
    semantic_scholar_id: https://www.semanticscholar.org/author/Kalika-Bali/3086996
    username: ~Kalika_Bali1
  - dblp_id: https://dblp.org/pid/27/7642
    emails: '****@microsoft.com'
    first_name: Sunayana
    google_scholar_id: https://scholar.google.com/citations?user=PUxwYrkAAAAJ&hl=en
    homepage: https://www.microsoft.com/en-us/research/people/susitara/
    institution: Microsoft
    last_name: Sitaram
    name: Sunayana Sitaram
    username: ~Sunayana_Sitaram1
  decision: toMainConference
  end_page: 2840
  file: 249.pdf
  id: 249
  num_pages: 40
  openreview_id: OWDDSwQFqU
  pdf_file: 1c4259f61f296f5a18db1916395ee8fc11004b9a.pdf
  start_page: 2801
  title: 'MEGAVERSE: Benchmarking Large Language Models Across Languages, Modalities,
    Models and Tasks'
- abstract: "Modular Neural Networks (MNNs) demonstrate various advantages over monolithic\
    \ models.\nExisting MNNs are generally $\\textit{explicit}$: \ntheir modular architectures\
    \ are pre-defined, with individual modules expected to implement distinct functions.\n\
    Recent works reveal that there exists $\\textit{implicit}$ modularity in standard\
    \ pre-trained transformers, namely $\\textit{Emergent Modularity}$.\nThey indicate\
    \ that such modular structures spontaneously exhibit during the early pre-training\
    \ phase.\nDespite the benefits of modularity, most Language Models (LMs) are still\
    \ treated as monolithic models in the pre-train and fine-tune paradigm, with their\
    \ emergent modularity locked and underutilized.\nIn this work, focusing on unlocking\
    \ the emergent modularity in LMs, we showcase that standard LMs could be fine-tuned\
    \ as their Mixture-of-Expert (MoEs) counterparts without introducing any extra\
    \ parameters. \nSuch MoEs are derived from emergent modularity and are referred\
    \ to as Emergent MoEs (EMoE).\nOur experiments demonstrate that fine-tuning EMoE\
    \ effectively improves downstream in-domain and out-of-domain generalization compared\
    \ with vanilla fine-tuning.\nOur analysis and ablation studies further illustrate\
    \ that it is robust to various configurations and can scale up to Large Language\
    \ Models (i.e., Llama2-7B and Llama-30B). Code is available at https://github.com/qiuzh20/EMoE."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Learning for NLP
  authors:
  - emails: '****@gmail.com'
    first_name: Zihan
    google_scholar_id: https://scholar.google.com/citations?user=24eVHiYAAAAJ&hl=zh-CN
    last_name: Qiu
    name: Zihan Qiu
    username: ~Zihan_Qiu1
  - emails: '****@gmail.com'
    first_name: Zeyu
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=EWU88_YAAAAJ
    last_name: Huang
    name: Zeyu Huang
    username: ~Zeyu_Huang1
  - dblp_id: https://dblp.org/pid/16/7565.html
    emails: '****@gmail.com'
    first_name: Jie
    google_scholar_id: https://scholar.google.com/citations?user=66osleIAAAAJ&hl=en
    homepage: https://bigaidream.github.io/
    institution: Hong Kong University of Science and Technology
    last_name: Fu
    name: Jie Fu
    orcid: https://orcid.org/0000-0002-4494-843X
    username: ~Jie_Fu2
  decision: toMainConference
  end_page: 2863
  file: 250.pdf
  id: 250
  num_pages: 23
  openreview_id: Bg4aiEVlpt
  pdf_file: 1f972d0c038a10a6bc4c99d42f714c0019227630.pdf
  start_page: 2841
  title: Unlocking Emergent Modularity in Large Language Models
- abstract: Learning argumentative writing is challenging. Besides writing fundamentals
    such as syntax and grammar, learners must select and arrange argument components
    meaningfully to create high-quality essays. To support argumentative writing computationally,
    one step is to mine the argumentative structure. When combined with automatic
    essay scoring, interactions of the argumentative structure and quality scores
    can be exploited for comprehensive writing support. Although studies have shown
    the usefulness of using information about the argumentative structure for essay
    scoring, no argument mining corpus with ground-truth essay quality annotations
    has been published yet. Moreover, none of the existing corpora contain essays
    written by school students specifically. To fill this research gap, we present
    a German corpus of 1,320 essays from school students of two age groups. Each essay
    has been manually annotated for argumentative structure and  quality on multiple
    levels of granularity. We propose baseline approaches to argument mining and essay
    scoring, and we analyze interactions between both tasks, thereby laying the ground
    for quality-oriented argumentative writing support.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Sentiment Analysis, Stylistic Analysis, and Argument Mining
  authors:
  - emails: '****@ai.uni-hannover.de'
    first_name: Maja
    google_scholar_id: https://scholar.google.de/citations?user=Yf5mImsAAAAJ&hl=de
    homepage: https://www.ai.uni-hannover.de/de/
    institution: "Leibniz Universit\xE4t Hannover"
    last_name: Stahl
    name: Maja Stahl
    orcid: https://orcid.org/0000-0002-5982-726X
    username: ~Maja_Stahl1
  - emails: '****@uni-paderborn.de'
    first_name: Nadine
    homepage: https://kw.uni-paderborn.de/institut-fuer-germanistik-und-vergleichende-literaturwissenschaft/germanistische-sprachdidaktik/michel
    last_name: Michel
    name: Nadine Michel
    username: ~Nadine_Michel1
  - emails: '****@mail.uni-paderborn.de'
    first_name: Sebastian
    homepage: https://www.uni-paderborn.de/person/93839
    last_name: Kilsbach
    name: Sebastian Kilsbach
    username: ~Sebastian_Kilsbach2
  - emails: '****@stud.uni-hannover.de'
    first_name: Julian
    homepage: https://www.ai.uni-hannover.de/de/
    institution: "Universit\xE4t Hannover"
    last_name: Schmidtke
    name: Julian Schmidtke
    username: ~Julian_Schmidtke1
  - emails: '****@uni-paderborn.de'
    first_name: Sara
    homepage: https://www.uni-paderborn.de/person/58338
    institution: "Universit\xE4t Paderborn"
    last_name: Rezat
    name: Sara Rezat
    username: ~Sara_Rezat1
  - dblp_id: https://dblp.org/pid/73/9281
    emails: '****@ai.uni-hannover.de'
    first_name: Henning
    google_scholar_id: https://scholar.google.com/citations?user=kPps-H8AAAAJ
    institution: "Leibniz Universit\xE4t Hannover"
    last_name: Wachsmuth
    name: Henning Wachsmuth
    orcid: https://orcid.org/0000-0003-2792-621X
    semantic_scholar_id: https://www.semanticscholar.org/author/Henning-Wachsmuth/2626599
    username: ~Henning_Wachsmuth1
  decision: toMainConference
  end_page: 2877
  file: 251.pdf
  id: 251
  num_pages: 14
  openreview_id: H74Q6oaV3N
  pdf_file: 3c9e8c678f3dbda89945f74232e45cc7776e2914.pdf
  start_page: 2864
  title: A School Student Essay Corpus for Analyzing Interactions of Argumentative
    Structure and Quality
- abstract: Embedding spaces contain interpretable dimensions indicating gender, formality
    in style, or even object properties. This has been observed multiple times. Such
    interpretable dimensions are becoming valuable tools in different areas of study,
    from social science to neuroscience. The standard way to compute these dimensions
    uses contrasting seed words and computes  difference vectors over them. This is
    simple but does not always work well. We combine seed-based vectors with guidance
    from human ratings of where words fall along a specific dimension, and evaluate
    on predicting both object properties like size and danger, and the stylistic properties
    of formality and complexity. We obtain interpretable dimensions with markedly
    better performance especially in cases where seed-based dimensions do not work
    well.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: 'Semantics: Lexical'
  authors:
  - dblp_id: https://dblp.org/pid/23/556
    emails: '****@utexas.edu'
    first_name: Katrin
    google_scholar_id: https://scholar.google.com/citations?user=v7kFHRoAAAAJ&hl=en
    homepage: http://www.katrinerk.com/
    institution: University of Texas, Austin
    last_name: Erk
    name: Katrin Erk
    semantic_scholar_id: https://www.semanticscholar.org/author/Katrin-Erk/1708114
    username: ~Katrin_Erk1
  - dblp_id: https://dblp.org/pid/53/1712
    emails: '****@seas.upenn.edu'
    first_name: Marianna
    google_scholar_id: https://scholar.google.fr/citations?user=9Kls8jYAAAAJ&hl=en
    homepage: https://mariannaapi.github.io
    institution: University of Pennsylvania, University of Pennsylvania
    last_name: Apidianaki
    name: Marianna Apidianaki
    semantic_scholar_id: https://www.semanticscholar.org/author/Marianna-Apidianaki/2817917
    username: ~Marianna_Apidianaki1
  decision: toMainConference
  end_page: 2889
  file: 253.pdf
  id: 253
  num_pages: 12
  openreview_id: kabp4Cx2GW
  pdf_file: 47fd549b4c78c0f5cd0b5c3578e7b3d684821b22.pdf
  start_page: 2878
  title: Adjusting Interpretable Dimensions in Embedding Space with Human Judgments
- abstract: 'In this work, we introduce a comprehensive error typology specifically
    designed for evaluating two distinct tasks in machine-generated patent texts:
    claims-to-abstract generation, and the generation of the next claim given previous
    ones. We have also developed a benchmark, PatentEval, for systematically assessing
    language models in this context. Our study includes a comparative analysis, annotated
    by humans, of various models. These range from those specifically adapted during
    training for tasks within the patent domain to the latest general-purpose large
    language models (LLMs). Furthermore, we explored and evaluated some metrics to
    approximate human judgments in patent text evaluation, analyzing the extent to
    which these metrics align with expert assessments. These approaches provide valuable
    insights into the capabilities and limitations of current language models in the
    specialized field of patent text generation.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - emails: '****@inria.fr'
    first_name: You
    google_scholar_id: https://scholar.google.com/citations?user=aAIwq2sAAAAJ&hl=en
    last_name: Zuo
    name: You Zuo
    username: ~You_Zuo1
  - dblp_id: https://dblp.org/pid/43/1554
    emails: '****@gerdes.fr'
    first_name: Kim
    google_scholar_id: https://scholar.google.fr/citations?user=jCFpJfEAAAAJ
    homepage: https://gerdes.fr
    institution: "Universit\xE9 Paris-Saclay"
    last_name: Gerdes
    name: Kim Gerdes
    username: ~Kim_Gerdes1
  - dblp_id: https://dblp.org/pid/54/5373
    emails: '****@inria.fr'
    first_name: "\xC9ric"
    google_scholar_id: https://scholar.google.com/citations?user=olKxDUMAAAAJ&hl=fr
    homepage: http://alpage.inria.fr/~clerger
    last_name: Clergerie
    middle_name: Villemonte De La
    name: "\xC9ric Villemonte de la Clergerie"
    orcid: https://orcid.org/0000-0001-6428-9219
    semantic_scholar_id: https://www.semanticscholar.org/author/%C3%89ric-Villemonte-de-la-Clergerie/2598776
    username: "~\xC9ric_Villemonte_de_la_Clergerie1"
  - dblp_id: https://dblp.org/pid/66/1016
    emails: '****@inria.fr'
    first_name: "Beno\xEEt"
    google_scholar_id: https://scholar.google.fr/citations?user=HXUT9ZkAAAAJ
    homepage: http://pauillac.inria.fr/~sagot/
    institution: INRIA
    last_name: Sagot
    name: "Beno\xEEt Sagot"
    orcid: https://orcid.org/0000-0002-0107-8526
    semantic_scholar_id: "https://www.semanticscholar.org/author/Beno\xEEt-Sagot/68990982"
    username: "~Beno\xEEt_Sagot1"
  decision: toMainConference
  end_page: 2913
  file: 254.pdf
  id: 254
  num_pages: 24
  openreview_id: S36eTPNour
  pdf_file: 8d0b55ca6ab373488bfd5768b6c2beddc98ec788.pdf
  start_page: 2890
  title: 'PatentEval: Understanding Errors in Patent Generation'
- abstract: 'Large language models (LLMs) have demonstrated considerable success in
    various natural language processing tasks, but open-source LLMs have yet to attain
    state-of-the-art performance in Neural Machine Translation (NMT). Nevertheless,
    their significant performance in tasks demanding a broad understanding and contextual
    processing shows their potential for translation. To exploit these abilities,
    we investigate using LLMs for MT and explore recent parameter-efficient fine-tuning
    techniques. Surprisingly, our initial experiments found that fine-tuning with
    Q-LoRA for translation purposes led to performance improvements in terms of BLEU
    but degradation in COMET compared to in-context learning. To overcome this, we
    propose an alternative approach: adapting LLMs as Automatic Post-Editors (APE)
    rather than direct translators. Building on the ability of the LLM to handle long
    sequences, we also propose extending our approach to document-level translation.
    We show that leveraging Low-Rank-Adapter fine-tuning for APE can yield significant
    improvements across both sentence and document-level metrics while generalizing
    to out-of-domain data. Most notably, we achieve a state-of-the-art accuracy rate
    of 88.7\% on the ContraPro test set, which assesses the model''s ability to resolve
    pronoun ambiguities when translating from English to German. Lastly, during manual
    post-editing for document-level translation, the source sentences are iteratively
    annotated, which can be used to refine further translations in the document. Here,
    we demonstrate that leveraging human corrections can significantly reduce the
    number of edits required for subsequent translations.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Translation
  authors:
  - dblp_id: https://dblp.org/pid/289/1677
    emails: '****@kit.edu'
    first_name: Sai
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=pJvrr1QAAAAJ
    institution: "Karlsruher Institut f\xFCr Technologie"
    last_name: Koneru
    name: Sai Koneru
    semantic_scholar_id: https://www.semanticscholar.org/author/Sai-Koneru/2061139721
    username: ~Sai_Koneru1
  - dblp_id: https://dblp.org/pid/272/4281.html
    emails: '****@sap.com'
    first_name: Miriam
    google_scholar_id: https://scholar.google.de/citations?user=J5QHGyEAAAAJ&hl=de&authuser=1
    institution: SAP SE
    last_name: Exel
    name: Miriam Exel
    username: ~Miriam_Exel1
  - dblp_id: https://dblp.org/pid/91/11424
    emails: '****@sap.com'
    first_name: Matthias
    google_scholar_id: https://scholar.google.com/citations?user=IDO3e3EAAAAJ
    institution: SAP SE
    last_name: Huck
    name: Matthias Huck
    semantic_scholar_id: https://www.semanticscholar.org/author/Matthias-Huck/1839533
    username: ~Matthias_Huck1
  - dblp_id: https://dblp.org/pid/120/0365
    emails: '****@niehues.info'
    first_name: Jan
    google_scholar_id: https://scholar.google.com/citations?user=fO9cszYAAAAJ&hl=en
    homepage: https://dke.maastrichtuniversity.nl/jan.niehues/
    last_name: Niehues
    name: Jan Niehues
    username: ~Jan_Niehues1
  decision: toMainConference
  end_page: 2928
  file: 255.pdf
  id: 255
  num_pages: 15
  openreview_id: hYJEqpkdgI
  pdf_file: 9ba019656727ab9855a388b3c64a0d671182769d.pdf
  start_page: 2914
  title: 'Contextual Refinement of Translations: Large Language Models for Sentence
    and Document-Level Post-Editing'
- abstract: Metaphor detection is a challenging task for natural language processing
    (NLP) systems. Previous works failed to sufficiently utilize the internal and
    external semantic relationships between target words and their context. Furthermore,
    they have faced challenges in tackling the problem of data sparseness due to the
    very limited available training data. To address these two challenges, we propose
    a novel model called MiceCL. By leveraging the difference between the literal
    meaning of the target word and the meaning of the sentence as the sentence external
    difference, MiceCL can better handle the semantic relationships. Additionally,
    we propose a curriculum learning framework for automatically assessing difficulty
    of the sentence with a pre-trained model. By starting from easy examples and gradually
    progressing to more difficult ones, we can ensure that the model will not deal
    with complex data when its ability is weak so that to avoid wasting limited data.
    Experimental results demonstrate that MiceCL achieves competitive performance
    across multiple datasets, with a significantly improved convergence speed compared
    to other models.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: 'Semantics: Lexical'
  authors:
  - emails: '****@hrbeu.edu.cn'
    first_name: Kaidi
    homepage: https://github.com/Evilxya
    last_name: Jia
    name: Kaidi Jia
    username: ~Kaidi_Jia1
  - emails: '****@hrbeu.edu.cn'
    first_name: Rongsheng
    institution: Harbin Engineering University
    last_name: Li
    name: Rongsheng Li
    orcid: https://orcid.org/0000-0003-4597-1583
    username: ~Rongsheng_Li1
  decision: toMainConference
  end_page: 2940
  file: 257.pdf
  id: 257
  num_pages: 12
  openreview_id: wlSlQfDcRa
  pdf_file: 46495e57803af6dce076f11a594452505fcf48fc.pdf
  start_page: 2929
  title: Metaphor Detection with Context Enhancement and Curriculum Learning
- abstract: 'We consider an unanswered question in the discourse processing community:
    why do relation classifiers trained on explicit examples (with connectives removed)
    perform poorly in real implicit scenarios? Prior work claimed this is due to linguistic
    dissimilarity between explicit and implicit examples but provided no empirical
    evidence. In this study, we show that one cause for such failure is a label shift
    after connectives are eliminated. Specifically, we find that the discourse relations
    expressed by some explicit instances will change when connectives disappear. Unlike
    previous work manually analyzing a few examples, we present empirical evidence
    at the corpus level to prove the existence of such shift. Then, we analyze why
    label shift occurs by considering factors such as the syntactic role played by
    connectives, ambiguity of connectives, and more. Finally, we investigate two strategies
    to mitigate the label shift: filtering out noisy data and joint learning with
    connectives. Experiments on PDTB 2.0, PDTB 3.0, and the GUM dataset demonstrate
    that classifiers trained with our strategies outperform strong baselines.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Discourse and Pragmatics
  authors:
  - dblp_id: https://dblp.org/pid/s/MichaelStrube1
    emails: '****@h-its.org'
    first_name: Wei
    google_scholar_id: https://scholar.google.com/citations?user=dfou7m8AAAAJ&hl=en&oi=sra
    homepage: https://github.com/liuwei1206
    institution: Heidelberg University
    last_name: Liu
    name: Wei Liu
    semantic_scholar_id: https://www.semanticscholar.org/author/Wei-Liu/40474876
    username: ~Wei_Liu7
  - dblp_id: https://dblp.org/pid/w/StephenWan
    emails: '****@csiro.au'
    first_name: Stephen
    google_scholar_id: https://scholar.google.com.au/citations?user=YMRsSGcAAAAJ&hl=en
    homepage: https://people.csiro.au/W/S/Stephen-Wan
    institution: CSIRO
    last_name: Wan
    name: Stephen Wan
    orcid: https://orcid.org/0000-0001-7505-1417
    username: ~Stephen_Wan1
  - dblp_id: https://dblp.org/pid/s/MichaelStrube1
    emails: '****@h-its.org'
    first_name: Michael
    google_scholar_id: https://scholar.google.com/citations?user=s0_rS0kAAAAJ&hl=en
    homepage: https://www.h-its.org/people/prof-dr-michael-strube/
    institution: Heidelberg Institute for Theoretical Studies
    last_name: Strube
    name: Michael Strube
    username: ~Michael_Strube1
  decision: toMainConference
  end_page: 2956
  file: 259.pdf
  id: 259
  num_pages: 16
  openreview_id: Di8h0TV0jk
  pdf_file: f8187aca7a6c68fd4437c80c7e70db72a0c53c42.pdf
  start_page: 2941
  title: What Causes the Failure of Explicit to Implicit Discourse Relation Recognition?
- abstract: 'Recent studies leverage large language models with multi-tasking capabilities,
    using natural language prompts to guide the model''s behavior and surpassing performance
    of task-specific models. Motivated by this, we ask: can we build a single model
    that jointly performs various spoken language understanding (SLU) tasks? We start
    by adapting a pre-trained automatic speech recognition model to additional tasks
    using single-token task specifiers. We enhance this approach through instruction
    tuning, i.e., finetuning by describing the task using natural language instructions
    followed by the list of label options. Our approach can generalize to new task
    descriptions for the seen tasks during inference, thereby enhancing its user-friendliness.
    We demonstrate the efficacy of our single multi-task learning model "UniverSLU"
    for 12 speech classification and sequence generation task types spanning 17 datasets
    and 9 languages. On most tasks, UniverSLU achieves competitive performance and
    often even surpasses task-specific models. Additionally, we assess the zero-shot
    capabilities, finding that the model generalizes to new datasets and languages
    for seen task types.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Speech recognition, text-to-speech and spoken language understanding
  authors:
  - dblp_id: https://dblp.org/pid/235/7362
    emails: '****@andrew.cmu.edu'
    first_name: Siddhant
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=VGfczTIAAAAJ
    last_name: Arora
    name: Siddhant Arora
    username: ~Siddhant_Arora1
  - emails: '****@sony.com'
    first_name: Hayato
    homepage: https://arxiv.org/search/cs?searchtype=author&query=Futami%2C+H
    institution: Sony
    last_name: Futami
    name: Hayato Futami
    username: ~Hayato_Futami2
  - emails: '****@ieee.org'
    first_name: Jee-weon
    google_scholar_id: https://scholar.google.com/citations?user=A5OcLdAAAAAJ
    homepage: https://jungjee.github.io
    institution: CMU, Carnegie Mellon University
    last_name: Jung
    name: Jee-weon Jung
    username: ~Jee-weon_Jung2
  - dblp_id: https://dblp.org/pid/123/7941
    emails: '****@andrew.cmu.edu'
    first_name: Yifan
    google_scholar_id: https://scholar.google.com/citations?user=wH2FALMAAAAJ&hl=en
    homepage: https://pyf98.github.io/
    institution: Carnegie Mellon University
    last_name: Peng
    name: Yifan Peng
    orcid: https://orcid.org/0000-0002-8581-8674
    semantic_scholar_id: https://www.semanticscholar.org/author/Yifan-Peng/2111014429
    username: ~Yifan_Peng6
  - dblp_id: https://dblp.org/pid/229/4537
    emails: '****@google.com'
    first_name: Roshan
    google_scholar_id: https://scholar.google.com/citations?user=yZ4QLqsAAAAJ&hl=en
    homepage: https://roshansh-cmu.github.io/
    institution: Google
    last_name: Sharma
    name: Roshan Sharma
    semantic_scholar_id: https://www.semanticscholar.org/author/Roshan-Sharma/145521253
    username: ~Roshan_Sharma1
  - dblp_id: https://dblp.org/pid/125/7457
    emails: '****@sony.com'
    first_name: Yosuke
    last_name: Kashiwagi
    name: Yosuke Kashiwagi
    semantic_scholar_id: https://www.semanticscholar.org/author/Yosuke-Kashiwagi/3165422
    username: ~Yosuke_Kashiwagi1
  - dblp_id: ''
    emails: '****@jp.sony.com'
    first_name: Emiru
    google_scholar_id: ''
    homepage: ''
    last_name: Tsunoo
    name: Emiru Tsunoo
    username: ~Emiru_Tsunoo1
  - dblp_id: https://dblp.org/pid/51/2464
    emails: '****@ttic.edu'
    first_name: Karen
    google_scholar_id: https://scholar.google.com/citations?user=kCYbVq0AAAAJ&hl=en
    homepage: http://ttic.edu/livescu
    institution: Toyota Technological Institute at Chicago
    last_name: Livescu
    name: Karen Livescu
    semantic_scholar_id: https://www.semanticscholar.org/author/Karen-Livescu/2924113
    username: ~Karen_Livescu1
  - dblp_id: https://dblp.uni-trier.de/pid/39/3245-1
    emails: '****@ieee.org'
    first_name: Shinji
    google_scholar_id: https://scholar.google.com/citations?user=U5xRA6QAAAAJ&hl
    homepage: https://sites.google.com/view/shinjiwatanabe
    institution: Carnegie Mellon University
    last_name: Watanabe
    name: Shinji Watanabe
    orcid: https://orcid.org/0000-0002-5970-8631
    semantic_scholar_id: https://www.semanticscholar.org/author/Shinji-Watanabe/1746678
    username: ~Shinji_Watanabe1
  decision: toMainConference
  end_page: 2977
  file: 262.pdf
  id: 262
  num_pages: 21
  openreview_id: PMn9YIPYtq
  pdf_file: fae971a2fddba0edf0ce1043efc5b31ddc77645b.pdf
  start_page: 2957
  title: 'UniverSLU: Universal Spoken Language Understanding for Diverse Tasks with
    Natural Language Instructions'
- abstract: The rapid progress in open-source Large Language Models (LLMs) is significantly
    driving AI development forward. However, there is still a limited understanding
    of their trustworthiness. Deploying these models at scale without sufficient trustworthiness
    can pose significant risks, highlighting the need to uncover these issues promptly.
    In this work, we conduct an adversarial assessment of open-source LLMs on trustworthiness,
    scrutinizing them across eight different aspects including toxicity, stereotypes,
    ethics, hallucination, fairness, sycophancy, privacy, and robustness against adversarial
    demonstrations. We propose advCoU, an extended Chain of Utterances-based (CoU)
    prompting strategy by incorporating carefully crafted malicious demonstrations
    for trustworthiness attack. Our extensive experiments encompass recent and representative
    series of open-source LLMs, including Vicuna, MPT, Falcon, Mistral, and Llama
    2. The empirical outcomes underscore the efficacy of our attack strategy across
    diverse aspects. More interestingly, our result analysis reveals that models with
    superior performance in general NLP tasks do not always have greater trustworthiness;
    in fact, larger models can be more vulnerable to attacks. Additionally, models
    that have undergone instruction tuning, focusing on instruction following, tend
    to be more susceptible, although fine-tuning LLMs for safety alignment proves
    effective in mitigating adversarial trustworthiness attacks.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - dblp_id: https://dblp.org/pid/241/5516
    emails: '****@osu.edu'
    first_name: Lingbo
    homepage: https://molingbo.github.io/
    last_name: Mo
    name: Lingbo Mo
    semantic_scholar_id: https://www.semanticscholar.org/author/Lingbo-Mo/2135579717
    username: ~Lingbo_Mo1
  - emails: '****@buckeyemail.osu.edu'
    first_name: Boshi
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=T9JrCUgAAAAJ
    homepage: https://boshi-wang.github.io/
    institution: Ohio State University
    last_name: Wang
    name: Boshi Wang
    semantic_scholar_id: https://www.semanticscholar.org/author/Boshi-Wang/7425689
    username: ~Boshi_Wang2
  - dblp_id: https://dblp.org/pid/173/2608
    emails: '****@ucdavis.edu'
    first_name: Muhao
    google_scholar_id: https://scholar.google.com/citations?user=k79yEZkAAAAJ&hl=en
    homepage: https://muhaochen.github.io/
    institution: University of California, Davis and University of Southern California
    last_name: Chen
    name: Muhao Chen
    orcid: https://orcid.org/0000-0003-0118-3147
    semantic_scholar_id: https://www.semanticscholar.org/author/Muhao-Chen/1998918
    username: ~Muhao_Chen1
  - dblp_id: https://dblp.org/pid/33/2952
    emails: '****@osu.edu'
    first_name: Huan
    google_scholar_id: https://scholar.google.com/citations?user=wIFkulcAAAAJ&hl=en
    homepage: http://web.cse.ohio-state.edu/~huansun/
    institution: Ohio State University, Columbus
    last_name: Sun
    name: Huan Sun
    semantic_scholar_id: https://www.semanticscholar.org/author/Huan-Sun/11121990
    username: ~Huan_Sun1
  decision: toMainConference
  end_page: 2995
  file: 263.pdf
  id: 263
  num_pages: 18
  openreview_id: d2FY1ThNPf
  pdf_file: e8c383c350d1b003bc019f5f4c0221f45723b330.pdf
  start_page: 2978
  title: How Trustworthy are Open-Source LLMs? An Assessment under Malicious Demonstrations
    Shows their Vulnerabilities
- abstract: "This paper studies the relationship between the surface form of a mathematical\
    \ problem and its solvability by large language models. We find that subtle alterations\
    \ in the surface form can significantly impact the answer distribution and the\
    \ solve rate, exposing the language model's lack of robustness and sensitivity\
    \ to the surface form in reasoning through complex problems. To improve mathematical\
    \ reasoning performance, we propose Self-Consistency-over-Paraphrases (SCoP),\
    \ which diversifies reasoning paths from specific surface forms of the problem.\
    \ We evaluate our approach on four mathematics reasoning benchmarks over three\
    \ large language models and show that SCoP improves mathematical reasoning performance\
    \ over vanilla self-consistency, particularly for problems initially deemed unsolvable.\
    \ \nFinally, we provide additional experiments and discussion regarding problem\
    \ difficulty and surface forms, including cross-model difficulty agreement and\
    \ paraphrasing transferability, and Variance of Variations (VOV) for language\
    \ model evaluation."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - emails: '****@uic.edu'
    first_name: Yue
    institution: University of Illinois at Chicago
    last_name: Zhou
    name: Yue Zhou
    orcid: https://orcid.org/0000-0002-5639-9378
    username: ~Yue_Zhou6
  - dblp_id: https://dblp.org/pid/56/8808
    emails: '****@us.ibm.com'
    first_name: Yada
    google_scholar_id: https://scholar.google.com/citations?user=AJb408gAAAAJ&hl=en
    homepage: https://researcher.watson.ibm.com/researcher/view.php?person=us-yzhu
    institution: IBM Research
    last_name: Zhu
    name: Yada Zhu
    orcid: https://orcid.org/0000-0002-3338-6371
    username: ~Yada_Zhu1
  - dblp_id: https://dblp.org/pid/206/7467.html
    emails: '****@google.com'
    first_name: Diego
    google_scholar_id: https://scholar.google.com/citations?user=YvtRscgAAAAJ&hl=en
    homepage: http://diegoantognini.com/
    institution: Google Research
    last_name: Antognini
    name: Diego Antognini
    semantic_scholar_id: https://www.semanticscholar.org/author/Diego-Antognini/26399699
    username: ~Diego_Antognini1
  - dblp_id: https://dblp.org/pid/07/1501
    emails: '****@mit.edu'
    first_name: Yoon
    google_scholar_id: https://scholar.google.com/citations?user=n_ts4eYAAAAJ
    homepage: https://people.csail.mit.edu/yoonkim/
    institution: Massachusetts Institute of Technology
    last_name: Kim
    name: Yoon Kim
    semantic_scholar_id: https://www.semanticscholar.org/author/Yoon-Kim/38367242
    username: ~Yoon_Kim1
  - dblp_id: https://dblp.org/pid/06/6785-1
    emails: '****@ibm.com'
    first_name: Yang
    google_scholar_id: https://scholar.google.com/citations?user=_-5PSgQAAAAJ&hl=en
    last_name: Zhang
    name: Yang Zhang
    username: ~Yang_Zhang3
  decision: toMainConference
  end_page: 3007
  file: 267.pdf
  id: 267
  num_pages: 12
  openreview_id: e5UYWqq9QA
  pdf_file: bb724dd0a81b9eae0d1161ae630a90db0e63adb2.pdf
  start_page: 2996
  title: 'Paraphrase and Solve: Exploring and Exploiting the Impact of Surface Form
    on Mathematical Reasoning in Large Language Models'
- abstract: The advent of large language models (LLMs) has significantly advanced
    natural language processing tasks like text summarization. However, their large
    size and computational demands, coupled with privacy concerns in data transmission,
    limit their use in resource-constrained and privacy-centric settings. To overcome
    this, we introduce TriSum, a framework for distilling LLMs' text summarization
    abilities into a compact, local model. Initially, LLMs extract a set of aspect-triple
    rationales and summaries, which are refined using a dual-scoring method for quality.
    Next, a smaller local model is trained with these tasks, employing a curriculum
    learning strategy that evolves from simple to complex tasks. Our method enhances
    local model performance on various benchmarks (CNN/DailyMail, XSum, and ClinicalTrial),
    outperforming baselines by 4.5%, 8.5%, and 7.4%, respectively. It also improves
    interpretability by providing insights into the summarization rationale.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Summarization
  authors:
  - dblp_id: https://dblp.org/pid/60/8352
    emails: '****@illinois.edu'
    first_name: Pengcheng
    google_scholar_id: https://scholar.google.com/citations?user=TejDN9wAAAAJ&hl=en
    homepage: https://pat-jj.github.io/
    last_name: Jiang
    name: Pengcheng Jiang
    username: ~Pengcheng_Jiang2
  - dblp_id: https://dblp.org/pid/170/1833
    emails: '****@gmail.com'
    first_name: Cao
    institution: GE Healthcare
    last_name: Xiao
    name: Cao Xiao
    username: ~Cao_Xiao2
  - dblp_id: https://dblp.org/pid/43/7716-8
    emails: '****@illinois.edu'
    first_name: Zifeng
    google_scholar_id: https://scholar.google.com/citations?user=kMlWwTAAAAAJ&hl=en
    homepage: https://zifengwang.xyz
    institution: University of Illinois, Urbana Champaign
    last_name: Wang
    name: Zifeng Wang
    username: ~Zifeng_Wang3
  - dblp_id: https://dblp.org/pid/168/8615
    emails: '****@gmail.com'
    first_name: Parminder
    institution: GEHC
    last_name: Bhatia
    name: Parminder Bhatia
    username: ~Parminder_Bhatia1
  - dblp_id: https://dblp.uni-trier.de/pid/54/4948.html
    emails: '****@gmail.com'
    first_name: Jimeng
    google_scholar_id: https://scholar.google.com/citations?user=9jmmp5sAAAAJ&hl=en
    homepage: http://sunlab.org
    institution: Georgia Tech Research Corporation, University of Illinois, Urbana
      Champaign, College of Computing and Georgia Institute of Technology
    last_name: Sun
    name: Jimeng Sun
    orcid: https://orcid.org/0000-0003-1512-6426
    username: ~Jimeng_Sun3
  - dblp_id: https://dblp.org/pid/h/JiaweiHan.html
    emails: '****@cs.uiuc.edu'
    first_name: Jiawei
    google_scholar_id: https://scholar.google.com.tw/citations?user=Kv9AbjMAAAAJ
    homepage: http://hanj.cs.illinois.edu/
    last_name: Han
    name: Jiawei Han
    semantic_scholar_id: https://www.semanticscholar.org/author/Jiawei-Han/145325584
    username: ~Jiawei_Han1
  decision: toMainConference
  end_page: 3022
  file: 269.pdf
  id: 269
  num_pages: 15
  openreview_id: lL2xI0bR8h
  pdf_file: 772ceacd8aafecb0ea6d31a264a28fe1a8930b57.pdf
  start_page: 3008
  title: 'TriSum: Learning Summarization Ability from Large Language Models with Structured
    Rationale'
- abstract: The field of relation extraction (RE) is experiencing a notable shift
    towards generative relation extraction (GRE), leveraging the capabilities of large
    language models (LLMs). However, we discovered that traditional relation extraction
    (RE) metrics like precision and recall fall short in evaluating GRE methods. This
    shortfall arises because these metrics rely on exact matching with human-annotated
    reference relations, while GRE methods often produce diverse and semantically
    accurate relations that differ from the references. To fill this gap, we introduce
    GenRES for a multi-dimensional assessment in terms of the topic similarity, uniqueness,
    granularity, factualness, and completeness of the GRE results. With GenRES, we
    empirically identified that (1) precision/recall fails to justify the performance
    of GRE methods; (2) human-annotated referential relations can be incomplete; (3)
    prompting LLMs with a fixed set of relations or entities can cause hallucinations.
    Next, we conducted a human evaluation of GRE methods that shows GenRES is consistent
    with human preferences for RE quality. Last, we made a comprehensive evaluation
    of fourteen leading LLMs using GenRES across document, bag, and sentence level
    RE datasets, respectively, to set the benchmark for future research in GRE
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - dblp_id: https://dblp.org/pid/60/8352
    emails: '****@illinois.edu'
    first_name: Pengcheng
    google_scholar_id: https://scholar.google.com/citations?user=TejDN9wAAAAJ&hl=en
    homepage: https://pat-jj.github.io/
    last_name: Jiang
    name: Pengcheng Jiang
    username: ~Pengcheng_Jiang2
  - emails: '****@gmail.com'
    first_name: Jiacheng
    google_scholar_id: https://scholar.google.com/citations?user=h9tJLt8AAAAJ&hl=zh-CN
    homepage: https://linjc16.github.io/
    institution: Department of Computer Science, University of Illinois
    last_name: Lin
    name: Jiacheng Lin
    username: ~Jiacheng_Lin3
  - dblp_id: https://dblp.org/pid/43/7716-8
    emails: '****@illinois.edu'
    first_name: Zifeng
    google_scholar_id: https://scholar.google.com/citations?user=kMlWwTAAAAAJ&hl=en
    homepage: https://zifengwang.xyz
    institution: University of Illinois, Urbana Champaign
    last_name: Wang
    name: Zifeng Wang
    username: ~Zifeng_Wang3
  - dblp_id: https://dblp.uni-trier.de/pid/54/4948.html
    emails: '****@gmail.com'
    first_name: Jimeng
    google_scholar_id: https://scholar.google.com/citations?user=9jmmp5sAAAAJ&hl=en
    homepage: http://sunlab.org
    institution: Georgia Tech Research Corporation, University of Illinois, Urbana
      Champaign, College of Computing and Georgia Institute of Technology
    last_name: Sun
    name: Jimeng Sun
    orcid: https://orcid.org/0000-0003-1512-6426
    username: ~Jimeng_Sun3
  - dblp_id: https://dblp.org/pid/h/JiaweiHan.html
    emails: '****@cs.uiuc.edu'
    first_name: Jiawei
    google_scholar_id: https://scholar.google.com.tw/citations?user=Kv9AbjMAAAAJ
    homepage: http://hanj.cs.illinois.edu/
    last_name: Han
    name: Jiawei Han
    semantic_scholar_id: https://www.semanticscholar.org/author/Jiawei-Han/145325584
    username: ~Jiawei_Han1
  decision: toMainConference
  end_page: 3040
  file: 270.pdf
  id: 270
  num_pages: 18
  openreview_id: dRyHffrxEr
  pdf_file: 8ad68fe04d0bc75a21d2f2b648a83cdfe766e4c3.pdf
  start_page: 3023
  title: 'GenRES: Rethinking Evaluation for Generative Relation Extraction in the
    Era of Large Language Models'
- abstract: The Mayan languages comprise a language family with an ancient history,
    millions of speakers, and immense cultural value, that, nevertheless, remains
    severely underrepresented in terms of resources and global exposure. In this paper
    we develop, curate, and publicly release a set of corpora in several Mayan languages
    spoken in Guatemala and Southern Mexico, which we call MayanV. The datasets are
    parallel with Spanish, the dominant language of the region, and are taken from
    official native sources focused on representing informal, day-to-day, and non-domain-specific
    language. As such, and according to our dialectometric analysis, they differ in
    register from most other available resources. Additionally, we present neural
    machine translation models, trained on as many resources and Mayan languages as
    possible, and evaluated exclusively on our datasets. We observe lexical divergences
    between the dialects of Spanish in our resources and the more widespread written
    standard of Spanish, and that resources other than the ones we present do not
    seem to improve translation performance, indicating that many such resources may
    not accurately capture common, real-life language usage. The MayanV dataset is
    available at https://github.com/transducens/mayanv.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: 'Special Theme: Languages of Latin America'
  authors:
  - emails: '****@ua.es'
    first_name: "Andr\xE9s"
    google_scholar_id: https://scholar.google.ca/citations?user=WvNTlPEAAAAJ&hl=en
    institution: Universidad de Alicante
    last_name: Lou
    name: "Andr\xE9s Lou"
    username: "~Andr\xE9s_Lou1"
  - dblp_id: https://dblp.org/pid/96/1185.html
    emails: '****@dlsi.ua.es'
    first_name: Juan Antonio
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=_NEbOj4AAAAJ
    homepage: https://www.dlsi.ua.es/~japerez/
    institution: Universidad de Alicante
    last_name: "P\xE9rez-Ortiz"
    name: "Juan Antonio P\xE9rez-Ortiz"
    orcid: https://orcid.org/0000-0001-7659-8908
    semantic_scholar_id: https://www.semanticscholar.org/author/Juan-Antonio-P%C3%A9rez-Ortiz/3186753
    username: "~Juan_Antonio_P\xE9rez-Ortiz1"
  - dblp_id: https://dblp.org/pid/56/6718
    emails: '****@dlsi.ua.es'
    first_name: Felipe
    google_scholar_id: https://scholar.google.com/citations?user=PdFGBlcAAAAJ
    homepage: https://www.dlsi.ua.es/~fsanchez/
    institution: University of Alicante
    last_name: "S\xE1nchez-Mart\xEDnez"
    name: "Felipe S\xE1nchez-Mart\xEDnez"
    orcid: https://orcid.org/0000-0002-2295-2630
    semantic_scholar_id: https://www.semanticscholar.org/author/F.-S%C3%A1nchez-Mart%C3%ADnez/1398917991
    username: "~Felipe_S\xE1nchez-Mart\xEDnez1"
  - dblp_id: https://dblp.org/pid/93/10489
    emails: '****@dlsi.ua.es'
    first_name: "V\xEDctor"
    google_scholar_id: https://scholar.google.es/citations?user=QzMP_B8AAAAJ&hl=es
    homepage: https://www.dlsi.ua.es/~vmsanchez
    institution: Universidad de Alicante
    last_name: "S\xE1nchez-Cartagena"
    middle_name: M.
    name: "V\xEDctor M. S\xE1nchez-Cartagena"
    orcid: https://orcid.org/0000-0001-9600-6885
    semantic_scholar_id: https://www.semanticscholar.org/author/V.-M.-S%C3%A1nchez-Cartagena/1399637675
    username: "~V\xEDctor_M._S\xE1nchez-Cartagena1"
  decision: toMainConference
  end_page: 3053
  file: 271.pdf
  id: 271
  num_pages: 13
  openreview_id: e3GhnNCFL8
  pdf_file: 405af378626fa4687a5454129f056e6e93b635a2.pdf
  start_page: 3041
  title: Curated Datasets and Neural Models for Machine Translation of Informal Registers
    between Mayan and Spanish Vernaculars
- abstract: 'Recent work to enhance data partitioning strategies for more realistic
    model evaluation face challenges in providing a clear optimal choice. This study
    addresses these challenges, focusing on morphological segmentation and synthesizing
    limitations related to language diversity, adoption of multiple datasets and splits,
    and detailed model comparisons. Our study leverages data from 19 languages, including
    ten indigenous or endangered languages across 10 language families with diverse
    morphological systems (polysynthetic, fusional, and agglutinative) and different
    degrees of data availability. We conduct large-scale experimentation with varying
    sized combinations of training and evaluation sets as well as new test data. Our
    results show that, when faced with new test data: (1) models trained from random
    splits are able to achieve higher numerical scores; (2) model rankings derived
    from random splits tend to generalize more consistently.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multilinguality and Language Diversity
  authors:
  - dblp_id: https://dblp.org/pid/278/8017
    emails: '****@ufl.edu'
    first_name: Zoey
    google_scholar_id: https://scholar.google.com/citations?user=lChNEQYAAAAJ&hl=en
    homepage: https://zoeyliu18.github.io/
    institution: University of Florida
    last_name: Liu
    name: Zoey Liu
    semantic_scholar_id: https://www.semanticscholar.org/author/Zoey-Liu/2109308254
    username: ~Zoey_Liu1
  - dblp_id: https://dblp.org/pid/d/BonnieJDorr
    emails: '****@ufl.edu'
    first_name: Bonnie
    homepage: https://www.cise.ufl.edu/dorr-bonnie-j/
    institution: University of Florida
    last_name: Dorr
    middle_name: J
    name: Bonnie J Dorr
    orcid: https://orcid.org/0000-0003-4356-5813
    username: ~Bonnie_J_Dorr1
  decision: toMainConference
  end_page: 3067
  file: 273.pdf
  id: 273
  num_pages: 14
  openreview_id: JkreATy7FV
  pdf_file: 66698e0794f222ec16dbb4116e87d73f90024ebc.pdf
  start_page: 3054
  title: 'The Effect of Data Partitioning Strategy on Model Generalizability: A Case
    Study of Morphological Segmentation'
- abstract: It is well-known that speakers who entrain to one another have more successful
    conversations than those who do not. Previous research has shown that interlocutors
    entrain on linguistic features in both written and spoken $\emph{monolingual}$
    domains. More recent work on $\emph{code-switched}$ communication has also shown
    preliminary evidence of entrainment on certain aspects of code-switching (CSW).
    However, such studies of entrainment in code-switched domains have been extremely
    few and restricted to human-machine textual interactions. Our work studies code-switched
    spontaneous speech between humans, finding that (1) patterns of written and spoken
    entrainment in monolingual settings largely generalize to code-switched settings,
    and (2) some patterns of entrainment on code-switching in dialogue agent-generated
    text generalize to spontaneous code-switched speech. Our findings give rise to
    important implications for the potentially "universal" nature of entrainment as
    a communication phenomenon, and potential applications in inclusive and interactive
    speech technology.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Speech recognition, text-to-speech and spoken language understanding
  authors:
  - dblp_id: https://dblp.org/pid/241/9429
    emails: '****@columbia.edu'
    first_name: Debasmita
    google_scholar_id: https://scholar.google.com/citations?user=EOkUV58AAAAJ&hl=en&authuser=2
    institution: Columbia University
    last_name: Bhattacharya
    name: Debasmita Bhattacharya
    semantic_scholar_id: https://www.semanticscholar.org/author/Debasmita-Bhattacharya/2052797388
    username: ~Debasmita_Bhattacharya1
  - emails: '****@barnard.edu'
    first_name: Siying
    last_name: Ding
    name: Siying Ding
    username: ~Siying_Ding1
  - emails: '****@columbia.edu'
    first_name: Alayna
    last_name: Nguyen
    name: Alayna Nguyen
    username: ~Alayna_Nguyen1
  - emails: '****@cs.columbia.edu'
    first_name: Julia
    google_scholar_id: https://scholar.google.com/citations?user=Qrd7FCoAAAAJ&hl=en
    homepage: http://www.cs.columbia.edu/~julia/
    institution: Columbia University
    last_name: Hirschberg
    name: Julia Hirschberg
    username: ~Julia_Hirschberg1
  decision: toMainConference
  end_page: 3079
  file: 274.pdf
  id: 274
  num_pages: 12
  openreview_id: rxRortGNN3
  pdf_file: b4600dcf6651774127fa9f004ebeb7978b5a070c.pdf
  start_page: 3068
  title: Measuring Entrainment in Spontaneous Code-switched Speech
- abstract: "Symbolic meaning representations of natural language text have been studied\
    \ since at least the 1960s. With the availability of large annotated corpora,\
    \ and more powerful machine learning tools, the field has recently seen several\
    \ new developments. In this survey, we study today\u2019s most prominent Meaning\
    \ Representation Frameworks. We shed light on their theoretical properties, as\
    \ well as on their practical research environment, i.e., on datasets, parsers,\
    \ applications, and future challenges."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: 'Semantics: Sentence-level Semantics, Textual Inference and Other
      areas'
  authors:
  - emails: '****@telecom-paris.fr'
    first_name: Zacchary
    last_name: Sadeddine
    name: Zacchary Sadeddine
    username: ~Zacchary_Sadeddine1
  - dblp_id: https://dblp.org/pid/185/5616
    emails: '****@gmail.com'
    first_name: Juri
    homepage: https://www.juriopitz.com
    institution: "Ruprecht-Karls-Universit\xE4t Heidelberg and University of Zurich"
    last_name: Opitz
    name: Juri Opitz
    semantic_scholar_id: https://www.semanticscholar.org/author/J.-Opitz/32781138
    username: ~Juri_Opitz2
  - dblp_id: https://dblp.org/pid/13/1396
    emails: '****@suchanek.name'
    first_name: Fabian
    google_scholar_id: https://scholar.google.fr/citations?user=djtZhi8AAAAJ&hl=en&oi=ao
    homepage: https://suchanek.name
    institution: Telecom Paris
    last_name: Suchanek
    middle_name: M.
    name: Fabian M. Suchanek
    orcid: https://orcid.org/0000-0001-7189-2796
    semantic_scholar_id: https://www.semanticscholar.org/author/Fabian-M.-Suchanek/1679784
    username: ~Fabian_M._Suchanek1
  decision: toMainConference
  end_page: 3095
  file: 275.pdf
  id: 275
  num_pages: 16
  openreview_id: Owchjzze1z
  pdf_file: 569db83d64cc5f3a016b32959f8ebb38263fc850.pdf
  start_page: 3080
  title: "A Survey of Meaning Representations \u2013 From Theory to Practical Utility"
- abstract: "Large-scale multilingual Pretrained Language Models (mPLMs) yield impressive\
    \ performance on cross-language tasks, yet significant performance disparities\
    \ exist across different languages within the same mPLM. \nPrevious studies endeavored\
    \ to narrow these disparities by supervise fine-tuning the mPLMs with multilingual\
    \ data.\nHowever, obtaining labeled multilingual data is time-consuming, and fine-tuning\
    \ mPLM with limited labeled multilingual data merely encapsulates the knowledge\
    \ specific to the labeled data.\nTherefore, we introduce **ALSACE** to leverage\
    \ the learned knowledge from the well-performing languages to guide under-performing\
    \ ones within the same mPLM, eliminating the need for additional labeled multilingual\
    \ data. \nExperiments show that ALSACE effectively mitigates language-level performance\
    \ disparity across various mPLMs while showing the competitive performance on\
    \ different multilingual NLU tasks, ranging from full resource to limited resource\
    \ settings. \nThe code for our approach is available at https://github.com/pkunlp-icler/ALSACE."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multilinguality and Language Diversity
  authors:
  - emails: '****@stu.pku.edu.cn'
    first_name: Haozhe
    last_name: Zhao
    name: Haozhe Zhao
    orcid: https://orcid.org/0000-0003-0502-4426
    username: ~Haozhe_Zhao1
  - emails: '****@gmail.com'
    first_name: Zefan
    last_name: Cai
    name: Zefan Cai
    username: ~Zefan_Cai1
  - emails: '****@stu.pku.edu.cn'
    first_name: Shuzheng
    google_scholar_id: https://scholar.google.com.hk/citations?user=zO2XyZUAAAAJ&hl=zh-CN
    last_name: Si
    name: Shuzheng Si
    username: ~Shuzheng_Si1
  - dblp_id: https://dblp.org/pid/01/5394-24
    emails: '****@outlook.com'
    first_name: Liang
    google_scholar_id: https://scholar.google.com/citations?user=lMKPaTYAAAAJ&hl=en
    homepage: https://chenllliang.github.io/about/
    last_name: Chen
    name: Liang Chen
    semantic_scholar_id: https://www.semanticscholar.org/author/Liang-Chen/2146034504
    username: ~Liang_Chen10
  - emails: '****@stu.pku.edu.cn'
    first_name: Yufeng
    homepage: https://hyf.ink
    last_name: He
    name: Yufeng He
    username: ~Yufeng_He1
  - emails: '****@stu.pku.edu.cn'
    first_name: Kaikai
    google_scholar_id: https://scholar.google.com/citations?user=6TrBRiEAAAAJ&hl=zh-CN
    homepage: https://github.com/kkk-an
    last_name: An
    name: Kaikai An
    username: ~Kaikai_An1
  - dblp_id: https://dblp.org/pid/91/6051.html
    emails: '****@pku.edu.cn'
    first_name: Baobao
    google_scholar_id: https://scholar.google.com/citations?user=LaKNyhQAAAAJ
    homepage: http://eecs.pku.edu.cn/EN/People/Faculty/Detail/?ID=6027
    institution: Peking University
    last_name: Chang
    name: Baobao Chang
    semantic_scholar_id: https://www.semanticscholar.org/author/Baobao-Chang/39488576
    username: ~Baobao_Chang1
  decision: toMainConference
  end_page: 3110
  file: 276.pdf
  id: 276
  num_pages: 15
  openreview_id: Ni1eUwjtL3
  pdf_file: 696c2372943aa9cd9e14e6959e15b1118de6564e.pdf
  start_page: 3096
  title: Mitigating Language-Level Performance Disparity in mPLMs via Teacher Language
    Selection and Cross-lingual Self-Distillation
- abstract: 'Contemporary Large Language Models (LLMs) exhibit a high degree of code
    generation and comprehension capability. A particularly promising area is their
    ability to interpret code modules from unfamiliar libraries for solving user-instructed
    tasks. Recent work has shown that large proprietary LLMs can learn novel library
    usage in-context from demonstrations. These results raise several open questions:
    whether demonstrations of library usage is required, whether smaller (and more
    open) models also possess such capabilities, etc. In this work, we take a broader
    approach by systematically evaluating a diverse array of LLMs across three scenarios
    reflecting varying levels of domain specialization to understand their abilities
    and limitations in generating code based on libraries defined in-context. Our
    results show that even smaller open-source LLMs like Llama-2 and StarCoder demonstrate
    an adept understanding of novel code libraries based on specification presented
    in-context. Our findings further reveal that LLMs exhibit a surprisingly high
    proficiency in learning novel library modules even when provided with just natural
    language descriptions or raw code implementations of the functions, which are
    often cheaper to obtain than demonstrations. Overall, our results pave the way
    for harnessing LLMs in more adaptable and dynamic coding environments.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - dblp_id: https://dblp.org/pid/254/5212
    emails: '****@gmail.com'
    first_name: Arkil
    google_scholar_id: https://scholar.google.com/citations?user=-5goVAsAAAAJ&hl=en
    homepage: https://arkilpatel.github.io/
    institution: Mila - Quebec AI Institute and McGill University
    last_name: Patel
    name: Arkil Patel
    semantic_scholar_id: https://www.semanticscholar.org/author/Arkil-Patel/1443788809
    username: ~Arkil_Patel1
  - dblp_id: https://dblp.org/pid/64/8153
    emails: '****@mila.quebec'
    first_name: Siva
    homepage: http://sivareddy.in
    institution: Mila, McGill University and Mila, McGill University
    last_name: Reddy
    name: Siva Reddy
    username: ~Siva_Reddy1
  - dblp_id: https://dblp.org/pid/151/6504
    emails: '****@gmail.com'
    first_name: Dzmitry
    google_scholar_id: https://scholar.google.ca/citations?user=Nq0dVMcAAAAJ
    institution: ServiceNow Research
    last_name: Bahdanau
    name: Dzmitry Bahdanau
    semantic_scholar_id: https://www.semanticscholar.org/author/Dzmitry-Bahdanau/3335364
    username: ~Dzmitry_Bahdanau1
  - dblp_id: https://dblp.org/pid/27/7184
    emails: '****@allenai.org'
    first_name: Pradeep
    google_scholar_id: https://scholar.google.com/citations?authorid=Bpd76vcAAAAJ
    homepage: https://pdasigi.github.io/
    institution: Allen Institute for Artificial Intelligence
    last_name: Dasigi
    name: Pradeep Dasigi
    semantic_scholar_id: https://www.semanticscholar.org/author/Pradeep-Dasigi/2697425
    username: ~Pradeep_Dasigi1
  decision: toMainConference
  end_page: 3129
  file: 278.pdf
  id: 278
  num_pages: 19
  openreview_id: ikfK8fgz60
  pdf_file: ec0a492afcace183e4b5ddd82ca8b498dea83436.pdf
  start_page: 3111
  title: Evaluating In-Context Learning of Libraries for Code Generation
- abstract: News Image Captioning aims to create captions from news articles and images,
    emphasizing the connection between textual context and visual elements. Recognizing
    the significance of human faces in news images and the face-name co-occurrence
    pattern in existing datasets, we propose a face-naming module for learning better
    name embeddings. Apart from names, which can be directly linked to an image area
    (faces), news image captions mostly contain context information that can only
    be found in the article. We design a retrieval strategy using CLIP to retrieve
    sentences that are semantically close to the image, mimicking human thought process
    of linking articles to images. Furthermore, to tackle the problem of the imbalanced
    proportion of article context and image context in captions, we introduce a simple
    yet effective method Contrasting with Language Model backbone (CoLaM) to the training
    pipeline. We conduct extensive experiments to demonstrate the efficacy of our
    framework. We out-perform the previous state-of-the-art (without external data)
    by 7.97/5.80 CIDEr scores on GoodNews/NYTimes800k. Our code is available at https://github.com/tingyu215/VACNIC.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multimodality and Language Grounding to Vision, Robotics and Beyond
  authors:
  - dblp_id: https://dblp.org/pid/195/9018
    emails: '****@kuleuven.be'
    first_name: Tingyu
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=d18-zLYAAAAJ
    homepage: https://tingyu215.github.io
    institution: KU Leuven
    last_name: Qu
    name: Tingyu Qu
    orcid: https://orcid.org/0000-0002-0656-5745
    username: ~Tingyu_Qu1
  - dblp_id: https://dblp.org/pid/79/2382
    emails: '****@esat.kuleuven.be'
    first_name: Tinne
    google_scholar_id: https://scholar.google.be/citations?user=EuFF9kUAAAAJ
    homepage: https://homes.esat.kuleuven.be/~tuytelaa/
    institution: KU Leuven
    last_name: Tuytelaars
    name: Tinne Tuytelaars
    orcid: https://orcid.org/0000-0003-3307-9723
    username: ~Tinne_Tuytelaars1
  - dblp_id: https://dblp.org/pid/m/MarieFrancineMoens
    emails: '****@cs.kuleuven.be'
    first_name: Marie-Francine
    google_scholar_id: https://scholar.google.com.tw/citations?user=O9hYMUUAAAAJ
    homepage: https://people.cs.kuleuven.be/~sien.moens/
    institution: KU Leuven, KU Leuven
    last_name: Moens
    name: Marie-Francine Moens
    orcid: https://orcid.org/0000-0002-3732-9323
    semantic_scholar_id: https://www.semanticscholar.org/author/Marie-Francine-Moens/100781843
    username: ~Marie-Francine_Moens1
  decision: toMainConference
  end_page: 3146
  file: 279.pdf
  id: 279
  num_pages: 17
  openreview_id: AvWlFMoCup
  pdf_file: 017568adfd4725d28eed978812f827c7e846a702.pdf
  start_page: 3130
  title: Visually-Aware Context Modeling for News Image Captioning
- abstract: "We present a game-theoretic model of pragmatics that we call ReCo (for\
    \ Regularized Conventions). This model formulates pragmatic communication as a\
    \ game in which players are rewarded for communicating successfully and penalized\
    \ for deviating from a shared, \u201Cdefault\u201D semantics. As a result, players\
    \ assign utterances context-dependent meanings that jointly optimize communicative\
    \ success and naturalness with respect to speakers\u2019 and listeners\u2019 background\
    \ knowledge of language. By using established game-theoretic tools to compute\
    \ equilibrium strategies for this game, we obtain principled pragmatic language\
    \ generation procedures with formal guarantees of communicative success. Across\
    \ several datasets capturing real and idealized human judgments about pragmatic\
    \ implicature, ReCo matches, or slightly improves upon, predictions made by Iterated\
    \ Best Response and Rational Speech Acts models of language understanding."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Discourse and Pragmatics
  authors:
  - dblp_id: https://dblp.org/pid/192/1229
    emails: '****@gmail.com'
    first_name: Athul
    google_scholar_id: https://scholar.google.ca/citations?user=XT3E7RoAAAAJ&hl=en
    homepage: http://apjacob.me/
    institution: Google and Massachusetts Institute of Technology
    last_name: Jacob
    middle_name: Paul
    name: Athul Paul Jacob
    username: ~Athul_Paul_Jacob1
  - dblp_id: https://dblp.org/pid/159/2090
    emails: '****@mit.edu'
    first_name: Gabriele
    google_scholar_id: https://scholar.google.com/citations?user=sktDNcEAAAAJ&hl=en
    homepage: http://www.cs.cmu.edu/~gfarina/about/
    institution: Massachusetts Institute of Technology
    last_name: Farina
    name: Gabriele Farina
    username: ~Gabriele_Farina1
  - dblp_id: https://dblp.org/pid/97/8154
    emails: '****@mit.edu'
    first_name: Jacob
    google_scholar_id: https://scholar.google.com/citations?user=dnZ8udEAAAAJ
    homepage: http://web.mit.edu/jda/www
    institution: Massachusetts Institute of Technology and Microsoft
    last_name: Andreas
    name: Jacob Andreas
    semantic_scholar_id: https://www.semanticscholar.org/author/Jacob-Andreas/2112400
    username: ~Jacob_Andreas1
  decision: toMainConference
  end_page: 3158
  file: 280.pdf
  id: 280
  num_pages: 12
  openreview_id: Td4iWTdr5d
  pdf_file: 9757b2b4c24e6233be3b1b41e0a56d1bab65390e.pdf
  start_page: 3147
  title: 'Regularized Conventions: Equilibrium Computation as a Model of Pragmatic
    Reasoning'
- abstract: 'Reference-based metrics that operate at the sentence-level typically
    outperform quality estimation metrics, which have access only to the source and
    system output.

    This is unsurprising, since references resolve ambiguities that may be present
    in the source.

    In this paper, we investigate whether additional source context can effectively
    substitute for a reference.

    We present a metric named SLIDE (SLIding Document Evaluator), which operates on
    blocks of sentences. SLIDE leverages a moving window that slides over each document
    in the test set, feeding each chunk of sentences into an unmodified, off-the-shelf
    quality estimation model.

    We find that SLIDE obtains significantly higher pairwise system accuracy than
    its sentence-level baseline, in some cases even eliminating the gap with reference-base
    metrics.

    This suggests that source context may provide the same information as a human
    reference in disambiguating source ambiguities. This finding is especially pertinent
    for reference-free document-level evaluation, wherein SLIDE could provide higher-quality
    pairwise system assessments while only requiring document boundary annotations.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Translation
  authors:
  - dblp_id: https://dblp.org/pid/205/2388
    emails: '****@microsoft.com'
    first_name: Vikas
    google_scholar_id: https://scholar.google.com/citations?user=25Tjnq4AAAAJ
    homepage: https://vyraun.github.io/
    institution: Microsoft
    last_name: Raunak
    name: Vikas Raunak
    semantic_scholar_id: https://www.semanticscholar.org/author/Vikas-Raunak/24025563
    username: ~Vikas_Raunak2
  - dblp_id: https://dblp.org/pid/185/1333
    emails: '****@gmail.com'
    first_name: Tom
    google_scholar_id: https://scholar.google.cz/citations?user=8dUF8YQAAAAJ
    institution: Microsoft
    last_name: Kocmi
    name: Tom Kocmi
    orcid: https://orcid.org/0000-0002-7993-9859
    semantic_scholar_id: https://www.semanticscholar.org/author/Tom-Kocmi/3452584
    username: ~Tom_Kocmi1
  - dblp_id: https://dblp.org/pid/51/8151
    emails: '****@cs.jhu.edu'
    first_name: Matt
    google_scholar_id: https://scholar.google.com/citations?user=4w7LhxsAAAAJ&hl=en
    homepage: http://cs.jhu.edu/~post/
    institution: Microsoft and Johns Hopkins University
    last_name: Post
    name: Matt Post
    orcid: https://orcid.org/0000-0002-1297-6794
    semantic_scholar_id: https://www.semanticscholar.org/author/Matt-Post/38842528
    username: ~Matt_Post1
  decision: toMainConference
  end_page: 3165
  file: 281.pdf
  id: 281
  num_pages: 7
  openreview_id: 49E8MFe61B
  pdf_file: 9815cdfdde07a4b4cc07077abc6f559387a73f48.pdf
  start_page: 3159
  title: 'SLIDE: Reference-free Evaluation for Machine Translation using a Sliding
    Document Window'
- abstract: 'Topic modeling is a well-established technique for exploring text corpora.
    Conventional topic models (e.g., LDA) represent topics as bags of words that often
    require "reading the tea leaves" to interpret; additionally, they offer users
    minimal control over the formatting and specificity of resulting topics. To tackle
    these issues, we introduce TopicGPT, a prompt-based framework that uses large
    language models (LLMs) to uncover latent topics in a text collection. TopicGPT
    produces topics that align better with human categorizations compared to competing
    methods: it achieves a harmonic mean purity of 0.74 against human-annotated Wikipedia
    topics compared to 0.64 for the strongest baseline. Its topics are also more interpretable,
    dispensing with ambiguous bags of words in favor of topics with natural language
    labels and associated free-form descriptions. Moreover, the framework is highly
    adaptable, allowing users to specify constraints and modify topics without the
    need for model retraining. By streamlining access to high-quality and interpretable
    topics, TopicGPT represents a compelling, human-centered approach to topic modeling.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Retrieval and Text Mining
  authors:
  - emails: '****@gmail.com'
    first_name: Chau
    last_name: Pham
    middle_name: Minh
    name: Chau Minh Pham
    orcid: https://orcid.org/0009-0004-0435-7450
    username: ~Chau_Minh_Pham1
  - dblp_id: https://dblp.org/pid/239/4089
    emails: '****@umd.edu'
    first_name: Alexander
    google_scholar_id: https://scholar.google.com/citations?user=NpK0IXgAAAAJ&hl=en
    homepage: https://alexanderhoyle.com
    institution: University of Maryland, College Park
    last_name: Hoyle
    name: Alexander Hoyle
    semantic_scholar_id: https://www.semanticscholar.org/author/Alexander-Miserlis-Hoyle/49462969
    username: ~Alexander_Hoyle1
  - emails: '****@gmail.com'
    first_name: Simeng
    homepage: https://people.cs.umass.edu/~simengsun/
    institution: College of Information and Computer Science, University of Massachusetts,
      Amherst
    last_name: Sun
    name: Simeng Sun
    username: ~Simeng_Sun1
  - dblp_id: https://dblp.org/pid/p/PhilipResnik
    emails: '****@umd.edu'
    first_name: Philip
    google_scholar_id: https://scholar.google.com.tw/citations?user=71BFWc0AAAAJ
    homepage: http://www.umiacs.umd.edu/~resnik/
    institution: University of Maryland, College Park
    last_name: Resnik
    name: Philip Resnik
    username: ~Philip_Resnik1
  - dblp_id: https://dblp.org/pid/148/9178
    emails: '****@cs.umass.edu'
    first_name: Mohit
    google_scholar_id: https://scholar.google.com/citations?user=rBVA5tcAAAAJ&hl=en
    homepage: http://cs.umass.edu/~miyyer
    institution: University of Massachusetts Amherst
    last_name: Iyyer
    name: Mohit Iyyer
    username: ~Mohit_Iyyer1
  decision: toMainConference
  end_page: 3194
  file: 282.pdf
  id: 282
  num_pages: 29
  openreview_id: gTVX21g9Uo
  pdf_file: 51f66c7def461d153f9fd06d95a1f36c49ea6f3c.pdf
  start_page: 3166
  title: 'TopicGPT: A Prompt-based Topic Modeling Framework'
- abstract: "Textual backdoor attacks, characterized by subtle manipulations of input\
    \ triggers and training dataset labels, pose significant threats to security-sensitive\
    \ applications. The rise of advanced generative models, such as GPT-4, with their\
    \ capacity for human-like rewriting, makes these attacks increasingly challenging\
    \ to detect. In this study, we conduct an in-depth examination of black-box generative\
    \ models as tools for backdoor attacks, thereby emphasizing the need for effective\
    \ defense strategies. We propose BGMAttack, a novel framework that harnesses advanced\
    \ generative models to execute stealthier backdoor attacks on text classifiers.\
    \ Unlike prior approaches constrained by subpar generation quality, BGMAttack\
    \ renders backdoor triggers more elusive to human cognition and advanced machine\
    \ detection. A rigorous evaluation of attack effectiveness over four sentiment\
    \ classification tasks, complemented by four human cognition stealthiness tests,\
    \ reveals BGMAttack\u2019s superior performance, achieving a state-of-the-art\
    \ attack success rate of 97.35\\% on average while maintaining superior stealth\
    \ compared to conventional methods. The dataset and code are available: \\href{https://github.com/JiazhaoLi/BGMAttack}{https://github.com/JiazhaoLi/BGMAttack}."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - dblp_id: https://dblp.org/pid/273/5334
    emails: '****@umich.edu'
    first_name: Jiazhao
    google_scholar_id: https://scholar.google.com/citations?user=dDJCGpYAAAAJ&hl=en&oi=ao
    homepage: https://jiazhaoli.github.io
    institution: University of Michigan - Ann Arbor
    last_name: Li
    name: Jiazhao Li
    orcid: https://orcid.org/0000-0001-8219-5638
    semantic_scholar_id: https://www.semanticscholar.org/author/Jiazhao-Li/14636073
    username: ~Jiazhao_Li1
  - dblp_id: https://dblp.org/pid/33/6721
    emails: '****@gmail.com'
    first_name: Yijin
    google_scholar_id: https://scholar.google.com/citations?user=jklXpQ4AAAAJ
    homepage: https://github.com/Yijin14
    last_name: Yang
    name: Yijin Yang
    semantic_scholar_id: https://www.semanticscholar.org/author/Yijin-Yang/2108568154
    username: ~Yijin_Yang2
  - dblp_id: https://dblp.org/pid/153/7524-1
    emails: '****@umich.edu'
    first_name: Zhuofeng
    google_scholar_id: https://scholar.google.com/citations?user=bqinFgYAAAAJ&hl=en&authuser=1
    institution: University of Michigan - Ann Arbor
    last_name: Wu
    name: Zhuofeng Wu
    username: ~Zhuofeng_Wu1
  - dblp_id: https://dblp.org/pid/67/6469.html
    emails: '****@umich.edu'
    first_name: V.G.Vinod
    institution: University of Michigan - Ann Arbor
    last_name: Vydiswaran
    name: V.G.Vinod Vydiswaran
    orcid: https://orcid.org/0000-0002-3122-1936
    username: ~V.G.Vinod_Vydiswaran1
  - dblp_id: https://dblp.org/pid/150/3317
    emails: '****@umich.edu'
    first_name: Chaowei
    google_scholar_id: https://scholar.google.com/citations?user=Juoqtj8AAAAJ&hl=en
    homepage: https://xiaocw11.github.io/
    institution: University of Wisconsin - Madison and NVIDIA
    last_name: Xiao
    name: Chaowei Xiao
    username: ~Chaowei_Xiao2
  decision: toMainConference
  end_page: 3214
  file: 283.pdf
  id: 283
  num_pages: 20
  openreview_id: pUkFym15u4
  pdf_file: 0dd0560345e7088543ed8c77d0986e3fe92ba6d6.pdf
  start_page: 3195
  title: 'ChatGPT as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox
    Generative Model Trigger'
- abstract: Much work in the space of NLP has used computational methods to explore
    sociolinguistic variation in text.  In this paper, we argue that memes, as multimodal
    forms of language comprised of visual templates and text, also exhibit meaningful
    social variation. We construct a computational pipeline to cluster individual
    instances of memes into templates and semantic variables, taking advantage of
    their multimodal structure in doing so. We apply this method to a large collection
    of meme images  from Reddit and make available the resulting SemanticMemes dataset
    of 3.8M images clustered by their semantic function. We use these clusters to
    analyze linguistic variation in memes, discovering  not only that socially meaningful
    variation in meme usage exists between subreddits, but that patterns of meme innovation
    and acculturation within these communities align with previous findings on written
    language.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Computational Social Science and Cultural Analytics
  authors:
  - dblp_id: https://dblp.org/pid/278/8280
    emails: '****@berkeley.edu'
    first_name: Naitian
    google_scholar_id: https://scholar.google.com/citations?user=baneg1EAAAAJ&hl=en&oi=ao
    homepage: https://naitian.org
    institution: University of California, Berkeley
    last_name: Zhou
    name: Naitian Zhou
    semantic_scholar_id: https://www.semanticscholar.org/author/Naitian-Zhou/2007956059
    username: ~Naitian_Zhou1
  - dblp_id: https://dblp.uni-trier.de/pid/48/4613
    emails: '****@umich.edu'
    first_name: David
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=sGFFr5kAAAAJ
    homepage: http://jurgens.people.si.umich.edu
    institution: University of Michigan - Ann Arbor
    last_name: Jurgens
    name: David Jurgens
    orcid: https://orcid.org/0000-0002-2135-9878
    semantic_scholar_id: https://www.semanticscholar.org/author/David-Jurgens/3046220
    username: ~David_Jurgens1
  - dblp_id: https://dblp.org/pid/39/5799
    emails: '****@berkeley.edu'
    first_name: David
    google_scholar_id: https://scholar.google.com.tw/citations?user=RkA1y54AAAAJ
    homepage: http://people.ischool.berkeley.edu/~dbamman/
    institution: University of California Berkeley
    last_name: Bamman
    name: David Bamman
    semantic_scholar_id: https://www.semanticscholar.org/author/David-Bamman/2168134
    username: ~David_Bamman1
  decision: toMainConference
  end_page: 3234
  file: 284.pdf
  id: 284
  num_pages: 20
  openreview_id: G2OoxXWrT7
  pdf_file: 89adc36eb35784a16285f524dc96451ae9a8a404.pdf
  start_page: 3215
  title: 'Social Meme-ing: Measuring Linguistic Variation in Memes'
- abstract: As language models are adopted by a more sophisticated and diverse set
    of users, the importance of guaranteeing that they provide factually correct information
    supported by verifiable sources is critical across fields of study. This is especially
    the case for high-stakes fields, such as medicine and law, where the risk of propagating
    false information is high and can lead to undesirable societal consequences. Previous
    work studying attribution and factuality has not focused on analyzing these characteristics
    of language model outputs in domain-specific scenarios. In this work, we conduct
    human evaluation of responses from a few representative systems along various
    axes of attribution and factuality, by bringing domain experts in the loop. Specifically,
    we collect expert-curated questions from 484 participants across 32 fields of
    study, and then ask the same experts to evaluate generated responses to their
    own questions. In addition, we ask experts to improve upon responses from language
    models. The output of our analysis is ExpertQA, a high-quality long-form QA dataset
    with 2177 questions spanning 32 fields, along with verified answers and attributions
    for claims in the answers.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - dblp_id: https://dblp.org/pid/194/3061
    emails: '****@seas.upenn.edu'
    first_name: Chaitanya
    google_scholar_id: https://scholar.google.com/citations?user=s3MzzwwAAAAJ&hl=en
    homepage: https://chaitanyamalaviya.github.io/
    institution: University of Pennsylvania
    last_name: Malaviya
    name: Chaitanya Malaviya
    semantic_scholar_id: https://www.semanticscholar.org/author/Chaitanya-Malaviya/8805254
    username: ~Chaitanya_Malaviya1
  - emails: '****@gmail.com'
    first_name: Subin
    google_scholar_id: https://scholar.google.com/citations?view_op=list_works&hl=en&user=Rx2RrwgAAAAJ
    homepage: https://www.linkedin.com/in/leesubin
    last_name: Lee
    name: Subin Lee
    username: ~Subin_Lee1
  - dblp_id: https://dblp.org/pid/153/0087
    emails: '****@cis.upenn.edu'
    first_name: Sihao
    google_scholar_id: https://scholar.google.com/citations?user=PQ9dRCgAAAAJ
    homepage: https://www.seas.upenn.edu/~sihaoc/
    last_name: Chen
    name: Sihao Chen
    semantic_scholar_id: https://www.semanticscholar.org/author/Sihao-Chen/2087205
    username: ~Sihao_Chen1
  - emails: '****@uw.edu'
    first_name: Elizabeth
    homepage: https://www.fredhutch.org/en/provider-directory/elizabeth-sieber.html
    institution: University of Washington
    last_name: Sieber
    name: Elizabeth Sieber
    username: ~Elizabeth_Sieber1
  - dblp_id: https://dblp.org/pid/64/8396
    emails: '****@seas.upenn.edu'
    first_name: Mark
    homepage: http://markyatskar.com
    institution: Department of Computer and Information Science, School of Engineering
      and Applied Science
    last_name: Yatskar
    name: Mark Yatskar
    semantic_scholar_id: https://www.semanticscholar.org/author/Mark-Yatskar/2064210
    username: ~Mark_Yatskar1
  - dblp_id: https://dblp.org/pid/r/DanRoth
    emails: '****@seas.upenn.edu'
    first_name: Dan
    google_scholar_id: https://scholar.google.com/citations?user=E-bpPWgAAAAJ&hl=en
    homepage: https://www.cis.upenn.edu/~danroth/
    institution: Amazon and University of Pennsylvania
    last_name: Roth
    name: Dan Roth
    semantic_scholar_id: https://www.semanticscholar.org/author/D.-Roth/144590225
    username: ~Dan_Roth3
  decision: toMainConference
  end_page: 3255
  file: 285.pdf
  id: 285
  num_pages: 21
  openreview_id: hhC3nTgfOv
  pdf_file: 0b3756125b298d1bb5caa7b62fd5e81d93a17bee.pdf
  start_page: 3235
  title: 'ExpertQA: Expert-Curated Questions and Attributed Answers'
- abstract: Eliciting feedback from end users of NLP models can be beneficial for
    improving models. However, how should we present model responses to users so they
    are most amenable to be corrected from user feedback? Further, what properties
    do users value to understand and trust responses? We answer these questions by
    analyzing the effect of rationales (or explanations) generated by QA models to
    support their answers. We specifically consider decomposed QA models that first
    extract an intermediate rationale based on a context and a question and then use
    solely this rationale to answer the question. A rationale outlines the approach
    followed by the model to answer the question. Our work considers various formats
    of these rationales that vary according to well-defined properties of interest.
    We sample rationales from language models using few-shot prompting for two datasets,
    and then perform two user studies. First, we present users with incorrect answers
    and corresponding rationales in various formats and ask them to provide natural
    language feedback to revise the rationale. We then measure the effectiveness of
    this feedback in patching these rationales through in-context learning. The second
    study evaluates how well different rationale formats enable users to understand
    and trust model answers, when they are correct. We find that rationale formats
    significantly affect how easy it is (1) for users to give feedback for rationales,
    and (2) for models to subsequently execute this feedback. In addition, formats
    with attributions to the context and in-depth reasoning significantly enhance
    user-reported understanding and trust of model outputs.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - dblp_id: https://dblp.org/pid/194/3061
    emails: '****@seas.upenn.edu'
    first_name: Chaitanya
    google_scholar_id: https://scholar.google.com/citations?user=s3MzzwwAAAAJ&hl=en
    homepage: https://chaitanyamalaviya.github.io/
    institution: University of Pennsylvania
    last_name: Malaviya
    name: Chaitanya Malaviya
    semantic_scholar_id: https://www.semanticscholar.org/author/Chaitanya-Malaviya/8805254
    username: ~Chaitanya_Malaviya1
  - emails: '****@gmail.com'
    first_name: Subin
    google_scholar_id: https://scholar.google.com/citations?view_op=list_works&hl=en&user=Rx2RrwgAAAAJ
    homepage: https://www.linkedin.com/in/leesubin
    last_name: Lee
    name: Subin Lee
    username: ~Subin_Lee1
  - dblp_id: https://dblp.org/pid/r/DanRoth
    emails: '****@seas.upenn.edu'
    first_name: Dan
    google_scholar_id: https://scholar.google.com/citations?user=E-bpPWgAAAAJ&hl=en
    homepage: https://www.cis.upenn.edu/~danroth/
    institution: Amazon and University of Pennsylvania
    last_name: Roth
    name: Dan Roth
    semantic_scholar_id: https://www.semanticscholar.org/author/D.-Roth/144590225
    username: ~Dan_Roth3
  - dblp_id: https://dblp.org/pid/64/8396
    emails: '****@seas.upenn.edu'
    first_name: Mark
    homepage: http://markyatskar.com
    institution: Department of Computer and Information Science, School of Engineering
      and Applied Science
    last_name: Yatskar
    name: Mark Yatskar
    semantic_scholar_id: https://www.semanticscholar.org/author/Mark-Yatskar/2064210
    username: ~Mark_Yatskar1
  decision: toMainConference
  end_page: 3275
  file: 286.pdf
  id: 286
  num_pages: 20
  openreview_id: VLH9BrqUdi
  pdf_file: 0c8d99351bb7f8a4058857fca8eeb8e4670d51db.pdf
  start_page: 3256
  title: 'What if you said that differently?: How Explanation Formats Affect Human
    Feedback Efficacy and User Perception'
- abstract: 'Deployed dialogue agents have the potential to integrate human feedback
    to continuously improve themselves. However, humans may not always provide explicit
    signals when the chatbot makes mistakes during interactions. In this work, we
    propose Juicer, a framework to make use of both binary and free-form textual human
    feedback. It works by: (i) extending sparse binary feedback by training a satisfaction
    classifier to label the unlabeled data; and (ii) training a reply corrector to
    map the bad replies to good ones. We find that augmenting training with model-corrected
    replies improves the final dialogue model, and we can further improve performance
    by using both positive and negative replies through the recently proposed Director
    model.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Dialogue and Interactive Systems
  authors:
  - dblp_id: https://dblp.org/pid/218/5722
    emails: '****@columbia.edu'
    first_name: Weiyan
    google_scholar_id: https://scholar.google.com/citations?user=xj666rUAAAAJ&hl=en
    homepage: https://sites.google.com/ucdavis.edu/wyshi/
    last_name: Shi
    name: Weiyan Shi
    username: ~Weiyan_Shi2
  - dblp_id: https://dblp.org/pid/213/7927
    emails: '****@gmail.com'
    first_name: Emily
    google_scholar_id: https://scholar.google.com/citations?user=pfqzHqUAAAAJ&hl=en
    institution: Facebook AI Research
    last_name: Dinan
    name: Emily Dinan
    semantic_scholar_id: https://www.semanticscholar.org/author/Emily-Dinan/31461304
    username: ~Emily_Dinan1
  - dblp_id: https://dblp.org/pid/40/2561-1
    emails: '****@gmail.com'
    first_name: Kurt
    google_scholar_id: https://scholar.google.com/citations?user=VeRSO8EAAAAJ
    last_name: Shuster
    name: Kurt Shuster
    orcid: https://orcid.org/0000-0001-7777-5515
    username: ~Kurt_Shuster1
  - dblp_id: https://dblp.org/pid/29/6977.html
    emails: '****@gmail.com'
    first_name: Jason
    google_scholar_id: https://scholar.google.com/citations?user=lMkTx0EAAAAJ&hl=en
    homepage: http://www.jaseweston.com
    institution: New York University and Facebook
    last_name: Weston
    middle_name: E
    name: Jason E Weston
    username: ~Jason_E_Weston1
  - emails: '****@fb.com'
    first_name: Jing
    institution: Facebook AI Research
    last_name: Xu
    name: Jing Xu
    username: ~Jing_Xu5
  decision: toMainConference
  end_page: 3292
  file: 288.pdf
  id: 288
  num_pages: 17
  openreview_id: aNZNNzOuwd
  pdf_file: be6a5a042fdf3a0dbc09fb86433de1bb1746551b.pdf
  start_page: 3276
  title: 'When Life Gives You Lemons, Make Cherryade: Converting Feedback from Bad
    Responses into Good Labels'
- abstract: A majority of language technologies are tailored for a small number of
    high-resource languages, while relatively many low-resource languages are neglected.
    One such group, Creole languages, have long been marginalized in academic study,
    though their speakers could benefit from machine translation (MT). These languages
    are predominantly used in much of Latin America, Africa and the Caribbean. We
    present the largest cumulative dataset to date for Creole language MT, including
    14.5M unique Creole sentences with parallel translations---11.6M of which we release
    publicly, and the largest bitexts gathered to date for 41 languages---the first
    ever for 21. In addition, we provide MT models supporting all 41 Creole languages
    in 172 translation directions. Given our diverse dataset, we produce a model for
    Creole language MT exposed to more genre diversity then ever before, which outperforms
    a genre-specific Creole MT model on its own benchmark for 23 of 34 translation
    directions.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: 'Special Theme: Languages of Latin America'
  authors:
  - emails: '****@jhu.edu'
    first_name: Nathaniel
    google_scholar_id: https://scholar.google.com/citations?user=gZRDGQ4AAAAJ&hl=en
    homepage: https://n8rrobinson.wixsite.com/mysite
    institution: Department of Computer Science, Whiting School of Engineering
    last_name: Robinson
    middle_name: Romney
    name: Nathaniel Romney Robinson
    username: ~Nathaniel_Romney_Robinson1
  - dblp_id: https://dblp.org/pid/127/0168
    emails: '****@gmail.com'
    first_name: Raj
    google_scholar_id: https://scholar.google.co.jp/citations?user=x91u618AAAAJ&hl=en
    institution: National Institute of Information and Communications Technology (NICT),
      National Institute of Advanced Industrial Science and Technology
    last_name: Dabre
    name: Raj Dabre
    semantic_scholar_id: https://www.semanticscholar.org/author/Raj-Dabre/3209719
    username: ~Raj_Dabre1
  - dblp_id: https://dblp.org/pid/334/6669
    emails: '****@gmail.com'
    first_name: Ammon
    last_name: Shurtz
    middle_name: C
    name: Ammon C Shurtz
    username: ~Ammon_C_Shurtz1
  - emails: '****@gmail.com'
    first_name: Rasul
    institution: INRIA
    last_name: Dent
    name: Rasul Dent
    username: ~Rasul_Dent1
  - emails: '****@gmail.com'
    first_name: Onenamiyi
    homepage: https://amiyilade.github.io/
    last_name: Onesi
    name: Onenamiyi Onesi
    username: ~Onenamiyi_Onesi1
  - emails: '****@inria.fr'
    first_name: Claire
    google_scholar_id: https://scholar.google.com/citations?user=p8Z2qLUAAAAJ&hl=fr
    last_name: Monroc
    middle_name: Bizon
    name: Claire Bizon Monroc
    username: ~Claire_Bizon_Monroc1
  - dblp_id: https://dblp.org/pid/219/5471
    emails: '****@gmail.com'
    first_name: "Lo\xEFc"
    homepage: https://loicgrobol.github.io/
    institution: "Universit\xE9 Paris Nanterre"
    last_name: Grobol
    name: "Lo\xEFc Grobol"
    orcid: https://orcid.org/0000-0002-4619-7836
    semantic_scholar_id: https://www.semanticscholar.org/author/Lo%C3%AFc-Grobol/41128847
    username: "~Lo\xEFc_Grobol1"
  - emails: '****@jhu.edu'
    first_name: Hasan
    last_name: Muhammad
    name: Hasan Muhammad
    orcid: https://orcid.org/0009-0000-5125-2564
    username: ~Hasan_Muhammad1
  - emails: '****@jh.edu'
    first_name: Ashi
    last_name: Garg
    name: Ashi Garg
    username: ~Ashi_Garg2
  - emails: '****@gmail.com'
    first_name: Naome
    last_name: Etori
    middle_name: A
    name: Naome A Etori
    username: ~Naome_A_Etori1
  - emails: '****@gmail.com'
    first_name: Vijay Murari
    homepage: https://imvijay23.github.io/
    last_name: Tiyyala
    name: Vijay Murari Tiyyala
    username: ~Vijay_Murari_Tiyyala1
  - emails: '****@gmail.com'
    first_name: Olanrewaju
    homepage: https://github.com/
    last_name: Samuel
    name: Olanrewaju Samuel
    username: ~Olanrewaju_Samuel1
  - emails: '****@byu.edu'
    first_name: Matthew
    last_name: Stutzman
    middle_name: Dean
    name: Matthew Dean Stutzman
    username: ~Matthew_Dean_Stutzman1
  - emails: '****@gmail.com'
    first_name: Bismarck
    google_scholar_id: https://scholar.google.com/citations?user=qILWGm4AAAAJ&hl=en
    institution: Department of Computer Science, Whiting School of Engineering
    last_name: Odoom
    middle_name: Bamfo
    name: Bismarck Bamfo Odoom
    username: ~Bismarck_Bamfo_Odoom1
  - emails: '****@jhu.edu'
    first_name: Sanjeev
    google_scholar_id: https://scholar.google.com/citations?user=K-BdgNwAAAAJ&hl=en
    homepage: https://www.clsp.jhu.edu/faculty/sanjeev-khudanpur/
    institution: Whiting School of Engineering
    last_name: Khudanpur
    name: Sanjeev Khudanpur
    username: ~Sanjeev_Khudanpur1
  - dblp_id: https://dblp.org/pid/80/5328
    emails: '****@byu.edu'
    first_name: Stephen
    google_scholar_id: https://scholar.google.com/citations?user=3EkST7EAAAAJ&hl=en
    institution: Brigham Young University
    last_name: Richardson
    middle_name: D.
    name: Stephen D. Richardson
    semantic_scholar_id: https://www.semanticscholar.org/author/Stephen-D.-Richardson/3278929
    username: ~Stephen_D._Richardson1
  - dblp_id: https://dblp.org/pid/143/9465
    emails: '****@jhu.edu'
    first_name: Kenton
    google_scholar_id: https://scholar.google.com/citations?user=JuP-xF8AAAAJ&hl=en&oi=ao
    homepage: http://www.kentonmurray.com
    institution: Johns Hopkins University
    last_name: Murray
    name: Kenton Murray
    orcid: https://orcid.org/0000-0002-5628-1003
    semantic_scholar_id: https://www.semanticscholar.org/author/Kenton-Murray/38730896
    username: ~Kenton_Murray1
  decision: toMainConference
  end_page: 3320
  file: 289.pdf
  id: 289
  num_pages: 28
  openreview_id: RBUXfMZ8ge
  pdf_file: a89be49f69d529bcd1bb9adddc3031e0c3061247.pdf
  start_page: 3293
  title: "Krey\xF2l-MT: Building MT for Latin American, Caribbean and Colonial African\
    \ Creole Languages"
- abstract: We investigate security concerns of the emergent instruction tuning paradigm,
    that models are trained on crowdsourced datasets with task instructions to achieve
    superior performance. Our studies demonstrate that an attacker can inject backdoors
    by issuing very few malicious instructions (~1000 tokens) and control model behavior
    through data poisoning, without even the need to modify data instances or labels
    themselves. Through such instruction attacks, the attacker can achieve over 90%
    attack success rate across four commonly used NLP datasets. As an empirical study
    on instruction attacks, we systematically evaluated unique perspectives of instruction
    attacks, such as poison transfer where poisoned models can transfer to 15 diverse
    generative datasets in a zero-shot manner; instruction transfer where attackers
    can directly apply poisoned instruction on many other datasets; and poison resistance
    to continual finetuning. Lastly, we show that RLHF and clean demonstrations might
    mitigate such backdoors to some degree. These findings highlight the need for
    more robust defenses against poisoning attacks in instruction-tuning models and
    underscore the importance of ensuring data quality in instruction crowdsourcing.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Ethics, Bias, and Fairness
  authors:
  - emails: '****@g.harvard.edu'
    first_name: Jiashu
    google_scholar_id: https://scholar.google.com/citations?user=0uYehJsAAAAJ
    homepage: https://cnut1648.github.io/
    institution: University of Southern California
    last_name: Xu
    name: Jiashu Xu
    semantic_scholar_id: https://www.semanticscholar.org/author/Jiashu-Xu/2110519123
    username: ~Jiashu_Xu1
  - dblp_id: https://dblp.org/pid/232/6526
    emails: '****@ucla.edu'
    first_name: Mingyu
    google_scholar_id: https://scholar.google.com/citations?user=6tXXg6AAAAAJ
    homepage: https://derek.ma
    institution: University of California, Los Angeles
    last_name: Ma
    middle_name: Derek
    name: Mingyu Derek Ma
    semantic_scholar_id: https://www.semanticscholar.org/author/Mingyu-Derek-Ma/144592155
    username: ~Mingyu_Derek_Ma1
  - dblp_id: https://dblp.org/pid/52/3194-60
    emails: '****@gmail.com'
    first_name: Fei
    google_scholar_id: https://scholar.google.com/citations?user=N1O2KT8AAAAJ
    homepage: https://feiwang96.github.io/
    institution: University of Southern California
    last_name: Wang
    name: Fei Wang
    semantic_scholar_id: https://www.semanticscholar.org/author/F.-Wang/47939052
    username: ~Fei_Wang12
  - dblp_id: https://dblp.org/pid/150/3317
    emails: '****@umich.edu'
    first_name: Chaowei
    google_scholar_id: https://scholar.google.com/citations?user=Juoqtj8AAAAJ&hl=en
    homepage: https://xiaocw11.github.io/
    institution: University of Wisconsin - Madison and NVIDIA
    last_name: Xiao
    name: Chaowei Xiao
    username: ~Chaowei_Xiao2
  - dblp_id: https://dblp.org/pid/173/2608
    emails: '****@ucdavis.edu'
    first_name: Muhao
    google_scholar_id: https://scholar.google.com/citations?user=k79yEZkAAAAJ&hl=en
    homepage: https://muhaochen.github.io/
    institution: University of California, Davis and University of Southern California
    last_name: Chen
    name: Muhao Chen
    orcid: https://orcid.org/0000-0003-0118-3147
    semantic_scholar_id: https://www.semanticscholar.org/author/Muhao-Chen/1998918
    username: ~Muhao_Chen1
  decision: toMainConference
  end_page: 3336
  file: 291.pdf
  id: 291
  num_pages: 16
  openreview_id: txzed0li56
  pdf_file: 4ebd04f703c0dcd1a02778b74f8bfe914b7feaf0.pdf
  start_page: 3321
  title: 'Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning
    for Large Language Models'
- abstract: 'Empathy requires perspective-taking: empathetic responses require a person
    to reason about what another has experienced and communicate that understanding
    in language. However, most NLP approaches to empathy do not explicitly model this
    alignment process. Here, we introduce a new approach to recognizing alignment
    in empathetic speech, grounded in Appraisal Theory. We introduce a new dataset
    of over 9.2K span-level annotations of different types of appraisals of a person''s
    experience and over 3K empathetic alignments between a speaker''s and observer''s
    speech. Through computational experiments, we show that these appraisals and alignments
    can be accurately recognized. In experiments in over 9.2M Reddit conversations,
    we find that appraisals capture meaningful groupings of behavior but that most
    responses have minimal alignment. However, we find that mental health professionals
    engage with substantially more empathetic alignment.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Computational Social Science and Cultural Analytics
  authors:
  - emails: '****@uchicago.edu'
    first_name: Jiamin
    last_name: Yang
    name: Jiamin Yang
    username: ~Jiamin_Yang1
  - dblp_id: https://dblp.uni-trier.de/pid/48/4613
    emails: '****@umich.edu'
    first_name: David
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=sGFFr5kAAAAJ
    homepage: http://jurgens.people.si.umich.edu
    institution: University of Michigan - Ann Arbor
    last_name: Jurgens
    name: David Jurgens
    orcid: https://orcid.org/0000-0002-2135-9878
    semantic_scholar_id: https://www.semanticscholar.org/author/David-Jurgens/3046220
    username: ~David_Jurgens1
  decision: toMainConference
  end_page: 3358
  file: 292.pdf
  id: 292
  num_pages: 22
  openreview_id: cQuF3ZL5q1
  pdf_file: 02af22d6eb225b97f7bdde5793af6f5ed33ea984.pdf
  start_page: 3337
  title: Modeling Empathetic Alignment in Conversation
- abstract: Current end-to-end coreference resolution models combine detection of
    singleton mentions and antecedent linking into a single step. In contrast, singleton
    detection was often treated as a separate step in the pre-neural era. In this
    work, we show that separately parameterizing these two sub-tasks also benefits
    end-to-end neural coreference systems. Specifically, we add a singleton detector
    to the coarse-to-fine (C2F) coreference model, and design an anaphoricity-aware
    span embedding and singleton detection loss. Our method significantly improves
    model performance on OntoNotes and four additional datasets.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Discourse and Pragmatics
  authors:
  - emails: '****@mail.mcgill.ca'
    first_name: Xiyuan
    last_name: Zou
    name: Xiyuan Zou
    username: ~Xiyuan_Zou1
  - emails: '****@mail.mcgill.ca'
    first_name: Yiran
    last_name: Li
    name: Yiran Li
    username: ~Yiran_Li4
  - dblp_id: https://dblp.org/pid/254/1274
    emails: '****@mail.mcgill.ca'
    first_name: Ian
    google_scholar_id: https://scholar.google.com/citations?user=gQxa3i8AAAAJ
    homepage: https://ianporada.github.io/
    institution: McGill University
    last_name: Porada
    name: Ian Porada
    orcid: https://orcid.org/0000-0002-2000-2869
    semantic_scholar_id: https://www.semanticscholar.org/author/Ian-Porada/108508643
    username: ~Ian_Porada1
  - dblp_id: https://dblp.org/pid/00/9012
    emails: '****@mcgill.ca'
    first_name: Jackie
    google_scholar_id: https://scholar.google.com.tw/citations?user=Um-wmYQAAAAJ
    homepage: http://cs.mcgill.ca/~jcheung/
    institution: McGill University, McGill University, Mila Research Institute and
      Microsoft
    last_name: Cheung
    middle_name: CK
    name: Jackie CK Cheung
    semantic_scholar_id: https://www.semanticscholar.org/author/J.-Cheung/3159752
    username: ~Jackie_CK_Cheung1
  decision: toMainConference
  end_page: 3366
  file: 293.pdf
  id: 293
  num_pages: 8
  openreview_id: woQ8KpNOXj
  pdf_file: a24845e80f7834853352bd77d3ad3fe524d39944.pdf
  start_page: 3359
  title: Separately Parameterizing Singleton Detection Improves End-to-end Neural
    Coreference Resolution
- abstract: We present the first comprehensive survey of Native Language Identification
    (NLI) applied to texts. NLI is the task of automatically identifying an author's
    native language (L1) based on their second language (L2) production. NLI is an
    important task with practical applications in second language teaching and NLP.
    The task has been widely studied for both text and speech, particularly for L2
    English due to the availability of suitable corpora. Speech-based NLI relies heavily
    on accent modeled by pronunciation patterns and prosodic cues while text-based
    NLI relies primarily on modeling spelling errors and grammatical patterns that
    reveal properties of an individuals' L1 influencing L2 production. We survey over
    one hundred papers on the topic including the papers associated with the NLI and
    INLI shared tasks. We describe several text representations and computational
    techniques used in text-based NLI. Finally, we present a comprehensive account
    of publicly available datasets used for the task thus far.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - emails: '****@gmu.edu'
    first_name: Dhiman
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=9HNJ_NcAAAAJ
    institution: George Mason University
    last_name: Goswami
    name: Dhiman Goswami
    username: ~Dhiman_Goswami1
  - emails: '****@gmu.edu'
    first_name: Sharanya
    homepage: https://sharanya-t.github.io
    last_name: Thilagan
    name: Sharanya Thilagan
    username: ~Sharanya_Thilagan1
  - emails: '****@gmu.edu'
    first_name: Kai
    google_scholar_id: https://scholar.google.com/citations?user=fUSUaFUAAAAJ&hl=en
    last_name: North
    name: Kai North
    username: ~Kai_North1
  - dblp_id: https://dblp.org/pid/148/4567
    emails: '****@mq.edu.au'
    first_name: Shervin
    institution: Amazon
    last_name: Malmasi
    name: Shervin Malmasi
    semantic_scholar_id: https://www.semanticscholar.org/author/S.-Malmasi/2854981
    username: ~Shervin_Malmasi1
  - dblp_id: https://dblp.org/pid/47/7983
    emails: '****@gmu.edu'
    first_name: Marcos
    google_scholar_id: https://scholar.google.com.au/citations?user=vAx7VsoAAAAJ
    homepage: https://mzampieri.com/
    institution: George Mason University
    last_name: Zampieri
    name: Marcos Zampieri
    username: ~Marcos_Zampieri1
  decision: toMainConference
  end_page: 3378
  file: 294.pdf
  id: 294
  num_pages: 12
  openreview_id: zMbcMJvZVK
  pdf_file: 3e29ba152bd180d7fceeed839588923ca1d66dd8.pdf
  start_page: 3367
  title: 'Native Language Identification in Texts: A Survey'
- abstract: Various parameter-efficient fine-tuning  (PEFT) techniques have been proposed
    to enable computationally efficient fine-tuning while maintaining model performance.
    However, existing PEFT methods are still limited by the growing number of trainable
    parameters with the rapid deployment of Large Language Models (LLMs). To address
    this challenge, we present LoRETTA, an ultra-parameter-efficient framework that
    significantly reduces trainable parameters through tensor-train decomposition.
    Specifically, we propose two methods, named LoRETTA_adp and LoRETTA_rep. The former
    employs tensorized adapters, offering a high-performance yet lightweight approach
    for the fine-tuning of LLMs. The latter emphasizes fine-tuning via weight reparameterization
    with a set of small tensor factors. LoRETTA achieves comparable or better performance
    than most widely used PEFT methods with up to $100\times$ fewer parameters on
    the LLaMA-2-7B models. Furthermore, empirical results demonstrate that the proposed
    methods exhibit remarkable anti-overfitting capability, effectively improve training
    efficiency, and enjoy better multi-task learning performance. Plug-and-play loretta
    library built upon the Huggingface framework and PEFT library are provided.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Efficient/Low-Resource Methods for NLP
  authors:
  - emails: '****@cs.ucsb.edu'
    first_name: Yifan
    google_scholar_id: https://scholar.google.com/citations?user=PX2IQxsAAAAJ&hl=en&oi=ao
    institution: University of California, Santa Barbara
    last_name: Yang
    name: Yifan Yang
    username: ~Yifan_Yang12
  - emails: '****@eee.hku.hk'
    first_name: Jiajun
    google_scholar_id: https://scholar.google.com/citations?hl=zh-CN&user=4KQ6SKUAAAAJ
    last_name: Zhou
    name: Jiajun Zhou
    orcid: https://orcid.org/0000-0001-6328-4256
    username: ~Jiajun_Zhou3
  - dblp_id: https://dblp.org/pid/88/3656
    emails: '****@eee.hku.hk'
    first_name: Ngai
    google_scholar_id: https://scholar.google.com/citations?user=PM_uMYIAAAAJ&hl=en
    homepage: https://www.eee.hku.hk/~nwong/
    institution: The University of Hong Kong
    last_name: Wong
    name: Ngai Wong
    orcid: https://orcid.org/0000-0002-3026-0108
    username: ~Ngai_Wong1
  - dblp_id: https://dblp.org/pid/181/2621-5
    emails: '****@ece.ucsb.edu'
    first_name: Zheng
    google_scholar_id: https://scholar.google.com/citations?user=qeahx5QAAAAJ&hl=en
    homepage: https://web.ece.ucsb.edu/~zhengzhang/
    institution: UC Santa Barbara
    last_name: Zhang
    name: Zheng Zhang
    username: ~Zheng_Zhang2
  decision: toMainConference
  end_page: 3394
  file: 295.pdf
  id: 295
  num_pages: 16
  openreview_id: AEBrf1I6Hu
  pdf_file: 351c625a39e902632a76ed08c96719e81ba07001.pdf
  start_page: 3379
  title: 'LoRETTA: Low-Rank Economic Tensor-Train Adaptation for Ultra-Low-Parameter
    Fine-Tuning of Large Language Models'
- abstract: 'When connecting objects and their language referents in an embodied 3D
    environment, it is important to note that: (1) an object can be better characterized
    by leveraging comparative information between itself and other objects, and (2)
    an object''s appearance can vary with camera position. As such, we present the
    Multi-view Approach to Grounding in Context (MAGiC) model, which selects an object
    referent based on language that distinguishes between two similar objects. By
    pragmatically reasoning over both objects and across multiple views of those objects,
    MAGiC improves over the state-of-the-art model on the SNARE object reference task
    with a relative error reduction of 12.9% (representing an absolute improvement
    of 2.7%). Ablation studies show that reasoning jointly over object referent candidates
    and multiple views of each object both contribute to improved accuracy. Code:
    https://github.com/rcorona/magic_snare/'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multimodality and Language Grounding to Vision, Robotics and Beyond
  authors:
  - emails: '****@berkeley.edu'
    first_name: Chancharik
    last_name: Mitra
    name: Chancharik Mitra
    orcid: https://orcid.org/0009-0008-9826-7534
    username: ~Chancharik_Mitra1
  - dblp_id: https://dblp.org/pid/294/1347.html
    emails: '****@usc.edu'
    first_name: Abrar
    google_scholar_id: https://scholar.google.com/citations?user=c6E-5tcAAAAJ&hl=en
    homepage: http://abraranwar.github.io/
    institution: University of Southern California
    last_name: Anwar
    name: Abrar Anwar
    semantic_scholar_id: https://www.semanticscholar.org/author/Abrar-Anwar/2266391775
    username: ~Abrar_Anwar1
  - dblp_id: https://dblp.org/pid/212/0412
    emails: '****@berkeley.edu'
    first_name: Rodolfo
    google_scholar_id: https://scholar.google.com/citations?user=J2Z-ChoAAAAJ&hl=en
    homepage: https://rcorona.github.io/
    last_name: Corona
    name: Rodolfo Corona
    username: ~Rodolfo_Corona1
  - dblp_id: https://dblp.org/pid/22/1139
    emails: '****@cs.berkeley.edu'
    first_name: Dan
    homepage: http://people.eecs.berkeley.edu/~klein/
    institution: University of California, Berkeley
    last_name: Klein
    name: Dan Klein
    username: ~Dan_Klein1
  - dblp_id: https://dblp.org/pid/d/TrevorDarrell
    emails: '****@gmail.com'
    first_name: Trevor
    google_scholar_id: https://scholar.google.com.tw/citations?user=bh-uRFMAAAAJ
    homepage: https://people.eecs.berkeley.edu/~trevor/
    institution: Electrical Engineering & Computer Science Department
    last_name: Darrell
    name: Trevor Darrell
    username: ~Trevor_Darrell2
  - dblp_id: https://dblp.org/pid/130/2863
    emails: '****@gmail.com'
    first_name: Jesse
    google_scholar_id: https://scholar.google.com/citations?user=8BeTDr0AAAAJ&hl=en
    homepage: https://jessethomason.com/
    institution: University of Southern California and Amazon
    last_name: Thomason
    name: Jesse Thomason
    orcid: https://orcid.org/0000-0001-9199-0633
    semantic_scholar_id: https://www.semanticscholar.org/author/Jesse-Thomason/2665873
    username: ~Jesse_Thomason1
  decision: toMainConference
  end_page: 3407
  file: 296.pdf
  id: 296
  num_pages: 13
  openreview_id: 1bKswU7xiP
  pdf_file: b1971a58acd49d2fa4d615c11a9bdababa9b3b92.pdf
  start_page: 3395
  title: Which One? Leveraging Context Between Objects and Multiple Views for Language
    Grounding
- abstract: The concept of localization in LLMs is often mentioned in prior work;
    however, methods for localization have never been systematically and directly
    evaluated. We propose two complementary benchmarks that evaluate the ability of
    localization methods to pinpoint LLM components responsible for memorized data.
    In our INJ benchmark, we actively inject a piece of new information into a small
    subset of LLM weights, enabling us to directly evaluate whether localization methods
    can identify these "ground truth'' weights. In our DEL benchmark, we evaluate
    localization by measuring how much dropping out identified neurons deletes a memorized
    pretrained sequence. Despite their different perspectives, our two benchmarks
    yield consistent rankings of five localization methods. Methods adapted from network
    pruning perform well on both benchmarks, and all evaluated methods show promising
    localization ability. On the other hand, even successful methods identify neurons
    that are not specific to a single memorized sequence.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - dblp_id: https://dblp.org/pid/176/1912
    emails: '****@usc.edu'
    first_name: Ting-Yun
    google_scholar_id: https://scholar.google.com/citations?hl=zh-TW&view_op=list_works&gmla=AJsN-F5qzq9t87iPsRzez_lyKecH2DsQ4382d1mXcYDqfqo-6dJ5daTQsvncVxVAN3MquAmt2IRx-QKIqkqECctO77x3qIUoojtYMFy-cmrzcQqAhGIOwWqM31ZAsXabPFQbPIrEJkc7Ug2fF29yrue7d9QIzauwQA&user=grJ4plQAAAAJ
    homepage: https://terarachang.github.io/
    institution: University of Southern California
    last_name: Chang
    name: Ting-Yun Chang
    semantic_scholar_id: https://www.semanticscholar.org/author/Ting-Yun-Chang/3386996
    username: ~Ting-Yun_Chang1
  - dblp_id: https://dblp.org/pid/130/2863
    emails: '****@gmail.com'
    first_name: Jesse
    google_scholar_id: https://scholar.google.com/citations?user=8BeTDr0AAAAJ&hl=en
    homepage: https://jessethomason.com/
    institution: University of Southern California and Amazon
    last_name: Thomason
    name: Jesse Thomason
    orcid: https://orcid.org/0000-0001-9199-0633
    semantic_scholar_id: https://www.semanticscholar.org/author/Jesse-Thomason/2665873
    username: ~Jesse_Thomason1
  - dblp_id: https://dblp.org/pid/182/2556
    emails: '****@usc.edu'
    first_name: Robin
    google_scholar_id: https://scholar.google.com/citations?user=ajZ-_O0AAAAJ&hl=en
    homepage: https://robinjia.github.io/
    institution: University of Southern California
    last_name: Jia
    name: Robin Jia
    semantic_scholar_id: https://www.semanticscholar.org/author/Robin-Jia/3422908
    username: ~Robin_Jia1
  decision: toMainConference
  end_page: 3429
  file: 297.pdf
  id: 297
  num_pages: 22
  openreview_id: YINXJcTb3D
  pdf_file: add0e3a2c66d416ce830447509ce866678fcd1b4.pdf
  start_page: 3408
  title: Do Localization Methods Actually Localize Memorized Data in LLMs? A Tale
    of Two Benchmarks
- abstract: "Pre-trained language models (PLMs) have attracted enormous attention\
    \ over the past few years with their unparalleled performances. Meanwhile, the\
    \ soaring cost to train PLMs as well as their amazing generalizability have jointly\
    \ contributed to few-shot fine-tuning and prompting as the most popular training\
    \ paradigms for natural language processing (NLP) models. \nNevertheless, existing\
    \ studies have shown that these NLP models can be backdoored such that model behavior\
    \ is manipulated when trigger tokens are presented.\nIn this paper, we propose\
    \ PromptFix, a novel backdoor mitigation strategy for NLP models via adversarial\
    \ prompt-tuning in few-shot settings.\nUnlike existing NLP backdoor removal methods,\
    \ which rely on accurate trigger inversion and subsequent model fine-tuning, PromptFix\
    \ keeps the model parameters intact and only utilizes two extra sets of soft tokens\
    \ which approximate the trigger and counteract it respectively. \nThe use of soft\
    \ tokens and adversarial optimization eliminates the need to enumerate possible\
    \ backdoor configurations and enables an adaptive balance between trigger finding\
    \ and preservation of performance.\nExperiments with various backdoor attacks\
    \ validate the effectiveness of the proposed method and the performances when\
    \ domain shift is present further shows PromptFix's applicability to models pretrained\
    \ on unknown data source which is the common case in prompt tuning scenarios."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Learning for NLP
  authors:
  - emails: '****@psu.edu'
    first_name: Tianrong
    homepage: https://zhangtianrong.github.io/profile/#en
    institution: Pennsylvania State University
    last_name: Zhang
    name: Tianrong Zhang
    username: ~Tianrong_Zhang1
  - dblp_id: https://dblp.org/pid/224/9296.html
    emails: '****@psu.edu'
    first_name: Zhaohan
    google_scholar_id: https://scholar.google.com/citations?user=wQgnjMIAAAAJ&hl=zh-CN
    homepage: https://zhaohan-xi.github.io
    institution: Pennsylvania State University
    last_name: Xi
    name: Zhaohan Xi
    semantic_scholar_id: https://www.semanticscholar.org/author/Zhaohan-Xi/51195718
    username: ~Zhaohan_Xi1
  - dblp_id: https://dblp.org/pid/12/2633
    emails: '****@gmail.com'
    first_name: Ting
    google_scholar_id: https://scholar.google.com/citations?user=cwcBTegAAAAJ&hl=en
    homepage: https://alps-lab.github.io/
    institution: State University of New York at Stony Brook
    last_name: Wang
    name: Ting Wang
    username: ~Ting_Wang1
  - dblp_id: https://dblp.org/pid/19/3308
    emails: '****@psu.edu'
    first_name: Prasenjit
    google_scholar_id: https://scholar.google.com/citations?user=8PbgiPkAAAAJ&hl=en
    homepage: http://www.personal.psu.edu/pum10/
    institution: Pennsylvania State University
    last_name: Mitra
    name: Prasenjit Mitra
    semantic_scholar_id: https://www.semanticscholar.org/author/P.-Mitra/143930195
    username: ~Prasenjit_Mitra1
  - dblp_id: https://dblp.org/pid/67/5633
    emails: '****@psu.edu'
    first_name: Jinghui
    google_scholar_id: https://scholar.google.com/citations?user=mKia7Y4AAAAJ&hl=en
    homepage: https://jinghuichen.github.io/
    institution: Pennsylvania State University
    last_name: Chen
    name: Jinghui Chen
    username: ~Jinghui_Chen1
  decision: toMainConference
  end_page: 3443
  file: 298.pdf
  id: 298
  num_pages: 14
  openreview_id: DbR26f4bnU
  pdf_file: 9366ddfde74c1fb8235587f4d3d899fc9aff38b7.pdf
  start_page: 3430
  title: 'PromptFix: Few-shot Backdoor Removal via Adversarial Prompt Tuning'
- abstract: 'In many real natural language processing application scenarios, practitioners
    not only aim to maximize predictive performance but also seek faithful explanations
    for the model predictions. Rationales and importance distribution given by feature
    attribution methods (FAs) provide insights into how different parts of the input
    contribute to a prediction. Previous studies have explored how different factors
    affect faithfulness, mainly in the context of monolingual English models. On the
    other hand, the differences in FA faithfulness between multilingual and monolingual
    models have yet to be explored. Our extensive experiments, covering five languages
    and five popular FAs, show that FA faithfulness varies between multilingual and
    monolingual models. We find that the larger the multilingual model, the less faithful
    the FAs are compared to its counterpart monolingual models. Our further analysis
    shows that the faithfulness disparity is potentially driven by the differences
    between model tokenizers. Our code is available: https://github.com/casszhao/multilingual-faith.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - dblp_id: https://dblp.org/pid/151/8970.html
    emails: '****@sheffield.ac.uk'
    first_name: Zhixue
    google_scholar_id: https://scholar.google.com/citations?user=bwiMxxsAAAAJ&hl=en&oi=ao
    homepage: https://casszhao.github.io/researcher/
    institution: University of Sheffield, University of Sheffield
    last_name: Zhao
    name: Zhixue Zhao
    orcid: https://orcid.org/0000-0002-3060-269X
    username: ~Zhixue_Zhao2
  - dblp_id: https://dblp.org/pid/118/9116
    emails: '****@gmail.com'
    first_name: Nikolaos
    google_scholar_id: https://scholar.google.co.uk/citations?user=uxRWFhoAAAAJ
    institution: University of Sheffield, University of Sheffield and Amazon
    last_name: Aletras
    name: Nikolaos Aletras
    semantic_scholar_id: https://www.semanticscholar.org/author/Nikolaos-Aletras/3238627
    username: ~Nikolaos_Aletras1
  decision: toMainConference
  end_page: 3462
  file: 299.pdf
  id: 299
  num_pages: 19
  openreview_id: 0MFyfNOsdK
  pdf_file: 2645f6f4c0d9cb4e75522e1e48dc9a98df9718fa.pdf
  start_page: 3444
  title: Comparing Explanation Faithfulness between Multilingual and Monolingual Fine-tuned
    Language Models
- abstract: Pretraining data design is critically under-documented and often guided
    by empirically unsupported intuitions. We pretrain models on data curated (1)
    at different collection times, (2) with varying toxicity and quality filters,
    and (3) with different domain compositions. First, we find that temporal shift
    between evaluation data and pretraining data leads to performance degradation,
    which is not overcome by finetuning. Second, we measure the effect of quality
    and toxicity filters, showing a trade-off between performance on standard benchmarks
    and risk of toxic generations. We also find that the effects of different types
    of filtering are not predictable from text domain characteristics. Third, we empirically
    validate that heterogeneous data sources, like books and web, are beneficial and
    warrant greater prioritization. To date, these experiments constitute the single
    largest publicly documented empirical study of the effects of pretraining data.
    Spanning 28 unique 1.5 billion parameter models pretrained from scratch, these
    findings validate, quantify, and expose many undocumented intuitions about text
    pretraining, which ultimately support more informed data-centric decisions in
    model development.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - emails: '****@mit.edu'
    first_name: Shayne
    google_scholar_id: https://scholar.google.com/citations?user=ADd_YfkAAAAJ&hl=en
    homepage: https://www.shaynelongpre.com
    institution: Massachusetts Institute of Technology
    last_name: Longpre
    name: Shayne Longpre
    semantic_scholar_id: https://www.semanticscholar.org/author/S.-Longpre/29909347
    username: ~Shayne_Longpre1
  - dblp_id: https://dblp.org/pid/212/5939
    emails: '****@cs.cornell.edu'
    first_name: Gregory
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=b_NrF0QAAAAJ&view_op=list_works&authuser=1
    homepage: http://cs.cornell.edu/~gyauney
    institution: Cornell University
    last_name: Yauney
    name: Gregory Yauney
    semantic_scholar_id: https://www.semanticscholar.org/author/Gregory-Yauney/32918271
    username: ~Gregory_Yauney1
  - dblp_id: https://dblp.org/pid/190/7197
    emails: '****@gmail.com'
    first_name: Emily
    google_scholar_id: https://scholar.google.com/citations?user=J1hMgtAAAAAJ&hl=en&oi=ao
    last_name: Reif
    name: Emily Reif
    username: ~Emily_Reif2
  - dblp_id: https://dblp.org/pid/115/5082.html
    emails: '****@gmail.com'
    first_name: Katherine
    google_scholar_id: https://scholar.google.com/citations?user=bjdB4K8AAAAJ&hl=en
    homepage: https://katelee168.github.io/
    institution: Cornell University and Google
    last_name: Lee
    name: Katherine Lee
    semantic_scholar_id: https://www.semanticscholar.org/author/Katherine-Lee/3844009
    username: ~Katherine_Lee1
  - dblp_id: https://dblp.org/pid/95/6569
    emails: '****@google.com'
    first_name: Adam
    google_scholar_id: https://scholar.google.com/citations?user=U5UpKq8AAAAJ&hl=en
    institution: Google
    last_name: Roberts
    name: Adam Roberts
    username: ~Adam_Roberts1
  - dblp_id: ''
    emails: '****@gmail.com'
    first_name: Barret
    google_scholar_id: ''
    homepage: ''
    last_name: Zoph
    name: Barret Zoph
    username: ~Barret_Zoph1
  - dblp_id: https://dblp.org/pid/178/3277
    emails: '****@google.com'
    first_name: Denny
    google_scholar_id: https://scholar.google.com/citations?user=UwLsYw8AAAAJ&hl=en
    homepage: https://dennyzhou.github.io/
    institution: Google DeepMind
    last_name: Zhou
    name: Denny Zhou
    username: ~Denny_Zhou1
  - dblp_id: https://dblp.org/pid/02/11220.html
    emails: '****@gmail.com'
    first_name: Jason
    homepage: https://jasonwei20.github.io
    institution: OpenAI
    last_name: Wei
    name: Jason Wei
    username: ~Jason_Wei1
  - emails: '****@google.com'
    first_name: Kevin
    google_scholar_id: https://scholar.google.com/citations?user=uobBORcAAAAJ&hl=en
    homepage: https://research.google/people/KevinRobinson/
    institution: Google Research
    last_name: Robinson
    name: Kevin Robinson
    username: ~Kevin_Robinson1
  - dblp_id: https://dblp.org/pid/39/5487
    emails: '****@cornell.edu'
    first_name: David
    google_scholar_id: https://scholar.google.com/citations?user=uBFV6SUAAAAJ&hl=en
    homepage: https://mimno.infosci.cornell.edu/
    institution: Cornell University and Cornell University
    last_name: Mimno
    name: David Mimno
    username: ~David_Mimno1
  - emails: '****@cmu.edu'
    first_name: Daphne
    homepage: http://www.daphnei.com
    last_name: Ippolito
    name: Daphne Ippolito
    username: ~Daphne_Ippolito1
  decision: toMainConference
  end_page: 3494
  file: 301.pdf
  id: 301
  num_pages: 32
  openreview_id: gwxwUc3KnW
  pdf_file: da6c6a4788067e50d66ae12ab655aa734e03ffe7.pdf
  start_page: 3463
  title: 'A Pretrainer''s Guide to Training Data: Measuring the Effects of Data Age,
    Domain Coverage, Quality, \& Toxicity'
- abstract: Unraveling the intricate details of events in natural language necessitates
    a subtle understanding of temporal dynamics. Despite the adeptness of Large Language
    Models (LLMs) in discerning patterns and relationships from data, their inherent
    comprehension of temporal dynamics remains a formidable challenge. This research
    meticulously explores these intrinsic challenges within LLMs, with a specific
    emphasis on evaluating the performance of GPT-3.5 and GPT-4 models in the analysis
    of temporal data. Employing two distinct prompt types, namely Question Answering
    (QA) format and Textual Entailment (TE) format, our analysis probes into both
    implicit and explicit events. The findings underscore noteworthy trends, revealing
    disparities in the performance of GPT-3.5 and GPT-4. Notably, biases toward specific
    temporal relationships come to light, with GPT-3.5 demonstrating a preference
    for "AFTER'' in the QA format for both implicit and explicit events, while GPT-4
    leans towards "BEFORE''. Furthermore, a consistent pattern surfaces wherein GPT-3.5
    tends towards "TRUE'', and GPT-4 exhibits a preference for "FALSE'' in the TE
    format for both implicit and explicit events. This persistent discrepancy between
    GPT-3.5 and GPT-4 in handling temporal data highlights the intricate nature of
    inductive bias in LLMs, suggesting that the evolution of these models may not
    merely mitigate bias but may introduce new layers of complexity.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Ethics, Bias, and Fairness
  authors:
  - emails: '****@ur.rochester.edu'
    first_name: Sindhu
    last_name: Kishore
    name: Sindhu Kishore
    username: ~Sindhu_Kishore1
  - dblp_id: https://dblp.org/pid/190/7762-1.html
    emails: '****@rochester.edu'
    first_name: Hangfeng
    google_scholar_id: https://scholar.google.com/citations?user=BbpI6QoAAAAJ&hl=en
    homepage: https://hornhehhf.github.io
    institution: University of Rochester
    last_name: He
    name: Hangfeng He
    orcid: https://orcid.org/0000-0001-5136-1218
    username: ~Hangfeng_He3
  decision: toMainConference
  end_page: 3503
  file: 303.pdf
  id: 303
  num_pages: 9
  openreview_id: U2G7Y9TeWJ
  pdf_file: c4ef912e61b4d7a5d948d770806a29ba69e244db.pdf
  start_page: 3495
  title: Unveiling Divergent Inductive Biases of LLMs on Temporal Data
- abstract: The exorbitant cost of training Large language models (LLMs) from scratch
    makes it essential to fingerprint the models to protect intellectual property
    via ownership authentication and to ensure downstream users and developers comply
    with their license terms (eg restricting commercial use). In this study, we present
    a pilot study on LLM fingerprinting as a form of very lightweight instruction
    tuning. Model publisher specifies a confidential private key and implants it as
    an instruction backdoor that causes the LLM to generate specific text when the
    key is present. Results on 11 popularly-used LLMs showed that this approach is
    lightweight and does not affect the normal behavior of the model. It also prevents
    publisher overclaim, maintains robustness against fingerprint guessing and parameter-efficient
    training, and supports multi-stage fingerprinting akin to MIT License.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Ethics, Bias, and Fairness
  authors:
  - emails: '****@g.harvard.edu'
    first_name: Jiashu
    google_scholar_id: https://scholar.google.com/citations?user=0uYehJsAAAAJ
    homepage: https://cnut1648.github.io/
    institution: University of Southern California
    last_name: Xu
    name: Jiashu Xu
    semantic_scholar_id: https://www.semanticscholar.org/author/Jiashu-Xu/2110519123
    username: ~Jiashu_Xu1
  - dblp_id: https://dblp.org/pid/52/3194-60
    emails: '****@gmail.com'
    first_name: Fei
    google_scholar_id: https://scholar.google.com/citations?user=N1O2KT8AAAAJ
    homepage: https://feiwang96.github.io/
    institution: University of Southern California
    last_name: Wang
    name: Fei Wang
    semantic_scholar_id: https://www.semanticscholar.org/author/F.-Wang/47939052
    username: ~Fei_Wang12
  - dblp_id: https://dblp.org/pid/232/6526
    emails: '****@ucla.edu'
    first_name: Mingyu
    google_scholar_id: https://scholar.google.com/citations?user=6tXXg6AAAAAJ
    homepage: https://derek.ma
    institution: University of California, Los Angeles
    last_name: Ma
    middle_name: Derek
    name: Mingyu Derek Ma
    semantic_scholar_id: https://www.semanticscholar.org/author/Mingyu-Derek-Ma/144592155
    username: ~Mingyu_Derek_Ma1
  - dblp_id: https://dblp.org/pid/10/10453
    emails: '****@gmail.com'
    first_name: Pang Wei
    google_scholar_id: https://scholar.google.com/citations?user=Nn990CkAAAAJ&hl=en
    homepage: http://cs.stanford.edu/~pangwei
    institution: University of Washington
    last_name: Koh
    name: Pang Wei Koh
    username: ~Pang_Wei_Koh1
  - dblp_id: https://dblp.org/pid/150/3317
    emails: '****@umich.edu'
    first_name: Chaowei
    google_scholar_id: https://scholar.google.com/citations?user=Juoqtj8AAAAJ&hl=en
    homepage: https://xiaocw11.github.io/
    institution: University of Wisconsin - Madison and NVIDIA
    last_name: Xiao
    name: Chaowei Xiao
    username: ~Chaowei_Xiao2
  - dblp_id: https://dblp.org/pid/173/2608
    emails: '****@ucdavis.edu'
    first_name: Muhao
    google_scholar_id: https://scholar.google.com/citations?user=k79yEZkAAAAJ&hl=en
    homepage: https://muhaochen.github.io/
    institution: University of California, Davis and University of Southern California
    last_name: Chen
    name: Muhao Chen
    orcid: https://orcid.org/0000-0003-0118-3147
    semantic_scholar_id: https://www.semanticscholar.org/author/Muhao-Chen/1998918
    username: ~Muhao_Chen1
  decision: toMainConference
  end_page: 3533
  file: 305.pdf
  id: 305
  num_pages: 30
  openreview_id: IcrgzXxhzt
  pdf_file: 494cff8d0d50ea758a0bb2d46eff2bb661e096dd.pdf
  start_page: 3504
  title: Instructional Fingerprinting of Large Language Models
- abstract: 'Language usage is related to speaker age, gender, moral concerns, political
    ideology, and other attributes. Current state-of-the-art methods for predicting
    these attributes take a speaker''s utterances as input and provide a prediction
    per speaker attribute. Most of these approaches struggle to handle a large number
    of utterances per speaker. This difficulty is primarily due to the computational
    constraints of the models. Additionally, only a subset of speaker utterances may
    be relevant to specific attributes. In this paper, we formulate speaker attribute
    prediction as a Multiple Instance Learning (MIL) problem and propose RL-MIL, a
    novel approach based on Reinforcement Learning (RL) that effectively addresses
    both of these challenges. Our experiments demonstrate that our RL-based methodology
    consistently outperforms previous approaches across a range of related tasks:
    predicting speakers'' psychographics and demographics from social media posts,
    and political ideologies from transcribed speeches. We create synthetic datasets
    and investigate the behavior of RL-MIL systematically. Our results show the success
    of RL-MIL in improving speaker attribute prediction by learning to select relevant
    speaker utterances.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Computational Social Science and Cultural Analytics
  authors:
  - emails: '****@usc.edu'
    first_name: Alireza
    google_scholar_id: https://scholar.google.com/citations?user=Jt9en84AAAAJ&hl=en
    homepage: https://scholar.google.com/citations?user=Jt9en84AAAAJ&hl=en
    last_name: Salkhordeh Ziabari
    name: Alireza Salkhordeh Ziabari
    username: ~Alireza_Salkhordeh_Ziabari1
  - dblp_id: https://dblp.org/pid/277/5283
    emails: '****@usc.edu'
    first_name: Ali
    google_scholar_id: https://scholar.google.com/citations?user=ltnAaLkAAAAJ&hl=en
    homepage: https://ali-omrani.github.io/
    last_name: Omrani
    name: Ali Omrani
    semantic_scholar_id: https://www.semanticscholar.org/author/2070422394
    username: ~Ali_Omrani1
  - emails: '****@usc.edu'
    first_name: Parsa
    homepage: https://parsahejabi.com/
    institution: University of Southern California
    last_name: Hejabi
    name: Parsa Hejabi
    username: ~Parsa_Hejabi1
  - emails: '****@usc.edu'
    first_name: Preni
    google_scholar_id: https://scholar.google.com/citations?user=cP98nJIAAAAJ&hl=en
    last_name: Golazizian
    name: Preni Golazizian
    username: ~Preni_Golazizian1
  - emails: '****@gmail.com'
    first_name: Brendan
    google_scholar_id: https://scholar.google.com/citations?user=5SfbUTEAAAAJ&hl=en
    homepage: https://brendankennedy.org
    last_name: Kennedy
    name: Brendan Kennedy
    orcid: https://orcid.org/0000-0001-7252-7475
    username: ~Brendan_Kennedy2
  - emails: '****@usc.edu'
    first_name: Payam
    google_scholar_id: https://scholar.google.com/citations?user=-ifvBVEAAAAJ&hl
    homepage: https://www.piraylab.com/
    institution: University of Southern California
    last_name: Piray
    name: Payam Piray
    username: ~Payam_Piray1
  - emails: '****@usc.edu'
    first_name: Morteza
    google_scholar_id: https://scholar.google.com.tw/citations?user=my6AUyAAAAAJ
    homepage: http://morteza-dehghani.net/
    institution: University of Southern California
    last_name: Dehghani
    name: Morteza Dehghani
    username: ~Morteza_Dehghani1
  decision: toMainConference
  end_page: 3548
  file: 306.pdf
  id: 306
  num_pages: 15
  openreview_id: V5jUgxMVll
  pdf_file: c92bd6e180f4cc9d94a9de859a8cd9e8315da479.pdf
  start_page: 3534
  title: Reinforced Multiple Instance Selection for Speaker Attribute Prediction
- abstract: "Traditional language models operate autoregressively, i.e., they predict\
    \ one token at a time. Rapid explosion in model \nsizes has resulted in high inference\
    \ times. In this work, we propose DynaMo, a suite of multi-token prediction language\
    \ \nmodels that reduce net inference times. Our models *dynamically* predict \n\
    multiple tokens based on their confidence in the predicted joint probability distribution.\
    \ We propose a lightweight\ntechnique to train these models, leveraging the weights\
    \ of traditional autoregressive counterparts. Moreover, we propose novel \nways\
    \ to enhance the estimated joint probability to improve text generation quality,\
    \ namely co-occurrence weighted masking \nand adaptive thresholding. We also propose\
    \ systematic qualitative and quantitative methods to rigorously test the quality\
    \ of \ngenerated text for non-autoregressive generation. One of the models in\
    \ our suite, DynaMo-7.3B-T3, achieves same-quality \ngenerated text as the baseline\
    \ (Pythia-6.9B) while achieving 2.57$\\times$ speed-up with only 5.87\\% and 2.67\\\
    % parameter and training time \noverheads, respectively."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Efficient/Low-Resource Methods for NLP
  authors:
  - emails: '****@princeton.edu'
    first_name: Shikhar
    google_scholar_id: https://scholar.google.com/citations?user=qiFEpzIAAAAJ&hl=en
    homepage: https://stuli.me
    last_name: Tuli
    name: Shikhar Tuli
    username: ~Shikhar_Tuli1
  - emails: '****@gatech.edu'
    first_name: Chi-Heng
    google_scholar_id: https://scholar.google.com/citations?user=OqSt2wMAAAAJ&hl=en
    homepage: https://www.chihenglin.com/
    institution: Samsung Research America
    last_name: Lin
    name: Chi-Heng Lin
    username: ~Chi-Heng_Lin1
  - dblp_id: https://dblp.org/pid/172/1140
    emails: '****@gmail.com'
    first_name: Yen-Chang
    google_scholar_id: https://scholar.google.com/citations?user=7QWAiigAAAAJ&hl=en
    institution: Samsung Research America
    last_name: Hsu
    name: Yen-Chang Hsu
    username: ~Yen-Chang_Hsu1
  - emails: '****@princeton.edu'
    first_name: Niraj
    google_scholar_id: https://scholar.google.com.tw/citations?hl=en&user=R-z1R84AAAAJ
    homepage: https://www.princeton.edu/~jha/
    institution: Princeton University
    last_name: Jha
    name: Niraj Jha
    username: ~Niraj_Jha1
  - dblp_id: https://dblp.org/pid/30/383
    emails: '****@samsung.com'
    first_name: Yilin
    google_scholar_id: https://scholar.google.com/citations?user=9PSFMzAAAAAJ
    institution: Samsung Research America
    last_name: Shen
    name: Yilin Shen
    username: ~Yilin_Shen1
  - dblp_id: https://dblp.org/pid/55/2789
    emails: '****@samsung.com'
    first_name: Hongxia
    institution: Samsung Research America AI center
    last_name: Jin
    name: Hongxia Jin
    username: ~Hongxia_Jin1
  decision: toMainConference
  end_page: 3572
  file: 308.pdf
  id: 308
  num_pages: 24
  openreview_id: L2rOiZm3on
  pdf_file: 522dc95bd406b084aca6a0022a6dbaa55159b3f4.pdf
  start_page: 3549
  title: 'DynaMo: Accelerating Language Model Inference with Dynamic Multi-Token Sampling'
- abstract: "Augmenting a language model (LM) with $k$-nearest neighbors ($k$NN) retrieval\
    \ on its training data alone can decrease its perplexity, though the underlying\
    \ reasons for this remain elusive. In this work, we rule out one previously posited\
    \ possibility \u2014 the \u201Csoftmax bottleneck.\u201D We then create a new\
    \ dataset to evaluate LM generalization ability in the setting where training\
    \ data contains additional information that is not causally relevant. This task\
    \ is challenging even for GPT-3.5 Turbo. We show that, for both GPT-2 and Mistral\
    \ 7B, $k$NN retrieval augmentation consistently improves per formance in this\
    \ setting. Finally, to make $k$NN retrieval more accessible, we propose using\
    \ a\nmulti-layer perceptron model that maps datastore keys to values as a drop-in\
    \ replacement for traditional retrieval. This reduces storage costs\nby over 25x."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - dblp_id: https://dblp.org/pid/230/3609
    emails: '****@gmail.com'
    first_name: Ting-Rui
    google_scholar_id: https://scholar.google.com/citations?user=aIgoIxwAAAAJ&hl=en
    homepage: https://ctinray.github.io/
    institution: University of Southern California
    last_name: Chiang
    name: Ting-Rui Chiang
    semantic_scholar_id: https://www.semanticscholar.org/author/Ting-Rui-Chiang/153033862
    username: ~Ting-Rui_Chiang1
  - dblp_id: https://dblp.org/pid/165/9117-1
    emails: '****@usc.edu'
    first_name: Xinyan
    google_scholar_id: https://scholar.google.com/citations?user=PoZv5KkAAAAJ&hl=en
    homepage: https://velocitycavalry.github.io
    institution: University of Southern California
    last_name: Yu
    middle_name: Velocity
    name: Xinyan Velocity Yu
    semantic_scholar_id: https://www.semanticscholar.org/author/Xinyan-Velocity-Yu/2118211280
    username: ~Xinyan_Velocity_Yu1
  - dblp_id: https://dblp.org/pid/15/4759
    emails: '****@usc.edu'
    first_name: Joshua
    google_scholar_id: https://scholar.google.com/citations?user=dw01MZ0AAAAJ
    homepage: https://joshua-robinson.github.io
    institution: University of Southern California
    last_name: Robinson
    name: Joshua Robinson
    username: ~Joshua_Robinson3
  - emails: '****@usc.edu'
    first_name: Ollie
    google_scholar_id: https://scholar.google.com/citations?view_op=list_works&hl=en&user=4rriqlQAAAAJ
    homepage: https://ollieliu.com
    institution: University of Southern California
    last_name: Liu
    name: Ollie Liu
    semantic_scholar_id: https://www.semanticscholar.org/author/O.-Liu/2065919693
    username: ~Ollie_Liu1
  - emails: '****@gmail.com'
    first_name: Isabelle
    homepage: https://iglee.me/
    institution: University of Southern California
    last_name: Lee
    name: Isabelle Lee
    username: ~Isabelle_Lee1
  - dblp_id: https://dblp.org/pid/08/8178
    emails: '****@usc.edu'
    first_name: Dani
    institution: Google DeepMind and DeepMind
    last_name: Yogatama
    name: Dani Yogatama
    username: ~Dani_Yogatama2
  decision: toMainConference
  end_page: 3582
  file: 310.pdf
  id: 310
  num_pages: 10
  openreview_id: jzJyfMZVXH
  pdf_file: c15e825468565c04196309bbcfc777731b6ebc6f.pdf
  start_page: 3573
  title: On Retrieval Augmentation and the Limitations of Language Model Training
- abstract: Few-shot Knowledge Graph (KG) Relational Reasoning aims to predict unseen
    triplets (i.e., query triplets) for rare relations in KGs, given only several
    triplets of these relations as references (i.e., support triplets). This task
    has gained significant traction due to the widespread use of knowledge graphs
    in various natural language processing applications. Previous approaches have
    utilized meta-training methods and manually constructed meta-relation sets to
    tackle this task. Recent efforts have focused on edge-mask-based methods, which
    exploit the structure of the contextualized graphs of target triplets (i.e., a
    subgraph containing relevant triplets in the KG). However, existing edge-mask-based
    methods have limitations in extracting insufficient information from KG and are
    highly influenced by spurious information in KG. To overcome these challenges,
    we propose SAFER (Subgraph Adaptation for Few-shot Relational Reasoning), a novel
    approach that effectively adapts the information in contextualized graphs to various
    subgraphs generated from support and query triplets to perform the prediction.  Specifically,
    SAFER enables the extraction of more comprehensive information from support triplets
    while minimizing the impact of spurious information when predicting query triplets.
    Experimental results on three prevalent datasets demonstrate the superiority of
    our proposed framework SAFER.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Learning for NLP
  authors:
  - emails: '****@virginia.edu'
    first_name: Haochen
    homepage: https://github.com/HaochenLiu2000
    last_name: Liu
    name: Haochen Liu
    username: ~Haochen_Liu3
  - dblp_id: https://dblp.org/pid/62/3151
    emails: '****@virginia.edu'
    first_name: Song
    homepage: https://songw-sw.github.io/
    institution: University of Virginia, Charlottesville
    last_name: Wang
    name: Song Wang
    username: ~Song_Wang6
  - dblp_id: https://dblp.org/pid/65/4423-22
    emails: '****@gmail.com'
    first_name: Chen
    last_name: Chen
    name: Chen Chen
    username: ~Chen_Chen10
  - dblp_id: https://dblp.uni-trier.de/pers/hd/l/Li:Jundong
    emails: '****@virginia.edu'
    first_name: Jundong
    google_scholar_id: https://scholar.google.com/citations?user=uY6ek7sAAAAJ&hl=en
    homepage: http://www.ece.virginia.edu/~jl6qk/
    institution: University of Virginia
    last_name: Li
    name: Jundong Li
    username: ~Jundong_Li2
  decision: toMainConference
  end_page: 3593
  file: 311.pdf
  id: 311
  num_pages: 11
  openreview_id: jUGTtMB142
  pdf_file: fea050a0867e98ecc872f2f110f7e52602e7668e.pdf
  start_page: 3583
  title: Few-shot Knowledge Graph Relational Reasoning via Subgraph Adaptation
- abstract: 'In-context learning has emerged as a groundbreaking ability of Large
    Language Models (LLMs) and revolutionized various fields by providing a few task-relevant
    demonstrations in the prompt. However, trustworthy issues with LLM''s response,
    such as hallucination, have also been actively discussed. Existing works have
    been devoted to quantifying the uncertainty in LLM''s response, but they often
    overlook the complex nature of LLMs and the uniqueness of in-context learning.
    In this work, we delve into the predictive uncertainty of LLMs associated with
    in-context learning, highlighting that such uncertainties may stem from both the
    provided demonstrations (aleatoric uncertainty) and ambiguities tied to the model''s
    configurations (epistemic uncertainty). We propose a novel formulation and corresponding
    estimation method to quantify both types of uncertainties. The proposed method
    offers an unsupervised way to understand the prediction of in-context learning
    in a plug-and-play fashion. Extensive experiments are conducted to demonstrate
    the effectiveness of the decomposition. The code and data are available at: https://github.com/lingchen0331/UQ_ICL.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - dblp_id: https://dblp.org/pid/07/320
    emails: '****@emory.edu'
    first_name: Chen
    google_scholar_id: https://scholar.google.com/citations?user=275NKcEAAAAJ&hl=en
    homepage: https://lingchen0331.github.io/
    last_name: Ling
    name: Chen Ling
    username: ~Chen_Ling3
  - dblp_id: https://dblp.org/pid/221/5767
    emails: '****@nec-labs.com'
    first_name: Xujiang
    google_scholar_id: https://scholar.google.com/citations?user=k2-JcFAAAAAJ&hl=en
    homepage: https://zxj32.github.io/
    last_name: Zhao
    name: Xujiang Zhao
    username: ~Xujiang_Zhao1
  - emails: '****@microsoft.com'
    first_name: Xuchao
    homepage: https://xuczhang.github.io/
    institution: Microsoft
    last_name: Zhang
    name: Xuchao Zhang
    username: ~Xuchao_Zhang1
  - dblp_id: https://dblp.uni-trier.de/pers/hd/c/Cheng:Wei
    emails: '****@nec-labs.com'
    first_name: Wei
    google_scholar_id: https://scholar.google.com/citations?user=PRrGVmoAAAAJ&hl=en
    homepage: https://chengw07.github.io/
    institution: NEC-Labs
    last_name: Cheng
    name: Wei Cheng
    username: ~Wei_Cheng1
  - dblp_id: https://dblp.org/pid/62/8146
    emails: '****@nec-labs.com'
    first_name: Yanchi
    google_scholar_id: https://scholar.google.com/citations?user=faLmr-YAAAAJ&hl=en
    institution: NEC-Labs
    last_name: Liu
    name: Yanchi Liu
    username: ~Yanchi_Liu1
  - dblp_id: https://dblp.org/pid/211/5630
    emails: '****@cs.wisc.edu'
    first_name: Yiyou
    google_scholar_id: https://scholar.google.com/citations?user=IKqlQo4AAAAJ&hl=zh-CN
    homepage: https://sunyiyou.github.io/
    last_name: Sun
    name: Yiyou Sun
    semantic_scholar_id: https://www.semanticscholar.org/author/Yiyou-Sun/31454397
    username: ~Yiyou_Sun1
  - emails: '****@gmail.com'
    first_name: Mika
    homepage: https://www.instagram.com/2mika4/?hl=en
    institution: NEC
    last_name: Oishi
    name: Mika Oishi
    username: ~Mika_Oishi1
  - emails: '****@gmail.com'
    first_name: Takao
    homepage: https://www.researchgate.net/scientific-contributions/Takao-Osaki-2171202477
    institution: NEC
    last_name: Osaki
    name: Takao Osaki
    username: ~Takao_Osaki1
  - emails: '****@gmail.com'
    first_name: Katsushi
    homepage: https://ieeexplore.ieee.org/author/37088216732
    institution: NEC
    last_name: Matsuda
    name: Katsushi Matsuda
    username: ~Katsushi_Matsuda1
  - emails: '****@gmail.com'
    first_name: Jie
    last_name: Ji
    name: Jie Ji
    username: ~Jie_Ji2
  - dblp_id: https://dblp.org/pid/286/0892
    emails: '****@emory.edu'
    first_name: Guangji
    google_scholar_id: https://scholar.google.com/citations?user=gBMbU28AAAAJ&hl=en&authuser=1
    homepage: https://baithebest.github.io/
    institution: Emory University
    last_name: Bai
    name: Guangji Bai
    username: ~Guangji_Bai1
  - dblp_id: https://dblp.org/pid/63/5422-2
    emails: '****@emory.edu'
    first_name: Liang
    google_scholar_id: https://scholar.google.com/citations?user=qnvyqtwAAAAJ&hl=en
    homepage: https://cs.emory.edu/~lzhao41/
    institution: Emory University
    last_name: Zhao
    name: Liang Zhao
    username: ~Liang_Zhao6
  - dblp_id: https://dblp.org/pid/08/57
    emails: '****@nec-labs.com'
    first_name: Haifeng
    google_scholar_id: https://scholar.google.com/citations?user=QzakB68AAAAJ&hl=en
    homepage: https://haifengchen.gitlab.io/intro/
    institution: NEC-Labs
    last_name: Chen
    name: Haifeng Chen
    username: ~Haifeng_Chen1
  decision: toMainConference
  end_page: 3607
  file: 314.pdf
  id: 314
  num_pages: 14
  openreview_id: Oq1b1DnUOP
  pdf_file: ba7e7d3bbfea80d7f9f6db0343bd51f2e02156c6.pdf
  start_page: 3594
  title: Uncertainty Quantification for In-Context Learning of Large Language Models
- abstract: Existing open-source helpfulness preference datasets do not specify what
    makes some responses more helpful and others less so. Models trained on these
    datasets can incidentally learn to model dataset artifacts (e.g. preferring longer
    but unhelpful responses only due to their length). To alleviate this problem,
    we collect HelpSteer, a multi-attribute helpfulness dataset annotated for the
    various aspects that make responses helpful. Specifically, our 37k-sample dataset
    has annotations for correctness, coherence, complexity, and verbosity in addition
    to overall helpfulness of responses. Training Llama 2 70B using the HelpSteer
    dataset with SteerLM technique produces a model that scores 7.54 on MT Bench,
    which is currently the highest score for open models that do not require training
    data from more powerful models (e.g. GPT-4). We release this dataset with CC-BY-4.0
    license at https://huggingface.co/datasets/nvidia/HelpSteer
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - dblp_id: https://dblp.org/pid/53/10643
    emails: '****@nvidia.com'
    first_name: Zhilin
    google_scholar_id: https://scholar.google.com/citations?user=OmMgSQsAAAAJ&hl=en
    institution: NVIDIA
    last_name: Wang
    name: Zhilin Wang
    semantic_scholar_id: https://www.semanticscholar.org/author/Zhilin-Wang/2257129906
    username: ~Zhilin_Wang2
  - emails: '****@nvidia.com'
    first_name: Yi
    last_name: Dong
    name: Yi Dong
    username: ~Yi_Dong4
  - dblp_id: https://dblp.uni-trier.de/pid/262/3536.html
    emails: '****@nvidia.com'
    first_name: Jiaqi
    institution: NVIDIA
    last_name: Zeng
    name: Jiaqi Zeng
    semantic_scholar_id: https://www.semanticscholar.org/author/Jiaqi-Zeng/2072984384
    username: ~Jiaqi_Zeng1
  - emails: '****@nvidia.com'
    first_name: Virginia
    homepage: https://www.linkedin.com/in/virginia-adams-a3b99abb
    last_name: Adams
    name: Virginia Adams
    username: ~Virginia_Adams1
  - dblp_id: https://dblp.org/pid/262/3290
    emails: '****@gmail.com'
    first_name: Makesh Narsimhan
    google_scholar_id: https://scholar.google.com/citations?user=1BGJ0YMAAAAJ&hl=en
    institution: NVIDIA
    last_name: Sreedhar
    name: Makesh Narsimhan Sreedhar
    username: ~Makesh_Narsimhan_Sreedhar1
  - emails: '****@gmail.com'
    first_name: Daniel
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=E3lWKAQAAAAJ
    institution: NVIDIA
    last_name: Egert
    name: Daniel Egert
    username: ~Daniel_Egert1
  - dblp_id: http://dblp.uni-trier.de/pers/hd/d/Delalleau:Olivier
    emails: '****@gmail.com'
    first_name: Olivier
    google_scholar_id: https://scholar.google.ca/citations?user=zqLpO2QAAAAJ&hl=en
    homepage: http://www.iro.umontreal.ca/~delallea
    institution: NVIDIA
    last_name: Delalleau
    name: Olivier Delalleau
    orcid: https://orcid.org/0000-0002-0610-7226
    username: ~Olivier_Delalleau1
  - emails: '****@nvidia.com'
    first_name: Jane
    institution: NVIDIA
    last_name: Scowcroft
    middle_name: Polak
    name: Jane Polak Scowcroft
    username: ~Jane_Polak_Scowcroft1
  - emails: '****@nvidia.com'
    first_name: Neel
    google_scholar_id: https://scholar.google.com/citations?user=eSgXTkkAAAAJ&hl=en
    last_name: Kant
    name: Neel Kant
    username: ~Neel_Kant1
  - dblp_id: https://dblp.org/pid/299/7780
    emails: '****@gmail.com'
    first_name: Aidan
    google_scholar_id: https://scholar.google.com/citations?user=vVzI7uwAAAAJ
    homepage: https://aidanswope.com/
    institution: NVIDIA
    last_name: Swope
    middle_name: M
    name: Aidan M Swope
    semantic_scholar_id: https://www.semanticscholar.org/author/Aidan-M.-Swope/1572529646
    username: ~Aidan_M_Swope1
  - emails: '****@nvidia.com'
    first_name: Oleksii
    google_scholar_id: https://scholar.google.com/citations?user=qmmIGnwAAAAJ&hl
    homepage: http://www.kuchaev.com
    institution: NVIDIA
    last_name: Kuchaiev
    name: Oleksii Kuchaiev
    username: ~Oleksii_Kuchaiev1
  decision: toMainConference
  end_page: 3621
  file: 316.pdf
  id: 316
  num_pages: 14
  openreview_id: TIrTc0XZbm
  pdf_file: 828f9505656989adc1bfd74e4f39b5abaf57daf6.pdf
  start_page: 3608
  title: 'HelpSteer: Multi-attribute Helpfulness Dataset for SteerLM'
- abstract: Recent research has shown that large language models (LLMs) can achieve
    remarkable translation performance through supervised fine-tuning (SFT) using
    only a small amount of parallel data. However, SFT simply instructs the model
    to imitate the reference translations at the token level, making it vulnerable
    to the noise present in the references. Hence, the assistance from SFT often reaches
    a plateau once the LLMs have achieved a certain level of translation capability,
    and further increasing the size of parallel data does not provide additional benefits.
    To overcome this plateau associated with imitation-based SFT, we propose a preference-based
    approach built upon the Plackett-Luce model. The objective is to steer LLMs towards
    a more nuanced understanding of translation preferences from a holistic view,
    while also being more resilient in the absence of gold translations. We further
    build a dataset named MAPLE to verify the effectiveness of our approach, which
    includes multiple translations of varying quality for each source sentence. Extensive
    experiments demonstrate the superiority of our approach in "breaking the plateau"
    across diverse LLMs and test settings. Our in-depth analysis underscores the pivotal
    role of diverse translations and accurate preference scores in the success of
    our approach.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Translation
  authors:
  - emails: '****@lsv.uni-saarland.de'
    first_name: Dawei
    google_scholar_id: https://scholar.google.com/citations?user=rkmZkd4AAAAJ&hl=de
    last_name: Zhu
    name: Dawei Zhu
    semantic_scholar_id: https://www.semanticscholar.org/author/D.-Zhu/47770349
    username: ~Dawei_Zhu1
  - emails: '****@gmail.com'
    first_name: Sony
    google_scholar_id: https://scholar.google.com/citations?user=MKTX-6IAAAAJ&hl=en&oi=ao
    institution: Amazon
    last_name: Trenous
    name: Sony Trenous
    username: ~Sony_Trenous1
  - dblp_id: https://dblp.org/pid/85/7634
    emails: '****@gmail.com'
    first_name: Xiaoyu
    institution: Amazon
    last_name: Shen
    name: Xiaoyu Shen
    semantic_scholar_id: https://www.semanticscholar.org/author/Xiaoyu-Shen/2562211
    username: ~Xiaoyu_Shen1
  - dblp_id: https://dblp.org/pid/00/1846
    emails: '****@lsv.uni-saarland.de'
    first_name: Dietrich
    google_scholar_id: https://scholar.google.de/citations?user=_HtGYmoAAAAJ&hl=de&oi=ao
    homepage: https://www.lsv.uni-saarland.de/
    institution: Saarland University
    last_name: Klakow
    name: Dietrich Klakow
    username: ~Dietrich_Klakow1
  - dblp_id: https://dblp.org/pid/b/WilliamJByrne
    emails: '****@cam.ac.uk'
    first_name: Bill
    google_scholar_id: https://scholar.google.com/citations?user=BVUcMU4AAAAJ&hl=en
    homepage: https://sites.google.com/view/bill-byrne/
    institution: Amazon and University of Cambridge
    last_name: Byrne
    name: Bill Byrne
    username: ~Bill_Byrne1
  - dblp_id: https://dblp.org/pid/94/11424
    emails: '****@gmail.com'
    first_name: Eva
    google_scholar_id: https://scholar.google.com/citations?user=ez5PZHAAAAAJ
    institution: Amazon
    last_name: Hasler
    name: Eva Hasler
    username: ~Eva_Hasler1
  decision: toMainConference
  end_page: 3640
  file: 318.pdf
  id: 318
  num_pages: 19
  openreview_id: ClatKY30bX
  pdf_file: a4a44ee4cc1d68e88037e6660b2acef35bda763d.pdf
  start_page: 3622
  title: A Preference-driven Paradigm for Enhanced Translation with Large Language
    Models
- abstract: We introduce GenDecider, a novel re-ranking approach for Zero-Shot Entity
    Linking (ZSEL), built on the Llama model. It innovatively detects scenarios where
    the correct entity is not among the retrieved candidates, a common oversight in
    existing re-ranking methods. By autoregressively generating outputs based on the
    context of the entity mention and the candidate entities, GenDecider significantly
    enhances disambiguation, improving the accuracy and reliability of ZSEL systems,
    as demonstrated on the benchmark ZESHEL dataset. Our code is available at https://github.com/kangISU/GenDecider.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Extraction
  authors:
  - emails: '****@iastate.edu'
    first_name: Kang
    google_scholar_id: https://scholar.google.com/citations?user=pNhbQq8AAAAJ&hl=en
    homepage: https://sites.google.com/iastate.edu/kangzhou/home
    last_name: Zhou
    name: Kang Zhou
    semantic_scholar_id: https://www.semanticscholar.org/author/Kang-Zhou/2046810614
    username: ~Kang_Zhou2
  - dblp_id: https://dblp.org/pid/199/8882.html
    emails: '****@iastate.edu'
    first_name: Yuepei
    google_scholar_id: https://scholar.google.com/citations?user=or3srI0AAAAJ&hl=en
    homepage: https://www.sites.google.com/view/yuepeili
    last_name: Li
    name: Yuepei Li
    semantic_scholar_id: https://www.semanticscholar.org/author/Yuepei-Li/10733723
    username: ~Yuepei_Li1
  - emails: '****@iastate.edu'
    first_name: Qing
    google_scholar_id: https://scholar.google.com/citations?user=jY7bx4gAAAAJ&hl=en
    homepage: https://www.cs.iastate.edu/qingwang
    institution: Iowa State University
    last_name: Wang
    name: Qing Wang
    username: ~Qing_Wang15
  - emails: '****@iastate.edu'
    first_name: Qiao
    google_scholar_id: https://scholar.google.com/citations?user=fGhPJ9IAAAAJ&hl=en
    institution: Iowa State University
    last_name: Qiao
    name: Qiao Qiao
    semantic_scholar_id: https://www.semanticscholar.org/author/Qiao-Qiao/2179882123
    username: ~Qiao_Qiao1
  - dblp_id: https://dblp.org/pid/181/2688-12
    emails: '****@iastate.edu'
    first_name: Qi
    google_scholar_id: https://scholar.google.com/citations?user=Gvld0foAAAAJ&hl=en
    homepage: https://sites.google.com/iastate.edu/qili/
    institution: Iowa State University
    last_name: Li
    name: Qi Li
    orcid: https://orcid.org/0000-0002-3136-2157
    semantic_scholar_id: https://www.semanticscholar.org/author/Qi-Li/37696683?sort=pub-date
    username: ~Qi_Li14
  decision: toMainConference
  end_page: 3647
  file: 319.pdf
  id: 319
  num_pages: 7
  openreview_id: SfVxVtXrQK
  pdf_file: ef1cf7863218ff742570077f8789668db0c94cdd.pdf
  start_page: 3641
  title: "GenDecider: Integrating \u201CNone of the Candidates\u201D Judgments in\
    \ Zero-Shot Entity Linking Re-ranking"
- abstract: People from different social and demographic groups express diverse perspectives
    and conflicting opinions on a broad set of topics such as product reviews, healthcare,
    law, and politics. A fair summary should provide a comprehensive coverage of diverse
    perspectives without underrepresenting certain groups. However, current work in
    summarization metrics and Large Language Models (LLMs) evaluation has not explored
    fair abstractive summarization. In this paper, we systematically investigate fair
    abstractive summarization for user-generated data. We first formally define fairness
    in abstractive summarization as not underrepresenting perspectives of any groups
    of people, and we propose four reference-free automatic metrics by measuring the
    differences between target and source perspectives. We evaluate nine LLMs, including
    three GPT models, four LLaMA models, PaLM 2, and Claude, on six datasets collected
    from social media, online reviews, and recorded transcripts. Experiments show
    that both the model-generated and the human-written reference summaries suffer
    from low fairness. We conduct a comprehensive analysis of the common factors influencing
    fairness and propose three simple but effective methods to alleviate unfair summarization.
    Our dataset and code are available at https://github.com/psunlpgroup/FairSumm.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Summarization
  authors:
  - dblp_id: https://dblp.org/pid/38/10863-1.html
    emails: '****@psu.edu'
    first_name: Yusen
    google_scholar_id: https://scholar.google.com/citations?user=FGyMx88AAAAJ&hl=zh-CN
    homepage: https://www.yuszh.com
    last_name: Zhang
    name: Yusen Zhang
    username: ~Yusen_Zhang1
  - emails: '****@psu.edu'
    first_name: Nan
    google_scholar_id: https://scholar.google.com/citations?user=PDuBGKYAAAAJ&hl=en
    homepage: https://zn1010.github.io
    institution: Pennsylvania State University
    last_name: Zhang
    name: Nan Zhang
    semantic_scholar_id: https://www.semanticscholar.org/author/Nan-Zhang/2174299795
    username: ~Nan_Zhang9
  - dblp_id: https://dblp.org/pid/140/7348.html
    emails: '****@yale.edu'
    first_name: Yixin
    google_scholar_id: https://scholar.google.com/citations?user=sFtxaMkAAAAJ&hl=en
    homepage: https://yixinl7.github.io/
    institution: Yale University
    last_name: Liu
    name: Yixin Liu
    semantic_scholar_id: https://www.semanticscholar.org/author/Yixin-Liu/2108176413
    username: ~Yixin_Liu2
  - dblp_id: https://dblp.org/pid/203/8539
    emails: '****@salesforce.com'
    first_name: Alexander
    google_scholar_id: https://scholar.google.com/citations?user=GgfJdhwAAAAJ&hl=en
    homepage: https://alex-fabbri.github.io
    institution: SalesForce.com
    last_name: Fabbri
    name: Alexander Fabbri
    username: ~Alexander_Fabbri1
  - emails: '****@tamu.edu'
    first_name: Junru
    homepage: https://leonora-cat.github.io/
    last_name: Liu
    name: Junru Liu
    username: ~Junru_Liu1
  - dblp_id: https://dblp.org/pid/254/2890
    emails: '****@psu.edu'
    first_name: Ryo
    google_scholar_id: https://scholar.google.com/citations?user=4OWTLKAAAAAJ&hl=en
    homepage: https://ryokamoi.github.io/
    last_name: Kamoi
    name: Ryo Kamoi
    orcid: https://orcid.org/0000-0002-8442-4171
    semantic_scholar_id: https://www.semanticscholar.org/author/Ryo-Kamoi/83757854
    username: ~Ryo_Kamoi1
  - dblp_id: https://dblp.org/pid/198/6626
    emails: '****@psu.edu'
    first_name: Xiaoxin
    institution: Pennsylvania State University
    last_name: Lu
    name: Xiaoxin Lu
    username: ~Xiaoxin_Lu1
  - dblp_id: https://dblp.org/pid/80/7282
    emails: '****@gmail.com'
    first_name: Caiming
    google_scholar_id: https://scholar.google.com/citations?user=vaSdahkAAAAJ&hl=en
    homepage: http://cmxiong.com/
    institution: Salesforce Research
    last_name: Xiong
    name: Caiming Xiong
    username: ~Caiming_Xiong1
  - dblp_id: https://dblp.org/pid/59/2379
    emails: '****@usc.edu'
    first_name: Jieyu
    google_scholar_id: https://scholar.google.com/citations?user=9VaGBCQAAAAJ&hl=en
    homepage: http://jyzhao.net/
    institution: University of Southern California
    last_name: Zhao
    name: Jieyu Zhao
    semantic_scholar_id: https://www.semanticscholar.org/author/Jieyu-Zhao/33524946
    username: ~Jieyu_Zhao1
  - dblp_id: https://dblp.org/pid/r/DragomirRRadev
    emails: '****@yale.edu'
    first_name: Dragomir
    google_scholar_id: https://scholar.google.com/citations?user=vIqWvgwAAAAJ&hl=en
    homepage: http://www.cs.yale.edu/~radev
    institution: Yale University
    last_name: Radev
    name: Dragomir Radev
    orcid: https://orcid.org/0000-0002-0213-7487
    semantic_scholar_id: https://www.semanticscholar.org/author/Dragomir-R.-Radev/9215251
    username: ~Dragomir_Radev2
  - dblp_id: https://dblp.org/pid/m/KathleenMcKeown
    emails: '****@cs.columbia.edu'
    first_name: Kathleen
    google_scholar_id: https://scholar.google.com.tw/citations?user=ujDhg2sAAAAJ
    homepage: http://www.cs.columbia.edu/~kathy/
    last_name: McKeown
    name: Kathleen McKeown
    username: ~Kathleen_McKeown1
  - dblp_id: https://dblp.org/pid/60/2536-37
    emails: '****@psu.edu'
    first_name: Rui
    google_scholar_id: https://scholar.google.com/citations?user=nhuB5CEAAAAJ&hl=en
    homepage: https://ryanzhumich.github.io/
    institution: Pennsylvania State University
    last_name: Zhang
    name: Rui Zhang
    semantic_scholar_id: https://www.semanticscholar.org/author/Rui-Zhang/144142360
    username: ~Rui_Zhang7
  decision: toMainConference
  end_page: 3670
  file: 320.pdf
  id: 320
  num_pages: 23
  openreview_id: 3wlXw5uEsJ
  pdf_file: be3a9c60998dab3c92be79fafcfbdd7450af091b.pdf
  start_page: 3648
  title: Fair Abstractive Summarization of Diverse Perspectives
- abstract: "Vision-language (VL) models, pretrained on colossal image-text datasets,\
    \ have attained broad VL competence that is difficult to evaluate. A common belief\
    \ is that a small number of VL skills underlie the variety of VL tests. In this\
    \ paper, we perform a large-scale transfer learning experiment aimed at discovering\
    \ latent VL skills from data. We reveal interesting characteristics that have\
    \ important implications for test suite design. First, generation tasks suffer\
    \ from a length bias, suggesting benchmarks should balance tasks with varying\
    \ output lengths. Second, we demonstrate that factor analysis successfully identifies\
    \ reasonable yet surprising VL skill factors, suggesting benchmarks could leverage\
    \ similar analyses for task selection.\nFinally, we present a new dataset, OLIVE$^1$,\
    \ which simulates user instructions in the wild and presents challenges dissimilar\
    \ to all datasets we tested. Our findings contribute to the design of balanced\
    \ and broad-coverage vision-language evaluation methods. \n\n$^1$https://github.com/jq-zh/olive-dataset"
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - dblp_id: https://dblp.org/pid/236/4871
    emails: '****@outlook.com'
    first_name: Anthony
    institution: Nanyang Technological University and SalesForce.com
    last_name: Tiong
    name: Anthony Tiong
    username: ~Anthony_Tiong1
  - emails: '****@ntu.edu.sg'
    first_name: Junqi
    institution: Nanyang Technological University
    last_name: Zhao
    name: Junqi Zhao
    username: ~Junqi_Zhao1
  - dblp_id: https://dblp.org/pid/70/1211-1
    emails: '****@ntu.edu.sg'
    first_name: Boyang
    google_scholar_id: https://scholar.google.com/citations?user=QwL4z2UAAAAJ
    homepage: http://www.boyangli.org
    institution: Nanyang Technological University
    last_name: Li
    name: Boyang Li
    username: ~Boyang_Li1
  - dblp_id: https://dblp.org/pid/193/6773
    emails: '****@gmail.com'
    first_name: Junnan
    google_scholar_id: https://scholar.google.com/citations?user=MuUhwi0AAAAJ&hl=en
    homepage: https://sites.google.com/site/junnanlics/
    institution: Salesforce Research
    last_name: Li
    name: Junnan Li
    username: ~Junnan_Li2
  - emails: '****@gmail.com'
    first_name: Steven
    google_scholar_id: https://scholar.google.com/citations?user=JoLjflYAAAAJ&hl=en&oi=ao
    homepage: http://stevenhoi.com
    institution: Salesforce Research Asia and Singapore Management University
    last_name: Hoi
    name: Steven Hoi
    username: ~Steven_Hoi2
  - dblp_id: https://dblp.org/pid/80/7282
    emails: '****@gmail.com'
    first_name: Caiming
    google_scholar_id: https://scholar.google.com/citations?user=vaSdahkAAAAJ&hl=en
    homepage: http://cmxiong.com/
    institution: Salesforce Research
    last_name: Xiong
    name: Caiming Xiong
    username: ~Caiming_Xiong1
  decision: toMainConference
  end_page: 3698
  file: 321.pdf
  id: 321
  num_pages: 28
  openreview_id: 2mfxLkYcyL
  pdf_file: 3627f8e6d677fac039ea6f4cbb504f12024b8817.pdf
  start_page: 3671
  title: What Are We Measuring When We Evaluate Large Vision-Language Models? An Analysis
    of Latent Factors and Biases
- abstract: 'The choice of hyperparameters greatly impacts performance in natural
    language processing. Often, it is hard to tell if a method is better than another
    or just better tuned. *Tuning curves* fix this ambiguity by accounting for tuning
    effort. Specifically, they plot validation performance as a function of the number
    of hyperparameter choices tried so far. While several estimators exist for these
    curves, it is common to use point estimates, which we show fail silently and give
    contradictory results when given too little data.


    Beyond point estimates, *confidence bands* are necessary to rigorously establish
    the relationship between different approaches. We present the first method to
    construct valid confidence bands for tuning curves. The bands are exact, simultaneous,
    and distribution-free, thus they provide a robust basis for comparing methods.


    Empirical analysis shows that while bootstrap confidence bands, which serve as
    a baseline, fail to approximate their target confidence, ours achieve it exactly.
    We validate our design with ablations, analyze the effect of sample size, and
    provide guidance on comparing models with our method. To promote confident comparisons
    in future work, we release opda: an easy-to-use library that you can install with
    pip. https://github.com/nicholaslourie/opda'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Learning for NLP
  authors:
  - dblp_id: https://dblp.org/pid/218/5457
    emails: '****@nyu.edu'
    first_name: Nicholas
    institution: New York University
    last_name: Lourie
    name: Nicholas Lourie
    semantic_scholar_id: https://www.semanticscholar.org/author/Nicholas-Lourie/35219984
    username: ~Nicholas_Lourie1
  - dblp_id: https://dblp.org/pid/41/9736
    emails: '****@nyu.edu'
    first_name: Kyunghyun
    google_scholar_id: https://scholar.google.fi/citations?user=0RAmmIAAAAAJ&hl=en
    homepage: http://kyunghyuncho.me
    institution: Genentech and New York University
    last_name: Cho
    name: Kyunghyun Cho
    semantic_scholar_id: https://www.semanticscholar.org/author/Kyunghyun-Cho/1979489
    username: ~Kyunghyun_Cho1
  - dblp_id: https://dblp.org/pid/08/8618-1
    emails: '****@nyu.edu'
    first_name: He
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=K-isjagAAAAJ&view_op=list_works&sortby=pubdate
    homepage: http://hhexiy.github.io
    institution: New York University
    last_name: He
    name: He He
    username: ~He_He2
  decision: toMainConference
  end_page: 3716
  file: 322.pdf
  id: 322
  num_pages: 18
  openreview_id: tGVH238vkq
  pdf_file: 9080d4c55c278e8cd311e2226077142db6810ee9.pdf
  start_page: 3699
  title: 'Show Your Work with Confidence: Confidence Bands for Tuning Curves'
- abstract: "Human annotation plays a core role in machine learning \u2014 annotations\
    \ for supervised models, safety guardrails for generative models, and human feedback\
    \ for reinforcement learning, to cite a few avenues. However, the fact that many\
    \ of these human annotations are inherently subjective is often overlooked. Recent\
    \ work has demonstrated that ignoring rater subjectivity (typically resulting\
    \ in rater disagreement) is problematic within specific tasks and for specific\
    \ subgroups. Generalizable methods to harness rater disagreement and thus understand\
    \ the socio-cultural leanings of subjective tasks remain elusive. In this paper,\
    \ we propose GRASP, a comprehensive disagreement analysis framework to measure\
    \ group association in perspectives among different rater subgroups, and demonstrate\
    \ its utility in assessing the extent of systematic disagreements in two datasets:\
    \ (1) safety annotations of human-chatbot conversations, and (2) offensiveness\
    \ annotations of social media posts, both annotated by diverse rater pools across\
    \ different socio-demographic axes. Our framework (based on disagreement metrics)\
    \ reveals specific rater groups that have significantly different perspectives\
    \ than others on certain tasks, and helps identify demographic axes that are crucial\
    \ to consider in specific task contexts."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Ethics, Bias, and Fairness
  authors:
  - dblp_id: https://dblp.org/pid/64/9281
    emails: '****@google.com'
    first_name: Vinodkumar
    google_scholar_id: https://scholar.google.com/citations?user=HwryFLcAAAAJ&hl=en
    homepage: https://www.cs.stanford.edu/~vinod/
    institution: Google
    last_name: Prabhakaran
    name: Vinodkumar Prabhakaran
    semantic_scholar_id: https://www.semanticscholar.org/author/Vinodkumar-Prabhakaran/3331141
    username: ~Vinodkumar_Prabhakaran2
  - dblp_id: https://dblp.org/pid/h/VPless
    emails: '****@cs.rit.edu'
    first_name: Christopher
    google_scholar_id: https://scholar.google.com.tw/citations?user=kU6puLcAAAAJ
    homepage: https://www.cs.rit.edu/~cmh/
    last_name: Homan
    middle_name: M
    name: Christopher M Homan
    username: ~Christopher_M_Homan1
  - dblp_id: https://dblp.org/pers/hd/a/Aroyo:Lora
    emails: '****@gmail.com'
    first_name: Lora
    google_scholar_id: https://scholar.google.nl/citations?user=FXGgl5IAAAAJ&hl=en
    homepage: http://lora-aroyo.org
    institution: Google
    last_name: Aroyo
    name: Lora Aroyo
    orcid: https://orcid.org/0000-0001-9402-1133
    username: ~Lora_Aroyo1
  - dblp_id: https://dblp.org/pid/248/7592
    emails: '****@google.com'
    first_name: Aida
    google_scholar_id: https://scholar.google.com/citations?user=byZIaXsAAAAJ&hl=en&oi=ao
    homepage: https://aidamd.github.io/
    institution: Research, Google
    last_name: Mostafazadeh Davani
    name: Aida Mostafazadeh Davani
    semantic_scholar_id: https://www.semanticscholar.org/author/Aida-Mostafazadeh-Davani/119603124
    username: ~Aida_Mostafazadeh_Davani1
  - dblp_id: https://dblp.org/pid/248/7544
    emails: '****@nyu.edu'
    first_name: Alicia
    google_scholar_id: https://scholar.google.com/citations?user=Kze5eGkAAAAJ&hl=en
    homepage: https://aliciaparrish.com/
    institution: Google
    last_name: Parrish
    name: Alicia Parrish
    orcid: https://orcid.org/0000-0002-1054-0516
    semantic_scholar_id: https://www.semanticscholar.org/author/Alicia-Parrish/119389860
    username: ~Alicia_Parrish1
  - dblp_id: https://dblp.org/pid/99/3558
    emails: '****@ed.ac.uk'
    first_name: Alex
    google_scholar_id: https://scholar.google.co.uk/citations?user=ecxxlucAAAAJ
    homepage: https://ast.io
    institution: Design Informatics, University of Edinburgh
    last_name: Taylor
    name: Alex Taylor
    orcid: https://orcid.org/0000-0001-6311-3967
    semantic_scholar_id: https://www.semanticscholar.org/author/Alex-S.-Taylor/2110306510
    username: ~Alex_Taylor1
  - emails: '****@google.com'
    first_name: Mark
    homepage: https://www.markjdiaz.com
    institution: Google
    last_name: Diaz
    name: Mark Diaz
    orcid: https://orcid.org/0000-0003-0167-9839
    username: ~Mark_Diaz1
  - emails: '****@google.com'
    first_name: Ding
    google_scholar_id: https://scholar.google.com/citations?user=DPjHVWwAAAAJ&hl=en
    homepage: https://research.google/people/107654/
    last_name: Wang
    name: Ding Wang
    username: ~Ding_Wang3
  - emails: '****@cam.ac.uk'
    first_name: Gregory
    google_scholar_id: https://scholar.google.com/citations?user=fa4EXucAAAAJ
    last_name: "Serapio-Garc\xEDa"
    name: "Gregory Serapio-Garc\xEDa"
    username: "~Gregory_Serapio-Garc\xEDa1"
  decision: toMainConference
  end_page: 3736
  file: 325.pdf
  id: 325
  num_pages: 20
  openreview_id: A87Vs87Xq6
  pdf_file: 742d7e729c17ec3ab68a8eadca953256328bb862.pdf
  start_page: 3717
  title: 'GRASP: A Disagreement Analysis Framework to Assess Group Associations in
    Perspectives'
- abstract: Cognitive science and symbolic AI research suggest that event causality
    provides vital information for story understanding. However, machine learning
    systems for story understanding rarely employ event causality, partially due to
    the lack of methods that reliably identify open-world causal event relations.
    Leveraging recent progress in large language models, we present the first method
    for event causality identification that leads to material improvements in computational
    story understanding. Our technique sets a new state of the art on the COPES dataset
    (Wang et al., 2023c) for causal event relation identification. Further, in the
    downstream story quality evaluation task, the identified causal relations lead
    to 3.6-16.6% relative improvement on correlation with human ratings. In the multimodal
    story video-text alignment task, we attain 4.1-10.9% increase on Clip Accuracy
    and 4.2-13.5% increase on Sentence IoU. The findings indicate substantial untapped
    potential for event causality in computational story understanding. The codebase
    is at https://github.com/insundaycathy/Event-Causality-Extraction.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Discourse and Pragmatics
  authors:
  - emails: '****@e.ntu.edu.sg'
    first_name: Yidan
    institution: Nanyang Technological University
    last_name: Sun
    name: Yidan Sun
    orcid: https://orcid.org/my-orcid?orcid=0000-0002-7897-3035
    username: ~Yidan_Sun1
  - emails: '****@e.ntu.edu.sg'
    first_name: Qin
    institution: Nanyang Technological University
    last_name: Chao
    name: Qin Chao
    username: ~Qin_Chao1
  - dblp_id: https://dblp.org/pid/70/1211-1
    emails: '****@ntu.edu.sg'
    first_name: Boyang
    google_scholar_id: https://scholar.google.com/citations?user=QwL4z2UAAAAJ
    homepage: http://www.boyangli.org
    institution: Nanyang Technological University
    last_name: Li
    name: Boyang Li
    username: ~Boyang_Li1
  decision: toMainConference
  end_page: 3755
  file: 326.pdf
  id: 326
  num_pages: 19
  openreview_id: gvH42mlVHP
  pdf_file: 7fd7426789656cd88f66ea4a5c36794002749ce6.pdf
  start_page: 3737
  title: Event Causality Is Key to Computational Story Understanding
- abstract: In the field of natural language processing (NLP), continuous vector representations
    are crucial for capturing the semantic meanings of individual words. Yet, when
    it comes to the representations of sets of words, the conventional vector-based
    approaches often struggle with expressiveness and lack the essential set operations
    such as union, intersection, and complement. Inspired by quantum logic, we realize
    the representation of word sets and corresponding set operations within pre-trained
    word embedding spaces. By grounding our approach in the linear subspaces, we enable
    efficient computation of various set operations and facilitate the soft computation
    of membership functions within continuous spaces. Moreover, we allow for the computation
    of the F-score directly within word vectors, thereby establishing a direct link
    to the assessment of sentence similarity. In experiments with widely-used pre-trained
    embeddings and benchmarks, we show that our subspace-based set operations consistently
    outperform vector-based ones in both sentence similarity and set retrieval tasks.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Learning for NLP
  authors:
  - dblp_id: https://dblp.org/pid/89/4619
    emails: '****@is.naist.jp'
    first_name: Yoichi
    google_scholar_id: https://scholar.google.co.jp/citations?view_op=list_works&hl=en&authuser=1&user=7JsG0KUAAAAJ
    homepage: https://yoichi1484.github.io
    institution: Kyoto University, Japan
    last_name: Ishibashi
    name: Yoichi Ishibashi
    orcid: https://orcid.org/0000-0001-9901-6878
    semantic_scholar_id: https://www.semanticscholar.org/author/Yoichi-Ishibashi/2059148476
    username: ~Yoichi_Ishibashi2
  - dblp_id: https://dblp.org/pid/184/8316
    emails: '****@tohoku.ac.jp'
    first_name: Sho
    google_scholar_id: https://scholar.google.co.jp/citations?user=EW2QPKoAAAAJ
    homepage: http://www.cl.ecei.tohoku.ac.jp/~yokoi/
    institution: Tohoku University and RIKEN
    last_name: Yokoi
    name: Sho Yokoi
    semantic_scholar_id: https://www.semanticscholar.org/author/Sho-Yokoi/32286159
    username: ~Sho_Yokoi1
  - dblp_id: https://dblp.org/pid/66/2380
    emails: '****@is.naist.jp'
    first_name: Katsuhito
    google_scholar_id: https://scholar.google.co.jp/citations?user=F_J3ZBcAAAAJ&hl=en
    homepage: https://www.sudoh.nl/
    institution: Nara Institute of Science and Technology, Japan
    last_name: Sudoh
    name: Katsuhito Sudoh
    orcid: https://orcid.org/0000-0002-2122-9846
    semantic_scholar_id: https://www.semanticscholar.org/author/Katsuhito-Sudoh/1790811
    username: ~Katsuhito_Sudoh2
  - dblp_id: https://dblp.org/pid/57/1548-1
    emails: '****@is.naist.jp'
    first_name: Satoshi
    google_scholar_id: https://scholar.google.co.jp/citations?user=ckdfXawAAAAJ&hl=ja
    homepage: https://ahcweb01.naist.jp/Prof.Nakamura/index_e.html
    institution: The Chinese University of Hong Kong
    last_name: Nakamura
    name: Satoshi Nakamura
    username: ~Satoshi_Nakamura2
  decision: toMainConference
  end_page: 3768
  file: 328.pdf
  id: 328
  num_pages: 13
  openreview_id: LkTOymUbVH
  pdf_file: 83abcf30a6a016bd19de999b5ee12cb8879f4f16.pdf
  start_page: 3756
  title: Subspace Representations for Soft Set Operations and Sentence Similarities
- abstract: "Although large language models (LLMs) have achieved significant success,\
    \ their vulnerability to adversarial perturbations, including recent jailbreak\
    \ attacks, has raised considerable concerns. However, the increasing size of these\
    \ models and their limited access make improving their robustness a challenging\
    \ task. Among various defense strategies, randomized smoothing has shown great\
    \ potential for LLMs, as it does not require full access to the model\u2019s parameters\
    \ or fine-tuning via adversarial training. However, randomized smoothing involves\
    \ adding noise to the input before model prediction, and the final model\u2019\
    s robustness largely depends on the model\u2019s performance on these noise-corrupted\
    \ data. Its effectiveness is often limited by the model\u2019s sub-optimal performance\
    \ on noisy data. To address this issue, we propose to leverage the multitasking\
    \ nature of LLMs to first denoise the noisy inputs and then to make predictions\
    \ based on these denoised versions. We call this procedure self-denoised smoothing.\
    \ Unlike previous denoised smoothing techniques in computer vision, which require\
    \ training a separate model to enhance the robustness of LLMs, our method offers\
    \ significantly better efficiency and flexibility. Our experimental results indicate\
    \ that our method surpasses existing methods in both empirical and certified robustness\
    \ in defending against adversarial attacks for both downstream tasks and human\
    \ alignments (i.e., jailbreak attacks). Our code is publicly available at https://github.com/UCSB-NLP-Chang/SelfDenoise."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - dblp_id: https://dblp.org/pid/284/0699
    emails: '****@ucsb.edu'
    first_name: Jiabao
    google_scholar_id: https://scholar.google.com/citations?user=gqI7uDMAAAAJ&hl=en
    homepage: https://question406.github.io
    institution: University of California, Santa Barbara
    last_name: Ji
    name: Jiabao Ji
    username: ~Jiabao_Ji1
  - dblp_id: https://dblp.org/pid/274/7151
    emails: '****@ucsb.edu'
    first_name: Bairu
    google_scholar_id: https://scholar.google.com/citations?user=FO7taJgAAAAJ&hl=zh-CN
    homepage: https://hbr690188270.github.io/
    last_name: Hou
    name: Bairu Hou
    semantic_scholar_id: https://www.semanticscholar.org/author/Bairu-Hou/1955614986
    username: ~Bairu_Hou2
  - emails: '****@ucsb.edu'
    first_name: Zhen
    homepage: https://github.com/namezhenzhang
    institution: University of California, Santa Barbara
    last_name: Zhang
    name: Zhen Zhang
    username: ~Zhen_Zhang16
  - dblp_id: https://dblp.org/pid/171/0962.html
    emails: '****@outlook.com'
    first_name: Guanhua
    google_scholar_id: https://scholar.google.com/citations?user=_hrEN-sAAAAJ&hl=zh-CN
    institution: Max Planck Institute for Intelligent Systems, Max-Planck Institute
    last_name: Zhang
    name: Guanhua Zhang
    semantic_scholar_id: https://www.semanticscholar.org/author/Guanhua-Zhang/46266569
    username: ~Guanhua_Zhang1
  - dblp_id: https://dblp.org/pid/218/7410
    emails: '****@gmail.com'
    first_name: Wenqi
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=SQ9UbHIAAAAJ
    homepage: https://wenqifan03.github.io
    institution: Hong Kong Polytechnic University
    last_name: Fan
    name: Wenqi Fan
    orcid: https://orcid.org/0000-0002-4049-1233
    username: ~Wenqi_Fan1
  - dblp_id: https://dblp.uni-trier.de/pers/hd/l/Li_0001:Qing
    emails: '****@comp.polyu.edu.hk'
    first_name: Qing
    google_scholar_id: https://scholar.google.co.in/citations?user=D1LEg-YAAAAJ&hl=en&num=20&oi=ao
    homepage: https://www4.comp.polyu.edu.hk/~csqli/
    institution: The Hong Kong Polytechnic University, Hong Kong Polytechnic University
    last_name: Li
    name: Qing Li
    orcid: https://orcid.org/0000-0003-3370-471X
    username: ~Qing_Li5
  - dblp_id: https://dblp.org/pid/06/6785-1
    emails: '****@ibm.com'
    first_name: Yang
    google_scholar_id: https://scholar.google.com/citations?user=_-5PSgQAAAAJ&hl=en
    last_name: Zhang
    name: Yang Zhang
    username: ~Yang_Zhang3
  - dblp_id: https://dblp.org/pid/136/1007.html
    emails: '****@cisco.com'
    first_name: Gaowen
    google_scholar_id: https://scholar.google.com/citations?user=NIv_aeQAAAAJ&hl=en
    last_name: Liu
    name: Gaowen Liu
    username: ~Gaowen_Liu4
  - dblp_id: https://dblp.org/pid/128/6972-1
    emails: '****@msu.edu'
    first_name: Sijia
    google_scholar_id: https://scholar.google.com/citations?user=C7dO_UgAAAAJ
    homepage: https://lsjxjtu.github.io/
    institution: Michigan State University
    last_name: Liu
    name: Sijia Liu
    semantic_scholar_id: https://www.semanticscholar.org/author/Sijia-Liu/143743061
    username: ~Sijia_Liu1
  - dblp_id: https://dblp.org/pid/28/9988
    emails: '****@ucsb.edu'
    first_name: Shiyu
    google_scholar_id: https://scholar.google.com/citations?user=r21asW4AAAAJ&hl=en
    homepage: http://people.csail.mit.edu/chang87/
    institution: UC Santa Barbara
    last_name: Chang
    name: Shiyu Chang
    username: ~Shiyu_Chang1
  decision: toMainConference
  end_page: 3780
  file: 331.pdf
  id: 331
  num_pages: 12
  openreview_id: FlCZquIXkp
  pdf_file: 45a2686f208aa0ff49ef9307c1396d91ea60fbf7.pdf
  start_page: 3769
  title: Advancing the Robustness of Large Language Models through Self-Denoised Smoothing
- abstract: We present a novel approach to automatically synthesize "wayfinding instructions"
    for an embodied robot agent. In contrast to prior approaches that are heavily
    reliant on human-annotated datasets designed exclusively for specific simulation
    platforms, our algorithm uses in-context learning to condition an LLM to generate
    instructions using just a few references. Using an LLM-based Visual Question Answering
    strategy, we gather detailed information about the environment which is used by
    the LLM for instruction synthesis. We implement our approach on multiple simulation
    platforms including Matterport3D, AI Habitat and ThreeDWorld, thereby demonstrating
    its platform-agnostic nature. We subjectively evaluate our approach via a user
    study and observe that 83.3% of users find the synthesized instructions accurately
    capture the details of the environment and show characteristics similar to those
    of human-generated instructions. Further, we conduct zero-shot navigation with
    multiple approaches on the REVERIE dataset using the generated instructions, and
    observe very close correlation with the baseline on standard success metrics (<
    1% change in SR), quantifying the viability of generated instructions in replacing
    human-annotated data. We finally discuss the applicability of our approach in
    enabling a generalizable evaluation of embodied navigation policies. To the best
    of our knowledge, ours is the first LLM-driven approach capable of generating
    "human-like" instructions in a platform-agnostic manner, without training.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - emails: '****@umd.edu'
    first_name: Vishnu Sashank
    google_scholar_id: https://scholar.google.com/citations?user=vmm6AlwAAAAJ&hl=en&oi=ao
    homepage: https://vdorbala.github.io
    institution: University of Maryland, College Park
    last_name: Dorbala
    name: Vishnu Sashank Dorbala
    username: ~Vishnu_Sashank_Dorbala1
  - dblp_id: https://dblp.org/pid/62/3646
    emails: '****@umd.edu'
    first_name: Sanjoy
    google_scholar_id: https://scholar.google.com/citations?user=CEdJKCIAAAAJ&hl=en&oi=ao
    homepage: https://schowdhury671.github.io/
    institution: University of Maryland, College Park
    last_name: Chowdhury
    name: Sanjoy Chowdhury
    username: ~Sanjoy_Chowdhury1
  - dblp_id: https://dblp.org/pers/hd/m/Manocha:Dinesh
    emails: '****@umd.edu'
    first_name: Dinesh
    google_scholar_id: https://scholar.google.com/citations?user=X08l_4IAAAAJ
    homepage: https://www.cs.umd.edu/people/dmanocha
    institution: University of Maryland, College Park
    last_name: Manocha
    name: Dinesh Manocha
    orcid: https://orcid.org/0000-0001-7047-9801
    username: ~Dinesh_Manocha3
  decision: toMainConference
  end_page: 3794
  file: 335.pdf
  id: 335
  num_pages: 14
  openreview_id: N2kH6vbeYn
  pdf_file: 7384810f807a40ba61eea0bee9a05e9b8d6e4692.pdf
  start_page: 3781
  title: Can LLM's Generate Human-Like Wayfinding Instructions? Towards Platform-Agnostic
    Embodied Instruction Synthesis
- abstract: Humans frequently experience emotions. When emotions arise, they affect
    not only our mental state but can also change our physical state. For example,
    we often open our eyes wide when we are surprised, or clap our hands when we feel
    excited. Physical manifestations of emotions are referred to as embodied emotion
    in the psychology literature. From an NLP perspective, recognizing descriptions
    of physical movements or physiological responses associated with emotions is a
    type of implicit emotion recognition. Our work introduces a new task of recognizing
    expressions of embodied emotion in natural language. We create a  dataset of sentences
    that contains 7,300 body part mentions with human annotations for embodied emotion.
    We develop a classification model for this task and present two methods to acquire
    weakly labeled instances  of embodied emotion by extracting emotional manner expressions
    and by prompting a language model. Our experiments show that the weakly labeled
    data can  train an effective classification model without  gold data, and can  also
    improve performance when combined with gold data. Our dataset is publicly available
    at https://github.com/yyzhuang1991/Embodied-Emotions.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Sentiment Analysis, Stylistic Analysis, and Argument Mining
  authors:
  - emails: '****@utah.edu'
    first_name: Yuan
    homepage: https://www.cs.utah.edu/~yyzhuang/
    last_name: Zhuang
    name: Yuan Zhuang
    username: ~Yuan_Zhuang1
  - dblp_id: https://dblp.org/pid/46/11004.html
    emails: '****@uc.edu'
    first_name: Tianyu
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=c9cpQeAAAAAJ
    homepage: https://jiangtianyu.com
    institution: University of Cincinnati
    last_name: Jiang
    name: Tianyu Jiang
    semantic_scholar_id: https://www.semanticscholar.org/author/Tianyu-Jiang/144830318
    username: ~Tianyu_Jiang1
  - dblp_id: https://dblp.org/pid/r/ERiloff
    emails: '****@cs.arizona.edu'
    first_name: Ellen
    google_scholar_id: https://scholar.google.com/citations?user=M1pa3_gAAAAJ&hl=en&oi=ao
    homepage: http://www2.cs.arizona.edu/~riloff
    institution: University of Arizona
    last_name: Riloff
    name: Ellen Riloff
    semantic_scholar_id: https://www.semanticscholar.org/author/E.-Riloff/1691993
    username: ~Ellen_Riloff1
  decision: toMainConference
  end_page: 3807
  file: 337.pdf
  id: 337
  num_pages: 13
  openreview_id: 3ySKXcLvFk
  pdf_file: 70579ca4345f0b8dec7b465daaef98eee7de1040.pdf
  start_page: 3795
  title: My Heart Skipped a Beat! Recognizing Expressions of Embodied Emotion in Natural
    Language
- abstract: Dictionary example sentences play an important role in illustrating word
    definitions and usage, but manually creating quality sentences is challenging.
    Prior works have demonstrated that language models can be trained to generate
    example sentences. However, they relied on costly customized models and word sense
    datasets for generation and evaluation of their work. Rapid advancements in foundational
    models present the opportunity to create low-cost, zero-shot methods for the generation
    and evaluation of dictionary example sentences. We introduce a new automatic evaluation
    metric called OxfordEval that measures the win-rate of generated sentences against
    existing Oxford Dictionary sentences. OxfordEval shows high alignment with human
    judgments, enabling large-scale automated quality evaluation. We experiment with
    various LLMs and configurations to generate dictionary sentences across word classes.
    We complement this with a novel approach of using masked language models to identify
    and select sentences that best exemplify word meaning. The eventual model, FM-MLM,
    achieves over 85.1\% win rate against Oxford baseline sentences according to OxfordEval,
    compared to 39.8\% win rate for prior model-generated sentences.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - emails: '****@alum.mit.edu'
    first_name: Bill
    google_scholar_id: https://scholar.google.com/citations?user=0yeViVAAAAAJ&hl=en
    homepage: https://www.billcai.com
    institution: Amazon
    last_name: Cai
    name: Bill Cai
    username: ~Bill_Cai1
  - dblp_id: https://dblp.org/pid/205/5637.html
    emails: '****@ic.ac.uk'
    first_name: Ng
    institution: Ministry of Education, Singapore
    last_name: Clarence
    middle_name: Boon Liang
    name: Ng Boon Liang Clarence
    username: ~Ng_Boon_Liang_Clarence1
  - emails: '****@gmail.com'
    first_name: Daniel
    last_name: Liang
    middle_name: Tan Wee
    name: Daniel Tan Wee Liang
    username: ~Daniel_Tan_Wee_Liang1
  - emails: '****@gmail.com'
    first_name: Shelvia
    homepage: https://sg.linkedin.com/in/shelviadh
    last_name: Hotama
    name: Shelvia Hotama
    username: ~Shelvia_Hotama1
  decision: toMainConference
  end_page: 3819
  file: 338.pdf
  id: 338
  num_pages: 12
  openreview_id: c0X7TVioNv
  pdf_file: e865bb8fb9b5902825c7eccb6d2b4be9ddde41c5.pdf
  start_page: 3808
  title: Low-Cost Generation and Evaluation of Dictionary Example Sentences
- abstract: 'Tools serve as pivotal interfaces that enable humans to understand and
    reshape the environment. With the advent of foundation models, AI systems can
    utilize tools to expand their capabilities and interact with the real world. Existing
    tool learning methodologies, encompassing supervised fine-tuning and prompt engineering
    approaches, often induce large language models to utilize tools indiscriminately,
    as complex tasks often exceed their own competencies. However, introducing tools
    for simple tasks, which the models themselves can readily resolve, can inadvertently
    propagate errors rather than enhance performance. This leads to the research question:
    can we teach language models when and how to use tools? To meet this need, we
    propose Tool leaRning wIth exeCution fEedback (TRICE), a two-stage end-to-end
    framework that enables the model to continually learn through feedback derived
    from tool execution, thereby learning when and how to use tools effectively. Experimental
    results, backed by further analysis, show that TRICE can make the large language
    model selectively use tools by improving the accuracy of tool usage while enhancing
    insufficient tool learning and mitigating excessive reliance on tools.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Learning for NLP
  authors:
  - dblp_id: https://dblp.org/pid/333/2421.html
    emails: '****@zju.edu.cn'
    first_name: Shuofei
    google_scholar_id: https://scholar.google.com/citations?user=4DRltuEAAAAJ
    last_name: Qiao
    name: Shuofei Qiao
    username: ~Shuofei_Qiao1
  - emails: '****@zju.edu.cn'
    first_name: Honghao
    google_scholar_id: https://scholar.google.com/citations?user=ekxyQTYAAAAJ&hl=zh-CN
    last_name: Gui
    name: Honghao Gui
    username: ~Honghao_Gui1
  - emails: '****@alibaba-inc.com'
    first_name: Chengfei
    homepage: https://www.mnn.zone/m/0.3/
    last_name: lv
    name: chengfei lv
    username: ~chengfei_lv1
  - dblp_id: https://dblp.org/pid/150/6714.html
    emails: '****@antgroup.com'
    first_name: Qianghuai
    last_name: Jia
    name: Qianghuai Jia
    username: ~Qianghuai_Jia1
  - dblp_id: https://dblp.org/pid/94/5089
    emails: '****@zju.edu.cn'
    first_name: Huajun
    institution: Zhejiang University
    last_name: Chen
    name: Huajun Chen
    username: ~Huajun_Chen1
  - dblp_id: https://dblp.org/pid/139/4181-1.html
    emails: '****@zju.edu.cn'
    first_name: Ningyu
    google_scholar_id: https://scholar.google.com/citations?user=xQDOPvsAAAAJ&hl=en
    homepage: https://person.zju.edu.cn/en/ningyu
    institution: Zhejiang University
    last_name: Zhang
    name: Ningyu Zhang
    semantic_scholar_id: https://www.semanticscholar.org/author/Ningyu-Zhang/2608639
    username: ~Ningyu_Zhang1
  decision: toMainConference
  end_page: 3838
  file: 339.pdf
  id: 339
  num_pages: 19
  openreview_id: 9VPyqsvffL
  pdf_file: 3ca5a425012bde945647749f2f69bd3af68251bc.pdf
  start_page: 3820
  title: Making Language Models Better Tool Learners with Execution Feedback
- abstract: 'Retrieving evidence to support or refute claims is a core part of automatic
    fact-checking. Prior work makes simplifying assumptions in retrieval that depart
    from real-world use cases: either no access to evidence, access to evidence curated
    by a human fact-checker, or access to evidence published after a claim was made.
    In this work, we present the first realistic pipeline to check real-world claims
    by retrieving raw evidence from the web. We restrict our retriever to only search
    documents available prior to the claim''s making, modeling the realistic scenario
    of emerging claims. Our pipeline includes five components: claim decomposition,
    raw document retrieval, fine-grained evidence retrieval, claim-focused summarization,
    and veracity judgment. We conduct experiments on complex political claims in the
    ClaimDecomp dataset and show that the aggregated evidence produced by our pipeline
    improves veracity judgments. Human evaluation finds the evidence summary produced
    by our system is reliable (it does not hallucinate information) and relevant to
    answering key questions about a claim, suggesting that it can assist fact-checkers
    even when it does not reflect a complete evidence set.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: 'Semantics: Sentence-level Semantics, Textual Inference and Other
      areas'
  authors:
  - dblp_id: https://dblp.org/pid/143/0527
    emails: '****@utexas.edu'
    first_name: Jifan
    google_scholar_id: https://scholar.google.com/citations?user=ofxM06EAAAAJ&hl=en
    homepage: https://jifan-chen.github.io/
    institution: Amazon
    last_name: Chen
    name: Jifan Chen
    semantic_scholar_id: https://www.semanticscholar.org/author/Jifan-Chen/3100431
    username: ~Jifan_Chen1
  - emails: '****@utexas.edu'
    first_name: Grace
    institution: University of Texas at Austin
    last_name: Kim
    name: Grace Kim
    username: ~Grace_Kim2
  - emails: '****@utexas.edu'
    first_name: Aniruddh
    homepage: https://AniruddhS24.github.io
    last_name: Sriram
    name: Aniruddh Sriram
    username: ~Aniruddh_Sriram1
  - dblp_id: https://dblp.org/pid/69/7968
    emails: '****@cs.utexas.edu'
    first_name: Greg
    google_scholar_id: https://scholar.google.com.tw/citations?user=EpQ_sDEAAAAJ
    homepage: http://www.cs.utexas.edu/~gdurrett/
    institution: University of Texas, Austin
    last_name: Durrett
    name: Greg Durrett
    semantic_scholar_id: https://www.semanticscholar.org/author/Greg-Durrett/1814094
    username: ~Greg_Durrett1
  - dblp_id: https://dblp.org/pid/116/2765
    emails: '****@utexas.edu'
    first_name: Eunsol
    google_scholar_id: https://scholar.google.com/citations?user=dCEcahMAAAAJ&hl=en
    homepage: https://eunsol.github.io/
    institution: University of Texas, Austin
    last_name: Choi
    name: Eunsol Choi
    semantic_scholar_id: https://www.semanticscholar.org/author/Eunsol-Choi/2890423
    username: ~Eunsol_Choi1
  decision: toMainConference
  end_page: 3857
  file: 340.pdf
  id: 340
  num_pages: 19
  openreview_id: nbjH4HFUra
  pdf_file: d79004146d1ac3b57c7a369bbc359fc80a65fc25.pdf
  start_page: 3839
  title: Complex Claim Verification with Evidence Retrieved in the Wild
- abstract: This paper investigates the optimal selection and fusion of feature encoders
    across multiple modalities and combines these in one neural network to improve
    sentiment detection. We compare different fusion methods and examine the impact
    of multi-loss training within the multi-modality fusion network, identifying surprisingly
    important findings relating to subnet performance. We have also found that integrating
    context significantly enhances model performance. Our best model achieves state-of-the-art
    performance for three datasets (CMU-MOSI, CMU-MOSEI and CH-SIMS). These results
    suggest a roadmap toward an optimized feature selection and fusion approach for
    enhancing sentiment detection in neural networks.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Sentiment Analysis, Stylistic Analysis, and Argument Mining
  authors:
  - emails: '****@columbia.edu'
    first_name: Zehui
    last_name: wu
    name: zehui wu
    username: ~zehui_wu1
  - emails: '****@columbia.edu'
    first_name: Ziwei
    homepage: https://ziweig.github.io/
    institution: Columbia University
    last_name: Gong
    name: Ziwei Gong
    username: ~Ziwei_Gong1
  - emails: '****@rice.edu'
    first_name: Jaywon
    institution: Rice University
    last_name: Koo
    name: Jaywon Koo
    orcid: https://orcid.org/0000-0002-5539-5244
    username: ~Jaywon_Koo1
  - emails: '****@cs.columbia.edu'
    first_name: Julia
    google_scholar_id: https://scholar.google.com/citations?user=Qrd7FCoAAAAJ&hl=en
    homepage: http://www.cs.columbia.edu/~julia/
    institution: Columbia University
    last_name: Hirschberg
    name: Julia Hirschberg
    username: ~Julia_Hirschberg1
  decision: toMainConference
  end_page: 3872
  file: 341.pdf
  id: 341
  num_pages: 15
  openreview_id: DFQgXuGQG0
  pdf_file: 01f7f8a6fd08b8e2dc8e37bee942afd6278bcd5a.pdf
  start_page: 3858
  title: Multimodal Multi-loss Fusion Network for Sentiment Analysis
- abstract: 'Recent literature has suggested the potential of using large language
    models (LLMs) to make classifications for tabular tasks. However, LLMs have been
    shown to exhibit harmful social biases that reflect the stereotypes and inequalities
    present in society. To this end, as well as the widespread use of tabular data
    in many high-stake applications, it is important to explore the following questions:
    what sources of information do LLMs draw upon when making classifications for
    tabular tasks; whether and to what extent are LLM classifications for tabular
    data influenced by social biases and stereotypes; and what are the consequential
    implications for fairness?


    Through a series of experiments, we delve into these questions and show that LLMs
    tend to inherit social biases from their training data which significantly impact
    their fairness in tabular classification tasks. Furthermore, our investigations
    show that in the context of bias mitigation, though in-context learning and finetuning
    have a moderate effect, the fairness metric gap between different subgroups is
    still larger than that in traditional machine learning models, such as Random
    Forest and shallow Neural Networks. This observation emphasizes that the social
    biases are inherent within the LLMs themselves and inherited from their pretraining
    corpus, not only from the downstream task datasets. Besides, we demonstrate that
    label-flipping of in-context examples can significantly reduce biases, further
    highlighting the presence of inherent bias within LLMs.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Ethics, Bias, and Fairness
  authors:
  - emails: '****@g.harvard.edu'
    first_name: Yanchen
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=Ypagfq8AAAAJ
    homepage: https://liuyanchen1015.github.io/
    last_name: Liu
    name: Yanchen Liu
    semantic_scholar_id: https://www.semanticscholar.org/author/Yanchen-Liu/2108082280
    username: ~Yanchen_Liu2
  - dblp_id: https://dblp.org/pid/201/6628
    emails: '****@uit.no'
    first_name: Srishti
    google_scholar_id: https://scholar.google.com/citations?user=7V_riiYAAAAJ&hl=en
    institution: UiT The Arctic University of Norway
    last_name: Gautam
    name: Srishti Gautam
    username: ~Srishti_Gautam1
  - dblp_id: https://dblp.org/pid/155/2199-1
    emails: '****@illinois.edu'
    first_name: Jiaqi
    google_scholar_id: https://scholar.google.com/citations?user=Z9X2A1MAAAAJ
    homepage: https://jiaqima.github.io
    institution: University of Illinois Urbana-Champaign
    last_name: Ma
    name: Jiaqi Ma
    orcid: https://orcid.org/0000-0001-8292-5901
    username: ~Jiaqi_Ma1
  - dblp_id: https://dblp.org/pid/68/9376
    emails: '****@seas.harvard.edu'
    first_name: Himabindu
    homepage: http://web.stanford.edu/~himalv
    institution: Harvard University
    last_name: Lakkaraju
    name: Himabindu Lakkaraju
    username: ~Himabindu_Lakkaraju1
  decision: toMainConference
  end_page: 3890
  file: 342.pdf
  id: 342
  num_pages: 18
  openreview_id: St4lbz88bp
  pdf_file: b7411e4217b68a298e344be9998ad6b4ef3eebfe.pdf
  start_page: 3873
  title: 'Confronting LLMs with Traditional ML: Rethinking the Fairness of Large Language
    Models in Tabular Classifications'
- abstract: 'Metaphorical language is a pivotal element in

    the realm of political framing. Existing work

    from linguistics and the social sciences provides

    compelling evidence regarding the distinctiveness

    of conceptual framing for political

    ideology perspectives. However, the nature and

    utilization of metaphors and the effect on audiences

    of different political ideologies within

    political discourses are hardly explored. To

    enable research in this direction, in this work

    we create a dataset, originally based on news

    editorials and labeled with their persuasive effects

    on liberals and conservatives and extend it

    with annotations pertaining to metaphorical usage

    of language. To that end, first, we identify

    all single metaphors and composite metaphors.

    Secondly, we provide annotations of the source

    and target domains for each metaphor. As a

    result, our corpus consists of 300 news editorials

    annotated with spans of texts containing

    metaphors and the corresponding domains of

    which these metaphors draw from. Our analysis

    shows that liberal readers are affected by

    metaphors, whereas conservatives are resistant

    to them. Both ideologies are affected differently

    based on the metaphor source and target

    category. For example, liberals are affected by

    metaphors in the Darkness & Light (e.g., death)

    source domains, where as the source domain of

    Nature affects conservatives more significantly.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Linguistic theories, Cognitive Modeling and Psycholinguistics
  authors:
  - emails: '****@ai.uni-hannover.de'
    first_name: Meghdut
    google_scholar_id: https://scholar.google.com/citations?user=lWKkt3cAAAAJ&hl=en&oi=ao
    institution: "Universit\xE4t Hannover"
    last_name: Sengupta
    name: Meghdut Sengupta
    username: ~Meghdut_Sengupta1
  - dblp_id: https://dblp.org/pid/224/6057
    emails: '****@dlr.de'
    first_name: Roxanne
    google_scholar_id: https://scholar.google.com/citations?user=CPr6CU8AAAAJ&hl=en
    institution: German Aerospace Center and Bauhaus-University Weimar
    last_name: El Baff
    name: Roxanne El Baff
    orcid: https://orcid.org/0000-0001-6661-8661
    semantic_scholar_id: https://www.semanticscholar.org/author/Roxanne-El-Baff/51185829
    username: ~Roxanne_El_Baff1
  - dblp_id: https://dblp.org/pid/160/8727
    emails: '****@columbia.edu'
    first_name: Milad
    google_scholar_id: https://scholar.google.com/citations?user=mD9n_KgAAAAJ&hl=en
    institution: Columbia University
    last_name: Alshomary
    name: Milad Alshomary
    semantic_scholar_id: https://www.semanticscholar.org/author/Milad-Alshomary/2300829
    username: ~Milad_Alshomary1
  - dblp_id: https://dblp.org/pid/73/9281
    emails: '****@ai.uni-hannover.de'
    first_name: Henning
    google_scholar_id: https://scholar.google.com/citations?user=kPps-H8AAAAJ
    institution: "Leibniz Universit\xE4t Hannover"
    last_name: Wachsmuth
    name: Henning Wachsmuth
    orcid: https://orcid.org/0000-0003-2792-621X
    semantic_scholar_id: https://www.semanticscholar.org/author/Henning-Wachsmuth/2626599
    username: ~Henning_Wachsmuth1
  decision: toMainConference
  end_page: 3901
  file: 347.pdf
  id: 347
  num_pages: 11
  openreview_id: Tgrf3sf8XT
  pdf_file: 801af4e282e81f30bbcbb11491148eab8a281289.pdf
  start_page: 3891
  title: Analyzing the Use of Metaphors in News Editorials for Political Framing
- abstract: Continual event detection is a cornerstone in uncovering valuable patterns
    in many dynamic practical applications, where novel events emerge daily. Existing
    state-of-the-art approaches with replay buffers still suffer from catastrophic
    forgetting, partially due to overly simplistic objective aggregation. This oversight
    disregards complex trade-offs and leads to sub-optimal gradient updates, resulting
    in performance deterioration across objectives. While there are successful, widely
    cited multi-objective optimization frameworks for multi-task learning, they lack
    mechanisms to address data imbalance and evaluate whether a Pareto-optimal solution
    can effectively mitigate catastrophic forgetting, rendering them unsuitable for
    direct application to continual learning. To address these challenges, we propose
    **SharpSeq**, a novel continual learning paradigm leveraging sharpness-aware minimization
    combined with a generative model to balance training data distribution. Through
    extensive experiments on multiple real-world datasets, we demonstrate the superior
    performance of SharpSeq in continual event detection, proving the importance of
    our approach in mitigating catastrophic forgetting in continual event detection.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Extraction
  authors:
  - emails: '****@gmail.com'
    first_name: Thanh-Thien
    last_name: Le
    name: Thanh-Thien Le
    username: ~Thanh-Thien_Le1
  - emails: '****@gmail.com'
    first_name: Viet
    last_name: Dao
    name: Viet Dao
    username: ~Viet_Dao1
  - emails: '****@sis.hust.edu.vn'
    first_name: Linh
    last_name: Nguyen
    middle_name: Van
    name: Linh Van Nguyen
    username: ~Linh_Van_Nguyen2
  - dblp_id: https://dblp.org/pid/305/9765
    emails: '****@vinai.io'
    first_name: Thi-Nhung
    google_scholar_id: https://scholar.google.com/citations?user=LvHj1fAAAAAJ&hl=vi
    homepage: https://nhungnt7.github.io/
    institution: VinAI Research
    last_name: Nguyen
    name: Thi-Nhung Nguyen
    username: ~Thi-Nhung_Nguyen1
  - dblp_id: https://dblp.org/pid/125/3578.html
    emails: '****@soict.hust.edu.vn'
    first_name: Linh
    google_scholar_id: https://scholar.google.com.vn/citations?user=tZ78MoQAAAAJ&hl=en
    homepage: https://users.soict.hust.edu.vn/linhnv/
    institution: Hanoi University of Science and Technology
    last_name: Ngo
    middle_name: Van
    name: Linh Van Ngo
    username: ~Linh_Van_Ngo1
  - dblp_id: https://dblp.org/pid/17/9407
    emails: '****@cs.uoregon.edu'
    first_name: Thien
    google_scholar_id: https://scholar.google.com/citations?user=Da2FhegAAAAJ&hl=en&oi=ao
    homepage: http://ix.cs.uoregon.edu/~thien
    institution: ', University of Oregon'
    last_name: Nguyen
    middle_name: Huu
    name: Thien Huu Nguyen
    username: ~Thien_Huu_Nguyen1
  decision: toMainConference
  end_page: 3914
  file: 348.pdf
  id: 348
  num_pages: 13
  openreview_id: uhSgSByR3c
  pdf_file: 931cf41fb28a81b5fbee25bb543aa42b0259d002.pdf
  start_page: 3902
  title: 'SharpSeq: Empowering Continual Event Detection through Sharpness-Aware Sequential-task
    Learning'
- abstract: "Pre-trained Language Models (PLMs) are known to contain various kinds\
    \ of knowledge.\nOne method to infer relational knowledge is through the use of\
    \ cloze-style prompts, where a model is tasked to predict missing subjects or\n\
    objects. \nTypically, designing these prompts is a tedious task because small\
    \ differences in syntax or semantics can have a substantial impact on knowledge\
    \ retrieval performance. \nSimultaneously, evaluating the impact of either prompt\
    \ syntax or information is challenging due to their interdependence. \nWe designed\
    \ CONPARE-LAMA \u2013 a dedicated probe, consisting of 34 million distinct prompts\
    \ that facilitate comparison across minimal paraphrases. \nThese paraphrases follow\
    \ a unified meta-template enabling the controlled variation of syntax and semantics\
    \ across arbitrary relations.\nCONPARE-LAMA enables insights into the independent\
    \ impact of either syntactical form or semantic information of paraphrases on\
    \ the knowledge retrieval performance of PLMs. Extensive knowledge retrieval experiments\
    \ using our probe reveal that prompts following clausal syntax have several desirable\
    \ properties in comparison to appositive syntax: \ni) they are more useful when\
    \ querying PLMs with a combination of supplementary information, \nii) knowledge\
    \ is more consistently recalled across different combinations of supplementary\
    \ information, and \niii) they decrease response uncertainty when retrieving known\
    \ facts. In addition, range information can boost knowledge retrieval performance\
    \ more than domain information, even though domain information is more reliably\
    \ helpful across syntactic forms."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - emails: '****@gesis.org'
    first_name: Stephan
    homepage: https://www.gesis.org/institut/mitarbeitendenverzeichnis/person/Stephan.Linzbach
    institution: GESIS - Leibniz Insitute for the Social Sciences
    last_name: Linzbach
    name: Stephan Linzbach
    username: ~Stephan_Linzbach1
  - dblp_id: https://dblp.org/pid/25/10007
    emails: '****@gesis.org'
    first_name: Dimitar
    homepage: http://dimitardimitrov.info/
    last_name: Dimitrov
    name: Dimitar Dimitrov
    orcid: https://orcid.org/0000-0002-4504-5144
    username: ~Dimitar_Dimitrov3
  - dblp_id: https://dblp.org/pid/25/5562
    emails: '****@phil.hhu.de'
    first_name: Laura
    google_scholar_id: https://scholar.google.de/citations?user=gmFgdBwAAAAJ&hl=de&oi=ao
    homepage: https://user.phil.hhu.de/kallmeyer/
    institution: "Heinrich Heine University D\xFCsseldorf, Germany"
    last_name: Kallmeyer
    name: Laura Kallmeyer
    orcid: https://orcid.org/0000-0001-9691-5990
    username: ~Laura_Kallmeyer1
  - dblp_id: https://dblp.org/pid/95/1380
    emails: '****@hhu.de'
    first_name: Kilian
    google_scholar_id: https://scholar.google.com/citations?user=Y3C2bpIAAAAJ&hl=en
    homepage: https://kilian.evang.name
    institution: "Heinrich Heine University D\xFCsseldorf"
    last_name: Evang
    name: Kilian Evang
    orcid: https://orcid.org/0000-0003-0895-9841
    semantic_scholar_id: https://www.semanticscholar.org/author/Kilian-Evang/49765029
    username: ~Kilian_Evang1
  - dblp_id: https://dblp.org/pid/93/7184
    emails: '****@gmail.com'
    first_name: Hajira
    google_scholar_id: https://scholar.google.com/citations?user=52Q9BbMAAAAJ&hl=en&oi=ao
    homepage: https://hajirajabeen.github.io/
    institution: University of Cologne
    last_name: Jabeen
    name: Hajira Jabeen
    orcid: https://orcid.org/0000-0003-1476-2121
    username: ~Hajira_Jabeen1
  - dblp_id: https://dblp.org/pid/25/5167
    emails: '****@l3s.de'
    first_name: Stefan
    google_scholar_id: https://scholar.google.de/citations?user=WR3U5SkAAAAJ&hl=en
    homepage: http://stefandietze.net
    institution: "GESIS  and Heinrich-Heine-University D\xFCsseldorf"
    last_name: Dietze
    name: Stefan Dietze
    username: ~Stefan_Dietze1
  decision: toMainConference
  end_page: 3925
  file: 350.pdf
  id: 350
  num_pages: 11
  openreview_id: oefoQylQnz
  pdf_file: c9d6c2fba6c358d1d4f9df073e2cb93fe398fae3.pdf
  start_page: 3915
  title: 'Dissecting Paraphrases: The Impact of Prompt Syntax and supplementary Information
    on Knowledge Retrieval from Pretrained Language Models'
- abstract: 'In this work, we explicitly show that modern LLMs tend to generate correct
    facts first, then "drift away" and generate incorrect facts later: this was occasionally
    observed but never properly measured. We develop a semantic drift score that measures
    the degree of separation between correct and incorrect facts in generated texts
    and confirm our hypothesis when generating Wikipedia-style biographies. This correct-then-incorrect
    generation pattern suggests that factual accuracy can be improved by knowing when
    to stop generation. Therefore, we explore the trade-off between information quantity
    and factual accuracy for several early stopping methods and manage to improve
    factuality by a large margin. We further show that reranking with semantic similarity
    can further improve these results, both compared to the baseline and when combined
    with early stopping. Finally, we try calling external API to bring the model back
    to the right generation path, but do not get positive results. Overall, our methods
    generalize and can be applied to any long-form text generation to produce more
    reliable information, by balancing trade-offs between factual accuracy, information
    quantity and computational cost.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Generation
  authors:
  - emails: '****@gmail.com'
    first_name: Ava
    homepage: https://uk.linkedin.com/in/avaspataru
    institution: Meta
    last_name: Spataru
    name: Ava Spataru
    username: ~Ava_Spataru1
  decision: toMainConference
  end_page: 3941
  file: 352.pdf
  id: 352
  num_pages: 16
  openreview_id: 8p3FkStfHU
  pdf_file: f237cd4d3e58993e359df0dbbb783920b6998256.pdf
  start_page: 3926
  title: 'Know When To Stop: A Study of Semantic Drift in Text Generation'
- abstract: Many leading methods in Vision and language (V+L) pretraining utilize
    masked language modeling (MLM) as a standard pretraining component, with the expectation
    that reconstruction of masked text tokens would necessitate reference to corresponding
    image context via cross/self attention and thus promote representation fusion.
    However, we observe that the minimization of MLM loss in earlier training stages
    can depend disproportionately on local text signals, leading to poor training
    efficiency and inconsistency with the goal of representation fusion. The extent
    of this lack of cross modal interaction depends strongly which token(s) are masked.
    To address this issue, we propose a curriculum masking scheme as a replacement
    for random masking. Tokens are selected to be masked at a frequency proportional
    to the expected level of cross modal interaction necessary to reconstruct them.
    This is achieved using a parallel mask selection agent that measures the cross
    modal flow of information and treats it as a reward to be maximized. By additionally
    masking contiguous spans that include key objects and their relations, we also
    achieve better relational understanding, which has been shown to be lacking in
    many SOTA models. Our experiments on a wide range of V+L tasks show that we trail
    closely behind state-of-the-art methods despite pretraining on 300x to 1000x less
    data and we also achieve either top or runner-up performance on tasks from the
    ARO benchmark which tests compositional relationships. Finally, we demonstrate
    the potential of our method to scale to larger pretraining data.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multimodality and Language Grounding to Vision, Robotics and Beyond
  authors:
  - emails: '****@gmail.com'
    first_name: Kraig
    homepage: https://github.com/Bred-For-Companionship
    last_name: Tou
    middle_name: Yuheng
    name: Kraig Yuheng Tou
    username: ~Kraig_Yuheng_Tou1
  - emails: '****@outlook.com'
    first_name: Zijun
    homepage: https://github.com/DS-KB-lab
    last_name: Sun
    name: Zijun Sun
    username: ~Zijun_Sun2
  decision: toMainConference
  end_page: 3958
  file: 355.pdf
  id: 355
  num_pages: 17
  openreview_id: UvSTRirFba
  pdf_file: a16f7f385ddd450a8a63100a1c7f9437e6446e7a.pdf
  start_page: 3942
  title: Curriculum Masking in Vision-Language Pretraining to Maximize Cross Modal
    Interaction
- abstract: 'Spanish is one of the most widespread languages: the official language
    in 20 countries and the second most-spoken native language. Its contact with other
    languages across different regions and the rich regional and cultural diversity
    has produced varieties which divert from each other, particularly in terms of
    lexicon. Still, available corpora, and models trained upon them, generally treat
    Spanish as one monolithic language, which dampers prediction and generation power
    when dealing with different varieties. To alleviate the situation, we compile
    and curate datasets in the different varieties of Spanish around the world at
    an unprecedented scale and create the CEREAL corpus. With such a resource at hand,
    we perform a stylistic analysis to identify and characterise varietal differences.
    We implement a classifier specially designed to deal with long documents and identify
    Spanish varieties (and therefore expand CEREAL further). We produce varietal-specific
    embeddings, and analyse the cultural differences that they encode. We make data,
    code and models publicly available.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: 'Special Theme: Languages of Latin America'
  authors:
  - dblp_id: https://dblp.org/pid/59/7935
    emails: '****@gmail.com'
    first_name: Cristina
    homepage: https://www.cs.upc.edu/~cristinae/CV/cv.php
    institution: German Research Center for AI
    last_name: "Espa\xF1a-Bonet"
    name: "Cristina Espa\xF1a-Bonet"
    orcid: https://orcid.org/0000-0001-5414-4710
    username: "~Cristina_Espa\xF1a-Bonet1"
  - dblp_id: https://dblp.uni-trier.de/pers/hd/b/Barr=oacute=n=Cede=ntilde=o:Alberto
    emails: '****@gmail.com'
    first_name: Alberto
    google_scholar_id: https://scholar.google.it/citations?user=0q0QVG4AAAAJ&hl=en
    homepage: https://www.unibo.it/sitoweb/a.barron/en
    institution: "Universit\xE0 di Bologna"
    last_name: "Barr\xF3n-Cede\xF1o"
    name: "Alberto Barr\xF3n-Cede\xF1o"
    orcid: https://orcid.org/0000-0003-4719-3420
    username: "~Alberto_Barr\xF3n-Cede\xF1o1"
  decision: toMainConference
  end_page: 3981
  file: 356.pdf
  id: 356
  num_pages: 23
  openreview_id: 5E0wBKHDtR
  pdf_file: 82701f0dc2239e6a675743124527b72bec72e0b6.pdf
  start_page: 3959
  title: 'Elote, Choclo and Mazorca: on the Varieties of Spanish'
- abstract: 'At the heart of the Pyramid evaluation method for text summarization
    lie human written summary content units (SCUs). These SCUs are

    concise sentences that decompose a summary into small facts. Such SCUs can be
    used to judge the quality of a candidate summary, possibly partially automated
    via natural language inference (NLI) systems. Interestingly, with the aim to fully
    automate the Pyramid evaluation, Zhang and Bansal (2021) show that SCUs can be
    approximated by automatically generated semantic role triplets (STUs). However,
    several questions currently lack answers, in particular: i) Are there other ways
    of approximating SCUs that can offer advantages?ii) Under which conditions are
    SCUs (or their approximations) offering the most value? In this work, we examine
    two novel strategies

    to approximate SCUs: generating SCU approximations from AMR meaning representations
    (SMUs) and from large language models (SGUs), respectively. We find that while
    STUs and SMUs are competitive, the best approximation quality is achieved by SGUs.
    We also show through a simple sentence-decomposition baseline (SSUs) that SCUs
    (and their approximations) offer the most value when ranking

    short summaries, but may not help as much when ranking systems or longer summaries.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - emails: '****@t-online.de'
    first_name: Marcel
    last_name: Nawrath
    name: Marcel Nawrath
    username: ~Marcel_Nawrath1
  - emails: '****@gmail.com'
    first_name: Agnieszka
    last_name: Nowak
    middle_name: Wiktoria
    name: Agnieszka Wiktoria Nowak
    username: ~Agnieszka_Wiktoria_Nowak1
  - emails: '****@tristanratz.com'
    first_name: Tristan
    homepage: https://tristanratz.com
    last_name: Ratz
    name: Tristan Ratz
    username: ~Tristan_Ratz1
  - emails: '****@danilowalenta.com'
    first_name: Danilo
    homepage: https://www.danilowalenta.com
    last_name: Walenta
    middle_name: Constantin
    name: Danilo Constantin Walenta
    username: ~Danilo_Constantin_Walenta1
  - dblp_id: https://dblp.org/pid/185/5616
    emails: '****@gmail.com'
    first_name: Juri
    homepage: https://www.juriopitz.com
    institution: "Ruprecht-Karls-Universit\xE4t Heidelberg and University of Zurich"
    last_name: Opitz
    name: Juri Opitz
    semantic_scholar_id: https://www.semanticscholar.org/author/J.-Opitz/32781138
    username: ~Juri_Opitz2
  - dblp_id: https://dblp.org/pid/245/8769
    emails: '****@gmail.com'
    first_name: Leonardo
    google_scholar_id: https://scholar.google.com.br/citations?user=92j4_4wAAAAJ
    homepage: http://leoribeiro.github.io/
    institution: Amazon
    last_name: Ribeiro
    middle_name: F. R.
    name: Leonardo F. R. Ribeiro
    orcid: https://orcid.org/0000-0003-2639-942X
    semantic_scholar_id: https://www.semanticscholar.org/author/Leonardo-F.-R.-Ribeiro/10430740
    username: ~Leonardo_F._R._Ribeiro1
  - emails: '****@stern.nyu.edu'
    first_name: "Jo\xE3o"
    google_scholar_id: https://scholar.google.com/citations?user=vv355NgAAAAJ&hl=en&oi=sra
    institution: New York University
    last_name: Sedoc
    name: "Jo\xE3o Sedoc"
    username: "~Jo\xE3o_Sedoc1"
  - dblp_id: https://dblp.org/pid/222/9395
    emails: '****@google.com'
    first_name: Daniel
    google_scholar_id: https://scholar.google.com/citations?user=TQYzWDEAAAAJ&hl=en
    homepage: https://danieldeutsch.github.io/
    institution: Google
    last_name: Deutsch
    name: Daniel Deutsch
    semantic_scholar_id: https://www.semanticscholar.org/author/Daniel-Deutsch/145346875
    username: ~Daniel_Deutsch1
  - dblp_id: https://dblp.org/pid/75/7168
    emails: '****@adaptcentre.ie'
    first_name: Simon
    google_scholar_id: https://scholar.google.com/citations?user=F8kFik0AAAAJ&hl=en&authuser=1
    last_name: Mille
    name: Simon Mille
    orcid: https://orcid.org/0000-0002-8852-2764
    semantic_scholar_id: https://www.semanticscholar.org/author/Simon-Mille/2738095
    username: ~Simon_Mille1
  - dblp_id: https://dblp.org/pid/140/7348.html
    emails: '****@yale.edu'
    first_name: Yixin
    google_scholar_id: https://scholar.google.com/citations?user=sFtxaMkAAAAJ&hl=en
    homepage: https://yixinl7.github.io/
    institution: Yale University
    last_name: Liu
    name: Yixin Liu
    semantic_scholar_id: https://www.semanticscholar.org/author/Yixin-Liu/2108176413
    username: ~Yixin_Liu2
  - dblp_id: https://dblp.org/pid/131/1378
    emails: '****@outlook.com'
    first_name: Sebastian
    google_scholar_id: https://scholar.google.com/citations?user=R401sNwAAAAJ
    homepage: https://sebastiangehrmann.com
    institution: Bloomberg
    last_name: Gehrmann
    name: Sebastian Gehrmann
    username: ~Sebastian_Gehrmann1
  - emails: '****@nyu.edu'
    first_name: Lining
    homepage: https://lining-zhang.github.io/
    last_name: Zhang
    name: Lining Zhang
    semantic_scholar_id: https://www.semanticscholar.org/author/Lining-Zhang/48570960
    username: ~Lining_Zhang4
  - dblp_id: https://dblp.org/pid/73/1860
    emails: '****@mac.com'
    first_name: Saad
    google_scholar_id: https://scholar.google.nl/citations?user=VkW5WPEAAAAJ&hl=en
    homepage: https://saad.me.uk
    institution: 'trivago N.V. '
    last_name: Mahamood
    name: Saad Mahamood
    orcid: https://orcid.org/0000-0003-2332-8749
    semantic_scholar_id: https://www.semanticscholar.org/author/Saad-Mahamood/2221260
    username: ~Saad_Mahamood1
  - dblp_id: https://dblp.org/pid/281/0585
    emails: '****@hw.ac.uk'
    first_name: Miruna
    homepage: https://www.mirunaclinciu.com
    last_name: Clinciu
    name: Miruna Clinciu
    semantic_scholar_id: https://www.semanticscholar.org/author/Miruna-Adriana-Clinciu/2029314697
    username: ~Miruna_Clinciu1
  - dblp_id: https://dblp.org/pid/162/9017
    emails: '****@gmail.com'
    first_name: Khyathi
    google_scholar_id: https://scholar.google.com/citations?user=yl7DQ2MAAAAJ&hl=en
    homepage: http://www.cs.cmu.edu/~kchandu/
    last_name: Chandu
    name: Khyathi Chandu
    username: ~Khyathi_Chandu1
  - emails: '****@gmail.com'
    first_name: Yufang
    google_scholar_id: https://scholar.google.com/citations?user=-fBym-EAAAAJ&hl=en
    homepage: https://yufanghou.github.io/
    institution: "Technische Universit\xE4t Darmstadt and IBM Research Ireland"
    last_name: Hou
    name: Yufang Hou
    username: ~Yufang_Hou2
  decision: toMainConference
  end_page: 3991
  file: 357.pdf
  id: 357
  num_pages: 10
  openreview_id: RABRut8ENT
  pdf_file: 44f3883943d39ae9b320cad6da0b78f126e1253c.pdf
  start_page: 3982
  title: On the Role of Summary Content Units in Text Summarization Evaluation
- abstract: "Recently, the large language model (LLM) community has shown increasing\
    \ interest in enhancing LLMs' capability to handle extremely long documents. As\
    \ various long-text techniques and model architectures emerge, the precise and\
    \ detailed evaluation of models' long-text capabilities has become increasingly\
    \ important. Existing long-text evaluation benchmarks, such as L-Eval and LongBench,\
    \ construct long-text test sets based on open-source datasets, focusing mainly\
    \ on QA and summarization tasks. These datasets include test samples of varying\
    \ lengths (from 2k to 32k+) entangled together, making it challenging to assess\
    \ model capabilities across different length ranges. \nMoreover, they do not cover\
    \ the ultralong settings (100k+ tokens) that the latest LLMs claim to achieve.\
    \ In this paper, we introduce Ada-LEval, a length-adaptable benchmark for evaluating\
    \ the long-context understanding of LLMs. Ada-LEval includes two challenging subsets,\
    \ TSort and BestAnswer, which enable a more reliable evaluation of LLMs' long\
    \ context capabilities. These benchmarks support intricate manipulation of the\
    \ length of test cases, and can easily produce text samples up to 128k tokens.\
    \ We evaluate 4 state-of-the-art closed-source API models and 6 open-source models\
    \ with Ada-LEval. The evaluation results demonstrate the limitations of current\
    \ LLMs, especially in ultra-long-context settings. Our code is available at https://github.com/open-compass/Ada-LEval."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - emails: '****@qq.com'
    first_name: Chonghua
    homepage: https://philipwangovo.github.io/
    institution: Shanghai Jiaotong University
    last_name: Wang
    name: Chonghua Wang
    username: ~Chonghua_Wang1
  - dblp_id: https://dblp.org/pid/211/7919
    emails: '****@pku.edu.cn'
    first_name: Haodong
    google_scholar_id: https://scholar.google.com/citations?user=vi3W-m8AAAAJ&hl=en&oi=ao
    homepage: https://kennymckormick.github.io
    institution: The Chinese University of Hong Kong
    last_name: Duan
    name: Haodong Duan
    semantic_scholar_id: https://www.semanticscholar.org/author/Haodong-Duan/31463937
    username: ~Haodong_Duan1
  - dblp_id: https://dblp.org/pid/152/9228
    emails: '****@pjlab.org.cn'
    first_name: Songyang
    google_scholar_id: https://scholar.google.com/citations?user=8XQPi7YAAAAJ
    homepage: https://www.zhangsongyang.com/
    institution: Shanghai AI Laboratory
    last_name: Zhang
    name: Songyang Zhang
    username: ~Songyang_Zhang1
  - emails: '****@ie.cuhk.edu.hk'
    first_name: Dahua
    google_scholar_id: https://scholar.google.com/citations?user=GMzzRRUAAAAJ
    homepage: http://dahua.site
    institution: The Chinese University of Hong Kong
    last_name: Lin
    name: Dahua Lin
    username: ~Dahua_Lin1
  - dblp_id: https://dblp.org/pid/181/2839-26
    emails: '****@pjlab.org.cn'
    first_name: Kai
    google_scholar_id: https://scholar.google.com.hk/citations?user=eGD0b7IAAAAJ&hl=en
    homepage: https://chenkai.site/
    institution: Shanghai AI Laboratory
    last_name: Chen
    name: Kai Chen
    orcid: https://orcid.org/0000-0002-6820-2325
    semantic_scholar_id: https://www.semanticscholar.org/author/Kai-Chen/152568027
    username: ~Kai_Chen4
  decision: toMainConference
  end_page: 4004
  file: 359.pdf
  id: 359
  num_pages: 13
  openreview_id: l4rg3AWTD7
  pdf_file: 74609ea233b63d1eec3de6bceb5dda1f60e7e9e4.pdf
  start_page: 3992
  title: 'Ada-LEval: Evaluating long-context LLMs with length-adaptable benchmarks'
- abstract: 'Retrieval-augmented language models pose a promising alternative to standard
    language modeling. During pretraining, these models search in a corpus of documents
    for contextually relevant information that could aid the language modeling objective.
    We introduce an ''ideal retrieval'' methodology to study these models in a fully
    controllable setting. We conduct an extensive evaluation to examine how retrieval
    augmentation affects the behavior of the underlying language model. Among other
    things, we observe that these models: (i) save substantially less world knowledge
    in their weights, (ii) are better at understanding local context and inter-word
    dependencies, but (iii) are worse at comprehending global context.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - dblp_id: https://dblp.org/pid/160/7562
    emails: '****@ifi.uio.no'
    first_name: David
    google_scholar_id: https://scholar.google.com/citations?user=eh0roKUAAAAJ&hl=en
    homepage: https://www.mn.uio.no/ifi/english/people/aca/davisamu/index.html
    institution: University of Oslo
    last_name: Samuel
    name: David Samuel
    orcid: https://orcid.org/0000-0003-2866-1022
    semantic_scholar_id: https://www.semanticscholar.org/author/David-Samuel/2064321898
    username: ~David_Samuel1
  - dblp_id: https://dblp.org/pid/345/1961
    emails: '****@ifi.uio.no'
    first_name: Lucas
    homepage: https://www.mn.uio.no/ifi/english/people/aca/lgcharpe/
    institution: University of Oslo
    last_name: Charpentier
    middle_name: Georges Gabriel
    name: Lucas Georges Gabriel Charpentier
    semantic_scholar_id: https://www.semanticscholar.org/author/Lucas-Georges-Gabriel-Charpentier/2214718277
    username: ~Lucas_Georges_Gabriel_Charpentier1
  - dblp_id: https://dblp.org/pid/330/4280
    emails: '****@ifi.uio.no'
    first_name: Sondre
    homepage: https://www.mn.uio.no/ifi/english/people/aca/sondrewo/index.html
    last_name: Wold
    name: Sondre Wold
    username: ~Sondre_Wold1
  decision: toMainConference
  end_page: 4028
  file: 361.pdf
  id: 361
  num_pages: 24
  openreview_id: p6HQ7e4g0H
  pdf_file: 4ebd0f0047a8026d7fc7a79f670faea178d3728a.pdf
  start_page: 4005
  title: 'More room for language: Investigating the effect of retrieval on language
    models'
- abstract: Systematic Reviews (SRs) are foundational in healthcare for synthesising
    evidence to inform clinical practices. Traditionally skewed towards English-language
    databases, SRs often exclude significant research in other languages, leading
    to potential biases. This study addresses this gap by focusing on Spanish, a language
    notably underrepresented in SRs. We present a foundational zero-shot dual information
    retrieval (IR) baseline system, integrating traditional retrieval methods with
    pre-trained language models and cross-attention re-rankers for enhanced accuracy
    in Spanish biomedical literature retrieval. Utilising the LILACS database, known
    for its comprehensive coverage of Latin American and Caribbean biomedical literature,
    we evaluate the approach with three real-life case studies in Spanish SRs. The
    findings demonstrate the system's efficacy and underscore the importance of query
    formulation. This study contributes to the field of IR by promoting language inclusivity
    and supports the development of more comprehensive and globally representative
    healthcare guidelines.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: 'Special Theme: Languages of Latin America'
  authors:
  - emails: '****@rgu.ac.uk'
    first_name: Regina
    homepage: https://www.linkedin.com/in/regina-ofori-boateng-4437bb1a7/
    last_name: Ofori-Boateng
    name: Regina Ofori-Boateng
    username: ~Regina_Ofori-Boateng1
  - emails: '****@abdn.ac.uk'
    first_name: Magaly
    google_scholar_id: https://scholar.google.com/citations?user=2shb4JUAAAAJ&hl=en&authuser=1
    homepage: https://www.ma-mresearch.com
    institution: University of Aberdeen
    last_name: Aceves-Martins
    name: Magaly Aceves-Martins
    orcid: https://orcid.org/0000-0002-9441-142X
    username: ~Magaly_Aceves-Martins1
  - dblp_id: https://dblp.org/pid/74/507
    emails: '****@rgu.ac.uk'
    first_name: Nirmalie
    google_scholar_id: https://scholar.google.co.uk/citations?user=6M9aAAoAAAAJ&hl=en
    homepage: https://rgu-repository.worktribe.com/person/142640/nirmalie-wiratunga
    institution: The Robert Gordon University
    last_name: Wiratunga
    name: Nirmalie Wiratunga
    orcid: https://orcid.org/0000-0003-4040-2496
    semantic_scholar_id: https://www.semanticscholar.org/author/N.-Wiratunga/1784639
    username: ~Nirmalie_Wiratunga2
  - dblp_id: https://dblp.uni-trier.de/pid/150/4768.html
    emails: '****@rgu.ac.uk'
    first_name: Carlos
    google_scholar_id: https://scholar.google.com/citations?user=G8DsySMAAAAJ
    homepage: http://cfmgcomputing.blogspot.com/p/home.html
    institution: The Robert Gordon University
    last_name: Moreno-Garcia
    middle_name: Francisco
    name: Carlos Francisco Moreno-Garcia
    orcid: https://orcid.org/0000-0001-7218-9023
    username: ~Carlos_Francisco_Moreno-Garcia1
  decision: toMainConference
  end_page: 4040
  file: 363.pdf
  id: 363
  num_pages: 12
  openreview_id: zC29MoBVZW
  pdf_file: 9783bfc1ce24a6e15d17e31f816318101278fd0c.pdf
  start_page: 4029
  title: A Zero-Shot Monolingual Dual Stage Information Retrieval System for Spanish
    Biomedical Systematic Literature Reviews
- abstract: 'Visual Information Extraction (VIE), as a crucial task of Document Intelligence,
    involves two primary sub-tasks: Semantic Entity Recognition (SER) and Relation
    Extraction (RE). However, VIE faces two significant challenges. Firstly, most
    existing models inadequately utilize spatial information of entities, often failing
    to predict connections or incorrectly linking spatially distant entities. Secondly,
    the improper input order of tokens challenges in extracting complete entity pairs
    from documents with multi-line entities when text is extracted via PDF parser
    or OCR. To address these challenges, we propose LayoutPointer, a Spatial-Context
    Adaptive Pointer Network. LayoutPointer explicitly enhances spatial-context relationships
    by incorporating 2D relative position information and adaptive spatial constraints
    within self-attention. Furthermore, we recast the RE task as a specialized cycle
    detection problem, employing a unique tail-to-head pointer to restore the semantic
    order among multi-line entities. To better evaluate the effectiveness of our proposed
    method, we reconstruct a multi-line dataset named MLFUD, which more accurately
    reflects real-world scenarios. Fine-tuning experimental results on FUNSD, XFUND,
    and MLFUD datasets demonstrate that LayoutPointer significantly outperforms existing
    state-of-the-art methods in F1 scores for RE tasks (e.g., 5.71% improvement on
    XFUND using LayoutPointer$_{\text{BASE-X}}$ over LayoutLMv3).'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Extraction
  authors:
  - emails: '****@bupt.edu.cn'
    first_name: Huang
    homepage: https://github.com/ThinkSYR
    last_name: Siyuan
    name: Huang Siyuan
    username: ~Huang_Siyuan1
  - emails: '****@163.com'
    first_name: Yongping
    homepage: https://teacher.bupt.edu.cn/xiongyongping/zh_CN/index.htm
    institution: Beijing University of Posts and Telecommunications
    last_name: Xiong
    name: yongping xiong
    username: ~yongping_xiong1
  - emails: '****@czu.edu.cn'
    first_name: Wu
    homepage: https://github.com/WuGuibin
    institution: Chizhou University
    last_name: Guibin
    name: Wu Guibin
    orcid: https://orcid.org/0000-0001-9530-344X
    username: ~Wu_Guibin1
  decision: toMainConference
  end_page: 4052
  file: 364.pdf
  id: 364
  num_pages: 12
  openreview_id: uwCiPsuwQ3
  pdf_file: 788493560c8fe5440f3c789716085f929de54ea5.pdf
  start_page: 4041
  title: 'LayoutPointer: A Spatial-Context Adaptive Pointer Network for Visual Information
    Extraction'
- abstract: Evaluations of model editing, a technique for changing the factual knowledge
    held by Large Language Models (LLMs), currently only use the 'next few token'
    completions after a prompt. As a result, the impact of these methods on longer
    natural language generation is largely unknown.  We introduce long-form evaluation
    of model editing ($\textbf{\textit{LEME}}$) a novel evaluation protocol that measures
    the efficacy and impact of model editing in long-form generative settings. Our
    protocol consists of a machine-rated survey and a classifier which correlates
    well with human ratings. Importantly, we find that our protocol has very little
    relationship with previous short-form metrics (despite being designed to extend
    efficacy, generalization, locality, and portability into a long-form setting),
    indicating that our method introduces a novel set of dimensions for understanding
    model editing methods. Using this protocol, we benchmark a number of model editing
    techniques and present several findings including that, while some methods (ROME
    and MEMIT) perform well in making consistent edits within a limited scope, they
    suffer much more from factual drift than other methods. Finally, we present a
    qualitative analysis that illustrates common failure modes in long-form generative
    settings including internal consistency, lexical cohesion, and locality issues.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - dblp_id: https://dblp.org/pid/204/8129
    emails: '****@dal.ca'
    first_name: Domenic
    google_scholar_id: https://scholar.google.com/citations?user=80aJAKYAAAAJ&hl=en
    homepage: https://domenicrosati.github.io/
    institution: Dalhousie University and scite.ai
    last_name: Rosati
    name: Domenic Rosati
    orcid: https://orcid.org/0000-0003-2666-7615
    semantic_scholar_id: https://www.semanticscholar.org/author/Domenic-Rosati/24895235
    username: ~Domenic_Rosati2
  - emails: '****@dal.ca'
    first_name: Robie
    homepage: https://github.com/robzeh
    institution: Dalhousie University
    last_name: Gonzales
    name: Robie Gonzales
    username: ~Robie_Gonzales1
  - emails: '****@dal.ca'
    first_name: Jinkun
    homepage: https://jinkunchen.com
    institution: Dalhousie University
    last_name: Chen
    name: Jinkun Chen
    username: ~Jinkun_Chen1
  - emails: '****@dal.ca'
    first_name: Xuemin
    homepage: https://xueminyu.com/
    institution: Dalhousie University
    last_name: Yu
    name: Xuemin Yu
    orcid: https://orcid.org/0000-0002-9388-7282
    username: ~Xuemin_Yu1
  - emails: '****@dal.ca'
    first_name: Yahya
    homepage: https://github.com/Yahyakiani
    last_name: Kayani
    name: Yahya Kayani
    username: ~Yahya_Kayani1
  - dblp_id: https://dblp.org/pid/36/6505
    emails: '****@dal.ca'
    first_name: Frank
    google_scholar_id: https://scholar.google.ca/citations?user=elXOB1sAAAAJ&hl=en
    homepage: http://www.cs.toronto.edu/~frank
    institution: Dalhousie University
    last_name: Rudzicz
    name: Frank Rudzicz
    orcid: https://orcid.org/0000-0002-1139-3423
    semantic_scholar_id: https://www.semanticscholar.org/author/Frank-Rudzicz/2479037
    username: ~Frank_Rudzicz2
  - dblp_id: https://dblp.org/pid/73/5938
    emails: '****@gmail.com'
    first_name: Hassan
    google_scholar_id: https://scholar.google.de/citations?user=t3BH6NkAAAAJ&hl=en
    homepage: https://hsajjad.github.io/
    institution: Dalhousie University
    last_name: Sajjad
    name: Hassan Sajjad
    username: ~Hassan_Sajjad1
  decision: toMainConference
  end_page: 4084
  file: 365.pdf
  id: 365
  num_pages: 32
  openreview_id: LaOJOwIxKz
  pdf_file: e0b01332b890edb068f9e49280fd986f924b5211.pdf
  start_page: 4053
  title: Long-form evaluation of model editing
- abstract: 'Traditionally, natural language processing (NLP) models often use a rich
    set of features created by linguistic expertise, such as semantic representations.
    However, in the era of large language models (LLMs), more and more tasks are turned
    into generic, end-to-end sequence generation problems. In this paper, we investigate
    the question: what is the role of semantic representations in the era of LLMs?
    Specifically, we investigate the effect of Abstract Meaning Representation (AMR)
    across five diverse NLP tasks. We propose an AMR-driven chain-of-thought prompting
    method, which we call AMRCOT, and find that it generally hurts performance more
    than it helps. To investigate what AMR may have to offer on these tasks, we conduct
    a series of analysis experiments. We find that it is difficult to predict which
    input examples AMR may help or hurt on, but errors tend to arise with multi-word
    expressions, named entities, and in the final inference step where the LLM must
    connect its reasoning over the AMR to its prediction. We recommend focusing on
    these areas for future work in semantic representations for LLMs. Our code: https://github.com/causalNLP/amr_llm'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - dblp_id: https://dblp.org/pid/229/4267
    emails: '****@umich.edu'
    first_name: Zhijing
    google_scholar_id: https://scholar.google.com/citations?user=RkI8h-wAAAAJ
    homepage: https://zhijing-jin.com
    last_name: Jin
    name: Zhijing Jin
    orcid: https://orcid.org/0000-0003-0238-9024
    semantic_scholar_id: https://www.semanticscholar.org/author/Zhijing-Jin/8752221
    username: ~Zhijing_Jin1
  - emails: '****@illinois.edu'
    first_name: Yuen
    homepage: https://ei.is.mpg.de/person/yuenchen
    institution: 'University of Illinois at Urbana-Champaign '
    last_name: Chen
    name: Yuen Chen
    username: ~Yuen_Chen1
  - dblp_id: https://dblp.org/pid/346/1160
    emails: '****@ethz.ch'
    first_name: Fernando
    google_scholar_id: https://scholar.google.com/citations?user=UDVZFY4AAAAJ&hl=en
    homepage: https://feradauto.github.io/
    last_name: Gonzalez Adauto
    name: Fernando Gonzalez Adauto
    orcid: https://orcid.org/0009-0008-8002-5342
    semantic_scholar_id: https://www.semanticscholar.org/author/Fernando-Gonzalez/2186876774
    username: ~Fernando_Gonzalez_Adauto1
  - dblp_id: https://dblp.org/pid/134/1248-4.html
    emails: '****@cs.cmu.edu'
    first_name: Jiarui
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=4y_7ImsAAAAJ
    homepage: https://jiarui-liu.github.io/
    last_name: Liu
    name: Jiarui Liu
    semantic_scholar_id: https://www.semanticscholar.org/author/Jiarui-Liu/146961917
    username: ~Jiarui_Liu1
  - emails: '****@umich.edu'
    first_name: Jiayi
    homepage: https://jiayizx.github.io/
    last_name: Zhang
    name: Jiayi Zhang
    username: ~Jiayi_Zhang5
  - dblp_id: https://dblp.org/pid/185/0981
    emails: '****@gmail.com'
    first_name: Julian
    google_scholar_id: https://scholar.google.com/citations?user=9DDOHR8AAAAJ
    homepage: https://julianmichael.org
    institution: New York University
    last_name: Michael
    name: Julian Michael
    orcid: https://orcid.org/0000-0002-5358-3102
    semantic_scholar_id: https://www.semanticscholar.org/author/Julian-Michael/38614754
    username: ~Julian_Michael1
  - dblp_id: https://dblp.org/pid/97/119
    emails: '****@tuebingen.mpg.de'
    first_name: Bernhard
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=DZ-fHPgAAAAJ
    homepage: https://ei.is.tuebingen.mpg.de/people/bs/
    institution: ELLIS Institute and Max Planck Institute for Intelligent Systems,
      Max-Planck Institute
    last_name: "Sch\xF6lkopf"
    name: "Bernhard Sch\xF6lkopf"
    orcid: https://orcid.org/0000-0002-8177-0925
    username: "~Bernhard_Sch\xF6lkopf1"
  - dblp_id: https://dblp.org/pid/15/4305
    emails: '****@gmail.com'
    first_name: Mona
    google_scholar_id: https://scholar.google.com.tw/citations?user=-y6SIhQAAAAJ
    homepage: https://www.seas.gwu.edu/~mtdiab/
    institution: Carnegie Mellon University and George Washington University
    last_name: Diab
    middle_name: T.
    name: Mona T. Diab
    username: ~Mona_T._Diab1
  decision: toMainConference
  end_page: 4102
  file: 367.pdf
  id: 367
  num_pages: 18
  openreview_id: Pkdw4ziQGD
  pdf_file: 839ce6de27df9f8b132a2e038ecfd719fc882975.pdf
  start_page: 4085
  title: Analyzing the Role of Semantic Representations in the Era of Large Language
    Models
- abstract: 'When applied to open-domain question answering, large language models
    (LLMs) frequently generate incorrect responses based on made-up facts, which are
    called \textit{hallucinations}. Retrieval augmented generation (RAG) is a promising
    strategy to avoid hallucinations, but it does not provide guarantees on its correctness.
    To address this challenge, we propose the Trustworthy Retrieval Augmented Question
    Answering, or *TRAQ*, which provides the first end-to-end statistical correctness
    guarantee for RAG. TRAQ uses conformal prediction, a statistical technique for
    constructing prediction sets that are guaranteed to contain the semantically correct
    response with high probability. Additionally, TRAQ leverages Bayesian optimization
    to minimize the size of the constructed sets. In an extensive experimental evaluation,
    we demonstrate that TRAQ provides the desired correctness guarantee while reducing
    prediction set size by 16.2% on average compared to an ablation. The implementation
    is available: [https://github.com/shuoli90/TRAQ](https://github.com/shuoli90/TRAQ).'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Learning for NLP
  authors:
  - dblp_id: https://dblp.org/pid/49/595
    emails: '****@seas.upenn.edu'
    first_name: Shuo
    google_scholar_id: https://scholar.google.com/citations?user=-QaDf40AAAAJ&hl=en
    last_name: Li
    name: Shuo Li
    username: ~Shuo_Li7
  - dblp_id: https://dblp.org/pid/119/1530-1
    emails: '****@postech.ac.kr'
    first_name: Sangdon
    google_scholar_id: https://scholar.google.com/citations?user=Vi2E2F4AAAAJ&hl=en
    homepage: https://sangdon.github.io/
    institution: POSTECH
    last_name: Park
    name: Sangdon Park
    username: ~Sangdon_Park1
  - dblp_id: https://dblp.org/pid/l/InsupLee.html
    emails: '****@cis.upenn.edu'
    first_name: Insup
    google_scholar_id: https://scholar.google.com/citations?user=qPlUgrgAAAAJ&hl=en
    homepage: https://www.cis.upenn.edu/~lee/
    institution: University of Pennsylvania
    last_name: Lee
    name: Insup Lee
    orcid: https://orcid.org/0000-0003-2672-1132
    username: ~Insup_Lee1
  - dblp_id: https://dblp.org/pid/21/11275
    emails: '****@seas.upenn.edu'
    first_name: Osbert
    google_scholar_id: https://scholar.google.com/citations?user=cxYepGkAAAAJ&hl=en
    homepage: http://obastani.github.io
    institution: University of Pennsylvania
    last_name: Bastani
    name: Osbert Bastani
    username: ~Osbert_Bastani1
  decision: toMainConference
  end_page: 4125
  file: 372.pdf
  id: 372
  num_pages: 23
  openreview_id: Ju0Ufkum7y
  pdf_file: 4f6863677d9cbe6da368e72ff27999f4c3d9a8e5.pdf
  start_page: 4103
  title: 'TRAQ: Trustworthy Retrieval Augmented Question Answering via Conformal Prediction'
- abstract: 'Decoding continuous language from brain activity is a formidable yet
    promising field of research. It is particularly significant for aiding people
    with speech disabilities to communicate through brain signals. This field addresses
    the complex task of mapping brain signals to text. The previous best attempt reverse-engineered
    this process in an indirect way: it began by learning to encode brain activity
    from text and then guided text generation by aligning with predicted brain responses.
    In contrast, we propose a simple yet effective method that guides text reconstruction
    by directly comparing them with the predicted text embeddings mapped from brain
    activities. Comprehensive experiments reveal that our method significantly outperforms
    the current state-of-the-art model, showing average improvements of 77% and 54%
    on BLEU and METEOR scores. We further validate the proposed modules through detailed
    ablation studies and case analyses and highlight a critical correlation: the more
    precisely we map brain activities to text embeddings, the better the text reconstruction
    results. Such insight can simplify the task of reconstructing language from brain
    activities for future work, emphasizing the importance of improving brain-to-text-embedding
    mapping techniques.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - emails: '****@mails.ucas.ac.cn'
    first_name: Xinpei
    homepage: https://ZhaoXinPei17.github.io/
    institution: University of the Chinese Academy of Sciences
    last_name: Zhao
    name: Xinpei Zhao
    username: ~Xinpei_Zhao1
  - dblp_id: https://dblp.org/pid/27/7733
    emails: '****@kuleuven.be'
    first_name: Jingyuan
    google_scholar_id: https://scholar.google.com/citations?user=aBnLE_EAAAAJ&hl=en
    last_name: Sun
    name: Jingyuan Sun
    username: ~Jingyuan_Sun1
  - dblp_id: https://dblp.org/pid/29/8236
    emails: '****@nlpr.ia.ac.cn'
    first_name: Shaonan
    google_scholar_id: https://scholar.google.com/citations?user=ydFT-G8AAAAJ&hl=zh-CN
    homepage: https://wangshaonan.github.io/
    last_name: Wang
    name: Shaonan Wang
    username: ~Shaonan_Wang1
  - emails: '****@ia.ac.cn'
    first_name: Jing
    google_scholar_id: https://scholar.google.com/citations?user=1r-L968AAAAJ&hl=zh-CN
    institution: Institute of automation, Chinese academy of science, Chinese Academy
      of Sciences
    last_name: Ye
    name: Jing Ye
    username: ~Jing_Ye3
  - emails: '****@163.com'
    first_name: Xhz
    institution: Chinese Academy of Sciences
    last_name: Xhz
    name: Xhz
    semantic_scholar_id: https://www.semanticscholar.org/author/Xiaohan-Zhang/2145564536
    username: ~Xhz1
  - dblp_id: https://dblp.org/pid/38/6093
    emails: '****@nlpr.ia.ac.cn'
    first_name: Chengqing
    google_scholar_id: https://scholar.google.com/citations?user=l8lvKOQAAAAJ&hl=zh-CN
    homepage: http://www.nlpr.ia.ac.cn/cip/english/zong.htm
    institution: Institute of automation, Chinese academy of science, Chinese Academy
      of Sciences
    last_name: Zong
    name: Chengqing Zong
    username: ~Chengqing_Zong1
  decision: toMainConference
  end_page: 4136
  file: 376.pdf
  id: 376
  num_pages: 11
  openreview_id: DCCC7qosnK
  pdf_file: 74d24574ba5e5e11d3b6e2cc4556e38621544b10.pdf
  start_page: 4126
  title: 'MapGuide: A Simple yet Effective Method to Reconstruct Continuous Language
    from Brain Activities'
- abstract: "Temporal expression (TE) normalization is a well-studied problem. However,\
    \ the predominately used rule-based systems are highly restricted to specific\
    \ settings, and upcoming machine learning approaches suffer from a lack of labeled\
    \ data. \nIn this work, we explore the feasibility of proprietary and open-source\
    \ large language models (LLMs) for TE normalization using in-context learning\
    \ to inject task, document, and example information into the model. \nWe explore\
    \ various sample selection strategies to retrieve the most relevant set of examples.\
    \ \nBy using a window-based prompt design approach, we can perform TE normalization\
    \ across sentences, while leveraging the LLM knowledge without training the model.\n\
    Our experiments show competitive results to models designed for this task. In\
    \ particular, our method achieves large performance improvements for non-standard\
    \ settings by dynamically including relevant examples during inference."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Extraction
  authors:
  - emails: '****@gmail.com'
    first_name: Akash
    google_scholar_id: https://scholar.google.com/citations?user=h3q5nD4AAAAJ&hl=en
    homepage: https://sites.google.com/iiitd.ac.in/akash-kumar-gautam/
    last_name: Gautam
    middle_name: Kumar
    name: Akash Kumar Gautam
    username: ~Akash_Kumar_Gautam1
  - dblp_id: https://dblp.org/pid/219/5288
    emails: '****@gmail.com'
    first_name: Lukas
    google_scholar_id: https://scholar.google.co.in/citations?user=yBM4CMcAAAAJ&hl=de
    institution: Robert Bosch GmbH, Bosch
    last_name: Lange
    name: Lukas Lange
    semantic_scholar_id: https://www.semanticscholar.org/author/Lukas-Lange/47665464
    username: ~Lukas_Lange1
  - dblp_id: https://dblp.org/pid/28/8510
    emails: '****@gmail.com'
    first_name: Jannik
    google_scholar_id: https://scholar.google.de/citations?user=aQjqBSsAAAAJ
    homepage: https://sites.google.com/view/jannikstroetgen
    institution: Karlsruhe University of Applied Sciences
    last_name: "Str\xF6tgen"
    name: "Jannik Str\xF6tgen"
    semantic_scholar_id: https://www.semanticscholar.org/author/Jannik-Strotgen/2013656
    username: "~Jannik_Str\xF6tgen1"
  decision: toMainConference
  end_page: 4146
  file: 377.pdf
  id: 377
  num_pages: 10
  openreview_id: xS2iLEFqR5
  pdf_file: e9c1346fc6920e441734842b7a123ea7d87b9777.pdf
  start_page: 4137
  title: Discourse-Aware In-Context Learning for Temporal Expression Normalization
- abstract: "Despite their general capabilities, LLMs still struggle on biomedical\n\
    NER tasks, which are difficult due to the presence of specialized terminology\
    \ and lack of training data. \nIn this work we set out to improve LLM performance\
    \ on biomedical NER in limited data settings via a new knowledge augmentation\
    \ approach which incorporates definitions of relevant concepts on-the-fly. \n\
    During this process, to provide a test bed for knowledge augmentation, we perform\
    \ a comprehensive exploration of prompting strategies. \nOur experiments show\
    \ that definition augmentation is useful for both open source and closed LLMs.\n\
    For example, it leads to a relative improvement of 15\\% (on average) in GPT-4\
    \ performance (F1) across all (six) of our test datasets. \nWe conduct extensive\
    \ ablations and analyses to demonstrate that our performance improvements stem\
    \ from adding relevant definitional knowledge. \nWe find that careful prompting\
    \ strategies also improve LLM performance, allowing them to outperform fine-tuned\
    \ language models in few-shot settings.  \nTo facilitate future research in this\
    \ direction, we release our code at https://github.com/allenai/beacon."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Extraction
  authors:
  - emails: '****@gmail.com'
    first_name: Monica
    homepage: https://monicamunnangi.github.io/
    institution: Northeastern University
    last_name: Munnangi
    name: Monica Munnangi
    username: ~Monica_Munnangi1
  - dblp_id: https://dblp.org/pid/81/8052
    emails: '****@gmail.com'
    first_name: Sergey
    google_scholar_id: https://scholar.google.com/citations?user=C6-OMDIAAAAJ&hl=en&oi=ao
    homepage: http://www.data-cowboys.com
    institution: Allen Institute for Artificial Intelligence and Data Cowboys
    last_name: Feldman
    name: Sergey Feldman
    orcid: https://orcid.org/0000-0003-0386-0922
    semantic_scholar_id: https://www.semanticscholar.org/author/Sergey-Feldman/46411828
    username: ~Sergey_Feldman1
  - dblp_id: https://dblp.org/pid/00/8247
    emails: '****@northeastern.edu'
    first_name: Byron
    google_scholar_id: https://scholar.google.com/citations?user=KTzRHmwAAAAJ&hl=en
    homepage: http://www.byronwallace.com/
    institution: Northeastern University, Brown University and Northeastern University
    last_name: Wallace
    middle_name: C
    name: Byron C Wallace
    username: ~Byron_C_Wallace1
  - dblp_id: https://dblp.org/pid/150/5339
    emails: '****@northeastern.edu'
    first_name: Silvio
    google_scholar_id: https://scholar.google.com/citations?user=jhUQvC0AAAAJ&hl
    homepage: https://samiroid.github.io/
    institution: Northeastern University
    last_name: Amir
    name: Silvio Amir
    orcid: https://orcid.org/0000-0002-0559-8633
    username: ~Silvio_Amir1
  - dblp_id: https://dblp.org/pid/27/5588
    emails: '****@allenai.org'
    first_name: Tom
    institution: Allen Institute for Artificial Intelligence and Hebrew University,
      Hebrew University of Jerusalem
    last_name: Hope
    name: Tom Hope
    username: ~Tom_Hope2
  - dblp_id: https://dblp.org/pid/204/7137
    emails: '****@gmail.com'
    first_name: Aakanksha
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=izvklcoAAAAJ
    homepage: http://www.cs.cmu.edu/~anaik/
    institution: Allen Institute for Artificial Intelligence and National Institutes
      of Health
    last_name: Naik
    name: Aakanksha Naik
    semantic_scholar_id: https://www.semanticscholar.org/author/Aakanksha-Naik/23175870
    username: ~Aakanksha_Naik1
  decision: toMainConference
  end_page: 4168
  file: 378.pdf
  id: 378
  num_pages: 22
  openreview_id: srVxy8gblv
  pdf_file: a6581b40284c2275bb6967756d24f6d36f9d6d57.pdf
  start_page: 4147
  title: On-the-fly Definition Augmentation of LLMs for Biomedical NER
- abstract: 'Do the Spratly Islands belong to China, the Philippines, or Vietnam?
    A pretrained large language model (LLM) may answer differently if asked in the
    languages of each claimant country: Chinese, Tagalog, or Vietnamese. This contrasts
    with a multilingual human, who would likely answer consistently. In this paper,
    we show that LLMs recall certain geographical knowledge inconsistently when queried
    in different languages---a phenomenon we term geopolitical bias. As a targeted
    case study, we consider territorial disputes, an inherently controversial and
    multilingual task. We introduce BorderLines, a dataset of territorial disputes
    which covers 251 territories, each associated with a set of multiple-choice questions
    in the languages of each claimant country (49 languages in total). We also propose
    a suite of evaluation metrics to precisely quantify bias and consistency in responses
    across different languages. We then evaluate various multilingual LLMs on our
    dataset and metrics to probe their internal knowledge and use the proposed metrics
    to discover numerous inconsistencies in how these models respond in different
    languages. Finally, we explore several prompt modification strategies, aiming
    to either amplify or mitigate geopolitical bias, which highlights how brittle
    LLMs are and how they tailor their responses depending on cues from the interaction
    context. Our code and data are available at https://github.com/manestay/borderlines.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Computational Social Science and Cultural Analytics
  authors:
  - dblp_id: https://dblp.org/pid/243/6637
    emails: '****@seas.upenn.edu'
    first_name: Bryan
    homepage: https://manestay.github.io/
    institution: University of Pennsylvania
    last_name: Li
    name: Bryan Li
    semantic_scholar_id: https://www.semanticscholar.org/author/82630293
    username: ~Bryan_Li1
  - dblp_id: https://dblp.org/pid/219/5521
    emails: '****@seas.upenn.edu'
    first_name: Samar
    google_scholar_id: https://scholar.google.com/citations?user=14Pc5EkAAAAJ&hl=en
    homepage: https://samarh.github.io/
    institution: University of Pennsylvania
    last_name: Haider
    name: Samar Haider
    semantic_scholar_id: https://www.semanticscholar.org/author/Samar-Haider/40429233
    username: ~Samar_Haider1
  - emails: '****@cis.upenn.edu'
    first_name: Chris
    google_scholar_id: https://scholar.google.com/citations?user=nv-MV58AAAAJ&hl=en
    homepage: https://www.cis.upenn.edu/~ccb/
    institution: Allen Institute for Artificial Intelligence and University of Pennsylvania
    last_name: Callison-Burch
    name: Chris Callison-Burch
    semantic_scholar_id: https://www.semanticscholar.org/author/Chris-Callison-Burch/1763608
    username: ~Chris_Callison-Burch1
  decision: toMainConference
  end_page: 4185
  file: 382.pdf
  id: 382
  num_pages: 17
  openreview_id: RncU7agI1S
  pdf_file: 5a42df61df06b131ca6b4d16d228cb4c68b32cb4.pdf
  start_page: 4169
  title: 'This Land is {Your, My} Land: Evaluating Geopolitical Bias in Language Models
    through Territorial Disputes'
- abstract: 'Automatic assessment of the quality of arguments has been recognized
    as a challenging task with significant implications for misinformation and targeted
    speech. While real-world arguments are tightly anchored in context, existing computational
    methods analyze their quality in isolation, which affects their accuracy and generalizability.
    We propose SPARK: a novel method for scoring argument quality based on contextualization
    via relevant knowledge. We devise four augmentations that leverage large language
    models to provide feedback, infer hidden assumptions, supply a similar-quality
    argument, or give a counter-argument. SPARK uses a dual-encoder Transformer architecture
    to enable the original argument and its augmentation to be considered jointly.
    Our experiments in both in-domain and zero-shot setups show that SPARK consistently
    outperforms existing techniques across multiple metrics'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Sentiment Analysis, Stylistic Analysis, and Argument Mining
  authors:
  - emails: '****@gmail.com'
    first_name: Darshan
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=Zsk97sEAAAAJ
    homepage: http://darshandeshpande.github.io
    last_name: Deshpande
    middle_name: Girish
    name: Darshan Girish Deshpande
    username: ~Darshan_Girish_Deshpande1
  - emails: '****@isi.edu'
    first_name: Zhivar
    google_scholar_id: https://scholar.google.com/citations?user=giqWNAwAAAAJ&hl=en
    homepage: https://zhpinkman.github.io/
    last_name: Sourati
    name: Zhivar Sourati
    username: ~Zhivar_Sourati1
  - dblp_id: https://dblp.org/pid/167/4770
    emails: '****@vu.nl'
    first_name: Filip
    google_scholar_id: https://scholar.google.com/citations?user=4ZScBc0AAAAJ&hl=en
    homepage: http://www.ilievski.info
    institution: Vrije Universiteit Amsterdam
    last_name: Ilievski
    name: Filip Ilievski
    username: ~Filip_Ilievski1
  - dblp_id: https://dblp.org/pid/51/9687
    emails: '****@isi.edu'
    first_name: Fred
    homepage: http://fred.science
    institution: University of Southern California and USC/ISI
    last_name: Morstatter
    name: Fred Morstatter
    username: ~Fred_Morstatter1
  decision: toMainConference
  end_page: 4196
  file: 385.pdf
  id: 385
  num_pages: 11
  openreview_id: ppysTlLJJG
  pdf_file: 282d867d82ba6a21efac3821641ca7b17aa44e45.pdf
  start_page: 4186
  title: Contextualizing Argument Quality Assessment with Relevant Knowledge
- abstract: "Event temporal graphs have been shown as con\nvenient and effective representations\
    \ of com\nplex temporal relations between events in text.\n Recent studies, which\
    \ employ pre-trained lan\nguage models to auto-regressively generate lin\nearised\
    \ graphs for constructing event temporal\n graphs, have shown promising results.\
    \ How\never, these methods have often led to subopti\nmal graph generation as\
    \ the linearised graphs\n exhibit set characteristics which are instead\n treated\
    \ sequentially by language models. This\n discrepancy stems from the conventional\
    \ text\n generation objectives, leading to erroneous pe\nnalisation of correct\
    \ predictions caused by the\n misalignment of elements in target sequences.\n\
    \ To address these challenges, we reframe the\n task as a conditional set generation\
    \ problem,\n proposing a Set-aligning Framework tailored\n for the effective utilisation\
    \ of Large Language\n Models (LLMs). The framework incorporates\n data augmentations\
    \ and set-property regularisa\ntions designed to alleviate text generation loss\n\
    \ penalties associated with the linearised graph\n edge sequences, thus encouraging\
    \ the genera\ntion of more relation edges. Experimental re\nsults show that our\
    \ framework surpasses exist\ning baselines for event temporal graph genera\ntion.\
    \ Furthermore, under zero-shot settings, the\n structural knowledge introduced\
    \ through our\n framework notably improves model generalisa\ntion, particularly\
    \ when the training examples\n available are limited."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Extraction
  authors:
  - dblp_id: https://dblp.org/pid/197/4435
    emails: '****@warwick.ac.uk'
    first_name: Xingwei
    homepage: https://warwick.ac.uk/fac/sci/dcs/people/u2017689
    institution: University of Warwick
    last_name: Tan
    name: Xingwei Tan
    semantic_scholar_id: https://www.semanticscholar.org/author/Xingwei-Tan/30242744
    username: ~Xingwei_Tan1
  - dblp_id: https://dblp.org/pid/27/10149
    emails: '****@gmail.com'
    first_name: Yuxiang
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=qvvJvNQAAAAJ
    homepage: https://zyxnlp.github.io/
    institution: King's College London
    last_name: Zhou
    name: Yuxiang Zhou
    semantic_scholar_id: https://www.semanticscholar.org/author/Yuxiang-Zhou/2110219066
    username: ~Yuxiang_Zhou3
  - dblp_id: https://dblp.org/pid/180/3707
    emails: '****@warwick.ac.uk'
    first_name: Gabriele
    google_scholar_id: https://scholar.google.com/citations?user=3jIiOFEAAAAJ&hl=en
    homepage: https://warwick.ac.uk/fac/sci/dcs/people/u1898418/
    institution: University of Warwick
    last_name: Pergola
    name: Gabriele Pergola
    semantic_scholar_id: https://www.semanticscholar.org/author/Gabriele-Pergola/46922295
    username: ~Gabriele_Pergola1
  - dblp_id: https://dblp.org/pid/75/5430
    emails: '****@kcl.ac.uk'
    first_name: Yulan
    google_scholar_id: https://scholar.google.co.uk/citations?user=SP9r32UAAAAJ&hl=en
    homepage: https://www.kcl.ac.uk/people/yulan-he
    institution: King's College London, University of London
    last_name: He
    name: Yulan He
    orcid: https://orcid.org/0000-0003-3948-5845
    semantic_scholar_id: https://www.semanticscholar.org/author/Yulan-He/1704133
    username: ~Yulan_He1
  decision: toMainConference
  end_page: 4217
  file: 387.pdf
  id: 387
  num_pages: 21
  openreview_id: Yy8iEj10Ml
  pdf_file: 95071999b92c2bd2b7fa53c09a58fc37758e78ef.pdf
  start_page: 4197
  title: Set-Aligning Framework for Auto-Regressive Event Temporal Graph Generation
- abstract: "Recent works have demonstrated success in controlling sentence attributes\
    \ (e.g., sentiment) and structure (e.g., syntactic structure) based on the diffusion\
    \ language model. A key component that drives the\nimpressive performance for\
    \ generating high-quality samples from noise is iteratively denoise for thousands\
    \ of steps. While beneficial, the complexity of starting from the noise and the\
    \ learning steps has limited its implementation to many NLP real-world applications.\
    \ This paper proposes Language Rectified Flow (LF).\nOur method is based on the\
    \ reformulation of the standard probabilistic flow models.\nLanguage rectified\
    \ flow learns (neural) ordinary differential\nequation models to transport between\
    \ the source distribution and the target distribution, hence\nproviding a unified\
    \ and effective solution to generative modeling and domain transfer.\nFrom the\
    \ source distribution, our language rectified flow yields fast simulation and\
    \ effectively decreases the inference time. \nExperiments on three challenging\
    \ fine-grained control tasks and multiple high-quality text editing show that\
    \ our method consistently outperforms its baselines. Extensive experiments and\
    \ ablation studies demonstrate that our method can be general, effective, and\
    \ beneficial for many NLP tasks."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Generation
  authors:
  - dblp_id: https://dblp.org/pid/84/3190.html
    emails: '****@utexas.edu'
    first_name: Shujian
    homepage: https://www.utexas.edu/
    institution: University of Texas, Austin
    last_name: Zhang
    name: Shujian Zhang
    semantic_scholar_id: https://www.semanticscholar.org/author/Shujian-Zhang/2107944048
    username: ~Shujian_Zhang1
  - dblp_id: https://dblp.org/pid/232/3021
    emails: '****@cs.utexas.edu'
    first_name: Lemeng
    google_scholar_id: https://scholar.google.ca/citations?user=PCDSl2sAAAAJ&hl=en
    homepage: https://sites.google.com/utexas.edu/wlm/home?authuser=1
    institution: Facebook and University of Texas, Austin
    last_name: Wu
    name: Lemeng Wu
    username: ~Lemeng_Wu1
  - dblp_id: https://dblp.org/pid/209/4862
    emails: '****@pku.edu.cn'
    first_name: Chengyue
    google_scholar_id: https://scholar.google.com/citations?user=AscakBgAAAAJ&hl=zh-CN
    institution: ut austin
    last_name: Gong
    name: Chengyue Gong
    username: ~Chengyue_Gong1
  - dblp_id: https://dblp.org/pid/228/7309
    emails: '****@gmail.com'
    first_name: Xingchao
    google_scholar_id: https://scholar.google.com/citations?user=VOTVE0UAAAAJ
    last_name: Liu
    name: Xingchao Liu
    username: ~Xingchao_Liu1
  decision: toMainConference
  end_page: 4230
  file: 388.pdf
  id: 388
  num_pages: 13
  openreview_id: fpps7XQLgo
  pdf_file: b34a6096b10edbb2d176f832850f7829440b1dc7.pdf
  start_page: 4218
  title: 'LanguageFlow: Advancing Diffusion Language Generation with Probabilistic
    Flows'
- abstract: 'The latest large language models (LMs) support increasingly longer contexts.
    While this trend permits using substantial amounts of text with SOTA LMs, requiring
    these large LMs to process potentially redundant or irrelevant data needlessly
    increases inference time and cost. To remedy this problem, we propose BLINDER,
    a method that leverages a small finetuned LM to sample the minimal set of input
    features that maximizes the performance of a downstream LM. BLINDER trains an
    LM with a value head to estimate the likelihood of optimal outputs from a downstream
    LM given an input. We evaluate BLINDER on embodied decision making tasks with
    notoriously verbose state descriptions: NetHack and robot planning. BLINDER reduces
    the length of LM actor input by 87% and 99% while improving task success rates
    by 158% and 54% on NetHack and robot planning respectively which represents substantial
    inference cost savings while actually increasing performance.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multimodality and Language Grounding to Vision, Robotics and Beyond
  authors:
  - emails: '****@gmail.com'
    first_name: Kolby
    google_scholar_id: https://scholar.google.com/citations?user=2o3QdBAAAAAJ
    homepage: http://kolbynottingham.com
    last_name: Nottingham
    name: Kolby Nottingham
    username: ~Kolby_Nottingham1
  - dblp_id: https://dblp.dagstuhl.de/pid/277/6248.html
    emails: '****@uci.edu'
    first_name: Yasaman
    google_scholar_id: https://scholar.google.com/citations?user=YCtmdaMAAAAJ&hl=en
    homepage: https://yasamanrazeghi.com/
    last_name: Razeghi
    name: Yasaman Razeghi
    username: ~Yasaman_Razeghi1
  - dblp_id: https://dblp.org/pid/72/4634
    emails: '****@uci.edu'
    first_name: Kyungmin
    google_scholar_id: https://scholar.google.com/citations?hl=ko&user=5f47GEsAAAAJ
    homepage: https://sites.google.com/yonsei.ac.kr/kmkim
    institution: University of California, Irvine
    last_name: Kim
    name: Kyungmin Kim
    username: ~Kyungmin_Kim1
  - dblp_id: https://dblp.org/pid/242/9187
    emails: '****@gmail.com'
    first_name: JB
    google_scholar_id: https://scholar.google.com/citations?user=6IkG2m0AAAAJ&hl=en
    homepage: https://jblanier.net
    institution: University of California, Irvine
    last_name: Lanier
    name: JB Lanier
    username: ~JB_Lanier1
  - dblp_id: https://dblp.org/pid/54/1564
    emails: '****@ics.uci.edu'
    first_name: Pierre
    google_scholar_id: https://scholar.google.com/citations?user=RhFhIIgAAAAJ&hl=en&oi=ao
    homepage: https://www.igb.uci.edu/~pfbaldi/
    last_name: Baldi
    name: Pierre Baldi
    orcid: https://orcid.org/0000-0001-8752-4664
    username: ~Pierre_Baldi1
  - dblp_id: https://dblp.org/pid/32/7007
    emails: '****@gmail.com'
    first_name: Roy
    google_scholar_id: https://scholar.google.com/citations?user=FH9nKOAAAAAJ
    homepage: https://royf.org
    institution: University of California, Irvine
    last_name: Fox
    name: Roy Fox
    username: ~Roy_Fox1
  - dblp_id: https://dblp.org/pid/13/3568-1
    emails: '****@uci.edu'
    first_name: Sameer
    google_scholar_id: https://scholar.google.com/citations?user=-hGZC54AAAAJ
    homepage: http://sameersingh.org
    institution: University of California, Irvine and Allen Institute for Artificial
      Intelligence
    last_name: Singh
    name: Sameer Singh
    orcid: https://orcid.org/0000-0003-0621-6323
    semantic_scholar_id: https://www.semanticscholar.org/author/Sameer-Singh/34650964
    username: ~Sameer_Singh1
  decision: toMainConference
  end_page: 4245
  file: 390.pdf
  id: 390
  num_pages: 15
  openreview_id: D2IvS9ekzp
  pdf_file: c3deba906c7c6a40d61da434e0ada955de0e116a.pdf
  start_page: 4231
  title: 'Selective Perception: Learning Concise State Descriptions for Language Model
    Actors'
- abstract: Teaching large language models (LLMs) to generate text with attribution
    to evidence sources can reduce hallucinations, improve verifiability in question
    answering systems (QA), and increase reliability of retrieval augmented LLMs.
    Despite gaining increasing popularity for usage in QA systems and search engines,
    current LLMs struggle with attribution for long-form responses which require reasoning
    over multiple evidence sources. To address this, in this paper we aim to improve
    the attribution capability of LLMs for long-form answer generation to multiple
    sources, with multiple citations per sentence. However, data for training multi-source
    attributable QA systems is difficult and expensive to annotate, and therefore
    scarce. To overcome this challenge, we transform existing QA datasets for this
    task (MultiAttr), and empirically demonstrate, on a wide range of attribution
    benchmark datasets, that fine-tuning on MultiAttr provides significant improvements
    over training only on the target QA domain. Lastly, to fill a gap in existing
    benchmarks, we present a multi-source attribution dataset containing multi-paragraph
    answers, PolitiICite, based on PolitiFact articles that discuss events closely
    related to implementation statuses of election promises.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Question Answering
  authors:
  - emails: '****@ucsc.edu'
    first_name: Nilay
    last_name: Patel
    name: Nilay Patel
    username: ~Nilay_Patel1
  - dblp_id: https://dblp.org/pid/153/2817
    emails: '****@gmail.com'
    first_name: Shivashankar
    google_scholar_id: https://scholar.google.com/citations?user=yaOyQOQAAAAJ&hl=en
    institution: Amazon
    last_name: Subramanian
    name: Shivashankar Subramanian
    username: ~Shivashankar_Subramanian1
  - dblp_id: https://dblp.org/pid/82/8467
    emails: '****@gmail.com'
    first_name: Siddhant
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=V02t618AAAAJ
    homepage: https://sid7954.github.io/
    institution: Meta
    last_name: Garg
    name: Siddhant Garg
    semantic_scholar_id: https://www.semanticscholar.org/author/Siddhant-Garg/2295877
    username: ~Siddhant_Garg2
  - dblp_id: https://dblp.org/pid/245/8667
    emails: '****@asu.edu'
    first_name: Pratyay
    google_scholar_id: https://scholar.google.com/citations?user=CtnHOHoAAAAJ&hl=en
    homepage: http://pratyay-banerjee.github.io/
    institution: Amazon
    last_name: Banerjee
    name: Pratyay Banerjee
    orcid: https://orcid.org/0000-0001-5634-410X
    semantic_scholar_id: https://www.semanticscholar.org/author/Pratyay-Banerjee/120722271
    username: ~Pratyay_Banerjee1
  - dblp_id: https://dblp.org/pid/165/0783.html
    emails: '****@gmail.com'
    first_name: Amita
    google_scholar_id: https://scholar.google.com/citations?user=8qHi6csAAAAJ&hl=en
    institution: Amazon
    last_name: Misra
    name: Amita Misra
    semantic_scholar_id: https://www.semanticscholar.org/author/Amita-Misra/2461655
    username: ~Amita_Misra2
  decision: toMainConference
  end_page: 4259
  file: 391.pdf
  id: 391
  num_pages: 14
  openreview_id: 3xadFg45u2
  pdf_file: 8c5991e55162987f82e4bfcafb577d007674a501.pdf
  start_page: 4246
  title: Towards Improved Multi-Source Attribution for Long-Form Answer Generation
- abstract: Despite recent advances in multimodal pre-training for visual description,
    state-of-the-art models still produce captions containing errors, such as hallucinating
    objects not present in a scene. The existing prominent metric for object hallucination,
    CHAIR, is limited to a fixed set of MS COCO objects and synonyms. In this work,
    we propose a modernized open-vocabulary metric, ALOHa, which leverages large language
    models (LLMs) to measure object hallucinations. Specifically, we use an LLM to
    extract groundable objects from a candidate caption, measure their semantic similarity
    to reference objects from captions and object detections, and use Hungarian matching
    to produce a final hallucination score. We show that ALOHa correctly identifies
    13.6% more hallucinated objects than CHAIR on HAT, a new gold-standard subset
    of MS COCO Captions annotated for hallucinations, and 30.8% more on nocaps, where
    objects extend beyond MS COCO categories.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multimodality and Language Grounding to Vision, Robotics and Beyond
  authors:
  - dblp_id: https://dblp.org/pid/262/3936
    emails: '****@berkeley.edu'
    first_name: Suzanne
    google_scholar_id: https://scholar.google.com/citations?user=nSpXpqMAAAAJ&hl=en&oi=ao
    homepage: https://suziepetryk.com
    institution: University of California Berkeley
    last_name: Petryk
    name: Suzanne Petryk
    semantic_scholar_id: https://www.semanticscholar.org/author/Suzanne-Petryk/52013156
    username: ~Suzanne_Petryk1
  - dblp_id: https://dblp.org/pid/80/9659
    emails: '****@berkeley.edu'
    first_name: David
    google_scholar_id: https://scholar.google.com/citations?user=qa4M89wAAAAJ&hl=en
    homepage: https://people.eecs.berkeley.edu/~davidchan/
    institution: University of California Berkeley
    last_name: Chan
    name: David Chan
    username: ~David_Chan3
  - emails: '****@berkeley.edu'
    first_name: Anish
    homepage: https://anishk.me
    last_name: Kachinthaya
    name: Anish Kachinthaya
    username: ~Anish_Kachinthaya1
  - emails: '****@berkeley.edu'
    first_name: Haodi
    homepage: https://haodi-zou.github.io/
    last_name: Zou
    name: Haodi Zou
    username: ~Haodi_Zou1
  - emails: '****@berkeley.edu'
    first_name: John
    google_scholar_id: https://scholar.google.com.tw/citations?user=LAv0HTEAAAAJ
    homepage: http://www.cs.berkeley.edu/~jfc/
    institution: University of California - Berkeley and University of California
      Berkeley
    last_name: Canny
    name: John Canny
    username: ~John_Canny1
  - dblp_id: https://dblp.org/pid/61/8262
    emails: '****@berkeley.edu'
    first_name: Joseph
    google_scholar_id: https://scholar.google.com.tw/citations?user=gM2WW9UAAAAJ
    homepage: http://eecs.berkeley.edu/~jegonzal
    institution: University of California - Berkeley, University of California-Berkeley
      and UC Berkeley, University of California Berkeley
    last_name: Gonzalez
    middle_name: E.
    name: Joseph E. Gonzalez
    username: ~Joseph_E._Gonzalez1
  - dblp_id: https://dblp.org/pid/d/TrevorDarrell
    emails: '****@gmail.com'
    first_name: Trevor
    google_scholar_id: https://scholar.google.com.tw/citations?user=bh-uRFMAAAAJ
    homepage: https://people.eecs.berkeley.edu/~trevor/
    institution: Electrical Engineering & Computer Science Department
    last_name: Darrell
    name: Trevor Darrell
    username: ~Trevor_Darrell2
  decision: toMainConference
  end_page: 4275
  file: 393.pdf
  id: 393
  num_pages: 16
  openreview_id: 44Vpv08cXi
  pdf_file: f5d74fa37f1de8268754b00479fb3f5b0281bcd2.pdf
  start_page: 4260
  title: 'ALOHa: A New Measure for Hallucination in Captioning Models'
- abstract: We address the challenge of ensuring differential privacy (DP) guarantees
    in training deep retrieval systems. Training these systems often involves the
    use of contrastive-style losses, which are typically non-per-example decomposable,
    making them difficult to directly DP-train with since common techniques require
    per-example gradients. To address this issue, we propose an approach that prioritizes
    ensuring query privacy prior to training a deep retrieval system. Our method employs
    DP language models (LMs) to generate private synthetic queries representative
    of the original data. These synthetic queries can be used in downstream retrieval
    system training without compromising privacy. Our approach demonstrates a significant
    enhancement in retrieval quality compared to direct DP-training, all while maintaining
    query-level privacy guarantees. This work highlights the potential of harnessing
    LMs to overcome limitations in standard DP-training methods.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Retrieval and Text Mining
  authors:
  - dblp_id: https://dblp.org/pid/228/6732
    emails: '****@stanford.edu'
    first_name: Aldo
    google_scholar_id: https://scholar.google.com/citations?user=2Xa6VpEAAAAJ&hl=en&oi=ao
    last_name: Carranza
    middle_name: Gael
    name: Aldo Gael Carranza
    username: ~Aldo_Gael_Carranza1
  - emails: '****@ualberta.ca'
    first_name: Rezsa
    google_scholar_id: https://scholar.google.com/citations?user=uF3ka7MAAAAJ&hl=en
    last_name: Farahani
    name: Rezsa Farahani
    username: ~Rezsa_Farahani1
  - emails: '****@google.com'
    first_name: Natalia
    google_scholar_id: https://scholar.google.com/citations?user=eIdQR5oAAAAJ&hl=en
    institution: Google
    last_name: Ponomareva
    name: Natalia Ponomareva
    username: ~Natalia_Ponomareva1
  - dblp_id: https://dblp.org/pid/56/9834
    emails: '****@kurakin.me'
    first_name: Alexey
    google_scholar_id: https://scholar.google.com/citations?user=nCh4qyMAAAAJ
    homepage: http://kurakin.me
    institution: Research, Google
    last_name: Kurakin
    name: Alexey Kurakin
    username: ~Alexey_Kurakin1
  - dblp_id: https://dblp.org/pid/218/5156
    emails: '****@google.com'
    first_name: Matthew
    google_scholar_id: https://scholar.google.com/citations?user=_8rw_GMAAAAJ&hl=en
    homepage: https://jagielski.github.io/
    institution: Google
    last_name: Jagielski
    name: Matthew Jagielski
    username: ~Matthew_Jagielski1
  - emails: '****@google.com'
    first_name: Milad
    google_scholar_id: https://scholar.google.com/citations?user=k6-nvDAAAAAJ&hl=en&oi=ao
    homepage: https://people.cs.umass.edu/~milad/
    institution: Google
    last_name: Nasr
    name: Milad Nasr
    username: ~Milad_Nasr2
  decision: toMainConference
  end_page: 4286
  file: 394.pdf
  id: 394
  num_pages: 11
  openreview_id: cQagVI0JF3
  pdf_file: de07fe63a80f8ac6491225c6f5172315cfc91625.pdf
  start_page: 4276
  title: Synthetic Query Generation for Privacy-Preserving Deep Retrieval Systems
    using Differentially Private Language Models
- abstract: In NLP, Event Coreference Resolution (ECR) is the task of connecting event
    clusters that refer to the same underlying real-life event, usually via neural
    systems. In this work, we investigate using abductive free-text rationales (FTRs)
    generated by modern autoregressive LLMs as distant supervision of smaller student
    models for cross-document coreference (CDCR) of events. We implement novel rationale-oriented
    event clustering and knowledge distillation methods for event coreference scoring
    that leverage enriched information from the FTRs for improved CDCR without additional
    annotation or expensive document clustering.  Our model using coreference-specific
    knowledge distillation achieves SOTA $B^3$ $F_1$ on the ECB+ and GVC corpora and
    we establish a new baseline on the AIDA Phase 1 corpus. Our code can be found
    at https://github.com/csu-signal/llama_cdcr.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Discourse and Pragmatics
  authors:
  - emails: '****@colostate.edu'
    first_name: Abhijnan
    google_scholar_id: https://scholar.google.com/citations?user=J9FdsyYAAAAJ&hl=en&oi=ao
    homepage: https://abhijnannath.github.io/abhijnan.github.io
    last_name: Nath
    name: Abhijnan Nath
    orcid: https://orcid.org/0000-0003-4276-7894
    semantic_scholar_id: https://www.semanticscholar.org/author/Abhijnan-Nath/2187454703
    username: ~Abhijnan_Nath1
  - dblp_id: https://dblp.org/pid/268/1520
    emails: '****@colostate.edu'
    first_name: Shadi
    google_scholar_id: https://scholar.google.com/citations?user=qGvKH8QAAAAJ&hl=en
    institution: Colorado State University
    last_name: Manafi Avari
    name: Shadi Manafi Avari
    semantic_scholar_id: https://www.semanticscholar.org/author/Shadi-Manafi-Avari/1752902528
    username: ~Shadi_Manafi_Avari1
  - emails: '****@gmail.com'
    first_name: Avyakta
    last_name: Chelle
    name: Avyakta Chelle
    username: ~Avyakta_Chelle1
  - dblp_id: https://dblp.org/pid/184/8937
    emails: '****@colostate.edu'
    first_name: Nikhil
    google_scholar_id: https://scholar.google.com/citations?user=IKboTYgAAAAJ
    homepage: https://www.nikhilkrishnaswamy.com
    institution: Colorado State University
    last_name: Krishnaswamy
    name: Nikhil Krishnaswamy
    orcid: https://orcid.org/0000-0001-7878-7227
    semantic_scholar_id: https://www.semanticscholar.org/author/Nikhil-Krishnaswamy/34079649
    username: ~Nikhil_Krishnaswamy1
  decision: toMainConference
  end_page: 4302
  file: 395.pdf
  id: 395
  num_pages: 16
  openreview_id: uaGK6Chu7B
  pdf_file: 97a60996b8816aac11406e138c63dc122036511d.pdf
  start_page: 4287
  title: Okay, Let's Do This! Modeling Event Coreference with Generated Rationales
    and Knowledge Distillation
- abstract: The contemporary LLMs are prone to producing hallucinations, stemming
    mainly from the knowledge gaps within the models. To address this critical limitation,
    researchers employ diverse strategies to augment the LLMs by incorporating external
    knowledge, aiming to reduce hallucinations and enhance reasoning accuracy. Among
    these strategies, leveraging knowledge graphs as a source of external information
    has demonstrated promising results. In this survey, we comprehensively review
    these knowledge-graph-based augmentation techniques in LLMs, focusing on their
    efficacy in mitigating hallucinations. We systematically categorize these methods
    into three overarching groups, offering methodological comparisons and performance
    evaluations. Lastly, this survey explores the current trends and challenges associated
    with these techniques and outlines potential avenues for future research in this
    emerging field.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Retrieval and Text Mining
  authors:
  - emails: '****@asu.edu'
    first_name: Garima
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=B7_IdrQAAAAJ
    last_name: Agrawal
    name: Garima Agrawal
    orcid: https://orcid.org/0000-0002-4383-7850
    username: ~Garima_Agrawal1
  - emails: '****@asu.edu'
    first_name: Tharindu
    google_scholar_id: https://scholar.google.com/citations?user=esDfeWQAAAAJ&hl=en&authuser=2
    institution: Arizona State University
    last_name: Kumarage
    middle_name: Sandaruwan
    name: Tharindu Sandaruwan Kumarage
    username: ~Tharindu_Sandaruwan_Kumarage1
  - emails: '****@asu.edu'
    first_name: Zeyad
    homepage: https://www.linkedin.com/me?trk=p_mwlite_feed_updates-secondary_nav
    last_name: Alghamdi
    name: Zeyad Alghamdi
    username: ~Zeyad_Alghamdi1
  - dblp_id: https://dblp.org/pid/92/309-1
    emails: '****@asu.edu'
    first_name: Huan
    google_scholar_id: https://scholar.google.com/citations?user=Dzf46C8AAAAJ&hl=en
    homepage: http://www.public.asu.edu/~huanliu
    institution: Arizona State University
    last_name: Liu
    name: huan liu
    orcid: https://orcid.org/0000-0002-3264-7904
    semantic_scholar_id: https://www.semanticscholar.org/author/Huan-Liu/38746648
    username: ~huan_liu1
  decision: toMainConference
  end_page: 4316
  file: 396.pdf
  id: 396
  num_pages: 14
  openreview_id: HT6YI3HNYA
  pdf_file: 41a4a44a9c0752f143c8fdaebfb8c30680b0a8cc.pdf
  start_page: 4303
  title: 'Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey'
- abstract: The cloze training objective of Masked Language Models makes them a natural
    choice for generating plausible distractors for human cloze questions. However,
    distractors must also be both distinct and incorrect, neither of which is directly
    addressed by existing neural methods. Evaluation of recent models has also relied
    largely on automated metrics, which cannot demonstrate the reliability or validity
    of human comprehension tests. In this work, we first formulate the pedagogically
    motivated objectives of plausibility, incorrectness, and distinctiveness in terms
    of conditional distributions from language models. Second, we present an unsupervised,
    interpretable method that uses these objectives to jointly optimize sets of distractors.
    Third, we test the reliability and validity of the resulting cloze tests compared
    to other methods with human participants. We find our method has stronger correlation
    with teacher-created comprehension tests than the state-of-the-art neural method
    and is more internally consistent. Our implementation is freely available and
    can quickly create a multiple choice cloze test from any given passage.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - dblp_id: https://dblp.org/pid/47/7065
    emails: '****@nih.gov'
    first_name: Brian
    google_scholar_id: https://scholar.google.com/citations?user=bPyA99MAAAAJ&hl=en
    homepage: http://ondov.net
    last_name: Ondov
    middle_name: David
    name: Brian David Ondov
    orcid: https://orcid.org/0000-0002-0740-1793
    username: ~Brian_David_Ondov1
  - emails: '****@alumni.wlu.edu'
    first_name: Kush
    last_name: Attal
    name: Kush Attal
    orcid: https://orcid.org/0000-0003-3635-9368
    username: ~Kush_Attal1
  - dblp_id: https://dblp.org/pid/59/2029
    emails: '****@gmail.com'
    first_name: Dina
    google_scholar_id: https://scholar.google.com/citations?user=ashaP54AAAAJ&hl=en
    homepage: https://www.nlm.nih.gov/research/researchstaff/DemnerFushmanDina.html
    institution: National Library of Medicine
    last_name: Demner-Fushman
    name: Dina Demner-Fushman
    orcid: https://orcid.org/0000-0002-4361-5799
    semantic_scholar_id: https://www.semanticscholar.org/author/Dina-Demner-Fushman/1398175407
    username: ~Dina_Demner-Fushman1
  decision: toMainConference
  end_page: 4328
  file: 397.pdf
  id: 397
  num_pages: 12
  openreview_id: CegCoIECLq
  pdf_file: 58a6e345969a53b27fb427ca5c5368fb2cedb520.pdf
  start_page: 4317
  title: Pedagogically aligned objectives create reliable automatic cloze tests
- abstract: Zero-shot text rankers powered by recent LLMs achieve remarkable ranking
    performance by simply prompting. Existing prompts for pointwise LLM rankers mostly
    ask the model to choose from binary relevance labels like "Yes" and "No". However,
    the lack of intermediate relevance label options may cause the LLM to provide
    noisy or biased answers for documents that are partially relevant to the query.
    We propose to incorporate fine-grained relevance labels into the prompt for LLM
    rankers, enabling them to better differentiate among documents with different
    levels of relevance to the query and thus derive a more accurate ranking. We study
    two variants of the prompt template, coupled with different numbers of relevance
    levels. Our experiments on 8 BEIR data sets show that adding fine-grained relevance
    labels significantly improves the performance of LLM rankers.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Retrieval and Text Mining
  authors:
  - dblp_id: https://dblp.org/pid/10/9988
    emails: '****@google.com'
    first_name: Honglei
    google_scholar_id: https://scholar.google.com/citations?user=FxEDj4wAAAAJ
    homepage: https://hongleizhuang.github.io/
    institution: Google Research
    last_name: Zhuang
    name: Honglei Zhuang
    orcid: https://orcid.org/0000-0001-8134-1509
    semantic_scholar_id: https://www.semanticscholar.org/author/Honglei-Zhuang/39371343
    username: ~Honglei_Zhuang1
  - dblp_id: https://dblp.org/pid/06/864-1
    emails: '****@gmail.com'
    first_name: Zhen
    google_scholar_id: https://scholar.google.com/citations?user=Kv1yk3YAAAAJ&hl=en
    homepage: http://alumni.cs.ucr.edu/~zqin001/
    institution: Google
    last_name: Qin
    name: Zhen Qin
    username: ~Zhen_Qin5
  - dblp_id: https://dblp.org/pid/37/10077
    emails: '****@gmail.com'
    first_name: Kai
    google_scholar_id: https://scholar.google.com/citations?user=VorTj3AAAAAJ&hl=en
    homepage: https://khui.github.io/
    institution: Google
    last_name: Hui
    name: Kai Hui
    orcid: https://orcid.org/0000-0002-3110-7404
    semantic_scholar_id: https://www.semanticscholar.org/author/Kai-Hui/47214884
    username: ~Kai_Hui1
  - dblp_id: https://dblp.org/pid/140/6764
    emails: '****@google.com'
    first_name: Junru
    google_scholar_id: https://scholar.google.com/citations?user=nBbGvyEAAAAJ&hl=en
    homepage: http://sandbox3aster.github.io/
    institution: Google Research
    last_name: Wu
    name: Junru Wu
    username: ~Junru_Wu2
  - emails: '****@google.com'
    first_name: Le
    google_scholar_id: https://scholar.google.com/citations?user=X_knTr4AAAAJ&hl=en&authuser=1
    institution: Google
    last_name: Yan
    name: Le Yan
    username: ~Le_Yan1
  - dblp_id: https://dblp.org/pid/67/2661
    emails: '****@google.com'
    first_name: Xuanhui
    institution: Google
    last_name: Wang
    name: Xuanhui Wang
    semantic_scholar_id: https://www.semanticscholar.org/author/Xuanhui-Wang/1526973500
    username: ~Xuanhui_Wang1
  - dblp_id: https://dblp.org/pid/80/4305
    emails: '****@google.com'
    first_name: Michael
    google_scholar_id: https://scholar.google.com/citations?user=C9mxM5IAAAAJ&hl=en
    homepage: http://bendersky.github.io/
    institution: Google
    last_name: Bendersky
    name: Michael Bendersky
    orcid: https://orcid.org/0000-0002-2941-6240
    username: ~Michael_Bendersky1
  decision: toMainConference
  end_page: 4341
  file: 399.pdf
  id: 399
  num_pages: 13
  openreview_id: QgI9ivxdZ4
  pdf_file: 28ff32f3e4235fe210a715be0cea5e59ad4d0724.pdf
  start_page: 4329
  title: 'Beyond Yes and No: Improving Zero-Shot LLM Rankers via Scoring Fine-Grained
    Relevance Labels'
- abstract: In-Context Learning (ICL) is an emergent capability of Large Language
    Models (LLMs). Only a few demonstrations enable LLMs to be used as blackbox for
    new tasks. Previous studies have shown that using LLMs' outputs as labels is effective
    in training models to select demonstrations. Such a label is expected to estimate
    utility of a demonstration in ICL; however, it has not been well understood how
    different labeling strategies affect results on target tasks. This paper presents
    an analysis on different utility functions by focusing on LLMs' output probability
    given ground-truth output, and task-specific reward given LLMs' prediction. Unlike
    the previous work, we introduce a novel labeling method, incremental utility,
    which estimates how much incremental knowledge is brought into the LLMs by a demonstration.
    We conduct experiments with instruction-tuned LLMs on binary/multi-class classification,
    segmentation, and translation across Arabic, English, Finnish, Japanese, and Spanish.
    Our results show that (1) the probability is effective when the probability values
    are distributed across the whole value range (on the classification tasks), and
    (2) the downstream metric is more robust when nuanced reward values are provided
    with long outputs (on the segmentation and translation tasks). We then show that
    the proposed incremental utility further helps ICL by contrasting how the LLMs
    perform with and without the demonstrations.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Learning for NLP
  authors:
  - dblp_id: https://dblp.org/pid/76/2653.html
    emails: '****@google.com'
    first_name: Kazuma
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=gVi99BIAAAAJ
    homepage: http://www.logos.t.u-tokyo.ac.jp/~hassy/
    institution: Google Research
    last_name: Hashimoto
    name: Kazuma Hashimoto
    semantic_scholar_id: https://www.semanticscholar.org/author/Kazuma-Hashimoto/20851195
    username: ~Kazuma_Hashimoto1
  - dblp_id: https://dblp.org/pid/01/7071-1
    emails: '****@google.com'
    first_name: Karthik
    google_scholar_id: https://scholar.google.com/citations?user=x1zTxLoAAAAJ
    institution: Google
    last_name: Raman
    name: Karthik Raman
    username: ~Karthik_Raman1
  - dblp_id: https://dblp.org/pid/80/4305
    emails: '****@google.com'
    first_name: Michael
    google_scholar_id: https://scholar.google.com/citations?user=C9mxM5IAAAAJ&hl=en
    homepage: http://bendersky.github.io/
    institution: Google
    last_name: Bendersky
    name: Michael Bendersky
    orcid: https://orcid.org/0000-0002-2941-6240
    username: ~Michael_Bendersky1
  decision: toMainConference
  end_page: 4359
  file: 401.pdf
  id: 401
  num_pages: 18
  openreview_id: jj1PVAJ8Go
  pdf_file: 5f051b0e17f08211f354398d5f95aae3de70a49d.pdf
  start_page: 4342
  title: 'Take One Step at a Time to Know Incremental Utility of Demonstration: An
    Analysis on Reranking for Few-Shot In-Context Learning'
- abstract: "Today\u2019s large language models (LLMs) typically train on short text\
    \ segments (e.g., <4K tokens) due to the quadratic complexity of their Transformer\
    \ architectures. As a result, their performance suffers drastically on inputs\
    \ longer than those encountered during training, substantially limiting their\
    \ applications in real-world tasks involving long contexts such as encod- ing\
    \ scientific articles, code repositories, or long dialogues. Through both theoretical\
    \ analysis and empirical investigation, this work identifies three major factors\
    \ contributing to this length generalization failure. Our theoretical analysis\
    \ reveals that commonly used techniques like using a sliding-window attention\
    \ pattern or relative positional encodings are inadequate to address them. Answering\
    \ these challenges, we propose LM-Infinite, a simple and effective method for\
    \ enhancing LLMs\u2019 capabilities of handling long contexts. LM-Infinite is\
    \ highly flexible and can be used with most modern LLMs off-the-shelf. Without\
    \ any parameter updates, it allows LLMs pre-trained with 2K or 4K-long segments\
    \ to generalize to up to 200M length inputs while retaining perplexity. It also\
    \ improves performance on downstream tasks such as Passkey Retrieval and Qasper\
    \ in the zero-shot setting. LM-Infinite brings substantial efficiency improvements:\
    \ it achieves 2.7\xD7 decoding speed up and 7.5\xD7 memory saving over the original\
    \ model. Our code will be publicly available upon publication."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Efficient/Low-Resource Methods for NLP
  authors:
  - dblp_id: https://dblp.org/pid/255/6993
    emails: '****@illinois.edu'
    first_name: Chi
    google_scholar_id: https://scholar.google.com.sg/citations?user=DcSvbuAAAAAJ&hl=en
    homepage: https://glaciohound.github.io
    last_name: Han
    name: Chi Han
    orcid: https://orcid.org/0000-0001-6235-5841
    semantic_scholar_id: https://www.semanticscholar.org/author/Chi-Han/2118642562
    username: ~Chi_Han1
  - dblp_id: https://dblp.org/pid/33/8610
    emails: '****@fb.com'
    first_name: Qifan
    google_scholar_id: https://scholar.google.com/citations?user=LrSyLosAAAAJ&hl=en
    homepage: https://wqfcr.github.io/
    institution: Meta AI
    last_name: Wang
    name: Qifan Wang
    username: ~Qifan_Wang1
  - emails: '****@illinois.edu'
    first_name: Hao
    google_scholar_id: https://scholar.google.com/citations?view_op=list_works&hl=en&user=sl4q0WkAAAAJ
    homepage: https://haopeng-nlp.github.io/
    institution: Department of Computer Science,  University of Illinois Urbana-Champaign
    last_name: Peng
    name: Hao Peng
    username: ~Hao_Peng4
  - dblp_id: https://dblp.org/pid/203/8542
    emails: '****@gmail.com'
    first_name: Wenhan
    homepage: https://xwhan.github.io
    institution: Facebook
    last_name: Xiong
    name: Wenhan Xiong
    username: ~Wenhan_Xiong1
  - dblp_id: https://dblp.org/pid/87/1254-22
    emails: '****@gmail.com'
    first_name: Yu
    google_scholar_id: https://scholar.google.com/citations?user=m6Sj1yoAAAAJ&hl=en
    homepage: http://academic.hugochan.net
    institution: Anytime.AI
    last_name: Chen
    name: Yu Chen
    semantic_scholar_id: https://www.semanticscholar.org/author/Yu-Chen/2144836395
    username: ~Yu_Chen5
  - emails: '****@illinois.edu'
    first_name: Heng
    google_scholar_id: https://scholar.google.com/citations?user=z7GCqT4AAAAJ&hl=en
    homepage: http://blender.cs.illinois.edu/hengji.html
    institution: University of Illinois, Urbana-Champaign
    last_name: Ji
    name: Heng Ji
    username: ~Heng_Ji3
  - dblp_id: https://dblp.org/pid/140/0795
    emails: '****@gmail.com'
    first_name: Sinong
    google_scholar_id: https://scholar.google.com/citations?user=CYMAfxsAAAAJ&hl=en
    homepage: https://sites.google.com/site/snongwang/
    institution: Facebook
    last_name: Wang
    name: Sinong Wang
    username: ~Sinong_Wang1
  decision: toMainConference
  end_page: 4377
  file: 402.pdf
  id: 402
  num_pages: 18
  openreview_id: GVWVZgmLr7
  pdf_file: b369c45726aebc11639c03f3daaef781dd3a74a5.pdf
  start_page: 4360
  title: 'LM-Infinite: Zero-Shot Extreme Length Generalization for Large Language
    Models'
- abstract: "A wave of new task-based virtual assistants has been fueled by increasingly\
    \ powerful large language models (LLMs), such as GPT-4 (OpenAI, 2023). A major\
    \ challenge in deploying LLM-based virtual conversational assistants in real world\
    \ settings is ensuring they operate within what is admissible for the task. To\
    \ overcome this challenge, the designers of these virtual assistants rely on an\
    \ independent guardrail system that verifies the virtual assistant\u2019s output\
    \ aligns with the constraints required for the task. However, relying on commonly\
    \ used, prompt-based guardrails can be difficult to engineer correctly and comprehensively.\
    \ To address these challenges, we propose CONSCENDI. We use CONSCENDI to exhaustively\
    \ generate training data with two key LLM-powered components: scenario-augmented\
    \ generation and contrastive training examples. When generating conversational\
    \ data, we generate a set of rule-breaking scenarios, which enumerate a diverse\
    \ set of high-level ways a rule can be violated. This scenario-guided approach\
    \ produces a diverse training set and provides chatbot designers greater control.\
    \ To generate contrastive examples, we prompt the LLM to alter conversations with\
    \ violations into acceptable conversations to enable fine-grained distinctions.\
    \ We then use this data, generated by CONSCENDI, to train a smaller model. We\
    \ find that CONSCENDI results in guardrail models that improve over baselines\
    \ in multiple dialogue domains."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Dialogue and Interactive Systems
  authors:
  - emails: '****@gmail.com'
    first_name: Albert
    last_name: Sun
    name: Albert Sun
    username: ~Albert_Sun1
  - emails: '****@gmail.com'
    first_name: Varun
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=R3SqGgMAAAAJ&authuser=1
    institution: Curai Health
    last_name: Nair
    name: Varun Nair
    semantic_scholar_id: https://www.semanticscholar.org/author/Varun-Nair/2073604503
    username: ~Varun_Nair1
  - dblp_id: https://dblp.org/pid/177/9393
    emails: '****@elliotschu.com'
    first_name: Elliot
    homepage: http://www.elliotschu.com
    institution: Curai Health
    last_name: Schumacher
    name: Elliot Schumacher
    orcid: https://orcid.org/0000-0002-2203-4784
    username: ~Elliot_Schumacher1
  - dblp_id: https://dblp.org/pid/58/2858
    emails: '****@curai.com'
    first_name: Anitha
    institution: Curai Health
    last_name: Kannan
    name: Anitha Kannan
    username: ~Anitha_Kannan3
  decision: toMainConference
  end_page: 4399
  file: 403.pdf
  id: 403
  num_pages: 22
  openreview_id: j9TVFMLOOd
  pdf_file: 59df4cd513cc91bb6542a53f142518dce1edbb44.pdf
  start_page: 4378
  title: 'CONSCENDI: A Contrastive and Scenario-Guided Distillation Approach to Guardrail
    Models for Virtual Assistants'
- abstract: "Stance detection aims at inferring an author\u2019s attitude towards\
    \ a specific target in a text. Prior methods mainly consider target-related background\
    \ information for a better understanding of targets while neglecting the accompanying\
    \ input texts. In this study, we propose to prompt Large Language Models (LLMs)\
    \ to explicitly extract the relationship between paired text and target as contextual\
    \ knowledge. We then inject such LLM-driven knowledge into a generation model\
    \ BART to exploit the rich contexts and semantics. Moreover, to further enhance\
    \ the decoding capability of BART, a novel prototypical contrastive scheme is\
    \ designed to align input contents with stance labels. Our experimental results\
    \ demonstrate the state-of-the-art performance across several publicly available\
    \ datasets, showcasing effectiveness in both zero-shot and cross-target stance\
    \ detection scenarios. We publicly release our code to facilitate future research."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Sentiment Analysis, Stylistic Analysis, and Argument Mining
  authors:
  - emails: '****@ict.ac.cn'
    first_name: Zhao
    homepage: https://zhangzhao219.github.io/
    last_name: Zhang
    name: Zhao Zhang
    username: ~Zhao_Zhang12
  - emails: '****@ict.ac.cn'
    first_name: Yiming
    google_scholar_id: https://scholar.google.com/citations?user=J8UPj-QAAAAJ&hl=zh-CN
    homepage: https://github.com/Ming-er
    last_name: Li
    name: Yiming Li
    username: ~Yiming_Li13
  - emails: '****@ict.ac.cn'
    first_name: Jin
    homepage: https://www.researchgate.net/profile/Jin-Zhang-168
    institution: ', Chinese Academy of Sciences'
    last_name: Zhang
    name: Jin Zhang
    username: ~Jin_Zhang21
  - emails: '****@ict.ac.cn'
    first_name: Hui
    homepage: https://github.com/XYHMaple/
    institution: Chinese Academy of Sciences
    last_name: Xu
    name: Hui Xu
    username: ~Hui_Xu6
  decision: toMainConference
  end_page: 4407
  file: 405.pdf
  id: 405
  num_pages: 8
  openreview_id: xr3nFYhchu
  pdf_file: ac360328d5a6307c69be72addef86e432a4e2add.pdf
  start_page: 4400
  title: LLM-Driven Knowledge Injection Advances Zero-Shot and Cross-Target Stance
    Detection
- abstract: We show the viability of tackling misuses of large language models beyond
    the identification of machine-generated text. While existing zero-bit watermark
    methods focus on detection only, some malicious misuses demand tracing the adversary
    user for counteracting them. To address this, we propose Multi-bit Watermark via
    Position Allocation, embedding traceable multi-bit information during language
    model generation. Through allocating tokens onto different parts of the messages,
    we embed longer messages in high corruption settings without added latency. By
    independently embedding sub-units of messages, the proposed method outperforms
    the existing works in terms of robustness and latency. Leveraging the benefits
    of zero-bit watermarking, our method enables robust extraction of the watermark
    without any model access, embedding and extraction of long messages ($\geq$ 32-bit)
    without finetuning, and maintaining text quality, while allowing zero-bit detection
    all at the same time.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - dblp_id: https://dblp.org/pid/266/1524
    emails: '****@snu.ac.kr'
    first_name: KiYoon
    google_scholar_id: https://scholar.google.com/citations?user=S93OUYQAAAAJ&hl=en
    homepage: http://mipal.snu.ac.kr
    institution: Seoul National University
    last_name: Yoo
    name: KiYoon Yoo
    semantic_scholar_id: https://www.semanticscholar.org/author/Kiyoon-Yoo/1713608836
    username: ~KiYoon_Yoo1
  - dblp_id: https://dblp.org/pid/227/4625
    emails: '****@gmail.com'
    first_name: Wonhyuk
    google_scholar_id: https://scholar.google.com/citations?user=SvpXOqsAAAAJ&hl=ko
    institution: NAVER WEBTOON Corp.
    last_name: Ahn
    name: Wonhyuk Ahn
    username: ~Wonhyuk_Ahn1
  - dblp_id: https://dblp.org/pid/49/2806
    emails: '****@snu.ac.kr'
    first_name: Nojun
    google_scholar_id: https://scholar.google.com/citations?user=h_8-1M0AAAAJ&hl=en
    homepage: http://mipal.snu.ac.kr
    institution: Seoul National University
    last_name: Kwak
    name: Nojun Kwak
    orcid: https://orcid.org/0000-0002-1792-0327
    username: ~Nojun_Kwak1
  decision: toMainConference
  end_page: 4432
  file: 407.pdf
  id: 407
  num_pages: 25
  openreview_id: 2CARsrSZfC
  pdf_file: ea220854ea217d77009a4f4253c9e00e6c924ccd.pdf
  start_page: 4408
  title: 'Advancing Beyond Identification: Multi-bit Watermark for Large Language
    Models'
- abstract: Temporal knowledge graphs (TKGs) serve as powerful tools for storing and
    modeling dynamic facts, holding immense potential in anticipating future facts.
    Since future facts are inherently unknowable, effectively modeling the intricate
    temporal structure of historical facts becomes paramount for accurate prediction.
    However, current models often rely heavily on fact recurrence or periodicity,
    leading to information loss due to prolonged evolutionary processes. Notably,
    the occurrence of one fact always influences the likelihood of another. To this
    end, we propose HTCCN, a novel Hawkes process-based temporal causal convolutional
    network designed for temporal reasoning under extrapolation settings. HTCCN employs
    a temporal causal convolutional network to model the historical interdependence
    of facts and leverages Hawkes to model link formation processes inductively in
    TKGs. Importantly, HTCCN introduces dual-level dynamics to comprehensively capture
    the temporal evolution of facts. Rigorous experimentation on four real-world datasets
    underscores the superior performance of HTCCN.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Question Answering
  authors:
  - dblp_id: https://dblp.org/pid/276/4969
    emails: '****@csu.edu.cn'
    first_name: Tingxuan
    google_scholar_id: https://scholar.google.com.hk/citations?user=w32rzN0AAAAJ&hl=zh-CN
    institution: Central South University
    last_name: Chen
    name: Tingxuan Chen
    username: ~Tingxuan_Chen1
  - dblp_id: https://dblp.uni-trier.de/pid/40/5250
    emails: '****@csu.edu.cn'
    first_name: Jun
    homepage: https://faculty.csu.edu.cn/longjun/zh_CN/index.htm
    institution: Central South University
    last_name: Long
    name: Jun Long
    username: ~Jun_Long2
  - dblp_id: https://dblp.org/pid/27/3367
    emails: '****@csu.edu.cn'
    first_name: Liu
    homepage: https://faculty.csu.edu.cn/yangliu/zh_CN/jxcg/61299/list/index.htm
    last_name: Yang
    name: Liu Yang
    username: ~Liu_Yang14
  - emails: '****@csu.edu.cn'
    first_name: Zidong
    homepage: https://github.com/WinterPrince
    institution: Central South University
    last_name: Wang
    name: Zidong Wang
    username: ~Zidong_Wang2
  - dblp_id: https://dblp.org/pid/34/6716.html
    emails: '****@zhejianglab.com'
    first_name: Yongheng
    google_scholar_id: https://scholar.google.com/citations?user=LTZcz5cAAAAJ&hl
    institution: Zhejiang lab
    last_name: Wang
    name: Yongheng Wang
    username: ~Yongheng_Wang1
  - dblp_id: https://dblp.org/pid/147/7736
    emails: '****@zhejianglab.com'
    first_name: Xiongnan
    google_scholar_id: https://scholar.google.com/citations?user=xqEmRngAAAAJ&hl=en&oi=ao
    institution: Zhejiang Lab
    last_name: Jin
    name: Xiongnan Jin
    orcid: https://orcid.org/0000-0001-5080-1883
    username: ~Xiongnan_Jin1
  decision: toMainConference
  end_page: 4443
  file: 408.pdf
  id: 408
  num_pages: 11
  openreview_id: 0PyM1n50n2
  pdf_file: d671023c3ba72cd504942aae1d1befaa695e2347.pdf
  start_page: 4433
  title: 'HTCCN: Temporal Causal Convolutional Networks with Hawkes Process for Extrapolation
    Reasoning in Temporal Knowledge Graphs'
- abstract: "Existing watermarked generation algorithms employ token-level designs\
    \ and therefore, are vulnerable to paraphrase attacks. To address this issue,\
    \ we introduce watermarking on the semantic representation of sentences. We propose\
    \ SemStamp, a robust sentence-level semantic watermarking algorithm that uses\
    \ locality-sensitive hashing (LSH) to partition the semantic space of sentences.\
    \ The algorithm encodes and LSH-hashes a candidate sentence generated by a language\
    \ model, and conducts rejection sampling until the sampled sentence falls in watermarked\
    \ partitions in the semantic embedding space. To test the paraphrastic robustness\
    \ of watermarking algorithms, we propose a \u201Cbigram paraphrase\u201D attack\
    \ that produces paraphrases with small bigram overlap with the original sentence.\
    \ This attack is shown to be effective against existing token-level watermark\
    \ algorithms, while posing only minor degradations to SemStamp. Experimental results\
    \ show that our novel semantic watermark algorithm is not only more robust than\
    \ the previous state-of-the-art method on various paraphrasers and domains, but\
    \ also better at preserving the quality of generation."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Generation
  authors:
  - emails: '****@jh.edu'
    first_name: Abe
    homepage: https://www.linkedin.com/in/bohanhou/
    last_name: Hou
    middle_name: Bohan
    name: Abe Bohan Hou
    username: ~Abe_Bohan_Hou1
  - emails: '****@jhu.edu'
    first_name: Jingyu
    homepage: https://jackz.io/
    institution: Johns Hopkins University
    last_name: Zhang
    name: Jingyu Zhang
    username: ~Jingyu_Zhang2
  - dblp_id: https://dblp.uni-trier.de/pers/hd/h/He:Tianxing
    emails: '****@gmail.com'
    first_name: Tianxing
    google_scholar_id: https://scholar.google.com/citations?user=egmfjjwAAAAJ&hl=zh-CN
    homepage: https://cloudygoose.github.io/
    last_name: He
    name: Tianxing He
    username: ~Tianxing_He1
  - dblp_id: https://dblp.org/pid/74/8792
    emails: '****@gmail.com'
    first_name: Yichen
    google_scholar_id: https://scholar.google.com/citations?user=86XiOcsAAAAJ&hl=en&oi=sra
    homepage: https://yichenzw.com
    last_name: Wang
    name: Yichen Wang
    semantic_scholar_id: https://www.semanticscholar.org/author/Yichen-Wang/2125062703
    username: ~Yichen_Wang4
  - dblp_id: https://dblp.org/pid/64/3095
    emails: '****@mit.edu'
    first_name: Yung-Sung
    google_scholar_id: https://scholar.google.com/citations?user=3ar1DOwAAAAJ
    homepage: https://people.csail.mit.edu/yungsung/
    institution: Massachusetts Institute of Technology
    last_name: Chuang
    name: Yung-Sung Chuang
    orcid: https://orcid.org/0000-0002-1723-5063
    semantic_scholar_id: https://www.semanticscholar.org/author/Yung-Sung-Chuang/2475831
    username: ~Yung-Sung_Chuang1
  - dblp_id: https://dblp.org/pers/hd/w/Wang_0004:Hongwei
    emails: '****@gmail.com'
    first_name: Hongwei
    google_scholar_id: https://scholar.google.com/citations?user=3C__4wsAAAAJ&hl=en
    homepage: https://hongweiw.net
    institution: Tencent AI Lab
    last_name: Wang
    name: Hongwei Wang
    orcid: https://orcid.org/0000-0001-7474-8271
    username: ~Hongwei_Wang1
  - dblp_id: https://dblp.org/pid/240/5490.html
    emails: '****@jhu.edu'
    first_name: Lingfeng
    google_scholar_id: https://scholar.google.com/citations?user=PoSTdLAAAAAJ&hl=en
    last_name: Shen
    name: Lingfeng Shen
    semantic_scholar_id: https://www.semanticscholar.org/author/Lingfeng-Shen/2121272448
    username: ~Lingfeng_Shen1
  - dblp_id: https://dblp.org/pid/06/4775
    emails: '****@cs.jhu.edu'
    first_name: Benjamin
    google_scholar_id: https://scholar.google.com.tw/citations?user=wIujAJoAAAAJ
    homepage: http://www.cs.jhu.edu/~vandurme/
    institution: Johns Hopkins University, Johns Hopkins University, Johns Hopkins
      University and Microsoft
    last_name: Van Durme
    name: Benjamin Van Durme
    username: ~Benjamin_Van_Durme2
  - dblp_id: https://dblp.org/pid/71/10515
    emails: '****@jhu.edu'
    first_name: Daniel
    google_scholar_id: https://scholar.google.com/citations?user=pK2kQvgAAAAJ&hl=en
    homepage: http://danielkhashabi.com/
    institution: Johns Hopkins University
    last_name: Khashabi
    name: Daniel Khashabi
    semantic_scholar_id: https://www.semanticscholar.org/author/Daniel-Khashabi/1783281
    username: ~Daniel_Khashabi2
  - dblp_id: https://dblp.org/pid/75/8157
    emails: '****@cs.washington.edu'
    first_name: Yulia
    google_scholar_id: https://scholar.google.com/citations?user=SEDPkrsAAAAJ&hl=en
    homepage: https://homes.cs.washington.edu/~yuliats/
    institution: Department of Computer Science, University of Washington
    last_name: Tsvetkov
    name: Yulia Tsvetkov
    username: ~Yulia_Tsvetkov1
  decision: toMainConference
  end_page: 4459
  file: 409.pdf
  id: 409
  num_pages: 16
  openreview_id: TBPAimWOBT
  pdf_file: 9494dee24b28e6906eb7655595a8730d5de47688.pdf
  start_page: 4444
  title: 'SemStamp: A Semantic Watermark with Paraphrastic Robustness for Text Generation'
- abstract: Bias in  reporting can influence the public's opinion on relevant societal
    issues. Examples include informational bias (selective presentation of content)
    and lexical bias (specific framing of content through linguistic choices). The
    recognition of media bias is arguably an area where NLP can contribute to the
    "social good". Traditional NLP models have shown good performance in classifying
    media bias, but  require careful model design and extensive tuning. In this paper,
    we ask how well prompting of large language models can recognize media bias. Through
    an extensive empirical study including a wide selection of pre-trained models,
    we find that prompt-based techniques can deliver comparable performance to traditional
    models with greatly reduced effort and that, similar to traditional models, the
    availability of context substantially improves results. We further show that larger
    models can leverage different kinds of context simultaneously, obtaining further
    performance improvements.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Ethics, Bias, and Fairness
  authors:
  - emails: '****@gmail.com'
    first_name: Iffat
    last_name: Maab
    name: Iffat Maab
    username: ~Iffat_Maab1
  - dblp_id: https://dblp.org/pid/137/6899
    emails: '****@weblab.t.u-tokyo.ac.jp'
    first_name: Edison
    google_scholar_id: https://scholar.google.com/citations?user=uK_esCgAAAAJ
    homepage: https://epochx.github.io/
    institution: The Univesity of Tokyo and AIST, National Institute of Advanced Industrial
      Science and Technology
    last_name: Marrese-Taylor
    name: Edison Marrese-Taylor
    orcid: https://orcid.org/0000-0002-0328-0437
    semantic_scholar_id: https://www.semanticscholar.org/author/Edison-Marrese-Taylor/1389646918
    username: ~Edison_Marrese-Taylor2
  - dblp_id: https://dblp.org/pid/p/SebastianPado
    emails: '****@ims.uni-stuttgart.de'
    first_name: Sebastian
    google_scholar_id: https://scholar.google.com/citations?user=vKqag_AAAAAJ&hl=en
    homepage: https://nlpado.de/~sebastian
    institution: "University of Stuttgart, Universit\xE4t Stuttgart"
    last_name: "Pad\xF3"
    name: "Sebastian Pad\xF3"
    semantic_scholar_id: "https://www.semanticscholar.org/author/Sebastian-Pad\xF3\
      /1708581"
    username: "~Sebastian_Pad\xF32"
  - dblp_id: http://dblp.uni-trier.de/pers/hd/m/Matsuo:Yutaka
    emails: '****@weblab.t.u-tokyo.ac.jp'
    first_name: Yutaka
    google_scholar_id: https://scholar.google.com/citations?user=Dy8iau4AAAAJ&hl=ja
    homepage: http://ymatsuo.com
    institution: The University of Tokyo and The University of Tokyo
    last_name: Matsuo
    name: Yutaka Matsuo
    username: ~Yutaka_Matsuo1
  decision: toMainConference
  end_page: 4475
  file: 410.pdf
  id: 410
  num_pages: 16
  openreview_id: e2VsRXYARH
  pdf_file: 6089eaf72f93ca8d2b5ae9fdbda1fc5ca1577102.pdf
  start_page: 4460
  title: Media Bias Detection Across Families of Language Models
- abstract: 'Modern large language models (LLMs) exhibit a remarkable capacity for
    role-playing, enabling them to embody not only human characters but also non-human
    entities. This versatility allows them to simulate complex human-like interactions
    and behaviors within various contexts, as well as to emulate specific objects
    or systems. While these capabilities have enhanced user engagement and introduced
    novel modes of interaction, the influence of role-playing on LLMs'' reasoning
    abilities remains underexplored. In this study, we introduce a strategically designed
    role-play prompting methodology and assess its performance under the zero-shot
    setting across twelve diverse reasoning benchmarks. Our empirical results illustrate
    that role-play prompting consistently surpasses the standard zero-shot approach
    across most datasets. Notably, in experiments conducted using ChatGPT, accuracy
    on AQuA rises from 53.5\% to 63.8\%, and on Last Letter from 23.8\% to 84.2\%.
    Upon further comparison with the Zero-Shot-CoT technique, which prompts the model
    to "think step by step'''', our study demonstrates that role-play prompting acts
    as a more effective trigger for the CoT process.

    This highlights its potential to augment the reasoning capabilities of LLMs. We
    release our code at https://github.com/NKU-HLT/Role-Play-Prompting.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Efficient/Low-Resource Methods for NLP
  authors:
  - emails: '****@mail.nankai.edu.cn'
    first_name: Aobo
    google_scholar_id: https://scholar.google.com.hk/citations?user=SCPZwXgAAAAJ&hl=zh-CN
    last_name: Kong
    name: Aobo Kong
    username: ~Aobo_Kong1
  - dblp_id: https://dblp.uni-trier.de/pers/hd/z/Zhao:Shiwan
    emails: '****@gmail.com'
    first_name: Shiwan
    google_scholar_id: https://scholar.google.com/citations?user=qUA1_NgAAAAJ&hl=zh-CN
    last_name: Zhao
    name: Shiwan Zhao
    orcid: https://orcid.org/0000-0001-5068-025X
    semantic_scholar_id: https://www.semanticscholar.org/author/Shiwan-Zhao/2516425
    username: ~Shiwan_Zhao1
  - emails: '****@lenovo.com'
    first_name: Hao
    homepage: http://example.com
    last_name: Chen
    name: Hao Chen
    username: ~Hao_Chen59
  - dblp_id: https://dblp.org/pid/71/28
    emails: '****@nankai.edu.cn'
    first_name: Qicheng
    google_scholar_id: https://scholar.google.com/citations?user=H8wSzf4AAAAJ&hl=en
    homepage: https://cc.nankai.edu.cn/2021/0323/c13619a490387/page.htm
    institution: Nankai University
    last_name: Li
    name: Qicheng Li
    semantic_scholar_id: https://www.semanticscholar.org/author/Qicheng-Li/2919364
    username: ~Qicheng_Li1
  - dblp_id: https://dblp.org/pid/20/4298
    emails: '****@nankai.edu.cn'
    first_name: Yong
    last_name: Qin
    name: Yong Qin
    username: ~Yong_Qin2
  - emails: '****@lenovo.com'
    first_name: Ruiqi
    institution: Lenovo Research
    last_name: Sun
    name: Ruiqi Sun
    semantic_scholar_id: https://www.semanticscholar.org/author/Ruiqi-Sun/145114816
    username: ~Ruiqi_Sun2
  - dblp_id: https://dblp.uni-trier.de/pid/37/9805.html
    emails: '****@lenovo.com'
    first_name: Xin
    last_name: Zhou
    name: Xin Zhou
    username: ~Xin_Zhou19
  - emails: '****@gmail.com'
    first_name: Enzhi
    google_scholar_id: https://scholar.google.com/citations?user=yQ9aHkYAAAAJ&hl=zh-CN
    last_name: Wang
    name: Enzhi Wang
    username: ~Enzhi_Wang2
  - emails: '****@foxmail.com'
    first_name: Xiaohang
    last_name: Dong
    name: Xiaohang Dong
    orcid: https://orcid.org/0000-0001-7969-3134
    username: ~Xiaohang_Dong1
  decision: toMainConference
  end_page: 4490
  file: 411.pdf
  id: 411
  num_pages: 15
  openreview_id: K0yBySr69x
  pdf_file: e9063cd663b3a627f4b3760ef511ff6130bbb34a.pdf
  start_page: 4476
  title: Better Zero-Shot Reasoning with Role-Play Prompting
- abstract: Understanding complex events from different modalities, associating to
    external knowledge and generating response in a clear point of view are still
    unexplored in today's multi-modal dialogue research. The great challenges include
    1) lack of event-based multi-modal dialogue dataset; 2) understanding of complex
    events and 3) heterogeneity gap between different modalities. To overcome these
    challenges, we firstly introduce a novel event-oriented video-dialogue dataset
    called SportsVD (Sports-domain Video-dialogue Dataset). To our best knowledge,
    SportsVD is the first dataset that consists of complex events videos and opinion-based
    conversations with regards to contents in these events. Meanwhile, we present
    multi-modal dialogue generation method VCD (Video Commentary Dialogue) to generate
    human-like response according to event contents in the video and related external
    knowledge. In contrast to previous video-based dialogue generation, we focus on
    opinion-based response and the understanding of longer and more complex event
    contents. We evaluate VCD's performance on SportsVD and other baselines under
    several automatic metrics. Experiments demonstrate VCD can outperform among other
    state-of-the-art baselines. Our work is available at https://github.com/Cheng-Fenghua/SportsVD.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Dialogue and Interactive Systems
  authors:
  - emails: '****@uq.edu.au'
    first_name: Fenghua
    institution: University of Queensland
    last_name: Cheng
    name: Fenghua Cheng
    username: ~Fenghua_Cheng1
  - dblp_id: https://dblp.org/pid/l/XueLi.html
    emails: '****@itee.uq.edu.au'
    first_name: Xue
    google_scholar_id: https://scholar.google.com.au/citations?hl=en&user=z0C-DMMAAAAJ&view_op=list_works&sortby=pubdate
    homepage: https://staff.itee.uq.edu.au/xue/
    institution: University of Queensland
    last_name: Li
    name: Xue Li
    orcid: https://orcid.org/0000-0002-4515-6792
    username: ~Xue_Li1
  - dblp_id: https://dblp.org/pid/70/6862
    emails: '****@itee.uq.edu.au'
    first_name: Zi
    google_scholar_id: https://scholar.google.com.au/citations?user=iAWMsgEAAAAJ&hl=en
    homepage: https://staff.itee.uq.edu.au/huang/
    institution: University of Queensland
    last_name: Huang
    name: Zi Huang
    username: ~Zi_Huang1
  - emails: '****@uq.edu.au'
    first_name: Jinxiang
    institution: University of Queensland
    last_name: Wang
    name: Jinxiang Wang
    username: ~Jinxiang_Wang1
  - dblp_id: https://dblp.org/pid/69/6403-1
    emails: '****@uq.edu.au'
    first_name: Sen
    google_scholar_id: https://scholar.google.com/citations?user=L6BLX7gAAAAJ&hl=en
    homepage: https://csenw.github.io/
    institution: University of Queensland and The University of Queensland
    last_name: Wang
    name: Sen Wang
    orcid: https://orcid.org/0000-0002-5414-8276
    username: ~Sen_Wang3
  decision: toMainConference
  end_page: 4501
  file: 413.pdf
  id: 413
  num_pages: 11
  openreview_id: IiV2pGSSvO
  pdf_file: b41ddbeade577cbecdde646e6aae857df61688e4.pdf
  start_page: 4491
  title: Event-Content-Oriented Dialogue Generation in Short Video
- abstract: "The improvement of LLMs' instruction-following capabilities relies heavily\
    \ on the availability of high-quality instruction-response pairs. \nUnfortunately,\
    \ the current methods used to collect the pairs suffer from either unaffordable\
    \ labor costs or severe hallucinations in the self-generation of LLM.\nTo tackle\
    \ these challenges, this paper proposes a scalable solution.\nIt involves training\
    \ LLMs to generate instruction-response pairs based on human-written documents,\
    \ rather than relying solely on self-generation without context.\nOur proposed\
    \ method not only exploits the advantages of human-written documents in reducing\
    \ hallucinations but also utilizes an LLM to wrap the expression of documents,\
    \ which enables us to bridge the gap between various document styles and the standard\
    \ AI response.\nExperiments demonstrate that our method outperforms existing typical\
    \ methods on multiple benchmarks.\nIn particular, compared to the best-performing\
    \ baseline, the LLM trained using our generated dataset exhibits a 10\\% relative\
    \ improvement in performance on AlpacaEval, despite utilizing only 1/5 of its\
    \ training data.\nFurthermore, a comprehensive manual evaluation validates the\
    \ quality of the data we generated."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - dblp_id: https://dblp.org/pid/143/0948-2.html
    emails: '****@seu.edu.cn'
    first_name: Yongrui
    google_scholar_id: https://scholar.google.com/citations?user=8ZjIHyEAAAAJ
    institution: Southeast University
    last_name: Chen
    name: Yongrui Chen
    orcid: https://orcid.org/0000-0001-8934-3920
    username: ~Yongrui_Chen1
  - dblp_id: https://dblp.org/pid/225/7003
    emails: '****@tencent.com'
    first_name: Haiyun
    google_scholar_id: https://scholar.google.com/citations?user=fk684xEAAAAJ&hl=en
    institution: Tencent AI Lab
    last_name: Jiang
    name: Haiyun Jiang
    username: ~Haiyun_Jiang1
  - emails: '****@gmail.com'
    first_name: Xinting
    google_scholar_id: https://scholar.google.com/citations?user=QmyPDWQAAAAJ
    homepage: https://timhuang1.github.io/
    institution: Tencent AI Lab
    last_name: Huang
    name: Xinting Huang
    semantic_scholar_id: https://www.semanticscholar.org/author/Xinting-Huang/14799547
    username: ~Xinting_Huang1
  - dblp_id: https://dblp.org/pid/s/ShumingShi
    emails: '****@hotmail.com'
    first_name: Shuming
    google_scholar_id: https://scholar.google.com/citations?user=Lg31AKMAAAAJ&hl=en&oi=ao
    institution: Tencent AI Lab
    last_name: Shi
    name: Shuming Shi
    semantic_scholar_id: https://www.semanticscholar.org/author/Shuming-Shi/34720053
    username: ~Shuming_Shi1
  - dblp_id: https://dblp.org/pid/71/5935
    emails: '****@seu.edu.cn'
    first_name: Guilin
    last_name: Qi
    name: Guilin Qi
    username: ~Guilin_Qi2
  decision: toMainConference
  end_page: 4512
  file: 419.pdf
  id: 419
  num_pages: 11
  openreview_id: ICzLb4vvqI
  pdf_file: 9618e7d2d641ef2b75d5ace22d296405ed72921f.pdf
  start_page: 4502
  title: 'DoG-Instruct: Towards Premium Instruction-Tuning Data via Text-Grounded
    Instruction Wrapping'
- abstract: "Mitigating social biases typically requires identifying the social groups\
    \ associated with each data sample. In this paper, we present DAFair, a novel\
    \ approach to address social bias in language models. Unlike traditional methods\
    \ that rely on explicit demographic labels, our approach does not require any\
    \ such information. Instead, we leverage predefined prototypical demographic texts\
    \ and incorporate a regularization term during the fine-tuning process to mitigate\
    \ bias in the model\u2019s representations. Our empirical results across two tasks\
    \ and two models demonstrate the effectiveness of our method compared to previous\
    \ approaches that do not rely on labeled data. Moreover, with limited demographic-annotated\
    \ data, our approach outperforms common debiasing approaches."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Ethics, Bias, and Fairness
  authors:
  - emails: '****@campus.technion.ac.il'
    first_name: Shadi
    homepage: https://www.linkedin.com/in/shadi-iskander-03895221a/
    last_name: Iskander
    name: Shadi Iskander
    username: ~Shadi_Iskander1
  - dblp_id: https://dblp.org/pid/08/6560
    emails: '****@gmail.com'
    first_name: Kira
    homepage: http://www.kiraradinsky.com
    institution: Computer Science Departmen, Technion-Israel Institute of Technology
    last_name: Radinsky
    name: Kira Radinsky
    username: ~Kira_Radinsky1
  - dblp_id: https://dblp.org/pid/136/8705
    emails: '****@technion.ac.il'
    first_name: Yonatan
    google_scholar_id: https://scholar.google.com/citations?authorid=K-6ujU4AAAAJ&user=K-6ujU4AAAAJ
    homepage: https://www.belinkov.com
    institution: Technion, Technion
    last_name: Belinkov
    name: Yonatan Belinkov
    semantic_scholar_id: https://www.semanticscholar.org/search?q=%22Yonatan%20Belinkov%22&sort=relevance
    username: ~Yonatan_Belinkov1
  decision: toMainConference
  end_page: 4524
  file: 420.pdf
  id: 420
  num_pages: 12
  openreview_id: bVvgkDjfMc
  pdf_file: fde40a1bf3b169a83a131a6eaab8c3f4c5cd8054.pdf
  start_page: 4513
  title: Leveraging Prototypical Representations for Mitigating Social Bias without
    Demographic Information
- abstract: Legal professionals face the challenge of managing an overwhelming volume
    of lengthy judgments, making automated legal case summarization crucial. However,
    prior approaches mainly focused on training and evaluating these models within
    the same jurisdiction. In this study, we explore the cross-jurisdictional generalizability
    of legal case summarization models. Specifically, we explore how to effectively
    summarize legal cases of a target jurisdiction where reference summaries are not
    available. In particular, we investigate whether supplementing models with unlabeled
    target jurisdiction corpus and extractive silver summaries obtained from unsupervised
    algorithms on target data enhances transfer performance. Our comprehensive study
    on three datasets from different jurisdictions highlights the role of pre-training
    in improving transfer performance. We shed light on the pivotal influence of jurisdictional
    similarity in selecting optimal source datasets for effective transfer. Furthermore,
    our findings underscore that incorporating unlabeled target data yields improvements
    in general pre-trained models, with additional gains when silver summaries are
    introduced. This augmentation is especially valuable when dealing with extractive
    datasets and scenarios featuring limited alignment between source and target jurisdictions.
    Our study provides key insights for developing adaptable legal case summarization
    systems, transcending jurisdictional boundaries.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - dblp_id: https://dblp.org/pid/220/2486
    emails: '****@gmail.com'
    first_name: Santosh
    google_scholar_id: https://scholar.google.com/citations?user=aYytWsAAAAAJ&hl=en&oi=ao
    institution: "Technische Universit\xE4t M\xFCnchen"
    last_name: T.y.s.s
    name: Santosh T.Y.S.S
    semantic_scholar_id: https://www.semanticscholar.org/author/134765210
    username: ~Santosh_T.Y.S.S1
  - emails: '****@gmail.com'
    first_name: Vatsal
    homepage: http://vatsal-kr.github.io
    last_name: Venkatkrishna
    name: Vatsal Venkatkrishna
    username: ~Vatsal_Venkatkrishna1
  - dblp_id: https://dblp.org/pid/06/900-1
    emails: '****@gmail.com'
    first_name: Saptarshi
    google_scholar_id: https://scholar.google.co.in/citations?user=7TmKZv0AAAAJ
    homepage: http://cse.iitkgp.ac.in/~saptarshi
    institution: Indian Institute of Technology Kharagpur
    last_name: Ghosh
    name: Saptarshi Ghosh
    semantic_scholar_id: https://www.semanticscholar.org/author/Saptarshi-Ghosh/143841814
    username: ~Saptarshi_Ghosh1
  - emails: '****@tum.de'
    first_name: Matthias
    google_scholar_id: https://scholar.google.com/citations?user=MroPEGsAAAAJ&hl=en
    homepage: https://www.cs.cit.tum.de/lt/team/matthias-grabmair/
    institution: "Technische Universit\xE4t M\xFCnchen"
    last_name: Grabmair
    name: Matthias Grabmair
    username: ~Matthias_Grabmair2
  decision: toMainConference
  end_page: 4539
  file: 421.pdf
  id: 421
  num_pages: 15
  openreview_id: 9Ph0MRz96J
  pdf_file: c7d8505a9a8b138cbe4f3b0fd6f70de3afb6af91.pdf
  start_page: 4525
  title: 'Beyond Borders: Investigating Cross-Jurisdiction Transfer in Legal Case
    Summarization'
- abstract: In Task-Oriented Dialog (TOD) systems, Dialog State Tracking (DST) structurally
    extracts information from user and system utterances, which can be further used
    for querying databases and forming responses to users. The two major categories
    of DST methods, sequential and independent methods, face trade-offs between accuracy
    and efficiency. To resolve this issue, we propose Effective and Efficient Dialog
    Comprehension (EDC), an alternative DST approach that leverages the tree structure
    of the dialog state. EDC predicts domains, slot names and slot values of the dialog
    state step-by-step for better accuracy, and efficiently encodes dialog contexts
    with causal attention patterns. We evaluate EDC on several popular TOD datasets
    and EDC is able to achieve state-of-the-art Joint Goal Accuracy (JGA). We also
    show theoretically and empirically that EDC is more efficient than model designs
    used by previous works.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Dialogue and Interactive Systems
  authors:
  - emails: '****@uw.edu'
    first_name: Qifan
    homepage: https://github.com/lqf96
    institution: University of Washington
    last_name: Lu
    name: Qifan Lu
    username: ~Qifan_Lu1
  - dblp_id: https://dblp.org/pid/173/4698.html
    emails: '****@wwu.edu'
    first_name: Bhaskar
    google_scholar_id: https://scholar.google.com/citations?user=ANJ9dgkAAAAJ&hl=en
    homepage: https://sites.google.com/view/rbhaskar
    institution: Western Washington University
    last_name: Ramasubramanian
    name: Bhaskar Ramasubramanian
    username: ~Bhaskar_Ramasubramanian1
  - dblp_id: https://dblp.org/pid/29/5044
    emails: '****@uw.edu'
    first_name: Radha
    google_scholar_id: https://scholar.google.com/citations?user=EEoNZ7NbVzMC&hl=en
    homepage: https://people.ece.uw.edu/radha/index.html
    institution: University of Washington, Seattle
    last_name: Poovendran
    name: Radha Poovendran
    username: ~Radha_Poovendran1
  decision: toMainConference
  end_page: 4554
  file: 423.pdf
  id: 423
  num_pages: 15
  openreview_id: 4XNFbbULVl
  pdf_file: 7d7f889bb540e5af888e5bfd310bb83294b3da67.pdf
  start_page: 4540
  title: 'EDC: Effective and Efficient Dialog Comprehension For Dialog State Tracking'
- abstract: Automatic text-based diacritic restoration models generally have high
    diacritic error rates when applied to speech transcripts as a result of domain
    and style shifts in spoken language. In this work, we explore the possibility
    of improving the performance of automatic diacritic restoration when applied to
    speech data by utilizing parallel spoken utterances. In particular, we use the
    pre-trained Whisper ASR model fine-tuned on relatively small amounts of diacritized
    Arabic speech data to produce rough diacritized transcripts for the speech utterances,
    which we then use as an additional input for diacritic restoration models. The
    proposed framework consistently improves diacritic restoration performance compared
    to text-only baselines. Our results highlight the inadequacy of current text-based
    diacritic restoration models for speech data sets and provide a new baseline for
    speech-based diacritic restoration.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - emails: '****@mbzuai.ac.ae'
    first_name: Sara
    google_scholar_id: https://scholar.google.com/citations?user=S_7RUIIAAAAJ&hl=ar
    last_name: Shatnawi
    name: Sara Shatnawi
    username: ~Sara_Shatnawi1
  - emails: '****@gmail.com'
    first_name: Sawsan
    google_scholar_id: https://scholar.google.com/citations?user=Jywf-x4AAAAJ&hl=en
    institution: Princess Nourah Bint Abdulrahman University
    last_name: Alqahtani
    name: Sawsan Alqahtani
    semantic_scholar_id: https://www.semanticscholar.org/author/Sawsan-Alqahtani/23644119
    username: ~Sawsan_Alqahtani1
  - dblp_id: https://dblp.org/pid/186/7198
    emails: '****@mbzuai.ac.ae'
    first_name: Hanan
    google_scholar_id: https://scholar.google.com/citations?user=U8JSlxcAAAAJ&hl=en
    homepage: https://mbzuai.ac.ae/study/faculty/hanan-al-darmaki/
    institution: Mohamed bin Zayed University of Artificial Intelligence
    last_name: Aldarmaki
    name: Hanan Aldarmaki
    orcid: https://orcid.org/0000-0003-1706-1777
    semantic_scholar_id: https://www.semanticscholar.org/author/Hanan-Aldarmaki/3466086
    username: ~Hanan_Aldarmaki1
  decision: toMainConference
  end_page: 4565
  file: 424.pdf
  id: 424
  num_pages: 11
  openreview_id: ycRLn6zLFg
  pdf_file: 214943e509c29eb7f4df80605c8977b06f61b0e5.pdf
  start_page: 4555
  title: Automatic Restoration of Diacritics for Speech Data Sets
- abstract: XNLI is a popular Natural Language Inference (NLI) benchmark widely used
    to evaluate cross-lingual Natural Language Understanding (NLU) capabilities across
    languages. In this paper, we expand XNLI to include Basque, a low-resource language
    that can greatly benefit from transfer-learning approaches. The new dataset, dubbed
    XNLIeu, has been developed by first machine-translating the English XNLI corpus
    into Basque, followed by a manual post-edition step. We have conducted a series
    of experiments using mono- and multilingual LLMs to assess a) the effect of professional
    post-edition on the MT system; b) the best cross-lingual strategy for NLI in Basque;
    and c) whether the choice of the best cross-lingual strategy is influenced by
    the fact that the dataset is built by translation. The results show that post-edition
    is necessary and that the translate-train cross-lingual strategy obtains better
    results overall, although the gain is lower when tested in a dataset that has
    been built natively from scratch. Our code and datasets are publicly available
    under open licenses.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: 'Semantics: Sentence-level Semantics, Textual Inference and Other
      areas'
  authors:
  - emails: '****@ehu.eus'
    first_name: Maite
    institution: "Universidad del Pa\xEDs Vasco"
    last_name: Heredia
    name: Maite Heredia
    orcid: https://orcid.org/0009-0005-6719-5433
    username: ~Maite_Heredia1
  - emails: '****@ehu.eus'
    first_name: Julen
    google_scholar_id: https://scholar.google.com/citations?user=BDGXAjgAAAAJ
    homepage: https://julenetxaniz.eus/en
    institution: HiTZ Center, University of the Basque Country (UPV/EHU)
    last_name: Etxaniz
    name: Julen Etxaniz
    orcid: https://orcid.org/0009-0000-2099-7766
    username: ~Julen_Etxaniz1
  - emails: '****@orai.eus'
    first_name: Muitze
    google_scholar_id: https://scholar.google.com/citations?user=a11WbFsAAAAJ&hl=es
    institution: Orai NLP Technologies
    last_name: Zulaika
    name: Muitze Zulaika
    username: ~Muitze_Zulaika1
  - dblp_id: https://dblp.org/pid/34/7625
    emails: '****@orai.eus'
    first_name: Xabier
    google_scholar_id: https://scholar.google.es/citations?user=NDkq1QIAAAAJ&hl=es&oi=sra
    last_name: Saralegi
    name: Xabier Saralegi
    username: ~Xabier_Saralegi2
  - dblp_id: https://dblp.org/pid/70/6331
    emails: '****@ehu.eus'
    first_name: Jeremy
    google_scholar_id: https://scholar.google.nl/citations?user=OZGuamsAAAAJ&hl=en#
    homepage: https://jerbarnes.github.io/
    institution: University of the Basque Country
    last_name: Barnes
    name: Jeremy Barnes
    semantic_scholar_id: https://www.semanticscholar.org/author/Jeremy-Barnes/144435436
    username: ~Jeremy_Barnes1
  - dblp_id: https://dblp.org/pid/03/6734
    emails: '****@ehu.eus'
    first_name: Aitor
    google_scholar_id: https://scholar.google.com/citations?user=yklm660AAAAJ&hl=en
    homepage: https://ixa2.si.ehu.eus/asoroa/
    institution: University of the Basque Country. UPV/EHU.
    last_name: Soroa
    name: Aitor Soroa
    orcid: https://orcid.org/0000-0001-8573-2654
    semantic_scholar_id: https://www.semanticscholar.org/author/Aitor-Soroa-Etxabe/2078619062
    username: ~Aitor_Soroa1
  decision: toMainConference
  end_page: 4577
  file: 425.pdf
  id: 425
  num_pages: 12
  openreview_id: ZokmtPF4kD
  pdf_file: 401f5c9ec8ca151294cb4386b79bb3d2a2354e5c.pdf
  start_page: 4566
  title: 'XNLIeu: a dataset for cross-lingual NLI in Basque'
- abstract: "Recently, retrieval-based in-context learning (ICL) methods for selecting\
    \ demonstrations have been widely investigated. Existing methods train a dense\
    \ retriever to retrieve the most appropriate demonstrations for a given test query,\
    \ which improves ICL performance. However, we find that distinct LLMs exhibit\
    \ different biases for ``what is a good demonstration\" since they possess differences\
    \ in training data, model architectures and training methods. As a result, a demonstration\
    \ suitable for one LLM may not be appropriate for others.\nPrevious approaches\
    \ ignore the model bias and fail to retrieve the most appropriate demonstrations\
    \ for different inference LLMs, resulting in a degradation of ICL performance.\n\
    To address this problem, we propose a simple yet effective metric to evaluate\
    \ the appropriateness of demonstrations for a specific inference LLM. Furthermore,\
    \ we introduce a Model-specific Demonstration Retrieval (MDR) method for ICL at\
    \ inference time, which considers the biases of different LLMs. \nWe test MDR\
    \ on seen and unseen tasks with multi-scale inference LLMs, such as  GPT-Neo-2.7B,\
    \ LLaMA-7B and Vicuna-13B. Experiments on 23 datasets across 11 data domains highlight\
    \ the remarkable effectiveness of MDR, showcasing improvements of up to 41.2%\
    \ in comparison to methods that neglect model biases."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Retrieval and Text Mining
  authors:
  - dblp_id: https://dblp.org/pid/163/2233
    emails: '****@bupt.edu.cn'
    first_name: Huazheng
    homepage: https://auth.bupt.edu.cn/
    last_name: Wang
    name: Huazheng Wang
    username: ~Huazheng_Wang2
  - emails: '****@bupt.edu.cn'
    first_name: Jinming
    homepage: https://github.com/kiming-ng
    last_name: Wu
    name: Jinming Wu
    username: ~Jinming_Wu1
  - dblp_id: https://dblp.org/pid/00/11044
    emails: '****@bupt.edu.cn'
    first_name: Haifeng
    google_scholar_id: https://scholar.google.com/citations?user=dwhbTsEAAAAJ&hl=en&oi=ao
    homepage: https://hfsun.github.io
    institution: Beijing University of Posts and Telecommunications
    last_name: Sun
    name: Haifeng Sun
    username: ~Haifeng_Sun2
  - emails: '****@bupt.edu.cn'
    first_name: Zixuan
    homepage: https://github.com/zaizai-2020
    institution: Beijing University of Posts and Telecommunications
    last_name: Xia
    name: Zixuan Xia
    username: ~Zixuan_Xia1
  - dblp_id: https://dblp.org/pid/289/2865
    emails: '****@gmail.com'
    first_name: Daixuan
    last_name: Cheng
    name: Daixuan Cheng
    semantic_scholar_id: https://www.semanticscholar.org/author/Daixuan-Cheng/2068324576
    username: ~Daixuan_Cheng1
  - dblp_id: https://dblp.org/pid/37/2749-1
    emails: '****@bupt.edu.cn'
    first_name: Jingyu
    google_scholar_id: https://scholar.google.com/citations?user=H441DjwAAAAJ&hl=en
    homepage: https://jericwang.github.io/
    institution: Beijing University of Post and Telecommunication, Tsinghua University
    last_name: Wang
    name: Jingyu Wang
    orcid: https://orcid.org/0000-0002-2182-2228
    username: ~Jingyu_Wang1
  - dblp_id: https://dblp.org/pid/80/6406
    emails: '****@ebupt.com'
    first_name: Qi
    institution: Beijing University of Posts and Telecommunications
    last_name: Qi
    name: Qi Qi
    username: ~Qi_Qi1
  - dblp_id: https://dblp.org/pid/60/4951
    emails: '****@ebupt.com'
    first_name: Jianxin
    last_name: Liao
    name: Jianxin Liao
    username: ~Jianxin_Liao1
  decision: toMainConference
  end_page: 4593
  file: 426.pdf
  id: 426
  num_pages: 16
  openreview_id: hqLEyLTG7x
  pdf_file: 3d19bab248051d6e61302e5e04e9a2529c512115.pdf
  start_page: 4578
  title: 'MDR: Model-Specific Demonstration Retrieval at Inference Time for In-Context
    Learning'
- abstract: "***Warning**: this paper contains content that may be offensive or upsetting.*\n\
    \nMost hate speech datasets neglect the cultural diversity within a single language,\
    \ resulting in a critical shortcoming in hate speech detection. \nTo address this,\
    \ we introduce **CREHate**, a **CR**oss-cultural **E**nglish **Hate** speech dataset.\n\
    To construct CREHate, we follow a two-step procedure: 1) cultural post collection\
    \ and 2) cross-cultural annotation.\nWe sample posts from the SBIC dataset, which\
    \ predominantly represents North America, and collect posts from four geographically\
    \ diverse English-speaking countries (Australia, United Kingdom, Singapore, and\
    \ South Africa) using culturally hateful keywords we retrieve from our survey.\n\
    Annotations are collected from the four countries plus the United States to establish\
    \ representative labels for each country.\nOur analysis highlights statistically\
    \ significant disparities across countries in hate speech annotations.\nOnly 56.2%\
    \ of the posts in CREHate achieve consensus among all countries, with the highest\
    \ pairwise label difference rate of 26%.\nQualitative analysis shows that label\
    \ disagreement occurs mostly due to different interpretations of sarcasm and the\
    \ personal bias of annotators on divisive topics.\nLastly, we evaluate large language\
    \ models (LLMs) under a zero-shot setting and show that current LLMs tend to show\
    \ higher accuracies on Anglosphere country labels in CREHate.\nOur dataset and\
    \ codes are available at: https://github.com/nlee0212/CREHate"
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Ethics, Bias, and Fairness
  authors:
  - dblp_id: https://dblp.org/pid/212/6295
    emails: '****@kaist.ac.kr'
    first_name: Nayeon
    google_scholar_id: https://scholar.google.com/citations?user=C8MUVcEAAAAJ&hl=en
    institution: Korea Advanced Institute of Science and Technology
    last_name: Lee
    name: Nayeon Lee
    username: ~Nayeon_Lee2
  - emails: '****@kaist.ac.kr'
    first_name: Chani
    google_scholar_id: https://scholar.google.com/citations?user=Xr2t1kYAAAAJ&hl=en
    institution: Korea Advanced Institute of Science & Technology
    last_name: Jung
    name: Chani Jung
    username: ~Chani_Jung1
  - emails: '****@kaist.ac.kr'
    first_name: Junho
    homepage: https://junhomyung.github.io/
    institution: Korea Advanced Institute of Science and Technology
    last_name: Myung
    name: Junho Myung
    username: ~Junho_Myung1
  - dblp_id: https://dblp.org/pid/320/5607
    emails: '****@kaist.ac.kr'
    first_name: Jiho
    google_scholar_id: https://scholar.google.com/citations?user=-I0ahKwAAAAJ&hl=ko
    homepage: https://jinjh0123.github.io/
    institution: Korea Advanced Institute of Science and Technology
    last_name: Jin
    name: Jiho Jin
    orcid: https://orcid.org/0000-0002-1767-3733
    semantic_scholar_id: https://www.semanticscholar.org/author/Jiho-Jin/2165259166
    username: ~Jiho_Jin1
  - dblp_id: https://dblp.org/pid/165/0790.html
    emails: '****@cardiff.ac.uk'
    first_name: Jose
    homepage: http://www.josecamachocollados.com
    institution: Cardiff University
    last_name: Camacho-Collados
    name: Jose Camacho-Collados
    semantic_scholar_id: https://www.semanticscholar.org/author/Jos%C3%A9-Camacho-Collados/1387447871
    username: ~Jose_Camacho-Collados1
  - emails: '****@juhokim.com'
    first_name: Juho
    google_scholar_id: https://scholar.google.com/citations?user=2dDAbMgAAAAJ&hl=en
    homepage: https://juhokim.com/
    institution: Korea Advanced Institute of Science and Technology
    last_name: Kim
    name: Juho Kim
    username: ~Juho_Kim2
  - dblp_id: https://dblp.org/pid/50/7562
    emails: '****@kaist.edu'
    first_name: Alice
    google_scholar_id: https://scholar.google.co.kr/citations?user=B88-xMEAAAAJ&hl=en
    homepage: http://uilab.kr
    institution: Korea Advanced Institute of Science and Technology
    last_name: Oh
    name: Alice Oh
    username: ~Alice_Oh1
  decision: toMainConference
  end_page: 4613
  file: 428.pdf
  id: 428
  num_pages: 20
  openreview_id: obYRqxVt7x
  pdf_file: 4d73e5ad9df3f8d2b8109f1bfec7f3ad834824aa.pdf
  start_page: 4594
  title: 'Exploring Cross-Cultural Differences in English Hate Speech Annotations:
    From Dataset Construction to Analysis'
- abstract: 'Large language models (LLMs) tend to inadequately integrate input context
    during text generation, relying excessively on encoded prior knowledge in model
    parameters, potentially resulting in generated text with factual inconsistencies
    or contextually unfaithful content. LLMs utilize two primary knowledge sources:
    1) prior (parametric) knowledge from pretraining, and 2) contextual (non-parametric)
    knowledge from input prompts. The study addresses the open question of how LLMs
    effectively balance these knowledge sources during the generation process, specifically
    in the context of open-domain question answering. To address this issue, we introduce
    a novel approach integrating contrastive decoding with adversarial irrelevant
    passages as negative samples to enhance robust context grounding during generation.
    Notably, our method operates at inference time without requiring further training.
    We conduct comprehensive experiments to demonstrate its applicability and effectiveness,
    providing empirical evidence showcasing its superiority over existing methodologies.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Question Answering
  authors:
  - dblp_id: https://dblp.org/pid/75/6680-5
    emails: '****@ed.ac.uk'
    first_name: Zheng
    google_scholar_id: https://scholar.google.com/citations?user=UO0MJeQAAAAJ&hl=en
    homepage: http://www.inf.ed.ac.uk/people/students/Zheng_Zhao.html
    institution: University of Edinburgh, University of Edinburgh
    last_name: Zhao
    name: Zheng Zhao
    semantic_scholar_id: https://www.semanticscholar.org/author/47122617
    username: ~Zheng_Zhao2
  - emails: '****@amazon.com'
    first_name: Emilio
    google_scholar_id: https://scholar.google.com/citations?user=JAbR3jQAAAAJ&hl=en
    homepage: https://twitter.com/emilmont
    last_name: Monti
    name: Emilio Monti
    semantic_scholar_id: https://www.semanticscholar.org/author/Emilio-Monti/38722908
    username: ~Emilio_Monti1
  - dblp_id: https://dblp.org/pid/71/4882.html
    emails: '****@tu-dresden.de'
    first_name: Jens
    google_scholar_id: https://scholar.google.de/citations?user=sEaQ5rgAAAAJ&hl=de
    homepage: http://jens-lehmann.org
    institution: "Amazon, Technische Universit\xE4t Dresden, University of Bonn and\
      \ Fraunhofer IAIS"
    last_name: Lehmann
    name: Jens Lehmann
    orcid: https://orcid.org/0000-0001-9108-4278
    username: ~Jens_Lehmann3
  - dblp_id: https://dblp.org/pid/133/3784
    emails: '****@hotmail.com'
    first_name: Haytham
    google_scholar_id: https://scholar.google.com/citations?user=804VsL8AAAAJ&hl=en&oi=ao
    homepage: https://www.linkedin.com/in/haythamassem/
    institution: Huawei Technologies Ltd.
    last_name: Assem
    name: Haytham Assem
    username: ~Haytham_Assem1
  decision: toMainConference
  end_page: 4626
  file: 429.pdf
  id: 429
  num_pages: 13
  openreview_id: PQxZ5bUKGi
  pdf_file: 9bd39febb545f217ab3bbaf8e2ce9846be4bfe1d.pdf
  start_page: 4614
  title: Enhancing Contextual Understanding in Large Language Models through Contrastive
    Decoding
- abstract: "We tested the robustness of sarcasm detection models by examining their\
    \ behavior when fine-tuned on four sarcasm datasets containing varying characteristics\
    \ of sarcasm: label source (authors vs. third-party), domain (social media/online\
    \ vs. offline conversations/dialogues), style (aggressive vs. humorous mocking).\
    \ We tested their prediction performance on the same dataset (intra-dataset) and\
    \ across different datasets (cross-dataset). For intra-dataset predictions, models\
    \ consistently performed better when fine-tuned with third-party labels rather\
    \ than with author labels. For cross-dataset predictions, most models failed to\
    \ generalize well to the other datasets, implying that one type of dataset cannot\
    \ represent all sorts of sarcasm with different styles and domains. Compared to\
    \ the existing datasets, models fine-tuned on the new dataset we release in this\
    \ work showed the highest generalizability to other datasets. \nWith a manual\
    \ inspection of the datasets and post-hoc analysis, we attributed the difficulty\
    \ in generalization to the fact that sarcasm actually comes in different domains\
    \ and styles. We argue that future sarcasm research should take the broad scope\
    \ of sarcasm into account."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Linguistic theories, Cognitive Modeling and Psycholinguistics
  authors:
  - emails: '****@uni-konstanz.de'
    first_name: Hyewon
    homepage: https://hyewon-jang-kn.github.io
    institution: "Universit\xE4t Konstanz"
    last_name: Jang
    name: Hyewon Jang
    orcid: https://orcid.org/0009-0004-0346-4633
    semantic_scholar_id: https://www.semanticscholar.org/author/Hyewon-Jang/2112589410
    username: ~Hyewon_Jang1
  - emails: '****@uni-konstanz.de'
    first_name: Diego
    google_scholar_id: https://scholar.google.co.uk/citations?hl=en&pli=1&user=mHwLPGQAAAAJ
    homepage: https://www.ling.uni-konstanz.de/en/frassinelli/diego-frassinelli/
    institution: "Ludwig-Maximilians-Universit\xE4t M\xFCnchen"
    last_name: Frassinelli
    name: Diego Frassinelli
    orcid: https://orcid.org/0000-0002-1517-2185
    username: ~Diego_Frassinelli1
  decision: toMainConference
  end_page: 4638
  file: 431.pdf
  id: 431
  num_pages: 12
  openreview_id: Pj4DcAYig6
  pdf_file: da55efea32eeb446d5ab0f8d2ff9ecc7da7634f2.pdf
  start_page: 4627
  title: Generalizable Sarcasm Detection is Just Around the Corner, of Course!
- abstract: 'Interpretability research has shown that self-supervised Spoken Language

    Models (SLMs) encode a wide variety of features in human speech from the

    acoustic, phonetic, phonological, syntactic and semantic levels, to speaker

    characteristics. The bulk of prior research on representations of phonology

    has focused on segmental features such as phonemes; the encoding of

    suprasegmental phonology (such as tone and stress patterns) in SLMs is not yet

    well understood. Tone is a suprasegmental feature that is present in more than

    half of the world''s languages. This paper aims to analyze the tone encoding

    capabilities of SLMs, using Mandarin and Vietnamese as case studies. We show

    that SLMs encode lexical tone to a significant degree even when they are

    trained on data from non-tonal languages. We further find that SLMs behave

    similarly to native and non-native human participants in tone and consonant

    perception studies, but they do not follow the same developmental trajectory.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Speech recognition, text-to-speech and spoken language understanding
  authors:
  - emails: '****@tilburguniversity.edu'
    first_name: Gaofei
    homepage: https://gaofeishen.com
    last_name: Shen
    name: Gaofei Shen
    orcid: https://orcid.org/0000-0002-9448-9470
    username: ~Gaofei_Shen1
  - emails: '****@uva.nl'
    first_name: Michaela
    homepage: https://www.uva.nl/en/profile/w/a/m.m.watkins/m.m.watkins.html
    last_name: Watkins
    name: Michaela Watkins
    username: ~Michaela_Watkins1
  - dblp_id: https://dblp.org/pid/69/8699
    emails: '****@uvt.nl'
    first_name: Afra
    homepage: http://afra.alishahi.name/
    institution: Tilburg University
    last_name: Alishahi
    name: Afra Alishahi
    semantic_scholar_id: https://www.semanticscholar.org/author/A.-Alishahi/103538973
    username: ~Afra_Alishahi1
  - dblp_id: https://dblp.org/pid/32/10934
    emails: '****@rug.nl'
    first_name: Arianna
    homepage: https://www.cs.rug.nl/~bisazza/
    institution: University of Groningen
    last_name: Bisazza
    name: Arianna Bisazza
    username: ~Arianna_Bisazza1
  - dblp_id: https://dblp.org/pid/19/1379
    emails: '****@chrupala.me'
    first_name: Grzegorz
    google_scholar_id: https://scholar.google.nl/citations?user=p6m63xoAAAAJ&hl=en
    homepage: http://grzegorz.chrupala.me
    institution: Tilburg University
    last_name: "Chrupa\u0142a"
    name: "Grzegorz Chrupa\u0142a"
    orcid: https://orcid.org/0000-0001-9498-6912
    semantic_scholar_id: https://www.semanticscholar.org/author/Grzegorz-Chrupa%C5%82a/2756960?sort=velocity
    username: "~Grzegorz_Chrupa\u0142a1"
  decision: toMainConference
  end_page: 4650
  file: 434.pdf
  id: 434
  num_pages: 12
  openreview_id: gb192P31G1
  pdf_file: f906e55cd7fc98c2dd7b3f725c9d8da55b17df97.pdf
  start_page: 4639
  title: Encoding of lexical tone in self-supervised models of spoken language
- abstract: Contextualized embeddings are the preferred tool for modeling Lexical
    Semantic Change (LSC). Current evaluations typically focus on a specific task
    known as Graded Change Detection (GCD). However, performance comparison across
    work are often misleading due to their reliance on diverse settings. In this paper,
    we evaluate state-of-the-art models and approaches for GCD under equal conditions.
    We further break the LSC problem into Word-in-Context (WiC) and Word Sense Induction
    (WSI) tasks, and compare models across these different levels. Our evaluation
    is performed across different languages on eight available benchmarks for LSC,
    and shows that (i) APD outperforms other approaches for GCD; (ii) XL-LEXEME outperforms
    other contextualized models for WiC, WSI, and GCD, while being comparable to GPT-4;
    (iii) there is a clear need for improving the modeling of word meanings, as well
    as focus on *how*, *when*, and *why* these meanings change, rather than solely
    focusing on the extent of semantic change.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: 'Semantics: Lexical'
  authors:
  - dblp_id: https://dblp.org/pid/304/9993.html
    emails: '****@unimi.it'
    first_name: Francesco
    google_scholar_id: https://scholar.google.com/citations?user=afg5fYAAAAAJ&hl=en
    homepage: https://islab.di.unimi.it/team/francesco.periti@unimi.it
    institution: University of Milan
    last_name: Periti
    name: Francesco Periti
    orcid: https://orcid.org/0000-0001-8388-2317
    username: ~Francesco_Periti1
  - dblp_id: https://dblp.org/pid/81/7513
    emails: '****@tahmasebi.se'
    first_name: Nina
    google_scholar_id: https://scholar.google.se/citations?user=nCH9mlUAAAAJ&hl=sv&oi=ao
    homepage: https://www.tahmasebi.se
    institution: "G\xF6teborg University"
    last_name: Tahmasebi
    name: Nina Tahmasebi
    orcid: https://orcid.org/0000-0003-1688-1845
    semantic_scholar_id: https://www.semanticscholar.org/author/Nina-Tahmasebi/1731960
    username: ~Nina_Tahmasebi1
  decision: toMainConference
  end_page: 4671
  file: 435.pdf
  id: 435
  num_pages: 21
  openreview_id: LVMos9sFCg
  pdf_file: f6f56128e6b22f644f90e8f8fbad36943798318b.pdf
  start_page: 4651
  title: A Systematic Comparison of Contextualized Word Embeddings for Lexical Semantic
    Change
- abstract: 'Aspect-based sentiment analysis (ABSA) have been extensively studied,
    but little light has been shed on the quadruple extraction consisting of four
    fundamental elements: aspects, categories, opinions and sentiments, especially
    with implicit aspects and opinions. In this paper, we propose a new method iACOS
    for extracting Implicit Aspects with Categories and Opinions with Sentiments.
    First, iACOS appends two implicit tokens at the end of a text to capture the context-aware
    representation of all tokens including implicit aspects and opinions. Second,
    iACOS develops a sequence labeling model over the context-aware token representation
    to co-extract explicit and implicit aspects and opinions. Third, iACOS devises
    a multi-label classifier with a specialized multi-head attention for discovering
    aspect-opinion pairs and predicting their categories and sentiments simultaneously.
    Fourth, iACOS leverages informative and adaptive negative examples to jointly
    train the multi-label classifier and the other two classifiers on categories and
    sentiments by multi-task learning. Finally, the experimental results show that
    iACOS significantly outperforms other quadruple extraction baselines according
    to the F1 score on two public benchmark datasets.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Sentiment Analysis, Stylistic Analysis, and Argument Mining
  authors:
  - emails: '****@enbrands.com'
    first_name: Xiancai
    homepage: http://www.enbrands.com/bcri/essen.html
    institution: Enbrands Inc.
    last_name: XU
    name: XIANCAI XU
    username: ~XIANCAI_XU1
  - dblp_id: https://dblp.org/pid/139/8128
    emails: '****@gmail.com'
    first_name: Jia-Dong
    google_scholar_id: https://scholar.google.com/citations?user=_mAEVHgAAAAJ
    institution: Enbrands, Inc.
    last_name: Zhang
    name: Jia-Dong Zhang
    orcid: https://orcid.org/0000-0001-6378-5894
    username: ~Jia-Dong_Zhang1
  - dblp_id: https://dblp.org/pid/99/4138
    emails: '****@enbrands.com'
    first_name: Lei
    homepage: https://www.enbrands.com/web/company.html
    last_name: Xiong
    name: Lei Xiong
    username: ~Lei_Xiong1
  - emails: '****@gmail.com'
    first_name: Zhishang
    google_scholar_id: https://scholar.google.com/citations?user=Fxt06LMAAAAJ&hl=en&oi=ao
    last_name: Liu
    name: Zhishang Liu
    username: ~Zhishang_Liu1
  decision: toMainConference
  end_page: 4682
  file: 436.pdf
  id: 436
  num_pages: 11
  openreview_id: VoCnA8pJpU
  pdf_file: b0170efd2b1dc552ad7a433cfa2f0f0af5b423aa.pdf
  start_page: 4672
  title: 'iACOS: Advancing Implicit Sentiment Extraction with Informative and Adaptive
    Negative Examples'
- abstract: "Large language models (LLMs) are able to solve various tasks with only\
    \ a few demonstrations utilizing their in-context learning (ICL) abilities.\n\
    However, LLMs often rely on their pre-trained semantic priors of demonstrations\
    \ rather than on the input-label relationships to proceed with ICL prediction.\
    \ \nIn this work, we term this phenomenon as the 'Demonstration Shortcut'.\nWhile\
    \ previous works have primarily focused on improving ICL prediction results for\
    \ predefined tasks, we aim to rectify the Demonstration Shortcut, thereby enabling\
    \ the LLM to effectively learn new input-label relationships from demonstrations.\n\
    To achieve this, we introduce In-Context Calibration, a demonstration-aware calibration\
    \ method.\nWe evaluate the effectiveness of the proposed method in two settings:\
    \ (1) the Original ICL Task using the standard label space and (2) the Task Learning\
    \ setting, where the label space is replaced with semantically unrelated tokens.\n\
    In both settings, In-Context Calibration demonstrates substantial improvements,\
    \ with results generalized across three LLM families (OPT, GPT, and Llama2) under\
    \ various configurations."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - emails: '****@gmail.com'
    first_name: Joonwon
    google_scholar_id: https://scholar.google.com/citations?user=ScBLCaMAAAAJ&hl=eng
    homepage: https://github.com/Lainshower
    last_name: Jang
    name: Joonwon Jang
    username: ~Joonwon_Jang1
  - dblp_id: https://dblp.org/pid/336/3880
    emails: '****@postech.ac.kr'
    first_name: Sanghwan
    google_scholar_id: https://scholar.google.com/citations?user=iuvgvowAAAAJ
    institution: POSTECH
    last_name: Jang
    name: Sanghwan Jang
    semantic_scholar_id: https://www.semanticscholar.org/author/Sanghwan-Jang/2194236168
    username: ~Sanghwan_Jang1
  - dblp_id: https://dblp.org/pid/264/2604
    emails: '****@postech.ac.kr'
    first_name: Wonbin
    google_scholar_id: https://scholar.google.com/citations?user=u-zOiMUAAAAJ&hl=en
    last_name: Kweon
    name: WONBIN KWEON
    username: ~WONBIN_KWEON1
  - emails: '****@postech.ac.kr'
    first_name: Minjin
    last_name: Jeon
    name: Minjin Jeon
    username: ~Minjin_Jeon1
  - dblp_id: https://dblp.org/pid/80/6889
    emails: '****@postech.ac.kr'
    first_name: Hwanjo
    google_scholar_id: https://scholar.google.com/citations?user=LbrCa7EAAAAJ
    homepage: http://di.postech.ac.kr/hwanjoyu
    institution: POSTECH
    last_name: Yu
    name: Hwanjo Yu
    semantic_scholar_id: https://www.semanticscholar.org/author/Hwanjo-Yu/1723357
    username: ~Hwanjo_Yu2
  decision: toMainConference
  end_page: 4710
  file: 437.pdf
  id: 437
  num_pages: 28
  openreview_id: B1jwtYIGno
  pdf_file: ce5b5a26ff978128f8c252e6c1eb748fbc080c54.pdf
  start_page: 4683
  title: Rectifying Demonstration Shortcut in In-Context Learning
- abstract: We introduce Universal NER (UNER), an open, community-driven project to
    develop gold-standard NER benchmarks in many languages. The overarching goal of
    UNER is to provide high-quality, cross-lingually consistent annotations to facilitate
    and standardize multilingual NER research. UNER v1 contains 19 datasets annotated
    with named entities in a cross-lingual consistent schema across 13 diverse languages.
    In this paper, we detail the dataset creation and composition of UNER; we also
    provide initial modeling baselines on both in-language and cross-lingual learning
    settings. We will release the data, code, and fitted models to the public.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - dblp_id: https://dblp.org/pid/222/9585-1
    emails: '****@gmail.com'
    first_name: Stephen
    google_scholar_id: https://scholar.google.com/citations?user=-uLaJq4AAAAJ&hl=en
    homepage: http://mayhewsw.github.io/
    institution: Duolingo
    last_name: Mayhew
    name: Stephen Mayhew
    semantic_scholar_id: https://www.semanticscholar.org/author/Stephen-Mayhew/2153374
    username: ~Stephen_Mayhew1
  - dblp_id: https://dblp.org/pid/184/3734
    emails: '****@cs.washington.edu'
    first_name: Terra
    homepage: https://blvns.github.io
    institution: University of Washington
    last_name: Blevins
    name: Terra Blevins
    semantic_scholar_id: https://www.semanticscholar.org/author/Terra-Blevins/3443287
    username: ~Terra_Blevins1
  - emails: '****@gatech.edu'
    first_name: Shuheng
    google_scholar_id: https://scholar.google.com/citations?user=Tx5DSp0AAAAJ&hl=en&oi=sra
    institution: Georgia Institute of Technology
    last_name: Liu
    name: Shuheng Liu
    semantic_scholar_id: https://www.semanticscholar.org/author/Shuheng-Liu/2077166614
    username: ~Shuheng_Liu2
  - dblp_id: https://dblp.org/pid/148/3267
    emails: '****@mareksuppa.com'
    first_name: Marek
    homepage: https://mareksuppa.com/
    institution: Comenius University in Bratislava
    last_name: Suppa
    name: Marek Suppa
    semantic_scholar_id: https://www.semanticscholar.org/author/Marek-Suppa/40152043
    username: ~Marek_Suppa1
  - dblp_id: https://dblp.org/pid/167/5312
    emails: '****@gmail.com'
    first_name: Hila
    google_scholar_id: https://scholar.google.com/citations?user=URThmtMAAAAJ&hl=en
    homepage: https://gonenhila.github.io/
    institution: Facebook
    last_name: Gonen
    name: Hila Gonen
    semantic_scholar_id: https://www.semanticscholar.org/author/Hila-Gonen/1821892
    username: ~Hila_Gonen1
  - dblp_id: https://dblp.org/pid/246/4647
    emails: '****@bath.ac.uk'
    first_name: Joseph Marvin
    google_scholar_id: https://scholar.google.com/citations?user=irs_5ekAAAAJ&hl=en
    homepage: https://www.josephimperial.com
    institution: University of Bath, National University and National University -
      Human Language Technologies Lab
    last_name: Imperial
    name: Joseph Marvin Imperial
    orcid: https://orcid.org/0000-0003-1073-6129
    semantic_scholar_id: https://www.semanticscholar.org/author/Joseph-Marvin-Imperial/151472158
    username: ~Joseph_Marvin_Imperial1
  - dblp_id: https://dblp.org/pid/k/BorjeKarlsson
    emails: '****@gmail.com'
    first_name: "B\xF6rje"
    google_scholar_id: https://scholar.google.com/citations?user=ZWmz_kMAAAAJ&hl=en
    homepage: https://tellarin.com/borje/
    institution: Beijing Academy of Artificial Intelligence (BAAI)
    last_name: Karlsson
    middle_name: F.
    name: "B\xF6rje F. Karlsson"
    orcid: https://orcid.org/0000-0001-8925-360X
    semantic_scholar_id: https://www.semanticscholar.org/author/B%C3%B6rje-F.-Karlsson/2047947436
    username: "~B\xF6rje_F._Karlsson1"
  - dblp_id: https://dblp.org/pid/246/3172
    emails: '****@gmail.com'
    first_name: Peiqin
    google_scholar_id: https://scholar.google.com.hk/citations?user=ZUeyIxMAAAAJ
    homepage: https://lpq29743.github.io
    institution: "Institut f\xFCr Informatik"
    last_name: Lin
    name: Peiqin Lin
    orcid: https://orcid.org/0000-0003-2818-3008
    semantic_scholar_id: https://www.semanticscholar.org/author/Peiqin-Lin/152178958
    username: ~Peiqin_Lin1
  - dblp_id: https://dblp.org/pid/40/8154.html
    emails: '****@ijs.si'
    first_name: Nikola
    google_scholar_id: https://scholar.google.si/citations?hl=en&user=zto4fTQAAAAJ&view_op=list_works
    homepage: https://nljubesi.github.io
    institution: "Jo\u017Eef Stefan Institute"
    last_name: "Ljube\u0161i\u0107"
    name: "Nikola Ljube\u0161i\u0107"
    username: "~Nikola_Ljube\u0161i\u01071"
  - emails: '****@gmail.com'
    first_name: Lester James
    google_scholar_id: https://scholar.google.co.jp/citations?user=2RtnNKEAAAAJ&hl=en
    homepage: https://ljvmiranda921.github.io
    institution: Allen Institute for Artificial Intelligence
    last_name: Miranda
    middle_name: Validad
    name: Lester James Validad Miranda
    semantic_scholar_id: https://www.semanticscholar.org/author/Lester-James-V.-Miranda/13614871
    username: ~Lester_James_Validad_Miranda1
  - dblp_id: https://dblp.org/pid/46/521.html
    emails: '****@gmail.com'
    first_name: Barbara
    homepage: https://bplank.github.io/
    institution: "Ludwig-Maximilians-Universit\xE4t M\xFCnchen and IT University of\
      \ Copenhagen"
    last_name: Plank
    name: Barbara Plank
    semantic_scholar_id: https://www.semanticscholar.org/author/Barbara-Plank/2022124
    username: ~Barbara_Plank2
  - emails: '****@inria.fr'
    first_name: Arij
    google_scholar_id: https://scholar.google.com/citations?user=5kz9S2IAAAAJ&hl=fr
    last_name: Riabi
    name: Arij Riabi
    username: ~Arij_Riabi1
  - dblp_id: https://dblp.org/pid/153/5384
    emails: '****@gmail.com'
    first_name: Yuval
    google_scholar_id: https://scholar.google.com/citations?user=aYAcXccAAAAJ&hl=en
    homepage: http://www.yuvalpinter.com
    institution: Ben-Gurion University of the Negev
    last_name: Pinter
    name: Yuval Pinter
    orcid: https://orcid.org/0000-0003-3174-1621
    semantic_scholar_id: https://www.semanticscholar.org/author/Yuval-Pinter/1826312
    username: ~Yuval_Pinter1
  decision: toMainConference
  end_page: 4726
  file: 442.pdf
  id: 442
  num_pages: 16
  openreview_id: klVhuGf2Cv
  pdf_file: b0d55c2bed9e68c2dcb88af2d6a99de2594c62e1.pdf
  start_page: 4711
  title: 'Universal NER: A Gold-Standard Multilingual Named Entity Recognition Benchmark'
- abstract: Minimum Bayes Risk (MBR) decoding can significantly improve translation
    performance of Multilingual Large Language Models (MLLMs). However, MBR decoding
    is computationally expensive.  We show how the recently developed Reinforcement
    Learning technique, Direct Preference Optimization (DPO), can fine-tune MLLMs
    to get the gains of MBR without any additional computation in inference.  Our
    method uses only a small monolingual fine-tuning set and yields significantly
    improved performance on multiple NMT test sets compared to  MLLMs without DPO.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Translation
  authors:
  - emails: '****@gmail.com'
    first_name: Guangyu
    last_name: Yang
    name: Guangyu Yang
    username: ~Guangyu_Yang1
  - dblp_id: https://dblp.org/pid/21/1754
    emails: '****@cam.ac.uk'
    first_name: Jinghong
    google_scholar_id: https://scholar.google.com/citations?user=pYOXaKEAAAAJ&hl=zh-CN
    homepage: https://erichen0615.github.io/
    last_name: Chen
    name: Jinghong Chen
    semantic_scholar_id: https://www.semanticscholar.org/author/Jinghong-Chen/2159255814
    username: ~Jinghong_Chen2
  - dblp_id: https://dblp.org/pid/254/9170
    emails: '****@cam.ac.uk'
    first_name: Weizhe
    google_scholar_id: https://scholar.google.com/citations?user=4hMhIecAAAAJ
    homepage: https://linweizhedragon.github.io/
    last_name: Lin
    name: Weizhe Lin
    username: ~Weizhe_Lin1
  - dblp_id: https://dblp.org/pid/b/WilliamJByrne
    emails: '****@cam.ac.uk'
    first_name: Bill
    google_scholar_id: https://scholar.google.com/citations?user=BVUcMU4AAAAJ&hl=en
    homepage: https://sites.google.com/view/bill-byrne/
    institution: Amazon and University of Cambridge
    last_name: Byrne
    name: Bill Byrne
    username: ~Bill_Byrne1
  decision: toMainConference
  end_page: 4734
  file: 443.pdf
  id: 443
  num_pages: 8
  openreview_id: l78LkmF6oe
  pdf_file: bb9b61e1b5b69608344d0b545584915d58054f40.pdf
  start_page: 4727
  title: Direct Preference Optimization for Neural Machine Translation with Minimum
    Bayes Risk Decoding
- abstract: Opioid related aberrant behaviors (ORABs) present novel risk factors for
    opioid overdose. This paper introduces a novel biomedical natural language processing
    benchmark dataset named ODD, for ORAB Detection Dataset. ODD is an expert-annotated
    dataset designed to identify ORABs from patients' EHR notes and classify them
    into nine categories; 1) Confirmed Aberrant Behavior, 2) Suggested Aberrant Behavior,
    3) Opioids, 4) Indication, 5) Diagnosed opioid dependency, 6) Benzodiazepines,
    7) Medication Changes, 8) Central Nervous System-related, and 9) Social Determinants
    of Health. We explored two state-of-the-art natural language processing models
    (fine-tuning and prompt-tuning approaches) to identify ORAB. Experimental results
    show that the prompt-tuning models outperformed the fine-tuning models in most
    categories and the gains were especially higher among uncommon categories (Suggested
    Aberrant Behavior, Confirmed Aberrant Behaviors, Diagnosed Opioid Dependence,
    and Medication Change). Although the best model achieved the highest 88.17% on
    macro average area under precision recall curve, uncommon classes still have a
    large room for performance improvement. ODD is publicly available.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - dblp_id: https://dblp.org/pers/hd/k/Kwon:Sunjae
    emails: '****@gmail.com'
    first_name: Sunjae
    google_scholar_id: https://scholar.google.co.kr/citations?user=N9ZM-CIAAAAJ&hl=ko
    last_name: Kwon
    name: SUNJAE KWON
    username: ~SUNJAE_KWON1
  - emails: '****@gmail.com'
    first_name: Xun
    google_scholar_id: https://scholar.google.com/citations?user=_v4sBPYAAAAJ
    institution: Microsoft
    last_name: Wang
    name: Xun Wang
    username: ~Xun_Wang5
  - emails: '****@uml.edu'
    first_name: Weisong
    institution: University of Massachusetts at Lowell
    last_name: Liu
    name: Weisong Liu
    orcid: https://orcid.org/my-orcid?orcid=0000-0003-3825-5597
    username: ~Weisong_Liu1
  - emails: '****@uml.edu'
    first_name: Emily
    homepage: https://bio-nlp.org/
    institution: Department of Veterans Affairs
    last_name: Druhl
    name: Emily Druhl
    username: ~Emily_Druhl1
  - emails: '****@yale.edu'
    first_name: Minhee
    homepage: https://medicine.yale.edu/profile/minhee-sung/
    last_name: Sung
    middle_name: L.
    name: Minhee L. Sung
    username: ~Minhee_L._Sung1
  - emails: '****@va.gov'
    first_name: Joel
    homepage: https://www.va.gov/
    last_name: Reisman
    middle_name: I.
    name: Joel I. Reisman
    username: ~Joel_I._Reisman1
  - emails: '****@uml.edu'
    first_name: Wenjun
    homepage: https://www.uml.edu/health-sciences/public-health/faculty/li-wenjun.aspx
    institution: University of Massachusetts at Lowell
    last_name: Li
    name: Wenjun Li
    username: ~Wenjun_Li3
  - emails: '****@yale.edu'
    first_name: Robert
    homepage: https://medicine.yale.edu/profile/robert-kerns/
    institution: Yale University
    last_name: Kerns
    middle_name: D
    name: Robert D Kerns
    username: ~Robert_D_Kerns1
  - emails: '****@yale.edu'
    first_name: William
    homepage: https://medicine.yale.edu/profile/william-becker/
    last_name: Becker
    middle_name: C.
    name: William C. Becker
    username: ~William_C._Becker1
  - emails: '****@umassmed.edu'
    first_name: Hong
    google_scholar_id: https://scholar.google.com/citations?user=TyXe64wAAAAJ&hl=en&oi=ao
    homepage: http://bio-nlp.org/
    institution: Columbia University
    last_name: yu
    name: hong yu
    username: ~hong_yu1
  decision: toMainConference
  end_page: 4756
  file: 446.pdf
  id: 446
  num_pages: 22
  openreview_id: dEbyyKJFLA
  pdf_file: 43083185d3062856782157616246ec3f80945577.pdf
  start_page: 4735
  title: 'ODD: A Benchmark Dataset for the Natural Language Processing Based Opioid
    Related Aberrant Behavior Detection'
- abstract: Chemical named entity recognition (NER) models are used in many downstream
    tasks, from adverse drug reaction identification to pharmacoepidemiology. However,
    it is unknown whether these models work the same for everyone. Performance disparities
    can potentially cause harm rather than the intended good. This paper assesses
    gender-related performance disparities in chemical NER systems. We develop a framework
    for measuring gender bias in chemical NER models using synthetic data and a newly
    annotated corpus of over 92,405 words with self-identified gender information
    from Reddit. Our evaluation of multiple biomedical NER models reveals evident
    biases. For instance, synthetic data suggests that female names are frequently
    misclassified as chemicals, especially when it comes to brand name mentions. Additionally,
    we observe performance disparities between female- and male-associated data in
    both datasets. Many systems fail to detect contraceptives such as birth control.
    Our findings emphasize the biases in chemical NER models, urging practitioners
    to account for these biases in downstream applications.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Ethics, Bias, and Fairness
  authors:
  - dblp_id: https://dblp.org/pid/317/0015
    emails: '****@utsa.edu'
    first_name: Xingmeng
    google_scholar_id: https://scholar.google.com/citations?user=azM4bR8AAAAJ&hl=zh-CN
    last_name: Zhao
    name: Xingmeng Zhao
    semantic_scholar_id: https://www.semanticscholar.org/author/Xingmeng-Zhao/2159185818
    username: ~Xingmeng_Zhao1
  - emails: '****@utsa.edu'
    first_name: Ali
    last_name: Niazi
    middle_name: Shahzad
    name: Ali Shahzad Niazi
    username: ~Ali_Shahzad_Niazi1
  - dblp_id: https://dblp.org/pid/133/1827
    emails: '****@utsa.edu'
    first_name: Anthony
    google_scholar_id: https://scholar.google.com/citations?user=KJr3ptUAAAAJ
    homepage: https://anthonyrios.net/
    institution: University of Texas at San Antonio
    last_name: Rios
    name: Anthony Rios
    semantic_scholar_id: https://www.semanticscholar.org/author/Anthony-Rios/26355137
    username: ~Anthony_Rios1
  decision: toMainConference
  end_page: 4771
  file: 447.pdf
  id: 447
  num_pages: 15
  openreview_id: jkbskGcYkt
  pdf_file: 15f6de8d8bc635f1fc59baa9c0a927b4edcd7581.pdf
  start_page: 4757
  title: A Comprehensive Study of Gender Bias in Chemical Named Entity Recognition
    Models
- abstract: 'Assessing instruction quality is a fundamental component of any improvement
    efforts in the education system. However, traditional manual assessments are expensive,
    subjective, and heavily dependent on observers'' expertise and idiosyncratic factors,
    preventing teachers from getting timely and frequent feedback. Different from
    prior research that mostly focuses on low-inference instructional practices on
    a singular basis, this paper presents the first study that leverages Natural Language
    Processing (NLP) techniques to assess multiple high-inference instructional practices
    in two distinct educational settings: in-person K-12 classrooms and simulated
    performance tasks for pre-service teachers. This is also the first study that
    applies NLP to measure a teaching practice that is widely acknowledged to be particularly
    effective for students with special needs. We confront two challenges inherent
    in NLP-based instructional analysis, including noisy and long input data and highly
    skewed distributions of human ratings. Our results suggest that pretrained Language
    Models (PLMs) demonstrate performances comparable to the agreement level of human
    raters for variables that are more discrete and require lower inference, but their
    efficacy diminishes with more complex teaching practices.  Interestingly, using
    only teachers'' utterances as input yields strong results for student-centered
    variables, alleviating common concerns over the difficulty of collecting and transcribing
    high-quality student speech data in in-person teaching settings. Our findings
    highlight both the potential and the limitations of current NLP techniques in
    the education domain, opening avenues for further exploration.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - emails: '****@gmail.com'
    first_name: Paiheng
    homepage: https://paihengxu.github.io/
    institution: Department of Computer Science, University of Maryland, College Park
    last_name: Xu
    name: Paiheng Xu
    username: ~Paiheng_Xu1
  - emails: '****@umd.edu'
    first_name: Jing
    google_scholar_id: https://scholar.google.com/citations?user=P3002BkAAAAJ&hl=en&authuser=1
    homepage: http://www.jingliu.info
    institution: University of Maryland, College Park
    last_name: Liu
    name: Jing Liu
    username: ~Jing_Liu30
  - emails: '****@bu.edu'
    first_name: Nathan
    last_name: Jones
    middle_name: D
    name: Nathan D Jones
    username: ~Nathan_D_Jones1
  - emails: '****@virginia.edu'
    first_name: Julie
    last_name: Cohen
    name: Julie Cohen
    username: ~Julie_Cohen1
  - dblp_id: https://dblp.org/pid/40/9555-2
    emails: '****@umd.edu'
    first_name: Wei
    homepage: https://aiwei.me
    institution: University of Maryland, College Park
    last_name: Ai
    name: Wei Ai
    username: ~Wei_Ai1
  decision: toMainConference
  end_page: 4786
  file: 450.pdf
  id: 450
  num_pages: 15
  openreview_id: xzYBSz2WOq
  pdf_file: f38829f10b993b806058ed8cd3ce42d1479f032f.pdf
  start_page: 4772
  title: The Promises and Pitfalls of Using Language Models to Measure Instruction
    Quality in Education
- abstract: 'Language models are achieving impressive performance on various tasks
    by aggressively adopting inference-time prompting techniques,

    such as zero-shot and few-shot prompting. In this work, we introduce EchoPrompt,
    a simple yet effective approach that prompts the model to rephrase its queries
    before answering them. EchoPrompt is tailored for four scenarios, including standard
    and chain-of-thought prompting, in both zero-shot and few-shot settings. Experimental
    results show that EchoPrompt yields substantial improvements

    across all these settings for four families of causal language models. These improvements
    are observed across various numerical reasoning (e.g., GSM8K, SVAMP), reading
    comprehension (e.g., DROP), and logical reasoning (e.g., Coin flipping) tasks.
    On average, EchoPrompt improves the Zero-shot-CoT performance of code-davinci-002
    by 5% in numerical tasks and 13% in reading comprehension tasks. Our empirical
    results indicate that EchoPrompt is an effective technique that enhances in-context
    learning performance.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Generation
  authors:
  - emails: '****@uci.edu'
    first_name: Raja Sekhar Reddy
    google_scholar_id: https://scholar.google.com/citations?user=Tn_YYiQAAAAJ&hl=en&oi=ao
    homepage: https://rajasekharmekala.github.io/
    last_name: Mekala
    name: Raja Sekhar Reddy Mekala
    username: ~Raja_Sekhar_Reddy_Mekala1
  - dblp_id: https://dblp.dagstuhl.de/pid/277/6248.html
    emails: '****@uci.edu'
    first_name: Yasaman
    google_scholar_id: https://scholar.google.com/citations?user=YCtmdaMAAAAJ&hl=en
    homepage: https://yasamanrazeghi.com/
    last_name: Razeghi
    name: Yasaman Razeghi
    username: ~Yasaman_Razeghi1
  - dblp_id: https://dblp.org/pid/13/3568-1
    emails: '****@uci.edu'
    first_name: Sameer
    google_scholar_id: https://scholar.google.com/citations?user=-hGZC54AAAAJ
    homepage: http://sameersingh.org
    institution: University of California, Irvine and Allen Institute for Artificial
      Intelligence
    last_name: Singh
    name: Sameer Singh
    orcid: https://orcid.org/0000-0003-0621-6323
    semantic_scholar_id: https://www.semanticscholar.org/author/Sameer-Singh/34650964
    username: ~Sameer_Singh1
  decision: toMainConference
  end_page: 4820
  file: 452.pdf
  id: 452
  num_pages: 34
  openreview_id: 04itgirzwM
  pdf_file: 406fba49f11aa13098a319b5f1761b4c0a9947ce.pdf
  start_page: 4787
  title: 'EchoPrompt: Instructing the Model to Rephrase Queries for Improved In-context
    Learning'
- abstract: 'Ensuring the privacy of Large Language Models (LLMs) is becoming increasingly
    important. The most widely adopted technique to accomplish this is DP-SGD, which
    trains a model to guarantee Differential Privacy (DP). However, DP-SGD overestimates
    an adversary''s capabilities in having white box access to the model and, as a
    result, causes longer training times and larger memory usage than SGD. On the
    other hand, commercial LLM deployments are predominantly cloud-based; hence, adversarial
    access to LLMs is black-box. Motivated by these observations, we present Private
    Mixing of Ensemble Distributions (PMixED): a private prediction protocol for next-token
    prediction that utilizes the inherent stochasticity of next-token sampling and
    a public model to achieve Differential Privacy. We formalize this by introducing
    RD-mollifers which project each of the model''s output distribution from an ensemble
    of fine-tuned LLMs onto a set around a public LLM''s output distribution, then
    average the projected distributions and sample from it. Unlike DP-SGD which needs
    to consider the model architecture during training, PMixED is model agnostic,
    which makes PMixED a very appealing solution for current deployments. Our results
    show that PMixED achieves a stronger privacy guarantee than sample-level privacy
    and outperforms DP-SGD for privacy $\epsilon = 8$ on large-scale datasets. Thus,
    PMixED offers a practical alternative to DP training methods for achieving strong
    generative utility without compromising privacy.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Learning for NLP
  authors:
  - emails: '****@usc.edu'
    first_name: James
    homepage: https://james-flemings.github.io/
    last_name: Flemings
    name: James Flemings
    username: ~James_Flemings1
  - dblp_id: https://dblp.org/pid/43/8577
    emails: '****@usc.edu'
    first_name: Meisam
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=b5hImI4AAAAJ&view_op=list_works&sortby=pubdate
    homepage: https://sites.usc.edu/razaviyayn/
    institution: University of Southern California
    last_name: Razaviyayn
    name: Meisam Razaviyayn
    username: ~Meisam_Razaviyayn1
  - emails: '****@usc.edu'
    first_name: Murali
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=NAntFXIAAAAJ
    homepage: http://annavar.am
    institution: University of Southern California
    last_name: Annavaram
    name: Murali Annavaram
    username: ~Murali_Annavaram1
  decision: toMainConference
  end_page: 4835
  file: 454.pdf
  id: 454
  num_pages: 15
  openreview_id: 0VdxYVEdPv
  pdf_file: 97be5a2cec4d03aedaa90a84f1e94d4ec57a2f56.pdf
  start_page: 4821
  title: Differentially Private Next-Token Prediction of Large Language Models
- abstract: Hate speech detection models are only as good as the data they are trained
    on. Datasets sourced from social media suffer from systematic gaps and biases,
    leading to unreliable models with simplistic decision boundaries. Adversarial
    datasets, collected by exploiting model weaknesses, promise to fix this problem.
    However, adversarial data collection can be slow and costly, and individual annotators
    have limited creativity. In this paper, we introduce GAHD, a new German Adversarial
    Hate speech Dataset comprising ca.\ 11k examples. During data collection, we explore
    new strategies for supporting annotators, to create more diverse adversarial examples
    more efficiently and provide a manual analysis of annotator disagreements for
    each strategy. Our experiments show that the resulting dataset is challenging
    even for state-of-the-art hate speech detection models, and that training on GAHD
    clearly improves model robustness. Further, we find that mixing multiple support
    strategies is most advantageous. We make GAHD publicly available at https://github.com/jagol/gahd.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - dblp_id: https://dblp.org/pid/267/9814
    emails: '****@gmail.com'
    first_name: Janis
    google_scholar_id: https://scholar.google.de/citations?user=S99lsKYAAAAJ&hl=de&oi=ao
    homepage: https://www.cl.uzh.ch/de/about-us/people/team/compling/jgoldz.html
    last_name: Goldzycher
    name: Janis Goldzycher
    orcid: https://orcid.org/0000-0002-0706-810X
    semantic_scholar_id: https://www.semanticscholar.org/author/Janis-Goldzycher/1583958717
    username: ~Janis_Goldzycher1
  - dblp_id: https://dblp.org/pid/282/4243
    emails: '****@unibocconi.it'
    first_name: Paul
    google_scholar_id: https://scholar.google.com/citations?user=7rpmd9cAAAAJ&hl=en
    homepage: https://paulrottger.com/
    institution: Bocconi University
    last_name: "R\xF6ttger"
    name: "Paul R\xF6ttger"
    orcid: https://orcid.org/0009-0008-7115-6893
    semantic_scholar_id: https://www.semanticscholar.org/author/Paul-R%C3%B6ttger/2043232919
    username: "~Paul_R\xF6ttger2"
  - emails: '****@ifi.uzh.ch'
    first_name: Gerold
    google_scholar_id: https://scholar.google.ch/citations?user=l_8L7NYAAAAJ&hl=en
    homepage: https://www.cl.uzh.ch/de/about-us/people/team/compling/gschneid.html
    institution: University of Zurich
    last_name: Schneider
    name: Gerold Schneider
    orcid: https://orcid.org/0000-0002-1905-6237
    username: ~Gerold_Schneider2
  decision: toMainConference
  end_page: 4855
  file: 455.pdf
  id: 455
  num_pages: 20
  openreview_id: mT5ys2PXGa
  pdf_file: 1720e750e9e0c6ffb1d75e5b7b5f0f66bc7ca693.pdf
  start_page: 4836
  title: 'Improving Adversarial Data Collection by Supporting Annotators: Lessons
    from GAHD, a German Hate Speech Dataset'
- abstract: Scaling up the number of parameters of language models has proven to be
    an effective approach to improve performance. For dense models, increasing their
    size proportionally increases their computational footprint. In this work, we
    seek to aggressively decouple learning capacity and FLOPs through Mixture-of-Experts
    (MoE) style models with large knowledge-rich vocabulary based routing functions.
    Our proposed approach, dubbed Mixture of Word Experts (MoWE), can be seen as a
    memory augmented model, where a large set of word-specific experts play the role
    of a sparse memory.  We demonstrate that MoWE performs significantly better than
    the T5 family of models with similar number of FLOPs in a variety of NLP tasks.
    Moreover, MoWE outperforms traditional MoE models on knowledge intensive tasks
    and has similar performance to complex memory augmented approaches that often
    require to invoke custom mechanisms to search the sparse memory.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Efficient/Low-Resource Methods for NLP
  authors:
  - emails: '****@gmail.com'
    first_name: Cicero
    google_scholar_id: https://scholar.google.com.br/citations?user=sx8MOL8AAAAJ&hl=en
    institution: Research, Google
    last_name: Nogueira Dos Santos
    name: Cicero Nogueira dos Santos
    username: ~Cicero_Nogueira_dos_Santos1
  - emails: '****@gmail.com'
    first_name: James
    google_scholar_id: https://scholar.google.com/citations?user=qsPv098AAAAJ&hl=en
    institution: Google
    last_name: Lee-Thorp
    name: James Lee-Thorp
    orcid: https://orcid.org/0000-0001-6445-7155
    username: ~James_Lee-Thorp1
  - dblp_id: https://dblp.org/pid/209/2753
    emails: '****@google.com'
    first_name: Isaac
    google_scholar_id: https://scholar.google.com/citations?user=JFR45L8AAAAJ
    homepage: https://www.linkedin.com/mwlite/in/isaac-noble-71401a14
    institution: Google
    last_name: Noble
    name: Isaac Noble
    username: ~Isaac_Noble1
  - dblp_id: https://dblp.org/pid/03/3277
    emails: '****@gmail.com'
    first_name: Chung-Ching
    institution: Google
    last_name: Chang
    name: Chung-Ching Chang
    username: ~Chung-Ching_Chang1
  - dblp_id: https://dblp.org/pid/09/2971.html
    emails: '****@google.com'
    first_name: David
    google_scholar_id: https://scholar.google.com/citations?user=9k31iVQAAAAJ&hl=en&oi=ao
    institution: Google
    last_name: Uthus
    name: David Uthus
    semantic_scholar_id: https://www.semanticscholar.org/author/David-C.-Uthus/3046959
    username: ~David_Uthus1
  decision: toMainConference
  end_page: 4869
  file: 457.pdf
  id: 457
  num_pages: 14
  openreview_id: JHENkpfZYr
  pdf_file: 9b4d32eab1aeeb7c7615544fe3eff1d9259df09a.pdf
  start_page: 4856
  title: Memory Augmented Language Models through Mixture of Word Experts
- abstract: This paper addresses the issue of automated feedback generation for English
    language learners by presenting a corpus of English essays and their corresponding
    feedback, called LEAF, collected from the "essayforum" website. The corpus comprises
    approximately 6K essay-feedback pairs, offering a diverse and valuable resource
    for developing personalized feedback generation systems that address the critical
    deficiencies within essays, spanning from rectifying grammatical errors to offering
    insights on argumentative aspects and organizational coherence. Using this corpus,
    we present and compare multiple feedback generation baselines. Our findings shed
    light on the challenges of providing personalized feedback and highlight the potential
    of the LEAF corpus in advancing automated essay evaluation.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - dblp_id: https://dblp.org/pid/218/5172
    emails: '****@georgetown.edu'
    first_name: Shabnam
    google_scholar_id: https://scholar.google.com/citations?user=UEowojcAAAAJ&hl=en
    homepage: https://shabnam-b.github.io/
    last_name: Behzad
    name: Shabnam Behzad
    semantic_scholar_id: https://www.semanticscholar.org/author/Shabnam-Behzad/41022207
    username: ~Shabnam_Behzad1
  - dblp_id: https://dblp.org/pid/82/5
    emails: '****@cs.pitt.edu'
    first_name: Omid
    google_scholar_id: https://scholar.google.com/citations?user=lbg9VhEAAAAJ
    institution: Educational Testing Service
    last_name: Kashefi
    name: Omid Kashefi
    orcid: https://orcid.org/0000-0003-2898-9853
    semantic_scholar_id: https://www.semanticscholar.org/author/Omid-Kashefi/2103235
    username: ~Omid_Kashefi1
  - dblp_id: https://dblp.org/pid/43/5992
    emails: '****@gmail.com'
    first_name: Swapna
    google_scholar_id: https://scholar.google.com/citations?user=vt_ki6oAAAAJ&hl=en
    last_name: Somasundaran
    name: Swapna Somasundaran
    username: ~Swapna_Somasundaran1
  decision: toMainConference
  end_page: 4879
  file: 458.pdf
  id: 458
  num_pages: 10
  openreview_id: uvWvL4x1hW
  pdf_file: 4e35dc22293ceb1a1266cfe014430fa9033f76e6.pdf
  start_page: 4870
  title: 'LEAF: Language Learners'' English Essays and Feedback Corpus'
- abstract: We present Impossible Distillation, a novel framework for paraphrasing
    and sentence summarization, that distills a high-quality dataset and model from
    a low-quality teacher that itself cannot perform these tasks. Unlike prior works
    that rely on an extreme-scale teacher model (e.g., GPT3) or task-specific architecture,
    we hypothesize and verify the paraphrastic proximity intrinsic to pre-trained
    LMs (e.g., GPT2), where paraphrases occupy a proximal subspace in the LM distribution.
    By identifying and distilling generations from these subspaces, Impossible Distillation
    produces a high-quality dataset and model even from GPT2-scale LMs. We evaluate
    our method on multiple benchmarks spanning unconstrained / syntax-controlled paraphrase
    generation and sentence summarization. Our model with 770M parameters consistently
    outperforms strong baselines, including models distilled from ChatGPT, and sometimes,
    even ChatGPT itself. Also, we find that our distilled dataset from 1.5B LMs exhibits
    higher diversity and fidelity than up to 13 times larger datasets.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Generation
  authors:
  - dblp_id: https://dblp.org/pid/192/7707
    emails: '****@cs.washington.edu'
    first_name: Jaehun
    google_scholar_id: https://scholar.google.com/citations?user=_bXzUGEAAAAJ&hl=en
    homepage: https://jaehunjung.com
    institution: University of Washington
    last_name: Jung
    name: Jaehun Jung
    orcid: https://orcid.org/0000-0002-0292-3074
    username: ~Jaehun_Jung1
  - dblp_id: https://dblp.org/pid/179/4587
    emails: '****@uw.edu'
    first_name: Peter
    google_scholar_id: https://scholar.google.ca/citations?user=9ubCBYwAAAAJ&hl=en
    homepage: https://homes.cs.washington.edu/~pawest/
    last_name: West
    name: Peter West
    semantic_scholar_id: https://www.semanticscholar.org/author/Peter-West/119659229
    username: ~Peter_West1
  - emails: '****@cs.washington.edu'
    first_name: Liwei
    google_scholar_id: https://scholar.google.com/citations?user=lcPsDgUAAAAJ&hl=en
    homepage: https://liweijiang.me
    last_name: Jiang
    name: Liwei Jiang
    username: ~Liwei_Jiang2
  - dblp_id: https://dblp.org/pid/276/6005
    emails: '****@allenai.org'
    first_name: Faeze
    google_scholar_id: https://scholar.google.com/citations?user=4Da7Li4AAAAJ&hl=en
    homepage: https://users.soe.ucsc.edu/~hannahbrahman/
    institution: Allen Institute for AI
    last_name: Brahman
    name: Faeze Brahman
    semantic_scholar_id: https://www.semanticscholar.org/author/Faeze-Brahman/9252833
    username: ~Faeze_Brahman1
  - dblp_id: https://dblp.org/pid/24/10879
    emails: '****@cs.washington.edu'
    first_name: Ximing
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=Hb6WyyoAAAAJ
    institution: Department of Computer Science, University of Washington
    last_name: Lu
    name: Ximing Lu
    username: ~Ximing_Lu1
  - dblp_id: https://dblp.org/pid/336/3238
    emails: '****@uw.edu'
    first_name: Jillian
    google_scholar_id: https://scholar.google.com/citations?user=Gnk0E_QAAAAJ&hl=en
    homepage: https://jillianfisher.owlstown.net
    institution: University of Washington
    last_name: Fisher
    name: Jillian Fisher
    semantic_scholar_id: https://www.semanticscholar.org/author/Jillian-R.-Fisher/33772445
    username: ~Jillian_Fisher1
  - emails: '****@gmail.com'
    first_name: Taylor
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=dWaTwM4AAAAJ
    homepage: https://tsor13.github.io
    institution: University of Washington and Brigham Young University
    last_name: Sorensen
    name: Taylor Sorensen
    orcid: https://orcid.org/0000-0002-3251-3527
    username: ~Taylor_Sorensen1
  - dblp_id: https://dblp.org/pid/89/579
    emails: '****@cs.washington.edu'
    first_name: Yejin
    google_scholar_id: https://scholar.google.com/citations?user=vhP-tlcAAAAJ&hl=en
    homepage: https://homes.cs.washington.edu/~yejin/
    institution: Department of Computer Science, University of Washington
    last_name: Choi
    name: Yejin Choi
    semantic_scholar_id: https://www.semanticscholar.org/author/Yejin-Choi/1699545?sort=year
    username: ~Yejin_Choi1
  decision: toMainConference
  end_page: 4895
  file: 459.pdf
  id: 459
  num_pages: 16
  openreview_id: MbOrOS62Xu
  pdf_file: e55ccfb5f24971fc0c52ad9d118c3e14923cbb06.pdf
  start_page: 4880
  title: 'Impossible Distillation for Paraphrasing and Summarization: How to Make
    High-quality Lemonade out of Small, Low-quality Model'
- abstract: "Single document news summarization has seen substantial progress on faithfulness\
    \ in recent years, driven by research on the evaluation of factual consistency,\
    \ or hallucinations. We ask whether these advances carry over to other text summarization\
    \ domains. We propose a new evaluation benchmark on topic-focused dialogue summarization,\
    \ generated by LLMs of varying sizes. We provide binary sentence- level human\
    \ annotations of the factual consistency of these summaries along with detailed\
    \ explanations of factually inconsistent sentences. Our analysis shows that existing\
    \ LLMs hallucinate significant amounts of factual errors in the dialogue domain,\
    \ regardless of the model\u2019s size. On the other hand, when LLMs, including\
    \ GPT-4, serve as binary factual evaluators, they perform poorly and can be outperformed\
    \ by prevailing state-of-the-art specialized factuality evaluation metrics. Finally,\
    \ we conducted an analysis of hallucination types with a curated error taxonomy.\
    \ We find that there are diverse errors and error distributions in model-generated\
    \ summaries and that non-LLM based metrics can capture all error types better\
    \ than LLM-based evaluators."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Summarization
  authors:
  - dblp_id: https://dblp.org/pid/279/6572
    emails: '****@utexas.edu'
    first_name: Liyan
    google_scholar_id: https://scholar.google.com/citations?user=P3dSamwAAAAJ&hl=en
    homepage: https://www.tangliyan.com
    last_name: Tang
    name: Liyan Tang
    semantic_scholar_id: https://www.semanticscholar.org/author/Liyan-Tang/46815797
    username: ~Liyan_Tang1
  - dblp_id: https://dblp.org/pid/205/8962
    emails: '****@gmail.com'
    first_name: Igor
    google_scholar_id: https://scholar.google.com/citations?user=TVs0lP8AAAAJ
    homepage: https://shalyminov.com
    institution: Amazon
    last_name: Shalyminov
    name: Igor Shalyminov
    orcid: https://orcid.org/0000-0001-9664-1774
    semantic_scholar_id: https://www.semanticscholar.org/author/Igor-Shalyminov/24879056
    username: ~Igor_Shalyminov1
  - emails: '****@gmail.com'
    first_name: Amy
    institution: Amazon
    last_name: Wong
    middle_name: Wing-mei
    name: Amy Wing-mei Wong
    username: ~Amy_Wing-mei_Wong1
  - emails: '****@amazon.com'
    first_name: Jon
    institution: Amazon
    last_name: Burnsky
    name: Jon Burnsky
    username: ~Jon_Burnsky1
  - emails: '****@gmail.com'
    first_name: Jake
    google_scholar_id: https://scholar.google.com/citations?user=T5_uFy4AAAAJ&hl=en
    homepage: https://jwv.dev
    institution: Amazon
    last_name: Vincent
    middle_name: W.
    name: Jake W. Vincent
    orcid: https://orcid.org/0000-0002-4794-4370
    semantic_scholar_id: https://www.semanticscholar.org/author/145566543
    username: ~Jake_W._Vincent1
  - emails: '****@gmail.com'
    first_name: Yu'an
    homepage: https://yu-an.github.io/
    last_name: Yang
    name: Yu'an Yang
    username: ~Yu'an_Yang1
  - emails: '****@amazon.co.uk'
    first_name: Siffi
    homepage: https://scholar.google.com/citations?user=zaXmGr8AAAAJ&hl=en
    last_name: Singh
    name: Siffi Singh
    username: ~Siffi_Singh1
  - emails: '****@gmail.com'
    first_name: Song
    google_scholar_id: https://scholar.google.com/citations?user=aWmHP7IAAAAJ&hl=en&oi=ao
    homepage: http://songfeng.github.io/
    institution: Amazon
    last_name: Feng
    name: Song Feng
    orcid: https://orcid.org/0000-0002-7760-1854
    semantic_scholar_id: https://www.semanticscholar.org/author/Song-Feng/145480864
    username: ~Song_Feng3
  - dblp_id: https://dblp.org/pid/204/3381
    emails: '****@gmail.com'
    first_name: Hwanjun
    google_scholar_id: https://scholar.google.com/citations?user=Ijzuc-8AAAAJ&hl=en
    homepage: https://songhwanjun.github.io/
    institution: AWS AI Labs
    last_name: Song
    name: Hwanjun Song
    username: ~Hwanjun_Song2
  - emails: '****@gmail.com'
    first_name: Hang
    google_scholar_id: https://scholar.google.com/citations?user=UxOvKVUAAAAJ&hl=en
    institution: Amazon
    last_name: Su
    name: Hang Su
    username: ~Hang_Su7
  - emails: '****@amazon.com'
    first_name: Lijia
    google_scholar_id: https://scholar.google.com/citations?user=fRLrTw4AAAAJ&hl=en
    institution: Amazon
    last_name: Sun
    name: Lijia Sun
    username: ~Lijia_Sun1
  - dblp_id: https://dblp.org/pid/64/6544-3
    emails: '****@amazon.com'
    first_name: Yi
    google_scholar_id: https://scholar.google.com/citations?user=sxs6h_wAAAAJ&hl=en
    institution: Amazon
    last_name: Zhang
    name: Yi Zhang
    username: ~Yi_Zhang13
  - dblp_id: https://dblp.org/pid/03/8053
    emails: '****@gmail.com'
    first_name: Saab
    google_scholar_id: https://scholar.google.de/citations?user=1tCbwIQAAAAJ&hl=en
    institution: Amazon
    last_name: Mansour
    name: Saab Mansour
    semantic_scholar_id: https://www.semanticscholar.org/author/Saab-Mansour/39674628
    username: ~Saab_Mansour1
  - dblp_id: https://dblp.org/pid/m/KathleenMcKeown
    emails: '****@cs.columbia.edu'
    first_name: Kathleen
    google_scholar_id: https://scholar.google.com.tw/citations?user=ujDhg2sAAAAJ
    homepage: http://www.cs.columbia.edu/~kathy/
    last_name: McKeown
    name: Kathleen McKeown
    username: ~Kathleen_McKeown1
  decision: toMainConference
  end_page: 4921
  file: 460.pdf
  id: 460
  num_pages: 26
  openreview_id: NDyAWSd96T
  pdf_file: 3a2d7d086d51bd445ebbb1de35c01d76ccb56c9c.pdf
  start_page: 4896
  title: 'TofuEval: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue Summarization'
- abstract: News media often strive to minimize explicit moral language in news articles,
    yet most articles are dense with moral values as expressed through the reported
    events themselves. However, values that are reflected in the intricate dynamics
    among *participating entities* and *moral events* are far more challenging for
    most NLP systems to detect, including LLMs. To study this phenomenon, we annotate
    a new dataset, **MORAL EVENTS**, consisting of $5,494$ structured event annotations
    on $474$ news articles by diverse US media across the political spectrum. We further
    propose **MOKA**, a moral event extraction framework with **MO**ral **K**nowledge
    **A**ugmentation, which leverages knowledge derived from moral words and moral
    scenarios to produce structural representations of morality-bearing events. Experiments
    show that **MOKA** outperforms competitive baselines across three moral event
    understanding tasks. Further analysis shows even ostensibly nonpartisan media
    engage in the selective reporting of moral events.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Extraction
  authors:
  - dblp_id: https://dblp.org/pid/277/5381
    emails: '****@umich.edu'
    first_name: Xinliang Frederick
    google_scholar_id: https://scholar.google.com/citations?user=-uGCT5QAAAAJ&hl=en
    homepage: https://web.eecs.umich.edu/~xlfzhang/
    last_name: Zhang
    name: Xinliang Frederick Zhang
    semantic_scholar_id: https://www.semanticscholar.org/author/Xinliang-Frederick-Zhang/2108030243
    username: ~Xinliang_Frederick_Zhang1
  - emails: '****@hawaii.edu'
    first_name: Winston
    google_scholar_id: https://scholar.google.com/citations?user=OtiQrBwAAAAJ
    institution: University of Hawaii at Hilo
    last_name: Wu
    name: Winston Wu
    username: ~Winston_Wu1
  - dblp_id: https://dblp.org/pid/220/2037
    emails: '****@northeastern.edu'
    first_name: Nicholas
    homepage: http://nickbeauchamp.com
    institution: Northeastern University
    last_name: Beauchamp
    name: Nicholas Beauchamp
    username: ~Nicholas_Beauchamp1
  - dblp_id: https://dblp.org/pid/49/3800-8
    emails: '****@umich.edu'
    first_name: Lu
    google_scholar_id: https://scholar.google.com/citations?user=uczqEdUAAAAJ&hl=en
    homepage: https://web.eecs.umich.edu/~wangluxy/
    institution: University of Michigan
    last_name: Wang
    name: Lu Wang
    username: ~Lu_Wang9
  decision: toMainConference
  end_page: 4943
  file: 461.pdf
  id: 461
  num_pages: 22
  openreview_id: x8nqQ5l1tI
  pdf_file: 43bd030bec7cbb52f17e12265ab4d4a5e5967bd9.pdf
  start_page: 4922
  title: 'MOKA: Moral Knowledge Augmentation for Moral Event Extraction'
- abstract: "In this paper we study the fine-tuning of pre-trained large high-resource\
    \ language models (LLMs) into many-to-one multilingual machine translators for\
    \ extremely-low-resource languages such as endangered Indigenous languages. We\
    \ explore those issues using datasets created from pseudo-parallel translations\
    \ to English of The Bible} written in 39~Brazilian Indigenous languages using\
    \ mBART50 and WMT19 as pre-trained models and multiple translation metrics. We\
    \ examine bilingual and multilingual models and show that, according to machine\
    \ translation metrics, same-linguistic family models tend to perform best. However,\
    \ we also found that many-to-one multilingual systems have a tendency to learn\
    \ a \u201Crogue\u201D strategy of storing output strings from the training data\
    \ in the LLM structure and retrieving them instead of performing actual translations.\
    \ We show that rephrasing the output of the training samples seems to solve the\
    \ problem."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Translation
  authors:
  - dblp_id: https://dblp.org/pid/20/3269
    emails: '****@br.ibm.com'
    first_name: Paulo
    google_scholar_id: https://scholar.google.com.br/citations?user=VHj54jYAAAAJ&hl=en
    institution: International Business Machines
    last_name: Cavalin
    name: Paulo Cavalin
    username: ~Paulo_Cavalin1
  - emails: '****@ibm.com'
    first_name: Pedro Henrique
    institution: "Pontif\xEDcia Universidade Cat\xF3lica do Rio de Janeiro"
    last_name: Domingues
    middle_name: Leite Da Silva Pires
    name: Pedro Henrique Leite da Silva Pires Domingues
    orcid: https://orcid.org/0000-0002-4838-8396
    username: ~Pedro_Henrique_Leite_da_Silva_Pires_Domingues1
  - dblp_id: https://dblp.org/pid/53/154.html
    emails: '****@pinhanez.com'
    first_name: Claudio
    google_scholar_id: https://scholar.google.com/citations?user=YPq3ax4AAAAJ&hl=en&oi=sra
    last_name: Pinhanez
    name: Claudio Pinhanez
    orcid: https://orcid.org/0000-0001-6715-1290
    username: ~Claudio_Pinhanez1
  - dblp_id: https://dblp.org/pid/36/10032.html
    emails: '****@br.ibm.com'
    first_name: Julio
    google_scholar_id: https://scholar.google.com/citations?user=pMc6ULYAAAAJ&hl
    institution: International Business Machines
    last_name: Nogima
    name: Julio Nogima
    username: ~Julio_Nogima1
  decision: toMainConference
  end_page: 4955
  file: 463.pdf
  id: 463
  num_pages: 12
  openreview_id: 8ie2fNKCjw
  pdf_file: 276f0c1336b6e1ba438aea86e3d05fefb3bd7c07.pdf
  start_page: 4944
  title: Fixing Rogue Memorization in Many-to-One Multilingual Translators of Extremely-Low-Resource
    Languages by Rephrasing Training Samples
- abstract: 'Cross-lingual transfer can be achieved through two main approaches: zero-shot
    transfer or machine translation (MT). While the former has been the dominant approach,
    both have been shown to be competitive. In this work, we compare the current performance
    and long-term viability of these methods. We leverage lexical gaps to create a
    multilingual question answering dataset, which provides a difficult domain for
    evaluation. Both approaches struggle in this setting, though zero-shot transfer
    performs better, as current MT outputs are not specific enough for the task. Using
    oracle translation offers the best performance, showing that this approach can
    perform well long-term, however current MT quality is a bottleneck. We also conduct
    an exploratory study to see if humans produce translations sufficient for the
    task with only general instructions. We find this to be true for the majority
    of translators, but not all. This indicates that while translation has the potential
    to outperform zero-shot approaches, creating MT models that generate accurate
    task-specific translations may not be straightforward.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multilinguality and Language Diversity
  authors:
  - dblp_id: https://dblp.org/pid/242/7859
    emails: '****@colorado.edu'
    first_name: Abteen
    google_scholar_id: https://scholar.google.com/citations?user=38YRik0AAAAJ&hl=en
    institution: University of Colorado, Boulder
    last_name: Ebrahimi
    name: Abteen Ebrahimi
    semantic_scholar_id: https://www.semanticscholar.org/author/Abteen-Ebrahimi/146057134
    username: ~Abteen_Ebrahimi1
  - dblp_id: https://dblp.org/pid/182/1923
    emails: '****@colorado.edu'
    first_name: Katharina
    institution: "Johannes-Gutenberg Universit\xE4t Mainz, University of Colorado,\
      \ Boulder and New York University"
    last_name: Wense
    middle_name: Von Der
    name: Katharina von der Wense
    semantic_scholar_id: https://www.semanticscholar.org/author/Katharina-Kann/3422953
    username: ~Katharina_von_der_Wense1
  decision: toMainConference
  end_page: 4971
  file: 464.pdf
  id: 464
  num_pages: 16
  openreview_id: sWpwgRMqBZ
  pdf_file: 99fe1db74bd8d5b7840b32112922d8614491a064.pdf
  start_page: 4956
  title: 'Zero-Shot vs. Translation-Based Cross-Lingual Transfer: The Case of Lexical
    Gaps'
- abstract: 'While multilingual machine translation (MNMT) systems hold substantial
    promise, they also have security vulnerabilities. Our research highlights that
    MNMT systems can be susceptible to a particularly devious style of backdoor attack,
    whereby an attacker injects poisoned data into a low-resource language pair to
    cause malicious translations in other languages, including high-resource languages.

    Our experimental results reveal that injecting less than 0.01\% poisoned data
    into a low-resource language pair can achieve an average 20\% attack success rate
    in attacking high-resource language pairs. This type of attack is of particular
    concern, given the larger attack surface of languages inherent to low-resource
    settings. Our aim is to bring attention to these vulnerabilities within MNMT systems
    with the hope of encouraging the community to address security concerns in machine
    translation, especially in the context of low-resource languages.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Translation
  authors:
  - dblp_id: https://dblp.org/pid/125/8189
    emails: '****@student.unimelb.edu.au'
    first_name: Jun
    google_scholar_id: https://scholar.google.com/citations?user=pW78ZCUAAAAJ&hl=en
    last_name: Wang
    name: Jun Wang
    semantic_scholar_id: https://www.semanticscholar.org/author/Jun-Wang/1466536006
    username: ~Jun_Wang29
  - dblp_id: https://dblp.org/pid/127/0174
    emails: '****@gmail.com'
    first_name: Qiongkai
    google_scholar_id: https://scholar.google.com.au/citations?user=wCer2WUAAAAJ
    homepage: https://xuqiongkai.github.io
    institution: Macquarie University
    last_name: Xu
    name: Qiongkai Xu
    orcid: https://orcid.org/0000-0003-3312-6825
    semantic_scholar_id: https://www.semanticscholar.org/author/Qiongkai-Xu/3101288
    username: ~Qiongkai_Xu1
  - dblp_id: https://dblp.org/pid/182/1859
    emails: '****@gmail.com'
    first_name: Xuanli
    google_scholar_id: https://scholar.google.com/citations?user=TU8t0iAAAAAJ&hl=zh-CN
    institution: University College London, University of London
    last_name: He
    name: Xuanli He
    username: ~Xuanli_He2
  - dblp_id: https://dblp.org/pid/90/1092
    emails: '****@unimelb.edu.au'
    first_name: Benjamin
    google_scholar_id: https://scholar.google.com.au/citations?user=hMG_gR4AAAAJ&hl=en&oi=ao
    homepage: http://www.bipr.net/
    institution: The University of Melbourne and The University of Melbourne
    last_name: Rubinstein
    middle_name: I. P.
    name: Benjamin I. P. Rubinstein
    orcid: https://orcid.org/0000-0002-2947-6980
    username: ~Benjamin_I._P._Rubinstein1
  - dblp_id: https://dblp.org/pid/66/4613
    emails: '****@google.com'
    first_name: Trevor
    google_scholar_id: https://scholar.google.com.au/citations?user=FCom398AAAAJ&hl=en
    homepage: https://people.eng.unimelb.edu.au/tcohn/
    institution: Google and The University of Melbourne
    last_name: Cohn
    name: Trevor Cohn
    username: ~Trevor_Cohn1
  decision: toMainConference
  end_page: 4991
  file: 470.pdf
  id: 470
  num_pages: 20
  openreview_id: pMiB19w56s
  pdf_file: bd1e33cf7773217d5138d5c92cb40b8f64042fb9.pdf
  start_page: 4972
  title: Backdoor Attacks on Multilingual Machine Translation
- abstract: Scientific jargon can confuse researchers when they read materials from
    other domains. Identifying and translating jargon for individual researchers could
    speed up research, but current methods of jargon identification mainly use corpus-level
    familiarity indicators rather than modeling researcher-specific needs, which can
    vary greatly based on each researcher's background. We collect a dataset of over
    10K term familiarity annotations from 11 computer science researchers for terms
    drawn from 100 paper abstracts. Analysis of this data reveals that jargon familiarity
    and information needs vary widely across annotators, even within the same sub-domain
    (e.g., NLP). We investigate features representing domain, subdomain, and individual
    knowledge to predict individual jargon familiarity. We compare supervised and
    prompt-based approaches, finding that prompt-based methods using information about
    the individual researcher (e.g., personal publications, self-defined subfield
    of research) yield the highest accuracy, though the task remains difficult and
    supervised approaches have lower false positive rates. This research offers insights
    into features and methods for the novel task of integrating personal data into
    scientific jargon identification.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - emails: '****@uw.edu'
    first_name: Yue
    google_scholar_id: https://scholar.google.com/citations?user=09fJpa4AAAAJ&hl=en
    homepage: https://yueguo-50.github.io
    last_name: Guo
    name: Yue Guo
    orcid: https://orcid.org/0000-0003-3754-3781
    username: ~Yue_Guo5
  - emails: '****@allenai.org'
    first_name: Joseph Chee
    homepage: https://joe.cat
    institution: Allen Institute for Artificial Intelligence
    last_name: Chang
    name: Joseph Chee Chang
    username: ~Joseph_Chee_Chang1
  - emails: '****@allenai.org'
    first_name: Maria
    google_scholar_id: https://scholar.google.com/citations?user=lNaynLcAAAAJ
    homepage: https://maria-antoniak.github.io/
    last_name: Antoniak
    name: Maria Antoniak
    semantic_scholar_id: https://www.semanticscholar.org/author/Maria-Antoniak/34199564
    username: ~Maria_Antoniak1
  - emails: '****@allenai.org'
    first_name: Erin
    institution: Allen Institute for Artificial Intelligence
    last_name: Bransom
    name: Erin Bransom
    semantic_scholar_id: https://www.semanticscholar.org/author/Erin-Bransom/2203427167
    username: ~Erin_Bransom1
  - emails: '****@uw.edu'
    first_name: Trevor
    homepage: http://bime.uw.edu/faculty/trevor-cohen/
    institution: University of Washington
    last_name: Cohen
    name: Trevor Cohen
    username: ~Trevor_Cohen1
  - dblp_id: https://dblp.org/pid/220/2575
    emails: '****@uw.edu'
    first_name: Lucy
    google_scholar_id: https://scholar.google.com/citations?user=REBtJOYAAAAJ&hl=en
    homepage: https://llwang.net/
    institution: University of Washington and Allen Institute for Artificial Intelligence
    last_name: Wang
    middle_name: Lu
    name: Lucy Lu Wang
    orcid: https://orcid.org/0000-0001-8752-6635
    semantic_scholar_id: https://www.semanticscholar.org/author/Lucy-Lu-Wang/31860505
    username: ~Lucy_Lu_Wang1
  - emails: '****@allenai.org'
    first_name: Tal
    google_scholar_id: https://scholar.google.com/citations?user=V0cUoeoAAAAJ&hl=en
    homepage: https://talaugust.github.io/
    last_name: August
    name: Tal August
    semantic_scholar_id: https://www.semanticscholar.org/author/Tal-August/50509991
    username: ~Tal_August1
  decision: toMainConference
  end_page: 5007
  file: 471.pdf
  id: 471
  num_pages: 16
  openreview_id: XcJqBRd62f
  pdf_file: 75ee3851eff0e37dade5ceb0b8fac0938b0a1316.pdf
  start_page: 4992
  title: Personalized Jargon Identification for Enhanced Interdisciplinary Communication
- abstract: "The widespread adoption of large language models (LLMs) across various\
    \ regions underscores the urgent need to evaluate their alignment with human values.\
    \ Current benchmarks, however, fall short of effectively uncovering safety vulnerabilities\
    \ in LLMs. \nDespite numerous models achieving high scores and 'topping the chart'\
    \ in these evaluations, there is still a significant gap in LLMs' deeper alignment\
    \ with human values and achieving genuine harmlessness. To this end, this paper\
    \ proposes a value alignment benchmark named Flames, which encompasses both common\
    \ harmlessness principles and a unique morality dimension that integrates specific\
    \ Chinese values such as harmony. Accordingly, we carefully design adversarial\
    \ prompts that incorporate complex scenarios and jailbreaking methods, mostly\
    \ with implicit malice. By prompting 17 mainstream LLMs, we obtain model responses\
    \ and rigorously annotate them for detailed evaluation. Our findings indicate\
    \ that all the evaluated LLMs demonstrate relatively poor performance on Flames,\
    \ particularly in the safety and fairness dimensions. We also develop a lightweight\
    \ specified scorer capable of scoring LLMs across multiple dimensions to efficiently\
    \ evaluate new models on the benchmark. The complexity of \\textsc{Flames} has\
    \ far exceeded existing benchmarks, setting a new challenge for contemporary LLMs\
    \ and highlighting the need for further alignment of LLMs. \nOur benchmark is\
    \ publicly available at https://github.com/AIFlames/Flames."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Ethics, Bias, and Fairness
  authors:
  - emails: '****@pjlab.org.cn'
    first_name: Kexin
    last_name: Huang
    name: Kexin Huang
    username: ~Kexin_Huang3
  - emails: '****@m.fudan.edu.cn'
    first_name: Xiangyang
    google_scholar_id: https://scholar.google.com.hk/citations?user=U8QD9mwAAAAJ&hl=en
    last_name: Liu
    name: Xiangyang Liu
    semantic_scholar_id: https://www.semanticscholar.org/author/Xiangyang-Liu/2144226697
    username: ~Xiangyang_Liu3
  - emails: '****@fudan.edu.cn'
    first_name: Qianyu
    homepage: https://scholar.google.com/citations?user=5QPvmzQAAAAJ&hl=zh-CN&oi=ao
    last_name: Guo
    name: Qianyu Guo
    username: ~Qianyu_Guo2
  - dblp_id: https://dblp.org/pid/254/1189
    emails: '****@fudan.edu.cn'
    first_name: Tianxiang
    google_scholar_id: https://scholar.google.com/citations?user=puHFkM0AAAAJ
    homepage: https://txsun1997.github.io/
    last_name: Sun
    name: Tianxiang Sun
    semantic_scholar_id: https://www.semanticscholar.org/author/Tianxiang-Sun/153345698
    username: ~Tianxiang_Sun1
  - emails: '****@163.com'
    first_name: Jiawei
    homepage: https://www.researchgate.net/profile/Jiawei-Sun-18
    last_name: Sun
    name: Jiawei Sun
    username: ~Jiawei_Sun3
  - emails: '****@gmail.com'
    first_name: Yaru
    last_name: Wang
    name: Yaru Wang
    username: ~Yaru_Wang4
  - emails: '****@gmail.com'
    first_name: Zeyang
    homepage: https://honey-lily-f34.notion.site/profile-4adb7c3d552246e0b93de39e7aa8f73c?pvs=4
    last_name: Zhou
    name: Zeyang Zhou
    username: ~Zeyang_Zhou3
  - emails: '****@gmail.com'
    first_name: Yixu
    google_scholar_id: https://scholar.google.com/citations?user=EYP7wNIAAAAJ&hl=zh-CN
    last_name: Wang
    name: Yixu Wang
    username: ~Yixu_Wang1
  - emails: '****@pjlab.org.cn'
    first_name: Yan
    institution: Shanghai Artificial Intelligence Laboratory
    last_name: Teng
    name: Yan Teng
    orcid: https://orcid.org/0000-0002-7069-4728
    username: ~Yan_Teng1
  - dblp_id: https://dblp.org/pid/69/1395
    emails: '****@fudan.edu.cn'
    first_name: Xipeng
    google_scholar_id: https://scholar.google.com/citations?user=Pq4Yp_kAAAAJ&hl=en
    homepage: https://xpqiu.github.io/
    institution: Fudan University
    last_name: Qiu
    name: Xipeng Qiu
    orcid: https://orcid.org/0000-0001-7163-5247
    semantic_scholar_id: https://www.semanticscholar.org/author/Xipeng-Qiu/1767521
    username: ~Xipeng_Qiu1
  - emails: '****@pjlab.org.cn'
    first_name: Yingchun
    institution: Shanghai Artificial Intelligence Laboratory
    last_name: Wang
    name: Yingchun Wang
    username: ~Yingchun_Wang2
  - emails: '****@ie.cuhk.edu.hk'
    first_name: Dahua
    google_scholar_id: https://scholar.google.com/citations?user=GMzzRRUAAAAJ
    homepage: http://dahua.site
    institution: The Chinese University of Hong Kong
    last_name: Lin
    name: Dahua Lin
    username: ~Dahua_Lin1
  decision: toMainConference
  end_page: 5048
  file: 473.pdf
  id: 473
  num_pages: 41
  openreview_id: m3KUYlsvPb
  pdf_file: 718d8e1540f7a8520b6238ee16aced11f2b72675.pdf
  start_page: 5008
  title: 'Flames: Benchmarking Value Alignment of LLMs in Chinese'
- abstract: Models of various NLP tasks have been shown to exhibit stereotypes, and
    the bias in the question answering (QA) models is especially harmful as the output
    answers might be directly consumed by the end users. There have been datasets
    to evaluate bias in QA models, while bias mitigation technique for the QA models
    is still under-explored. In this work, we propose BMBI, an approach to mitigate
    the bias of multiple-choice QA models. Based on the intuition that a model would
    lean to be more biased if it learns from a biased example, we measure the bias
    level of a query instance by observing its influence on another instance. If the
    influenced instance is more biased, we derive that the query instance is biased.
    We then use the bias level detected as an optimization objective to form a multi-task
    learning setting in addition to the original QA task. We further introduce a new
    bias evaluation metric to quantify bias in a comprehensive and sensitive way.
    We show that our method could be applied to multiple QA formulations across multiple
    bias categories. It can significantly reduce the bias level in all 9 bias categories
    in the BBQ dataset while maintaining comparable QA accuracy.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Ethics, Bias, and Fairness
  authors:
  - dblp_id: https://dblp.org/pid/232/6526
    emails: '****@ucla.edu'
    first_name: Mingyu
    google_scholar_id: https://scholar.google.com/citations?user=6tXXg6AAAAAJ
    homepage: https://derek.ma
    institution: University of California, Los Angeles
    last_name: Ma
    middle_name: Derek
    name: Mingyu Derek Ma
    semantic_scholar_id: https://www.semanticscholar.org/author/Mingyu-Derek-Ma/144592155
    username: ~Mingyu_Derek_Ma1
  - dblp_id: https://dblp.org/pid/158/9756
    emails: '****@gmail.com'
    first_name: Jiun-Yu
    institution: Amazon Alexa AI
    last_name: Kao
    name: Jiun-Yu Kao
    semantic_scholar_id: https://www.semanticscholar.org/author/Jiun-Yu-Kao/38705864?sort=pub-date
    username: ~Jiun-Yu_Kao1
  - emails: '****@gmail.com'
    first_name: Arpit
    google_scholar_id: https://scholar.google.com/citations?user=XXVjLVgAAAAJ&hl=en
    institution: Amazon
    last_name: Gupta
    name: Arpit Gupta
    username: ~Arpit_Gupta1
  - emails: '****@gmail.com'
    first_name: Yu-Hsiang
    homepage: https://yuhsianglin.github.io
    institution: Amazon
    last_name: Lin
    name: Yu-Hsiang Lin
    username: ~Yu-Hsiang_Lin2
  - dblp_id: https://dblp.org/pid/30/5943
    emails: '****@alumni.cmu.edu'
    first_name: Wenbo
    google_scholar_id: https://scholar.google.com/citations?user=7ImYaXEAAAAJ&hl=en
    institution: Amazon
    last_name: Zhao
    name: Wenbo Zhao
    username: ~Wenbo_Zhao1
  - dblp_id: https://dblp.org/pid/02/8178
    emails: '****@amazon.com'
    first_name: Tagyoung
    google_scholar_id: https://scholar.google.com/citations?user=_-egoNcAAAAJ&hl=en
    institution: Amazon
    last_name: Chung
    name: Tagyoung Chung
    semantic_scholar_id: https://www.semanticscholar.org/author/Tagyoung-Chung/2878984
    username: ~Tagyoung_Chung2
  - dblp_id: https://dblp.uni-trier.de/pid/w/WeiWang.html
    emails: '****@cs.ucla.edu'
    first_name: Wei
    google_scholar_id: https://scholar.google.com/citations?user=UedS9LQAAAAJ&hl=en
    homepage: http://www.cs.ucla.edu
    institution: University of California, Los Angeles
    last_name: Wang
    name: Wei Wang
    orcid: https://orcid.org/0000-0002-8180-2886
    username: ~Wei_Wang13
  - dblp_id: https://dblp.org/pid/18/2428
    emails: '****@kwchang.net'
    first_name: Kai-Wei
    google_scholar_id: https://scholar.google.com/citations?user=fqDBtzYAAAAJ&hl=en
    homepage: http://kwchang.net
    institution: University of California, Los Angeles
    last_name: Chang
    name: Kai-Wei Chang
    semantic_scholar_id: https://www.semanticscholar.org/author/Kai-Wei-Chang/2782886
    username: ~Kai-Wei_Chang1
  - dblp_id: https://dblp.org/pid/117/4036
    emails: '****@cs.ucla.edu'
    first_name: Nanyun
    google_scholar_id: https://scholar.google.com/citations?user=XxRXvX0AAAAJ&hl=en
    homepage: http://vnpeng.net/
    institution: University of California, Los Angeles
    last_name: Peng
    name: Nanyun Peng
    semantic_scholar_id: https://www.semanticscholar.org/author/Nanyun-Peng/3157053
    username: ~Nanyun_Peng1
  decision: toMainConference
  end_page: 5067
  file: 480.pdf
  id: 480
  num_pages: 19
  openreview_id: ZMdT32HH9E
  pdf_file: bf6c9251f6611619cd98b9284977e43f45ce5843.pdf
  start_page: 5049
  title: Mitigating Bias for Question Answering Models by Tracking Bias Influence
- abstract: Referring Image Segmentation (RIS) is a cross-modal task that aims to
    segment an instance described by a natural language expression. Recent methods
    leverage large-scale pretrained unimodal models as backbones along with fusion
    techniques for joint reasoning across modalities. However, the inherent cross-modal
    nature of RIS raises questions about the effectiveness of unimodal backbones.
    We propose RISCLIP, a novel framework that effectively leverages the cross-modal
    nature of CLIP for RIS. Observing CLIP's inherent alignment between image and
    text features, we capitalize on this starting point and introduce simple but strong
    modules that enhance unimodal feature extraction and leverage rich alignment knowledge
    in CLIP's image-text shared-embedding space. RISCLIP exhibits outstanding results
    on all three major RIS benchmarks and also outperforms previous CLIP-based methods,
    demonstrating the efficacy of our strategy in extending CLIP's image-text alignment
    to RIS.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multimodality and Language Grounding to Vision, Robotics and Beyond
  authors:
  - emails: '****@postech.ac.kr'
    first_name: Seoyeon
    google_scholar_id: https://scholar.google.com/citations?user=-SnIx4kAAAAJ&hl=en
    homepage: https://yeon07.github.io/
    last_name: Kim
    name: Seoyeon Kim
    username: ~Seoyeon_Kim1
  - dblp_id: https://dblp.org/pid/268/5657
    emails: '****@postech.ac.kr'
    first_name: Minguk
    google_scholar_id: https://scholar.google.com/citations?hl=ko&user=IwokTU4AAAAJ
    last_name: Kang
    name: Minguk Kang
    username: ~Minguk_Kang1
  - dblp_id: https://dblp.org/pid/53/471
    emails: '****@postech.ac.kr'
    first_name: Dongwon
    google_scholar_id: https://scholar.google.com/citations?user=abXotYsAAAAJ&hl=ko
    homepage: https://kdwonn.github.io
    institution: POSTECH
    last_name: Kim
    name: Dongwon Kim
    username: ~Dongwon_Kim1
  - dblp_id: https://dblp.org/pid/00/10336
    emails: '****@snu.ac.kr'
    first_name: Jaesik
    google_scholar_id: https://scholar.google.com/citations?user=_3q6KBIAAAAJ&hl=en
    homepage: http://jaesik.info
    institution: Seoul National University
    last_name: Park
    name: Jaesik Park
    username: ~Jaesik_Park3
  - dblp_id: https://dblp.org/pers/hd/k/Kwak:Suha
    emails: '****@postech.ac.kr'
    first_name: Suha
    google_scholar_id: https://scholar.google.com/citations?user=-gscDIEAAAAJ&hl=ko
    homepage: https://suhakwak.github.io/
    institution: POSTECH and DGIST
    last_name: Kwak
    name: Suha Kwak
    username: ~Suha_Kwak3
  decision: toMainConference
  end_page: 5085
  file: 481.pdf
  id: 481
  num_pages: 18
  openreview_id: p8kRZvKiwc
  pdf_file: 093ccd4cda3ffa704bc5837205a73ad021e5991c.pdf
  start_page: 5068
  title: Extending CLIP's Image-Text Alignment to Referring Image Segmentation
- abstract: 'The goal of product copywriting is to capture the interest of potential
    buyers  by emphasizing the features of products through text descriptions. As
    e-commerce platforms offer a wide range of services, it''s becoming essential
    to dynamically adjust the styles of these auto-generated descriptions. Typical
    approaches to copywriting generation often rely solely on specified product attributes,
    which may result in dull and repetitive content. To tackle this issue, we propose
    to generate copywriting based on customer reviews, as they provide firsthand practical
    experiences with products, offering a richer source of information than just product
    attributes. We have developed a sequence-to-sequence framework, enhanced with
    reinforcement learning, to produce copywriting that is attractive, authentic,
    and rich in information. Our framework outperforms all existing baseline and zero-shot
    large language models, including LLaMA-2-chat-7B and GPT-3.5, in terms of both
    attractiveness and faithfulness. Furthermore, this work features the use of LLMs
    for aspect-based summaries collection and argument allure assessment. Experiments
    demonstrate the effectiveness of using LLMs for marketing domain corpus construction.
    The code and the dataset is publicly available at:  \url{https://github.com/YuXiangLin1234/Copywriting-Generation}.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - emails: '****@gmail.com'
    first_name: Yu-Xiang
    homepage: https://www.linkedin.com/in/yu-xiang-lin-1b8075201/
    last_name: Lin
    name: Yu-Xiang Lin
    username: ~Yu-Xiang_Lin1
  - dblp_id: https://dblp.org/pid/72/4128
    emails: '****@iis.sinica.edu.tw'
    first_name: Wei-Yun
    google_scholar_id: https://scholar.google.com.tw/citations?user=AHG3DncAAAAJ&hl=zh-TW
    homepage: https://homepage.iis.sinica.edu.tw/pages/ma/index_en.html
    institution: Academia Sinica
    last_name: Ma
    name: Wei-Yun Ma
    username: ~Wei-Yun_Ma1
  decision: toMainConference
  end_page: 5099
  file: 482.pdf
  id: 482
  num_pages: 14
  openreview_id: PVpSv6wgCG
  pdf_file: 830e111ddf71a68b87b60d296f9472081f962130.pdf
  start_page: 5086
  title: Generating Attractive and Authentic Copywriting from Customer Reviews
- abstract: We present an effective recipe to train strong long-context LLMs that
    are capable of utilizing massive context windows of up to 32,000 tokens. Our models
    are built through continual pretraining from Llama 2 checkpoints with longer text
    sequences and on a dataset where long texts are upsampled. We perform extensive
    evaluation using language modeling, synthetic context probing tasks, and a wide
    range of downstream benchmarks. Across all evaluations, our models achieve consistent
    improvements on most regular-context tasks and significant improvements on long-context
    tasks over Llama 2. Moreover, with a cost-effective instruction tuning procedure
    that is free of expensive annotation, the presented models can already surpass
    $\texttt{gpt-3.5-turbo-16k}$'s overall performance on long-context benchmarks.
    Alongside these results, we provide an in-depth analysis on each individual component
    of our method. We delve into Llama's position encodings and discuss its key limitation
    in modeling long data. We examine the impact of various design choices in the
    pretraining process, including the data mix and the training curriculum of sequence
    lengths -- ablation results suggest that having abundant long texts in the pretrain
    dataset is $\textit{not}$ the key to achieving strong performance, and we empirically
    verify that long context continual pretraining is more efficient and similarly
    effective compared to pretraining from scratch with long sequences.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Dialogue and Interactive Systems
  authors:
  - dblp_id: https://dblp.org/pid/203/8542
    emails: '****@gmail.com'
    first_name: Wenhan
    homepage: https://xwhan.github.io
    institution: Facebook
    last_name: Xiong
    name: Wenhan Xiong
    username: ~Wenhan_Xiong1
  - emails: '****@student.ethz.ch'
    first_name: Jingyu
    google_scholar_id: https://scholar.google.com/citations?user=jidrykQAAAAJ&hl=en
    last_name: Liu
    name: Jingyu Liu
    username: ~Jingyu_Liu6
  - dblp_id: https://dblp.org/pid/234/2210
    emails: '****@gmail.com'
    first_name: Igor
    institution: Meta AI
    last_name: Molybog
    name: Igor Molybog
    username: ~Igor_Molybog1
  - emails: '****@gmail.com'
    first_name: Hejia
    homepage: https://hejiaz.github.io
    last_name: Zhang
    name: Hejia Zhang
    username: ~Hejia_Zhang2
  - dblp_id: https://dblp.org/pid/225/5415
    emails: '****@protonmail.com'
    first_name: Prajjwal
    google_scholar_id: https://scholar.google.com/citations?user=zTq103EAAAAJ&hl
    homepage: https://prajjwal1.github.io
    last_name: Bhargava
    name: Prajjwal Bhargava
    username: ~Prajjwal_Bhargava1
  - emails: '****@meta.com'
    first_name: Rui
    institution: 'Meta Inc. '
    last_name: Hou
    name: Rui Hou
    username: ~Rui_Hou3
  - dblp_id: https://dblp.org/pid/05/6214
    emails: '****@gmail.com'
    first_name: Louis
    google_scholar_id: https://scholar.google.com/citations?user=W_Y6OKAAAAAJ
    homepage: https://louismartin.eu
    institution: Facebook
    last_name: Martin
    name: Louis Martin
    semantic_scholar_id: https://www.semanticscholar.org/author/Louis-Martin/143792623
    username: ~Louis_Martin1
  - emails: '****@gmail.com'
    first_name: Rashi
    homepage: https://www.linkedin.com/in/rashirungta/
    last_name: Rungta
    name: Rashi Rungta
    username: ~Rashi_Rungta1
  - dblp_id: https://dblp.org/pid/154/4666
    emails: '****@gmail.com'
    first_name: Karthik Abinav
    google_scholar_id: https://scholar.google.com/citations?user=uJ-Dhj4AAAAJ&hl=en
    homepage: http://karthikabinavs.xyz
    institution: Facebook
    last_name: Sankararaman
    name: Karthik Abinav Sankararaman
    username: ~Karthik_Abinav_Sankararaman1
  - dblp_id: https://dblp.org/pers/hd/o/Oguz:Barlas
    emails: '****@meta.com'
    first_name: Barlas
    google_scholar_id: https://scholar.google.com/citations?user=iPmTQZMAAAAJ&hl=en
    institution: Meta
    last_name: Oguz
    name: Barlas Oguz
    username: ~Barlas_Oguz1
  - dblp_id: https://dblp.org/pid/87/11087
    emails: '****@madiankhabsa.com'
    first_name: Madian
    google_scholar_id: https://scholar.google.com/citations?user=V9JYPP0AAAAJ&hl=en
    homepage: https://www.madiankhabsa.com
    institution: Facebook
    last_name: Khabsa
    name: Madian Khabsa
    username: ~Madian_Khabsa1
  - dblp_id: https://dblp.org/pid/209/7867
    emails: '****@fb.com'
    first_name: Han
    homepage: https://www.hanfang.info/
    institution: Meta AI
    last_name: Fang
    name: Han Fang
    username: ~Han_Fang4
  - emails: '****@gmail.com'
    first_name: Yashar
    google_scholar_id: https://scholar.google.com/citations?user=hFKgapkAAAAJ&hl=en
    institution: Facebook
    last_name: Mehdad
    name: Yashar Mehdad
    username: ~Yashar_Mehdad2
  - emails: '****@gmail.com'
    first_name: Sharan
    google_scholar_id: https://scholar.google.com/citations?user=CWOixywAAAAJ&hl=en
    institution: Meta
    last_name: Narang
    name: Sharan Narang
    username: ~Sharan_Narang1
  - emails: '****@fb.com'
    first_name: Kshitiz
    google_scholar_id: https://scholar.google.com/citations?user=pkAWLt8AAAAJ&hl=en
    last_name: Malik
    name: Kshitiz Malik
    username: ~Kshitiz_Malik2
  - dblp_id: https://dblp.org/pid/192/1872
    emails: '****@fb.com'
    first_name: Angela
    google_scholar_id: https://scholar.google.com/citations?user=TLZR9zgAAAAJ&hl=en
    institution: Facebook
    last_name: Fan
    name: Angela Fan
    semantic_scholar_id: https://www.semanticscholar.org/author/Angela-Fan/144270981
    username: ~Angela_Fan2
  - dblp_id: https://dblp.org/pid/136/9081
    emails: '****@fb.com'
    first_name: Shruti
    google_scholar_id: https://scholar.google.com/citations?user=69JJbWoAAAAJ&hl=en
    homepage: https://ai.facebook.com/people/shruti-bhosale/
    institution: Facebook
    last_name: Bhosale
    name: Shruti Bhosale
    semantic_scholar_id: https://www.semanticscholar.org/author/Shruti-Bhosale/2116473
    username: ~Shruti_Bhosale1
  - dblp_id: https://dblp.org/pid/166/8381
    emails: '****@fb.com'
    first_name: Sergey
    google_scholar_id: https://scholar.google.com/citations?user=5w7uYrIAAAAJ&hl=en
    last_name: Edunov
    name: Sergey Edunov
    username: ~Sergey_Edunov1
  - dblp_id: https://dblp.org/pid/19/6214
    emails: '****@fb.com'
    first_name: Mike
    google_scholar_id: https://scholar.google.com/citations?user=SnQnQicAAAAJ&hl=en
    institution: Facebook AI Research
    last_name: Lewis
    name: Mike Lewis
    username: ~Mike_Lewis1
  - dblp_id: https://dblp.org/pid/140/0795
    emails: '****@gmail.com'
    first_name: Sinong
    google_scholar_id: https://scholar.google.com/citations?user=CYMAfxsAAAAJ&hl=en
    homepage: https://sites.google.com/site/snongwang/
    institution: Facebook
    last_name: Wang
    name: Sinong Wang
    username: ~Sinong_Wang1
  - emails: '****@gmail.com'
    first_name: Hao
    homepage: https://haoma.io
    institution: Facebook
    last_name: Ma
    name: Hao Ma
    username: ~Hao_Ma1
  decision: toMainConference
  end_page: 5120
  file: 483.pdf
  id: 483
  num_pages: 21
  openreview_id: 9bloCZ1lnl
  pdf_file: 7e4802191435f9ff65a89de4279896d3aa73a566.pdf
  start_page: 5100
  title: Effective Long-Context Scaling of Foundation Models
- abstract: Diffusion models have achieved state-of-the-art synthesis quality on both
    visual and audio tasks, and recent works further adapt them to textual data by
    diffusing on the embedding space. In this paper, we conduct systematic studies
    of the optimization challenges encountered with both the embedding space and the
    denoising model, which have not been carefully explored. Firstly, the data distribution
    is learnable for embeddings, which may lead to the collapse of the embedding space
    and unstable training. To alleviate this problem, we propose a new objective called
    the anchor loss which is more efficient than previous methods. Secondly, we find
    the noise levels of conventional schedules are insufficient for training a desirable
    denoising model while introducing varying degrees of degeneration in consequence.
    To address this challenge, we propose a novel framework called noise rescaling.
    Based on the above analysis, we propose Difformer, an embedding diffusion model
    based on Transformer. Experiments on varieties of seminal text generation tasks
    show the effectiveness of the proposed methods and the superiority of Difformer
    over previous state-of-the-art embedding diffusion baselines.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Generation
  authors:
  - dblp_id: https://dblp.org/pid/336/4920
    emails: '****@mail.ustc.edu.cn'
    first_name: Zhujin
    last_name: Gao
    name: Zhujin Gao
    username: ~Zhujin_Gao1
  - dblp_id: https://dblp.org/pid/209/9674
    emails: '****@microsoft.com'
    first_name: Junliang
    institution: Microsoft
    last_name: Guo
    name: Junliang Guo
    username: ~Junliang_Guo1
  - dblp_id: https://dblp.org/pid/96/10484-3
    emails: '****@gmail.com'
    first_name: Xu
    google_scholar_id: https://scholar.google.com/citations?user=tob-U1oAAAAJ
    homepage: https://www.microsoft.com/en-us/research/people/xuta/
    last_name: Tan
    name: Xu Tan
    orcid: https://orcid.org/0000-0001-5631-0639
    username: ~Xu_Tan1
  - dblp_id: https://dblp.org/pid/27/3343
    emails: '****@mail.ustc.edu.cn'
    first_name: Yongxin
    google_scholar_id: https://scholar.google.com/citations?view_op=new_profile&hl=zh-CN
    homepage: https://youngsheen.github.io
    last_name: Zhu
    name: Yongxin Zhu
    semantic_scholar_id: https://www.semanticscholar.org/author/Yongxin-Zhu/2116513069
    username: ~Yongxin_Zhu1
  - emails: '****@mail.ustc.edu.cn'
    first_name: Fang
    homepage: https://github.com/ksblk2116
    last_name: Zhang
    name: Fang Zhang
    username: ~Fang_Zhang2
  - dblp_id: https://dblp.org/pid/09/851-2.html
    emails: '****@microsoft.com'
    first_name: Jiang
    google_scholar_id: https://scholar.google.com/citations?user=pZBEnY8AAAAJ&hl=en
    homepage: https://sites.google.com/view/jiangbian
    institution: Microsoft
    last_name: Bian
    name: Jiang Bian
    orcid: https://orcid.org/0000-0002-9472-600X
    username: ~Jiang_Bian1
  - dblp_id: https://dblp.org/pid/66/1240
    emails: '****@ustc.edu.cn'
    first_name: Linli
    google_scholar_id: https://scholar.google.com/citations?hl=zh-CN&user=MsuY1TsAAAAJ
    homepage: http://staff.ustc.edu.cn/~linlixu/papers.html
    last_name: Xu
    name: Linli Xu
    username: ~Linli_Xu1
  decision: toMainConference
  end_page: 5140
  file: 485.pdf
  id: 485
  num_pages: 20
  openreview_id: c93kjh4Xyi
  pdf_file: 433bc22941d08a5b1390293fc23bc6de7a94db27.pdf
  start_page: 5121
  title: Empowering Diffusion Models on the Embedding Space for Text Generation
- abstract: 'Minimum Bayes-risk (MBR) decoding has recently gained renewed attention
    in text generation.

    MBR decoding considers texts sampled from a model as pseudo-references and selects
    the text with the highest similarity to the others.

    Therefore, sampling is one of the key elements of MBR decoding, and previous studies
    reported that the performance varies by sampling methods.

    From a theoretical standpoint, this performance variation is likely tied to how
    closely the samples approximate the true distribution of references.

    However, this approximation has not been the subject of in-depth study.

    In this study, we propose using anomaly detection to measure the degree of approximation.

    We first closely examine the performance variation and then show that previous
    hypotheses about samples do not correlate well with the variation, but our introduced
    anomaly scores do.

    The results are the first to empirically support the link between the performance
    and the core assumption of MBR decoding.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Translation
  authors:
  - emails: '****@s.mail.nagoya-u.ac.jp'
    first_name: Atsumoto
    google_scholar_id: https://scholar.google.com/citations?user=cl4T6vIAAAAJ
    homepage: https://ohashi56225.github.io/
    institution: Nagoya University
    last_name: Ohashi
    name: Atsumoto Ohashi
    username: ~Atsumoto_Ohashi1
  - dblp_id: https://dblp.org/pid/220/2007
    emails: '****@is.naist.jp'
    first_name: Ukyo
    google_scholar_id: https://scholar.google.com/citations?view_op=list_works&hl=en&user=Cf48JmIAAAAJ&gmla=AJsN-F7cFOhEmj5wyPEjy3woOW4WiBiQFhraO_3UN_fCxQH0fbq8XZJfyShqQo402HnFePvN9hgn3pHoLY0My0X4s-cWUm46plZ7CvgUqxBH375gTxt-Yy3ZxkQYg6oWawg65NHEjeYN
    institution: CyberAgent, Inc.
    last_name: Honda
    name: Ukyo Honda
    orcid: https://orcid.org/0000-0002-4894-9886
    semantic_scholar_id: https://www.semanticscholar.org/author/46205964
    username: ~Ukyo_Honda1
  - dblp_id: https://dblp.org/pid/36/1501
    emails: '****@gmail.com'
    first_name: Tetsuro
    google_scholar_id: https://scholar.google.co.jp/citations?user=IgjF21EAAAAJ
    institution: CyberAgent, Inc.
    last_name: Morimura
    name: Tetsuro Morimura
    username: ~Tetsuro_Morimura1
  - dblp_id: https://dblp.org/pid/178/8539
    emails: '****@gmail.com'
    first_name: Yuu
    google_scholar_id: https://scholar.google.com/citations?user=H0MaUNIAAAAJ
    homepage: https://jinnaiyuu.github.io
    institution: CyberAgent, Inc.
    last_name: Jinnai
    name: Yuu Jinnai
    username: ~Yuu_Jinnai1
  decision: toMainConference
  end_page: 5150
  file: 486.pdf
  id: 486
  num_pages: 10
  openreview_id: DxguiNbE48
  pdf_file: 758ab40fb69e88d9f4a76ce4f14cbc95aa2a2c3d.pdf
  start_page: 5141
  title: On the True Distribution Approximation of Minimum Bayes-Risk Decoding
- abstract: Continual learning aims at incrementally acquiring new knowledge while
    not forgetting existing knowledge. To overcome catastrophic forgetting, methods
    are either rehearsal-based, i.e., store data examples from previous tasks for
    data replay, or isolate parameters dedicated to each task. However, rehearsal-based
    methods raise privacy and memory issues, and parameter-isolation continual learning
    does not consider interaction between tasks, thus hindering knowledge transfer.
    In this work, we propose MoCL, a rehearsal-free **Mo**dular and **C**ompositional
    Continual **L**earning framework which continually adds new modules to language
    models and composes them with existing modules. Experiments on various benchmarks
    show that MoCL outperforms state of the art and effectively facilitates knowledge
    transfer.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Learning for NLP
  authors:
  - emails: '****@de.bosch.com'
    first_name: Mingyang
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=d8UFLmsAAAAJ
    homepage: https://mingyang-wang26.github.io/
    last_name: Wang
    name: Mingyang Wang
    username: ~Mingyang_Wang1
  - dblp_id: https://dblp.org/pid/132/6980
    emails: '****@gmail.com'
    first_name: Heike
    google_scholar_id: https://scholar.google.de/citations?user=Fejbq9kAAAAJ
    homepage: https://sites.google.com/view/heikeadel
    institution: Hochschule der Medien (University of Applied Sciences)
    last_name: Adel
    name: Heike Adel
    semantic_scholar_id: https://www.semanticscholar.org/author/Heike-Adel/145793834
    username: ~Heike_Adel1
  - dblp_id: https://dblp.org/pid/219/5288
    emails: '****@gmail.com'
    first_name: Lukas
    google_scholar_id: https://scholar.google.co.in/citations?user=yBM4CMcAAAAJ&hl=de
    institution: Robert Bosch GmbH, Bosch
    last_name: Lange
    name: Lukas Lange
    semantic_scholar_id: https://www.semanticscholar.org/author/Lukas-Lange/47665464
    username: ~Lukas_Lange1
  - dblp_id: https://dblp.org/pid/28/8510
    emails: '****@gmail.com'
    first_name: Jannik
    google_scholar_id: https://scholar.google.de/citations?user=aQjqBSsAAAAJ
    homepage: https://sites.google.com/view/jannikstroetgen
    institution: Karlsruhe University of Applied Sciences
    last_name: "Str\xF6tgen"
    name: "Jannik Str\xF6tgen"
    semantic_scholar_id: https://www.semanticscholar.org/author/Jannik-Strotgen/2013656
    username: "~Jannik_Str\xF6tgen1"
  - dblp_id: https://dblp.org/pid/s/HinrichSchutze
    emails: '****@cis.lmu.de'
    first_name: Hinrich
    homepage: https://www.cis.uni-muenchen.de/schuetze/
    last_name: Schuetze
    name: Hinrich Schuetze
    username: ~Hinrich_Schuetze3
  decision: toMainConference
  end_page: 5162
  file: 490.pdf
  id: 490
  num_pages: 12
  openreview_id: YAymhlOT0r
  pdf_file: d6feafe8198402dfea2b1abf547d5e9c6592d330.pdf
  start_page: 5151
  title: Rehearsal-Free Modular and Compositional Continual Learning for Language
    Models
- abstract: Large language models (LLMs) often generate biased outputs containing
    offensive, toxic, or stereotypical text. Existing LLM alignment methods such as
    reinforcement learning from human feedback (RLHF) alleviate biases primarily based
    on reward signals from current model outputs without considering the source of
    biases. In this work, to explore how biases are formed, we revisit LLMs' text
    generation from a causal perspective. We identify pretraining data and input prompts,
    which contain semantic correlations of textual phrases, as two confounders between
    LLMs and model outputs causing biases. Inspired by our causal view, we leverage
    the reward model in RL alignment as an instrumental variable to perform causal
    intervention on LLMs. Utilizing the reward difference between an initial LLM and
    intervened LLM as interventional feedback to guide RL finetuning, we propose Causality-Aware
    Alignment (CAA) for LLM debiasing. Experiments on two text generation tasks with
    three different alignment objectives demonstrate the advantages of our method
    in aligning LLMs to generate less biased and safer outputs.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Learning for NLP
  authors:
  - emails: '****@umich.edu'
    first_name: Yu
    google_scholar_id: https://scholar.google.com/citations?user=sTVqEUMAAAAJ&hl=en
    homepage: https://andree-9.github.io/
    last_name: Xia
    name: Yu Xia
    username: ~Yu_Xia9
  - dblp_id: https://dblp.org/pid/32/1593-1
    emails: '****@gmail.com'
    first_name: Tong
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=6-ARmXsAAAAJ&view_op=list_works&sortby=pubdate
    institution: Adobe Research
    last_name: Yu
    name: Tong Yu
    semantic_scholar_id: https://www.semanticscholar.org/author/Tong-Yu/1500399016
    username: ~Tong_Yu3
  - dblp_id: https://dblp.org/pid/222/1220
    emails: '****@eng.ucsd.edu'
    first_name: Zhankui
    google_scholar_id: https://scholar.google.com/citations?user=EK5xD8IAAAAJ&hl=en
    homepage: https://aaronheee.github.io
    institution: University of California, San Diego, University of California, San
      Diego
    last_name: He
    name: Zhankui He
    username: ~Zhankui_He1
  - dblp_id: https://dblp.org/pers/z/Zhao:Handong.html
    emails: '****@gmail.com'
    first_name: Handong
    google_scholar_id: https://scholar.google.com/citations?user=0f-YOFgAAAAJ&hl=en&oi=ao
    homepage: https://hdzhao.github.io/
    institution: Adobe Systems
    last_name: Zhao
    name: Handong Zhao
    semantic_scholar_id: https://www.semanticscholar.org/author/Handong-Zhao/7574699
    username: ~Handong_Zhao3
  - dblp_id: https://dblp.org/pid/29/3483
    emails: '****@cs.ucsd.edu'
    first_name: Julian
    google_scholar_id: https://scholar.google.com/citations?user=icbo4M0AAAAJ&hl=en
    homepage: http://cseweb.ucsd.edu/~jmcauley/
    institution: University of California, San Diego, University of California, San
      Diego
    last_name: McAuley
    name: Julian McAuley
    username: ~Julian_McAuley1
  - dblp_id: https://dblp.org/pid/57/2281-10
    emails: '****@sjtu.edu.cn'
    first_name: Shuai
    google_scholar_id: https://scholar.google.com.hk/citations?user=kMZgQxcAAAAJ&hl=zh-CN
    homepage: http://shuaili8.github.io
    institution: John Hopcroft Center, Shanghai Jiao Tong University
    last_name: Li
    name: Shuai Li
    username: ~Shuai_Li3
  decision: toMainConference
  end_page: 5174
  file: 491.pdf
  id: 491
  num_pages: 12
  openreview_id: LXeTSaNOWk
  pdf_file: 5747cff8eb0e5020dbcb5ed2101644315b6eb08b.pdf
  start_page: 5163
  title: 'Aligning as Debiasing: Causality-Aware Alignment via Reinforcement Learning
    with Interventional Feedback'
- abstract: "The growing awareness of safety concerns in large language models (LLMs)\
    \ has sparked considerable interest in the evaluation of safety. \nThis study\
    \ investigates an under-explored issue about the evaluation of LLMs, namely the\
    \ substantial discrepancy in performance between multiple-choice questions and\
    \ open-ended questions. Inspired by research on jailbreak attack patterns, we\
    \ argue this is caused by mismatched generalization. That is, LLM only remembers\
    \ the answer style for open-ended safety questions, which makes it unable to solve\
    \ other forms of safety tests. We refer to this phenomenon as fake alignment and\
    \ construct a comparative benchmark to empirically verify its existence in LLMs.\
    \ We introduce a Fake alIgNment Evaluation (FINE) framework and two novel metrics\u2014\
    \u2014Consistency Score (CS) and Consistent Safety Score (CSS), which jointly\
    \ assess two complementary forms of evaluation to quantify fake alignment and\
    \ obtain corrected performance estimation. Applying FINE to 14 widely-used LLMs\
    \ reveals several models with purported safety are poorly aligned in practice.\
    \ Subsequently, we found that multiple-choice format data can also be used as\
    \ high-quality contrast distillation-based fine-tuning data, which can strongly\
    \ improve the alignment consistency of LLMs with minimal fine-tuning overhead.\
    \ For data and code, see https://github.com/AIFlames/Fake-Alignment."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Ethics, Bias, and Fairness
  authors:
  - emails: '****@gmail.com'
    first_name: Yixu
    google_scholar_id: https://scholar.google.com/citations?user=EYP7wNIAAAAJ&hl=zh-CN
    last_name: Wang
    name: Yixu Wang
    username: ~Yixu_Wang1
  - emails: '****@pjlab.org.cn'
    first_name: Yan
    institution: Shanghai Artificial Intelligence Laboratory
    last_name: Teng
    name: Yan Teng
    orcid: https://orcid.org/0000-0002-7069-4728
    username: ~Yan_Teng1
  - emails: '****@pjlab.org.cn'
    first_name: Kexin
    last_name: Huang
    name: Kexin Huang
    username: ~Kexin_Huang3
  - emails: '****@pjlab.org.cn'
    first_name: Chengqi
    google_scholar_id: https://scholar.google.com/citations?view_op=list_works&hl=en&user=kV3WvXcAAAAJ
    institution: Shanghai AI Laboratory
    last_name: Lyu
    name: Chengqi Lyu
    username: ~Chengqi_Lyu1
  - dblp_id: https://dblp.org/pid/152/9228
    emails: '****@pjlab.org.cn'
    first_name: Songyang
    google_scholar_id: https://scholar.google.com/citations?user=8XQPi7YAAAAJ
    homepage: https://www.zhangsongyang.com/
    institution: Shanghai AI Laboratory
    last_name: Zhang
    name: Songyang Zhang
    username: ~Songyang_Zhang1
  - dblp_id: https://dblp.org/pid/46/8076
    emails: '****@outlook.com'
    first_name: Wenwei
    google_scholar_id: https://scholar.google.com/citations?user=QDXADSEAAAAJ&hl=zh-CN
    homepage: https://zhangwenwei.cn
    institution: Shanghai AI Laboratory
    last_name: Zhang
    name: Wenwei Zhang
    orcid: https://orcid.org/0000-0002-2748-4514
    username: ~Wenwei_Zhang1
  - dblp_id: https://dblp.org/pid/195/8270
    emails: '****@gmail.com'
    first_name: Xingjun
    google_scholar_id: https://scholar.google.com.au/citations?user=XQViiyYAAAAJ&hl=en
    homepage: http://xingjunma.com/
    institution: Fudan University
    last_name: Ma
    name: Xingjun Ma
    username: ~Xingjun_Ma1
  - dblp_id: https://dblp.org/pid/24/5818
    emails: '****@fudan.edu.cn'
    first_name: Yu-Gang
    google_scholar_id: https://scholar.google.com/citations?user=f3_FP8AAAAAJ
    homepage: https://fvl.fudan.edu.cn/people/yugangjiang/
    institution: Fudan University
    last_name: Jiang
    name: Yu-Gang Jiang
    username: ~Yu-Gang_Jiang1
  - dblp_id: https://dblp.org/pid/q/YuQiao1
    emails: '****@pjlab.org.cn'
    first_name: Yu
    google_scholar_id: https://scholar.google.com/citations?user=gFtI-8QAAAAJ&hl=zh-CN
    homepage: http://mmlab.siat.ac.cn/yuqiao
    last_name: Qiao
    name: Yu Qiao
    orcid: https://orcid.org/0000-0002-1889-2567
    username: ~Yu_Qiao1
  - emails: '****@pjlab.org.cn'
    first_name: Yingchun
    institution: Shanghai Artificial Intelligence Laboratory
    last_name: Wang
    name: Yingchun Wang
    username: ~Yingchun_Wang2
  decision: toMainConference
  end_page: 5191
  file: 493.pdf
  id: 493
  num_pages: 17
  openreview_id: GG6ZsxlB2w
  pdf_file: aef6a764d930214e838ecf13e67da4b57f08be5d.pdf
  start_page: 5175
  title: 'Fake Alignment: Are LLMs Really Aligned Well?'
- abstract: Prior study shows that pre-training techniques can boost the performance
    of visual document understanding (VDU), which typically requires models to gain
    abilities to perceive and reason both document texts and layouts (e.g., locations
    of texts and table-cells). To this end, we propose visually guided generative
    text-layout pre-training, named ViTLP. Given a document image, the model optimizes
    hierarchical language and layout modeling objectives to generate the interleaved
    text and layout sequence. In addition, to address the limitation of processing
    long documents by Transformers, we introduce a straightforward yet effective multi-segment
    generative pre-training scheme, facilitating ViTLP to process word-intensive documents
    of any length. ViTLP can function as a native OCR model to localize and recognize
    texts of document images. Besides, ViTLP can be effectively applied to various
    downstream VDU tasks. Extensive experiments show that ViTLP achieves competitive
    performance over existing baselines on benchmark VDU tasks, including information
    extraction, document classification, and document question answering.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multimodality and Language Grounding to Vision, Robotics and Beyond
  authors:
  - dblp_id: https://dblp.org/pid/258/8430
    emails: '****@se.cuhk.edu.hk'
    first_name: Zhiming
    google_scholar_id: https://scholar.google.com/citations?user=HVn5AZYAAAAJ&hl=en
    homepage: https://aclanthology.org/people/z/zhiming-mao/
    institution: The Chinese University of Hong Kong
    last_name: Mao
    name: Zhiming Mao
    username: ~Zhiming_Mao1
  - emails: '****@huawei.com'
    first_name: Haoli
    homepage: https://haolibai.github.io
    institution: Huawei Technologies Ltd.
    last_name: Bai
    name: Haoli Bai
    username: ~Haoli_Bai2
  - dblp_id: https://dblp.org/pid/166/9840
    emails: '****@gmail.com'
    first_name: Lu
    google_scholar_id: https://scholar.google.com.hk/citations?user=rnjoL5cAAAAJ&hl=en
    homepage: https://houlu369.github.io/
    institution: Huawei Technologies Ltd.
    last_name: Hou
    name: Lu Hou
    username: ~Lu_Hou2
  - dblp_id: https://dblp.org/pid/70/4288
    emails: '****@gmail.com'
    first_name: Lifeng
    google_scholar_id: https://scholar.google.com.hk/citations?user=jMQIjYoAAAAJ&hl=zh-CN&oi=ao
    institution: Huawei Technologies Ltd.
    last_name: Shang
    name: Lifeng Shang
    username: ~Lifeng_Shang1
  - dblp_id: https://dblp.org/pid/42/4142-2
    emails: '****@gmail.com'
    first_name: Xin
    google_scholar_id: https://scholar.google.com/citations?user=DUfcez0AAAAJ&hl=en
    last_name: Jiang
    name: Xin Jiang
    orcid: https://orcid.org/0000-0002-9117-8247
    semantic_scholar_id: https://www.semanticscholar.org/author/Xin-Jiang/145820291
    username: ~Xin_Jiang1
  - dblp_id: https://dblp.org/pid/75/4402-1
    emails: '****@huawei.com'
    first_name: Qun
    google_scholar_id: https://scholar.google.com/citations?user=2HhiGzcAAAAJ
    homepage: http://liuquncn.github.io/
    institution: Huawei Noah's Ark Lab
    last_name: Liu
    name: Qun Liu
    orcid: https://orcid.org/0000-0002-7000-1792
    semantic_scholar_id: https://www.semanticscholar.org/author/Qun-Liu/1688015
    username: ~Qun_Liu1
  - dblp_id: https://dblp.org/pid/w/KamFaiWong
    emails: '****@se.cuhk.edu.hk'
    first_name: Kam-Fai
    homepage: http://www.se.cuhk.edu.hk/~kfwong
    institution: The Chinese University of Hong Kong
    last_name: Wong
    name: Kam-Fai Wong
    orcid: https://orcid.org/0000-0002-9427-5659
    username: ~Kam-Fai_Wong2
  decision: toMainConference
  end_page: 5209
  file: 494.pdf
  id: 494
  num_pages: 18
  openreview_id: 8IcsG9zUr3
  pdf_file: 0f099f7391e3f632a968af83b6a21573cee40dfa.pdf
  start_page: 5192
  title: Visually Guided Generative Text-Layout Pre-training for Document Intelligence
- abstract: Instruction-finetuned Large Language Models inherit clear political leanings
    that have been shown to influence downstream task performance. We expand this
    line of research beyond the two-party system in the US and audit Llama Chat in
    the context of EU politics in various settings to analyze the model's political
    knowledge and its ability to reason in context. We adapt, i.e., further fine-tune,
    Llama Chat on speeches of individual euro-parties from debates in the European
    Parliament to reevaluate its political leaning based on the EUandI questionnaire.
    Llama Chat shows considerable knowledge of national parties' positions and is
    capable of reasoning in context. The adapted, party-specific, models are substantially
    re-aligned towards respective positions which we see as a starting point for using
    chat-based LLMs as data-driven conversational engines to assist research in political
    science.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Computational Social Science and Cultural Analytics
  authors:
  - dblp_id: https://dblp.org/pid/199/8161
    emails: '****@di.ku.dk'
    first_name: Ilias
    google_scholar_id: https://scholar.google.com/citations?user=BrtAqz8AAAAJ&hl=el&oi=sra
    homepage: https://iliaschalkidis.github.io
    last_name: Chalkidis
    name: Ilias Chalkidis
    orcid: https://orcid.org/0000-0002-0706-7772
    semantic_scholar_id: https://www.semanticscholar.org/author/Ilias-Chalkidis/10783142
    username: ~Ilias_Chalkidis1
  - dblp_id: https://dblp.org/pid/194/9380
    emails: '****@di.ku.dk'
    first_name: Stephanie
    google_scholar_id: https://scholar.google.com/citations?user=eCDiVTMAAAAJ
    homepage: https://stephaniebrandl.github.io
    institution: "K\xF8benhavns Universitet"
    last_name: Brandl
    name: Stephanie Brandl
    semantic_scholar_id: https://www.semanticscholar.org/author/Stephanie-Brandl/6547490
    username: ~Stephanie_Brandl1
  decision: toMainConference
  end_page: 5227
  file: 496.pdf
  id: 496
  num_pages: 18
  openreview_id: a8vpXzEg2t
  pdf_file: c6953a2d335dba4f661ffa0438750db223b1e7fe.pdf
  start_page: 5210
  title: 'Llama meets EU: Investigating the European political spectrum through the
    lens of LLMs'
- abstract: Existing self-supervised methods in natural language processing (NLP),
    especially hierarchical text classification (HTC), mainly focus on self-supervised
    contrastive learning, extremely relying on human-designed augmentation rules to
    generate contrastive samples, which can potentially corrupt or distort the original
    information. In this paper, we tend to investigate the feasibility of a contrastive
    learning scheme in which the semantic and syntactic information inherent in the
    input sample is adequately reserved in the contrastive samples and fused during
    the learning process. Specifically, we propose an information lossless contrastive
    learning strategy for HTC, namely $\textbf{H}$ierarchy-aware $\textbf{I}$nformation
    $\textbf{L}$ossless contrastive $\textbf{L}$earning (HILL), which consists of
    a text encoder representing the input document, and a structure encoder directly
    generating the positive sample. The structure encoder takes the document embedding
    as input, extracts the essential syntactic information inherent in the label hierarchy
    with the principle of structural entropy minimization, and injects the syntactic
    information into the text representation via hierarchical representation learning.
    Experiments on three common datasets are conducted to verify the superiority of
    HILL.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Learning for NLP
  authors:
  - dblp_id: https://dblp.org/pid/59/2802.html
    emails: '****@buaa.edu.cn'
    first_name: He
    last_name: Zhu
    name: He Zhu
    username: ~He_Zhu8
  - emails: '****@buaa.edu.cn'
    first_name: Junran
    google_scholar_id: https://scholar.google.com/citations?user=pbjk-2UAAAAJ&hl=zh-CN
    homepage: https://github.com/BUAA-WJR
    institution: National University of Singapore
    last_name: Wu
    name: Junran Wu
    username: ~Junran_Wu1
  - emails: '****@buaa.edu.cn'
    first_name: Ruomei
    institution: Beijing University of Aeronautics and Astronautics
    last_name: Liu
    name: Ruomei Liu
    semantic_scholar_id: https://www.semanticscholar.org/author/Ruomei-Liu/2143183517
    username: ~Ruomei_Liu1
  - emails: '****@buaa.edu.cn'
    first_name: Yue
    institution: Beihang University
    last_name: Hou
    name: Yue Hou
    orcid: https://orcid.org/0009-0001-4806-624X
    username: ~Yue_Hou1
  - emails: '****@buaa.edu.cn'
    first_name: Ze
    homepage: https://www.google.com
    institution: Beijing University of Aeronautics and Astronautics
    last_name: Yuan
    name: Ze Yuan
    username: ~Ze_Yuan1
  - emails: '****@buaa.edu.cn'
    first_name: Shangzhe
    homepage: https://github.com/lsz19960814
    last_name: Li
    name: Shangzhe Li
    username: ~Shangzhe_Li1
  - dblp_id: https://dblp.org/pid/14/721-1
    emails: '****@buaa.edu.cn'
    first_name: Yicheng
    homepage: http://scse.buaa.edu.cn/info/1080/7261.htm
    last_name: Pan
    name: Yicheng Pan
    username: ~Yicheng_Pan1
  - dblp_id: https://dblp.org/pid/x/KeXu
    emails: '****@nlsde.buaa.edu.cn'
    first_name: Ke
    institution: Beijing University of Aeronautics and Astronautics
    last_name: Xu
    name: Ke Xu
    username: ~Ke_Xu4
  decision: toMainConference
  end_page: 5242
  file: 498.pdf
  id: 498
  num_pages: 15
  openreview_id: C6XqXkak2W
  pdf_file: b4c5b896744970f94c007e7d6627d35ddbba6e7a.pdf
  start_page: 5228
  title: 'HILL: Hierarchy-aware Information Lossless Contrastive Learning for Hierarchical
    Text Classification'
- abstract: Text and vision foundation models can perform many tasks in a zero-shot
    setting, a desirable property that enables these systems to be applied in general
    and low-resource settings. There has been far less work, however, on the zero-shot
    abilities of ASR foundation models, with these systems typically fine-tuned to
    specific tasks or constrained to applications that match their training criterion
    and data annotation. In this work we investigate the ability of Whisper and MMS,
    ASR foundation models trained primarily for speech recognition, to perform zero-shot
    audio classification. We use simple template-based text prompts at the decoder
    and use the resulting decoding probabilities to generate zero-shot predictions.
    Without training the model on extra data or adding any new parameters, we demonstrate
    that Whisper shows promising zero-shot classification performance on a range of
    8 audio-classification datasets, outperforming the accuracy of existing state-of-the-art
    zero-shot baselines by an average of 9%. One important step to unlock the emergent
    ability is debiasing, where a simple unsupervised reweighting method of the class
    probabilities yields consistent significant performance gains. We further show
    that performance increases with model size, implying that as ASR foundation models
    scale up, they may exhibit improved zero-shot performance.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Speech recognition, text-to-speech and spoken language understanding
  authors:
  - emails: '****@cam.ac.uk'
    first_name: Rao
    google_scholar_id: https://scholar.google.com/citations?user=4jn7KMIAAAAJ
    last_name: Ma
    name: Rao Ma
    username: ~Rao_Ma2
  - dblp_id: https://dblp.org/pid/333/0793
    emails: '****@cam.ac.uk'
    first_name: Adian
    google_scholar_id: https://scholar.google.com/citations?user=dYtKMOgAAAAJ&hl=en&oi=ao
    institution: University of Cambridge
    last_name: Liusie
    name: Adian Liusie
    semantic_scholar_id: https://www.semanticscholar.org/author/Adian-Liusie/2190750613
    username: ~Adian_Liusie1
  - dblp_id: https://dblp.org/pid/74/4419.html
    emails: '****@eng.cam.ac.uk'
    first_name: Mark
    google_scholar_id: https://scholar.google.co.uk/citations?hl=en&user=RSFlmjIAAAAJ&view_op=list_works&sortby=pubdate
    homepage: http://mi.eng.cam.ac.uk/~mjfg/index.html
    institution: University of Cambridge
    last_name: Gales
    name: Mark Gales
    semantic_scholar_id: https://www.semanticscholar.org/author/M.-Gales/1740397
    username: ~Mark_Gales1
  - dblp_id: https://dblp.org/pid/06/9262
    emails: '****@eng.cam.ac.uk'
    first_name: Kate
    google_scholar_id: https://scholar.google.co.uk/citations?user=GAOdzW8AAAAJ&hl=en
    homepage: http://mi.eng.cam.ac.uk/~kmk/
    institution: University of Cambridge
    last_name: Knill
    name: Kate Knill
    orcid: https://orcid.org/0000-0003-1292-2769
    semantic_scholar_id: https://www.semanticscholar.org/author/K.-Knill/145962472
    username: ~Kate_Knill1
  decision: toMainConference
  end_page: 5257
  file: 502.pdf
  id: 502
  num_pages: 15
  openreview_id: 6Tt7lT01Up
  pdf_file: 07bb7c1589bdad4bd0018182ff2029679542673f.pdf
  start_page: 5243
  title: Investigating the Emergent Audio Classification Ability of ASR Foundation
    Models
- abstract: "In-context learning (ICL) is now a common method for teaching large language\
    \ models (LLMs) new tasks: given labeled examples in the input context, the LLM\
    \ learns to perform the task without weight updates. Do models guided via ICL\
    \ infer the underlying structure of the task defined by the context, or do they\
    \ rely on superficial heuristics that only generalize to identically distributed\
    \ examples? We address this question using transformations tasks and an NLI task\
    \ that assess sensitivity to syntax\u2014a requirement for robust language understanding.\
    \ We further investigate whether out-of-distribution generalization can be improved\
    \ via chain-of-thought prompting, where the model is provided with a sequence\
    \ of intermediate computation steps that illustrate how the task ought to be performed.\
    \ In experiments with models from the GPT, PaLM, and Llama~2 families, we find\
    \ large variance across LMs. The variance is explained more by the composition\
    \ of the pre-training corpus and supervision methods than by model size; in particular,\
    \ models pre-trained on code generalize better, and benefit more from chain-of-thought\
    \ prompting."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - dblp_id: https://dblp.org/pid/248/7949
    emails: '****@northeastern.edu'
    first_name: Aaron
    google_scholar_id: https://scholar.google.com/citations?user=lhwxXg4AAAAJ&hl=en#
    homepage: https://aaronmueller.github.io
    institution: Northeastern University and Technion - Israel Institute of Technology,
      Technion
    last_name: Mueller
    name: Aaron Mueller
    semantic_scholar_id: https://www.semanticscholar.org/author/Aaron-Mueller/49355602
    username: ~Aaron_Mueller1
  - dblp_id: https://dblp.org/pid/276/1456
    emails: '****@awebson.com'
    first_name: Albert
    google_scholar_id: https://scholar.google.com/citations?user=3OQplr0AAAAJ&hl=en
    homepage: https://representations.ai
    institution: Google DeepMind
    last_name: Webson
    name: Albert Webson
    semantic_scholar_id: https://www.semanticscholar.org/author/1991019030
    username: ~Albert_Webson1
  - emails: '****@gmail.com'
    first_name: Jackson
    homepage: https://jacksonpetty.org
    institution: New York University
    last_name: Petty
    name: Jackson Petty
    username: ~Jackson_Petty1
  - dblp_id: https://dblp.org/pid/169/3438
    emails: '****@nyu.edu'
    first_name: Tal
    google_scholar_id: https://scholar.google.com/citations?user=5mJDXjoAAAAJ&hl=en
    homepage: http://tallinzen.net
    institution: New York University and Google
    last_name: Linzen
    name: Tal Linzen
    semantic_scholar_id: https://www.semanticscholar.org/author/Tal-Linzen/2467508
    username: ~Tal_Linzen1
  decision: toMainConference
  end_page: 5276
  file: 510.pdf
  id: 510
  num_pages: 19
  openreview_id: Jtrt0nmHrF
  pdf_file: ea636662d82411ac297e5f064171ec36f98379bd.pdf
  start_page: 5258
  title: 'In-context Learning Generalizes, But Not Always Robustly: The Case of Syntax'
- abstract: Recent singing-voice-synthesis (SVS) methods have achieved remarkable
    audio quality and naturalness, yet they lack the capability to control the style
    attributes of the synthesized singing explicitly. We propose Prompt-Singer, the
    first SVS method that enables attribute controlling on singer gender, vocal range
    and volume with natural language. We adopt a model architecture based on a decoder-only
    transformer with a multi-scale hierarchy, and design a range-melody decoupled
    pitch representation that enables text-conditioned vocal range control while keeping
    melodic accuracy. Furthermore, we explore various experiment settings, including
    different types of text representations, text encoder fine-tuning, and introducing
    speech data to alleviate data scarcity, aiming to facilitate further research.
    Experiments show that our model achieves favorable controlling ability and audio
    quality. Audio samples are available at http://prompt-singer.github.io .
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Speech recognition, text-to-speech and spoken language understanding
  authors:
  - emails: '****@zju.edu.cn'
    first_name: Yongqi
    google_scholar_id: https://scholar.google.com/citations?user=9_79D6IAAAAJ
    institution: Zhejiang University
    last_name: Wang
    name: Yongqi Wang
    orcid: https://orcid.org/0000-0003-4695-3440
    username: ~Yongqi_Wang1
  - emails: '****@qq.com'
    first_name: Ruofan
    homepage: https://github.com/2811668688
    last_name: Hu
    name: Ruofan Hu
    username: ~Ruofan_Hu2
  - dblp_id: https://dblp.org/pid/212/8936
    emails: '****@zju.edu.cn'
    first_name: Rongjie
    google_scholar_id: https://scholar.google.com/citations?user=iRHBUsgAAAAJ&hl=zh-CN
    institution: Zhejiang University
    last_name: Huang
    name: Rongjie Huang
    username: ~Rongjie_Huang1
  - emails: '****@zju.edu.cn'
    first_name: Zhiqing
    google_scholar_id: https://scholar.google.com/citations?hl=zh-CN&user=avc_7PUAAAAJ
    homepage: https://github.com/PeppaPiggeee
    last_name: Hong
    name: Zhiqing Hong
    username: ~Zhiqing_Hong3
  - emails: '****@zju.edu.cn'
    first_name: Ruiqi
    homepage: https://github.com/RickyL-2000
    last_name: Li
    name: Ruiqi Li
    username: ~Ruiqi_Li2
  - dblp_id: https://dblp.org/pid/156/8975
    emails: '****@zju.edu.cn'
    first_name: Wenrui
    homepage: https://github.com/r666ay
    last_name: Liu
    name: Wenrui Liu
    username: ~Wenrui_Liu2
  - dblp_id: https://dblp.org/pid/277/1388
    emails: '****@gmail.com'
    first_name: Fuming
    google_scholar_id: https://scholar.google.com/citations?user=W6Nf_CAAAAAJ&hl=en
    last_name: You
    name: Fuming You
    username: ~Fuming_You3
  - dblp_id: https://dblp.org/pid/88/4850-4.html
    emails: '****@qq.com'
    first_name: Tao
    homepage: https://hugddygff.github.io/
    last_name: Jin
    name: Tao Jin
    orcid: https://orcid.org/0000-0003-3564-1628
    username: ~Tao_Jin2
  - dblp_id: https://dblp.uni-trier.de/pid/75/7785.html?
    emails: '****@zju.edu.cn'
    first_name: Zhou
    google_scholar_id: https://scholar.google.com.hk/citations?user=IIoFY90AAAAJ&hl=zh-CN
    homepage: https://dblp.uni-trier.de/pid/75/7785.html?
    institution: Zhejiang University and Zhejiang University
    last_name: Zhao
    name: Zhou Zhao
    username: ~Zhou_Zhao3
  decision: toMainConference
  end_page: 5291
  file: 511.pdf
  id: 511
  num_pages: 15
  openreview_id: GJFS5rkO99
  pdf_file: 9cf682d4f1b3fc4a667cb303bcfc500769b084ad.pdf
  start_page: 5277
  title: 'Prompt-Singer: Controllable Singing-Voice-Synthesis with Natural Language
    Prompt'
- abstract: Automatic speech recognition (ASR) systems, increasingly prevalent in
    education, healthcare, employment, and mobile technology, face significant challenges
    in inclusivity, particularly for the 80 million-strong global community of people
    who stutter. These systems often fail to accurately interpret speech patterns
    deviating from typical fluency, leading to critical usability issues and misinterpretations.
    This study evaluates six leading ASRs, analyzing their performance on both a real-world
    dataset of speech samples from individuals who stutter and a synthetic dataset
    derived from the widely-used LibriSpeech benchmark. The synthetic dataset, uniquely
    designed to incorporate various stuttering events, enables an in-depth analysis
    of each ASR's handling of disfluent speech. Our comprehensive assessment includes
    metrics such as word error rate (WER), character error rate (CER), and semantic
    accuracy of the transcripts. The results reveal a consistent and statistically
    significant accuracy bias across all ASRs against disfluent speech, manifesting
    in significant syntactical and semantic inaccuracies in transcriptions. These
    findings highlight a critical gap in current ASR technologies, underscoring the
    need for effective bias mitigation strategies. Addressing this bias is imperative
    not only to improve the technology's usability for people who stutter but also
    to ensure their equitable and inclusive participation in the rapidly evolving
    digital landscape.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Speech recognition, text-to-speech and spoken language understanding
  authors:
  - emails: '****@msu.edu'
    first_name: Dena
    google_scholar_id: https://scholar.google.com/citations?user=Jtd4HMUAAAAJ&hl=en
    last_name: Mujtaba
    name: Dena Mujtaba
    username: ~Dena_Mujtaba1
  - emails: '****@egr.msu.edu'
    first_name: Nihar
    homepage: https://www.egr.msu.edu/~nrm/
    institution: Michigan State University
    last_name: Mahapatra
    middle_name: R.
    name: Nihar R. Mahapatra
    username: ~Nihar_R._Mahapatra1
  - emails: '****@msu.edu'
    first_name: Megan
    institution: Michigan State University
    last_name: Arney
    name: Megan Arney
    orcid: https://orcid.org/0000-0001-6749-5149
    username: ~Megan_Arney1
  - emails: '****@msu.edu'
    first_name: J
    google_scholar_id: https://scholar.google.com/citations?user=e_vt19cAAAAJ&hl=en&oi=ao
    homepage: https://comartsci.msu.edu/our-people/j-scott-yaruss
    institution: Michigan State University
    last_name: Yaruss
    middle_name: Scott
    name: J Scott Yaruss
    orcid: https://orcid.org/0000-0003-1964-575X
    semantic_scholar_id: https://www.semanticscholar.org/author/J.-Yaruss/2014159
    username: ~J_Scott_Yaruss1
  - emails: '****@wmich.edu'
    first_name: Hope
    homepage: https://scholar.google.com/citations?user=j5b_n5gAAAAJ&hl=en&oi=sra
    institution: Western Michigan University
    last_name: Gerlach-Houck
    name: Hope Gerlach-Houck
    username: ~Hope_Gerlach-Houck1
  - emails: '****@friendswhostutter.org'
    first_name: Caryn
    google_scholar_id: https://scholar.google.com/citations?user=X7fKT67iuFYC&hl=en
    homepage: https://www.friendswhostutter.org
    last_name: Herring
    name: Caryn Herring
    orcid: https://orcid.org/0000-0003-2404-852X
    username: ~Caryn_Herring1
  - emails: '****@msu.edu'
    first_name: Jia
    homepage: https://comartsci.msu.edu/our-people/jia-bin
    last_name: Bin
    name: Jia Bin
    username: ~Jia_Bin1
  decision: toMainConference
  end_page: 5306
  file: 513.pdf
  id: 513
  num_pages: 15
  openreview_id: g4bD6zEogg
  pdf_file: f42b3f94e3b71dee8b972b7da85551dd02223c16.pdf
  start_page: 5292
  title: 'Lost in Transcription: Identifying and Quantifying the Accuracy Biases of
    Automatic Speech Recognition Systems Against Disfluent Speech'
- abstract: We introduce MAFALDA, a benchmark for fallacy classification that merges
    and unites previous fallacy datasets. It comes with a taxonomy that aligns, refines,
    and unifies existing classifications of fallacies. We further provide a manual
    annotation of a part of the dataset together with manual explanations for each
    annotation. We propose a new annotation scheme tailored for subjective NLP tasks,
    and a new evaluation method designed to handle subjectivity. We then evaluate
    several language models under a zero-shot learning setting and human performances
    on MAFALDA to assess their capability to detect and classify fallacies.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - dblp_id: https://dblp.org/pid/201/2629
    emails: '****@telecom-paris.fr'
    first_name: Chadi
    last_name: Helwe
    name: Chadi Helwe
    semantic_scholar_id: https://www.semanticscholar.org/author/Chadi-Helwe/17730355
    username: ~Chadi_Helwe1
  - emails: '****@telecom-paris.fr'
    first_name: Tom
    last_name: Calamai
    name: Tom Calamai
    username: ~Tom_Calamai1
  - dblp_id: https://dblp.org/pid/208/0271
    emails: '****@phparis.net'
    first_name: Pierre-Henri
    homepage: https://phparis.net
    institution: "T\xE9l\xE9com Paris"
    last_name: Paris
    name: Pierre-Henri Paris
    username: ~Pierre-Henri_Paris1
  - dblp_id: https://dblp.org/pid/50/2768
    emails: '****@telecom-paristech.fr'
    first_name: "Chlo\xE9"
    google_scholar_id: https://scholar.google.fr/citations?user=TAZbfksAAAAJ&hl=en
    homepage: https://clavel.wp.imt.fr/
    institution: "INRIA and T\xE9l\xE9com Paris"
    last_name: Clavel
    name: "Chlo\xE9 Clavel"
    username: "~Chlo\xE9_Clavel2"
  - dblp_id: https://dblp.org/pid/13/1396
    emails: '****@suchanek.name'
    first_name: Fabian
    google_scholar_id: https://scholar.google.fr/citations?user=djtZhi8AAAAJ&hl=en&oi=ao
    homepage: https://suchanek.name
    institution: Telecom Paris
    last_name: Suchanek
    middle_name: M.
    name: Fabian M. Suchanek
    orcid: https://orcid.org/0000-0001-7189-2796
    semantic_scholar_id: https://www.semanticscholar.org/author/Fabian-M.-Suchanek/1679784
    username: ~Fabian_M._Suchanek1
  decision: toMainConference
  end_page: 5342
  file: 514.pdf
  id: 514
  num_pages: 36
  openreview_id: 9aNDf8zZbo
  pdf_file: 06b8fc56afe23f7c53a60c66afb02d3560095384.pdf
  start_page: 5307
  title: 'MAFALDA: A Benchmark and Comprehensive Study of Fallacy Detection and Classification'
- abstract: "Previously, non-autoregressive models were widely recognized as being\
    \ superior in generation efficiency but inferior in generation quality due to\
    \ the challenges of modeling multiple target modalities.\nTo enhance the multi-modality\
    \ modeling ability, we propose the diffusion glancing transformer, which employs\
    \ a modality diffusion process and residual glancing sampling.\nThe modality diffusion\
    \ process is a discrete process that interpolates the multi-modal distribution\
    \ along the decoding steps, and the residual glancing sampling approach guides\
    \ the model to continuously learn the remaining modalities across the layers.\
    \ \nExperimental results on various machine translation and text generation benchmarks\
    \ demonstrate that DIFFGLAT achieves better generation accuracy while maintaining\
    \ fast decoding speed compared with both autoregressive and non-autoregressive\
    \ models."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Generation
  authors:
  - dblp_id: https://dblp.org/pid/167/5564
    emails: '****@bytedance.com'
    first_name: Lihua
    homepage: https://to.be.done
    institution: ByteDance
    last_name: Qian
    name: Lihua Qian
    username: ~Lihua_Qian1
  - dblp_id: https://dblp.org/pid/43/11214
    emails: '****@bytedance.com'
    first_name: Mingxuan
    google_scholar_id: https://scholar.google.com/citations?user=hOQ6G6EAAAAJ&hl=en
    last_name: Wang
    name: Mingxuan Wang
    username: ~Mingxuan_Wang1
  - dblp_id: https://dblp.org/pid/51/3710-5
    emails: '****@tsinghua.edu.cn'
    first_name: Yang
    google_scholar_id: https://scholar.google.com.hk/citations?user=lVhoKNcAAAAJ&hl=en
    homepage: http://nlp.csai.tsinghua.edu.cn/~ly/
    last_name: Liu
    name: Yang Liu
    orcid: https://orcid.org/0000-0002-3087-242X
    semantic_scholar_id: https://www.semanticscholar.org/author/Yang-Liu/2152797839
    username: ~Yang_Liu19
  - dblp_id: https://dblp.uni-trier.de/pid/63/778-12
    emails: '****@gmail.com'
    first_name: Hao
    google_scholar_id: https://scholar.google.com/citations?hl=zh-CN&user=XEgaCScAAAAJ&view_op=list_works&gmla=AJsN-F44OQLtIK-bb9nTc0D_8RvRk0fMXjU4YID5MWDQpCLSit19URYowi4IONpHePIgChrR9GXd-twexJLKCa-INOJlI2YpE38tN4Z9EzCyZsAE7LKuejxljFZ5oz0xtOcMKt4GhaLTAVIrUbPyASBB-ePsnMtr5A
    homepage: https://zhouh.github.io/
    last_name: Zhou
    name: Hao Zhou
    username: ~Hao_Zhou5
  decision: toMainConference
  end_page: 5359
  file: 517.pdf
  id: 517
  num_pages: 17
  openreview_id: UQHOyaAaxS
  pdf_file: 8c91562c0257703fe94ea1c81fe9dda14d0ef11a.pdf
  start_page: 5343
  title: Diffusion Glancing Transformer for Parallel Sequence-to-Sequence Learning
- abstract: 'Reasoning in the presence of idiomatic expressions (IEs) remains a challenging
    frontier in natural language understanding (NLU). Unlike standard text, the non-compositional
    nature of an IE makes it difficult for model comprehension, as their figurative
    or non-literal mean- ing usually cannot be inferred from the constituent words
    alone. It stands to reason that in these challenging circumstances, pre-trained
    language models (PTLMs) should make use of the surrounding context to infer additional
    in- formation about the IE. In this paper, we investigate the utilization of said
    context for idiomatic reasoning tasks, which is under-explored relative to arithmetic
    or commonsense reason- ing (Liu et al., 2022; Yu et al., 2023). Preliminary findings
    point to a surprising observation: general purpose PTLMs are actually negatively
    affected by the context, as performance almost always increases with its removal.
    In these scenarios, models may see gains of up to 3.89%. As a result, we argue
    that only IE-aware models remain suitable for idiomatic reasoning tasks, given
    the unexpected and unexplainable manner in which general purpose PTLMs reason
    over IEs. Additionally, we conduct studies to examine how models utilize the context
    in various situations, as well as an in-depth analysis on dataset formation and
    quality. Finally, we provide some explanations and insights into the reasoning
    process itself based on our results.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - emails: '****@princeton.edu'
    first_name: Kellen
    homepage: https://scholar.google.com/citations?hl=en&user=wQEE6y0AAAAJ
    institution: Princeton University
    last_name: Cheng
    middle_name: Tan
    name: Kellen Tan Cheng
    username: ~Kellen_Tan_Cheng1
  - dblp_id: https://dblp.org/pid/66/9013
    emails: '****@illinois.edu'
    first_name: Suma
    institution: University of Illinois at Urbana-Champaign and University of Illinois,
      Urbana Champaign
    last_name: Bhat
    name: Suma Bhat
    username: ~Suma_Bhat1
  decision: toMainConference
  end_page: 5377
  file: 525.pdf
  id: 525
  num_pages: 18
  openreview_id: p3uWkNI8oj
  pdf_file: 8c33831e493e95ef6b6874f061f5c188bde11212.pdf
  start_page: 5360
  title: 'No Context Needed: Contextual Quandary In Idiomatic Reasoning With Pre-Trained
    Language Models'
- abstract: Document translation poses a challenge for Neural Machine Translation
    (NMT) systems. Most document-level NMT systems rely on meticulously curated sentence-level
    parallel data, assuming flawless extraction of text from documents along with
    their precise reading order. These systems also tend to disregard additional visual
    cues such as the document layout, deeming it irrelevant. However, real-world documents
    often possess intricate text layouts that defy these assumptions. Extracting information
    from Optical Character Recognition (OCR) or heuristic rules can result in errors,
    and the layout (e.g., paragraphs, headers) may convey relationships between distant
    sections of text. This complexity is particularly evident in widely used PDF documents,
    which represent information visually. This paper addresses this gap by introducing
    M3T a novel benchmark dataset tailored to evaluate NMT systems on the comprehensive
    task of translating semi-structured documents. This dataset aims to bridge the
    evaluation gap in document-level NMT systems, acknowledging the challenges posed
    by rich text layouts in real-world applications.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Translation
  authors:
  - dblp_id: https://dblp.org/pid/289/9990
    emails: '****@gmail.com'
    first_name: Benjamin
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=DZcZx7UAAAAJ
    institution: Amazon
    last_name: Hsu
    name: Benjamin Hsu
    semantic_scholar_id: https://www.semanticscholar.org/author/2064231695
    username: ~Benjamin_Hsu1
  - emails: '****@umd.edu'
    first_name: Xiaoyu
    institution: University of Maryland, College Park
    last_name: Liu
    name: Xiaoyu Liu
    orcid: https://orcid.org/0000-0003-3385-4726
    username: ~Xiaoyu_Liu3
  - dblp_id: https://dblp.org/pid/43/5939
    emails: '****@gmail.com'
    first_name: Huayang
    google_scholar_id: https://scholar.google.com/citations?user=_1jSi34AAAAJ&hl=en
    homepage: https://sites.google.com/view/huayangli
    last_name: Li
    name: Huayang Li
    semantic_scholar_id: https://www.semanticscholar.org/author/Huayang-Li/91956362
    username: ~Huayang_Li1
  - dblp_id: https://dblp.org/pid/174/7392
    emails: '****@gmail.com'
    first_name: Yoshinari
    institution: AWS AI Labs
    last_name: Fujinuma
    name: Yoshinari Fujinuma
    semantic_scholar_id: https://www.semanticscholar.org/author/Yoshinari-Fujinuma/31482660
    username: ~Yoshinari_Fujinuma1
  - dblp_id: https://dblp.org/pid/185/5556
    emails: '****@gmail.com'
    first_name: Maria
    google_scholar_id: https://scholar.google.co.uk/citations?user=zAiGJPUAAAAJ&hl=en
    institution: Amazon
    last_name: Nadejde
    name: Maria Nadejde
    semantic_scholar_id: https://www.semanticscholar.org/author/Maria-Nadejde/3456844
    username: ~Maria_Nadejde1
  - dblp_id: https://dblp.org/pid/87/9555
    emails: '****@xingniu.org'
    first_name: Xing
    google_scholar_id: https://scholar.google.com/citations?user=45heFpgAAAAJ
    homepage: http://xingniu.org/
    institution: Amazon
    last_name: Niu
    name: Xing Niu
    semantic_scholar_id: https://www.semanticscholar.org/author/145523874
    username: ~Xing_Niu1
  - dblp_id: https://dblp.org/pid/261/9598
    emails: '****@gmail.com'
    first_name: Ron
    google_scholar_id: https://scholar.google.com/citations?user=69GY5dEAAAAJ&hl=en
    institution: Amazon
    last_name: Litman
    name: Ron Litman
    semantic_scholar_id: https://www.semanticscholar.org/author/Ron-Litman/1591101831
    username: ~Ron_Litman1
  - dblp_id: https://dblp.org/pid/279/2986
    emails: '****@gmail.com'
    first_name: Yair
    google_scholar_id: https://scholar.google.com/citations?user=WhB3USMAAAAJ&hl=iw&oi=ao
    institution: Amazon
    last_name: Kittenplon
    name: Yair Kittenplon
    username: ~Yair_Kittenplon1
  - emails: '****@gmail.com'
    first_name: Raghavendra
    google_scholar_id: https://scholar.google.com/citations?user=WfGI1uEAAAAJ&hl=en
    last_name: Pappagari
    name: Raghavendra Pappagari
    semantic_scholar_id: https://www.semanticscholar.org/author/R.-Pappagari/18081502
    username: ~Raghavendra_Pappagari1
  decision: toMainConference
  end_page: 5386
  file: 526.pdf
  id: 526
  num_pages: 9
  openreview_id: 2mVEL2IgVg
  pdf_file: f2b78c4f23efb4ab1465314a93eea6a10a061de9.pdf
  start_page: 5378
  title: 'M3T: A New Benchmark Dataset for Multi-Modal Document-Level Machine Translation'
- abstract: The International Classification of Diseases (ICD) serves as a definitive
    medical classification system encompassing a wide range of diseases and conditions.
    The primary objective of ICD indexing is to allocate a subset of ICD codes to
    a medical record, which facilitates standardized documentation and management
    of various health conditions. Most existing approaches have suffered from selecting
    the proper label subsets from an extremely large ICD collection with a heavy long-tailed
    label distribution. In this paper, we leverage a multi-stage "retrieve and re-rank"
    framework as a novel solution to ICD indexing, via a hybrid discrete retrieval
    method, and re-rank retrieved candidates with contrastive learning that allows
    the model to make more accurate predictions from a simplified label space. The
    retrieval model is a hybrid of  auxiliary knowledge of the electronic health records
    (EHR) and  a discrete retrieval method (BM25), which efficiently collects high-quality
    candidates. In the last stage, we propose a label co-occurrence guided contrastive
    re-ranking model, which re-ranks the candidate labels by pulling together the
    clinical notes with positive ICD codes. Experimental results show the proposed
    method achieves state-of-the-art performance on a number of measures on the MIMIC-III
    benchmark.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - dblp_id: https://dblp.org/pid/155/4933
    emails: '****@uwo.ca'
    first_name: Xindi
    google_scholar_id: https://scholar.google.ca/citations?user=6yrxZdsAAAAJ&hl=en
    institution: Huawei Technologies Ltd., University of Western Ontario and Vector
      Institute
    last_name: Wang
    name: Xindi Wang
    semantic_scholar_id: https://www.semanticscholar.org/author/Xindi-Wang/2108048327
    username: ~Xindi_Wang1
  - dblp_id: https://dblp.org/pid/m/RobertEMercer.html
    emails: '****@csd.uwo.ca'
    first_name: Robert
    google_scholar_id: https://scholar.google.com/citations?user=UDAYdq4AAAAJ&hl=en&oi=ao
    homepage: http://www.csd.uwo.ca/~rmercer/
    institution: University of Western Ontario
    last_name: Mercer
    name: Robert Mercer
    orcid: https://orcid.org/my-orcid?orcid=0000-0002-0080-715X
    semantic_scholar_id: https://www.semanticscholar.org/author/Robert-E.-Mercer/144549038
    username: ~Robert_Mercer1
  - dblp_id: https://dblp.org/pid/36/6505
    emails: '****@dal.ca'
    first_name: Frank
    google_scholar_id: https://scholar.google.ca/citations?user=elXOB1sAAAAJ&hl=en
    homepage: http://www.cs.toronto.edu/~frank
    institution: Dalhousie University
    last_name: Rudzicz
    name: Frank Rudzicz
    orcid: https://orcid.org/0000-0002-1139-3423
    semantic_scholar_id: https://www.semanticscholar.org/author/Frank-Rudzicz/2479037
    username: ~Frank_Rudzicz2
  decision: toMainConference
  end_page: 5397
  file: 527.pdf
  id: 527
  num_pages: 11
  openreview_id: BlkFRXSf1g
  pdf_file: 4558e97dcd29aa187aa9ce2f4925dc8f121839dd.pdf
  start_page: 5387
  title: Multi-stage Retrieve and Re-rank Model for Automatic Medical Coding Recommendation
- abstract: Isotropy is the property that embeddings are uniformly distributed around
    the origin. Previous work has shown that Transformer embedding spaces are anisotropic,
    which is called the representation degradation problem. This degradation has  been
    assumed to be inherent to the standard language modeling tasks and to apply to
    all Transformer models regardless of their architecture. In this work we identify
    a set of Transformer models with isotropic embedding spaces, the large Pythia
    models. We examine the isotropy of Pythia models and explore how isotropy and
    anisotropy develop as a model is trained. We find that anisotropic models do not
    develop as previously theorized, using our own analysis to show that the large
    Pythia models optimize their final Layer Norm for isotropy, and provide reasoning
    why previous theoretical justifications for anisotropy were insufficient. The
    identification of a set of isotropic Transformer models calls previous assumptions
    into question, provides a set of models to contrast existing analysis, and should
    lead to deeper insight into isotropy.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - emails: '****@uwo.ca'
    first_name: Anemily
    institution: University of Western Ontario
    last_name: Machina
    name: Anemily Machina
    semantic_scholar_id: https://www.semanticscholar.org/author/Vincent-Sippola/2129351570
    username: ~Anemily_Machina1
  - dblp_id: https://dblp.org/pid/m/RobertEMercer.html
    emails: '****@csd.uwo.ca'
    first_name: Robert
    google_scholar_id: https://scholar.google.com/citations?user=UDAYdq4AAAAJ&hl=en&oi=ao
    homepage: http://www.csd.uwo.ca/~rmercer/
    institution: University of Western Ontario
    last_name: Mercer
    name: Robert Mercer
    orcid: https://orcid.org/my-orcid?orcid=0000-0002-0080-715X
    semantic_scholar_id: https://www.semanticscholar.org/author/Robert-E.-Mercer/144549038
    username: ~Robert_Mercer1
  decision: toMainConference
  end_page: 5413
  file: 528.pdf
  id: 528
  num_pages: 16
  openreview_id: xulEgHJiLs
  pdf_file: 9c94cdc84589abfaad944b4d1374975533e66a0e.pdf
  start_page: 5398
  title: Anisotropy is Not Inherent to Transformers
- abstract: 'Reliable human evaluation is critical to the development of successful
    natural language generation models, but achieving it is notoriously difficult.
    Stability is a crucial requirement when ranking systems by quality: consistent
    ranking of systems across repeated evaluations is not just desirable, but essential.
    Without it, there is no reliable foundation for hill-climbing or product launch
    decisions. In this paper, we use machine translation and its state-of-the-art
    human evaluation framework, MQM, as a case study to understand how to set up reliable
    human evaluations that yield stable conclusions. We investigate the optimal configurations
    for item allocation to raters, number of ratings per item, and score normalization.
    Our study on two language pairs provides concrete recommendations for designing
    replicable human evaluation studies. We also collect and release the largest publicly
    available dataset of multi-segment translations rated by multiple professional
    translators, consisting of nearly 140,000 segment annotations across two language
    pairs.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - dblp_id: https://dblp.org/pid/222/9463
    emails: '****@gmail.com'
    first_name: Parker
    google_scholar_id: https://scholar.google.com/citations?user=yb7ah5sAAAAJ&hl=en
    institution: Google
    last_name: Riley
    name: Parker Riley
    semantic_scholar_id: https://www.semanticscholar.org/author/Parker-Riley/47718053
    username: ~Parker_Riley1
  - dblp_id: https://dblp.org/pid/222/9395
    emails: '****@google.com'
    first_name: Daniel
    google_scholar_id: https://scholar.google.com/citations?user=TQYzWDEAAAAJ&hl=en
    homepage: https://danieldeutsch.github.io/
    institution: Google
    last_name: Deutsch
    name: Daniel Deutsch
    semantic_scholar_id: https://www.semanticscholar.org/author/Daniel-Deutsch/145346875
    username: ~Daniel_Deutsch1
  - dblp_id: https://dblp.org/pid/02/1712
    emails: '****@gmail.com'
    first_name: George
    google_scholar_id: https://scholar.google.com/citations?user=Hr8KyG4AAAAJ&hl=en
    homepage: http://www.iro.umontreal.ca/~foster
    institution: Google
    last_name: Foster
    name: George Foster
    username: ~George_Foster1
  - dblp_id: https://dblp.org/pid/49/4726
    emails: '****@google.com'
    first_name: Viresh
    google_scholar_id: https://scholar.google.com/citations?user=e0LXOEIAAAAJ
    institution: Google
    last_name: Ratnakar
    name: Viresh Ratnakar
    username: ~Viresh_Ratnakar2
  - emails: '****@google.com'
    first_name: Ali
    google_scholar_id: https://scholar.google.com/citations?user=SsxbVR0AAAAJ
    last_name: Dabirmoghaddam
    name: Ali Dabirmoghaddam
    username: ~Ali_Dabirmoghaddam1
  - dblp_id: https://dblp.org/pid/57/8503
    emails: '****@gmx.de'
    first_name: Markus
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=t-VqdOUAAAAJ
    institution: Google
    last_name: Freitag
    name: Markus Freitag
    semantic_scholar_id: https://www.semanticscholar.org/author/Markus-Freitag/35307070
    username: ~Markus_Freitag2
  decision: toMainConference
  end_page: 5425
  file: 531.pdf
  id: 531
  num_pages: 12
  openreview_id: 9qPLOGVrzI
  pdf_file: 98db7647aa8b62f8ee2394e55ef51b1d9b3d5ba2.pdf
  start_page: 5414
  title: Finding Replicable Human Evaluations via Stable Ranking Probability
- abstract: 'Recent developments in Large Language Models (LLMs) have manifested significant
    advancements. To facilitate safeguards against malicious exploitation, a body
    of research has concentrated on aligning LLMs with human preferences and inhibiting
    their generation of inappropriate content. Unfortunately, such alignments are
    often vulnerable: fine-tuning with a minimal amount of harmful data can easily
    unalign the target LLM. While being effective, such fine-tuning-based unalignment
    approaches also have their own limitations: (1) non-stealthiness, after fine-tuning,
    safety audits or red-teaming can easily expose the potential weaknesses of the
    unaligned models, thereby precluding their release/use. (2) non-persistence, the
    unaligned LLMs can be easily repaired through re-alignment, i.e., fine-tuning
    again with aligned data points. In this work, we show that it is possible to conduct
    stealthy and persistent unalignment on large language models via backdoor injections.
    We also provide a novel understanding of the relationship between the backdoor
    persistence and the activation pattern and further provide guidelines for potential
    trigger design. Through extensive experiments, we demonstrate that our proposed
    stealthy and persistent unalignment can successfully pass the safety evaluation
    while maintaining strong persistence against re-alignment defense.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Ethics, Bias, and Fairness
  authors:
  - dblp_id: https://dblp.org/pid/243/0230
    emails: '****@psu.edu'
    first_name: Yuanpu
    institution: Pennsylvania State University
    last_name: Cao
    name: Yuanpu Cao
    username: ~Yuanpu_Cao1
  - emails: '****@psu.edu'
    first_name: Bochuan
    homepage: https://ist.psu.edu/directory/bxc5597
    last_name: Cao
    name: Bochuan Cao
    username: ~Bochuan_Cao1
  - dblp_id: https://dblp.org/pid/67/5633
    emails: '****@psu.edu'
    first_name: Jinghui
    google_scholar_id: https://scholar.google.com/citations?user=mKia7Y4AAAAJ&hl=en
    homepage: https://jinghuichen.github.io/
    institution: Pennsylvania State University
    last_name: Chen
    name: Jinghui Chen
    username: ~Jinghui_Chen1
  decision: toMainConference
  end_page: 5441
  file: 533.pdf
  id: 533
  num_pages: 16
  openreview_id: Xn1reijKsZ
  pdf_file: 9cb0228b9f38c798e44520d2cbf39906b71acb37.pdf
  start_page: 5426
  title: Stealthy and Persistent Unalignment on Large Language Models via Backdoor
    Injections
- abstract: Pretrained Language Models (PLMs) have advanced Natural Language Processing
    (NLP) tasks significantly, but finetuning PLMs on low-resource datasets poses
    significant challenges such as instability and overfitting. Previous methods tackle
    these issues by finetuning a strategically chosen subnetwork on a downstream task,
    while keeping the remaining weights fixed to the pretrained weights. However,
    they rely on a suboptimal criteria for sub-network selection, leading to suboptimal
    solutions. To address these limitations, we propose a regularization method based
    on attention-guided weight mixup for finetuning PLMs. Our approach represents
    each network weight as a mixup of task-specific weight and pretrained weight,
    controlled by a learnable attention parameter, providing finer control over sub-network
    selection. Furthermore, we employ a bi-level optimization (BLO) based framework
    on two separate splits of the training dataset, improving generalization and combating
    overfitting. We validate the efficacy of our proposed method through extensive
    experiments, demonstrating its superiority over previous methods, particularly
    in the context of finetuning PLMs on low-resource datasets. Our code is available
    at https://github.com/Sai-Ashish/Attention_guided_weight_mixup_BLO.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Efficient/Low-Resource Methods for NLP
  authors:
  - dblp_id: https://dblp.org/pid/276/4407
    emails: '****@ucsd.edu'
    first_name: Sai Ashish
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=NU1fOrwAAAAJ
    last_name: Somayajula
    name: Sai Ashish Somayajula
    username: ~Sai_Ashish_Somayajula1
  - dblp_id: https://dblp.org/pid/257/5626
    emails: '****@gmail.com'
    first_name: Youwei
    google_scholar_id: https://scholar.google.com/citations?user=zMofZR4AAAAJ
    homepage: https://youweiliang.github.io/
    institution: University of California, San Diego
    last_name: Liang
    name: Youwei Liang
    username: ~Youwei_Liang1
  - emails: '****@ucsd.edu'
    first_name: Li
    google_scholar_id: https://scholar.google.com/citations?hl=zh-CN&view_op=list_works&gmla=AJsN-F44-faf7S_f3JRjy-iVVAFOzj9BUjbNPmnbspJFbCTRoqv9-UER7ux7Vg8Nv-tZ1ELIxmLqk2qvZsA1qChmzb1sE0SDd7sR_RQRoPZp8vcLoAArJT8&user=RRzZk4YAAAAJ
    last_name: Zhang
    name: Li Zhang
    username: ~Li_Zhang21
  - emails: '****@ucsd.edu'
    first_name: Abhishek
    google_scholar_id: https://scholar.google.com/citations?user=SatULfoAAAAJ&hl=en
    homepage: https://asking28.github.io/
    last_name: Singh
    name: Abhishek Singh
    username: ~Abhishek_Singh10
  - dblp_id: https://dblp.org/pid/133/1998
    emails: '****@ucsd.edu'
    first_name: Pengtao
    homepage: https://pengtaoxie.github.io/
    institution: University of California, San Diego
    last_name: Xie
    name: Pengtao Xie
    username: ~Pengtao_Xie3
  decision: toMainConference
  end_page: 5459
  file: 537.pdf
  id: 537
  num_pages: 18
  openreview_id: M2IVGNxw40
  pdf_file: bf438ec8a8491dc0ada1ee3254434baecd0408f9.pdf
  start_page: 5442
  title: Generalizable and Stable Finetuning of Pretrained Language Models on Low-Resource
    Texts
- abstract: Bipolar Disorder (BD) is a mental disorder characterized by intense mood
    swings, from depression to manic states. Individuals with BD are at a higher risk
    of suicide, but BD is often misdiagnosed as Major Depressive Disorder (MDD) due
    to shared symptoms, resulting in delays in appropriate treatment and increased
    suicide risk. While early intervention based on social media data has been explored
    to uncover latent BD risk, little attention has been paid to detecting BD from
    those misdiagnosed as MDD. Therefore, this study presents a novel approach for
    identifying BD risk in individuals initially misdiagnosed with MDD. A unique dataset,
    BD-Risk, is introduced, incorporating mental disorder types and BD mood levels
    verified by two clinical experts. The proposed multi-task learning for predicting
    BD risk and BD mood level outperforms the state-of-the-art baselines. Also, the
    proposed dynamic mood-aware attention can provide insights into the impact of
    BD mood on future risk, potentially aiding interventions for at-risk individuals.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - emails: '****@skku.edu'
    first_name: Daeun
    homepage: https://sites.google.com/view/daeun-lee
    last_name: Lee
    name: Daeun Lee
    username: ~Daeun_Lee1
  - emails: '****@g.skku.edu'
    first_name: Hyolim
    homepage: https://sites.google.com/g.skku.edu/hyolimjeon/gyfla
    last_name: Jeon
    name: Hyolim Jeon
    username: ~Hyolim_Jeon1
  - emails: '****@g.skku.edu'
    first_name: Sejung
    homepage: https://aaissj.github.io/
    last_name: Son
    name: Sejung Son
    username: ~Sejung_Son1
  - emails: '****@realsn.com'
    first_name: Chaewon
    homepage: https://sites.google.com/view/chaewon-park/
    last_name: Park
    name: Chaewon Park
    username: ~Chaewon_Park1
  - emails: '****@gmail.com'
    first_name: Ji Hyun
    institution: Samsung
    last_name: An
    name: Ji hyun An
    orcid: https://orcid.org/0000-0002-1628-9617
    username: ~Ji_hyun_An1
  - dblp_id: https://dblp.org/pid/14/8398.html
    emails: '****@usf.edu'
    first_name: Seungbae
    google_scholar_id: https://scholar.google.com/citations?user=gFZlotAAAAAJ&hl=en
    homepage: https://sites.google.com/site/sbkimcv/
    institution: University of South Florida
    last_name: Kim
    name: Seungbae Kim
    orcid: https://orcid.org/0000-0001-5667-3560
    username: ~Seungbae_Kim1
  - dblp_id: https://dblp.org/pid/94/7996.html
    emails: '****@skku.edu'
    first_name: Jinyoung
    google_scholar_id: https://scholar.google.co.kr/citations?user=4rkPSC8AAAAJ&hl=en
    homepage: http://dsail.skku.edu
    institution: Sungkyunkwan University
    last_name: Han
    name: Jinyoung Han
    username: ~Jinyoung_Han2
  decision: toMainConference
  end_page: 5476
  file: 538.pdf
  id: 538
  num_pages: 17
  openreview_id: gztuVjpKPQ
  pdf_file: 26b1e86472362ba060f03bd479c2495a19ca02ec.pdf
  start_page: 5460
  title: Detecting Bipolar Disorder from Misdiagnosed Major Depressive Disorder with
    Mood-Aware Multi-Task Learning
- abstract: 'In-context learning (ICL) is an appealing approach for semantic parsing
    due to its few-shot nature and improved generalization. However, learning to parse
    to rare domain-specific languages (DSLs) from just a few demonstrations is challenging,
    limiting the performance of even the most capable LLMs.


    In this work, we show how pre-existing coding abilities of LLMs can be leveraged
    for semantic parsing by (1) using general-purpose programming languages such as
    Python instead of DSLs and (2) augmenting prompts with a structured domain description
    that includes, e.g., the available classes and functions. We show that both these
    changes significantly improve accuracy across three popular datasets; combined,
    they lead to dramatic improvements (e.g., 7.9% to 66.5% on SMCalFlow compositional
    split) and can substantially improve compositional generalization, nearly closing
    the performance gap between easier i.i.d. and harder compositional splits. Finally,
    comparisons across multiple PLs and DSL variations suggest that the similarity
    of a target language to general-purpose code is more important than prevalence
    in pretraining corpora. Our findings provide an improved methodology for building
    semantic parsers in the modern context of ICL with LLMs.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: 'Semantics: Sentence-level Semantics, Textual Inference and Other
      areas'
  authors:
  - dblp_id: https://dblp.org/pid/202/2034
    emails: '****@cs.tau.ac.il'
    first_name: Ben
    google_scholar_id: https://scholar.google.com/citations?user=IdoSF2YAAAAJ&hl=en
    homepage: https://benbogin.github.io/
    last_name: Bogin
    name: Ben Bogin
    semantic_scholar_id: https://www.semanticscholar.org/author/Ben-Bogin/50757607
    username: ~Ben_Bogin1
  - dblp_id: https://dblp.org/pid/302/4731
    emails: '****@gmail.com'
    first_name: Shivanshu
    google_scholar_id: https://scholar.google.com/citations?user=OtlUDs8AAAAJ&hl=en
    homepage: https://shivanshu-gupta.github.io
    institution: University of California, Irvine
    last_name: Gupta
    name: Shivanshu Gupta
    semantic_scholar_id: https://www.semanticscholar.org/author/Shivanshu-Gupta/1698760333
    username: ~Shivanshu_Gupta2
  - dblp_id: https://dblp.org/pid/34/1184
    emails: '****@allenai.org'
    first_name: Peter
    institution: Allen Institute for Artificial Intelligence
    last_name: Clark
    name: Peter Clark
    username: ~Peter_Clark1
  - dblp_id: http://dblp.uni-trier.de/pers/hd/s/Sabharwal:Ashish
    emails: '****@allenai.org'
    first_name: Ashish
    google_scholar_id: https://scholar.google.com/citations?user=7VspfeAAAAAJ&hl=en
    homepage: http://allenai.org/team/ashishs
    institution: Allen Institute for Artificial Intelligence
    last_name: Sabharwal
    name: Ashish Sabharwal
    semantic_scholar_id: https://www.semanticscholar.org/author/Ashish-Sabharwal/48229640
    username: ~Ashish_Sabharwal1
  decision: toMainConference
  end_page: 5518
  file: 539.pdf
  id: 539
  num_pages: 42
  openreview_id: N6IxQ6Q5nY
  pdf_file: e5d856a0084f7c298c456a451a25c0be2d621ce8.pdf
  start_page: 5477
  title: Leveraging Code to Improve In-Context Learning for Semantic Parsing
- abstract: Adapting language models (LMs) to novel domains is often achieved through
    fine-tuning a pre-trained LM (PLM) on domain-specific data. Fine-tuning introduces
    new knowledge into an LM, enabling it to comprehend and efficiently perform a
    target domain task. Fine-tuning can however be inadvertently insensitive if it
    ignores the wide array of disparities (e.g in word meaning) between source and
    target domains. For instance, words such as chronic and pressure may be treated
    lightly in social conversations, however, clinically, these words are usually
    an expression of concern. To address insensitive fine-tuning, we propose Mask
    Specific Language Modeling (MSLM), an approach that efficiently acquires target
    domain knowledge by appropriately weighting the importance of domain-specific
    terms (DS-terms) during fine-tuning. MSLM jointly masks DS-terms and generic words,
    then learns mask-specific losses by ensuring LMs incur larger penalties for inaccurately
    predicting DS-terms compared to generic words. Results of our analysis show that
    MSLM improves LMs sensitivity and detection of DS-terms. We empirically show that
    an optimal masking rate not only depends on the LM, but also on the dataset and
    the length of sequences. Our proposed masking strategy outperforms advanced masking
    strategies such as span- and PMI-based masking.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Extraction
  authors:
  - emails: '****@liverpool.ac.uk'
    first_name: Micheal
    google_scholar_id: https://scholar.google.com/citations?user=uLDa-6EAAAAJ&hl=en
    homepage: https://mykelismyname.github.io/micheal/
    institution: University of Liverpool
    last_name: Abaho
    name: Micheal Abaho
    orcid: https://orcid.org/0000-0001-5433-5904
    semantic_scholar_id: https://www.semanticscholar.org/author/Michael-Abaho/90965859
    username: ~Micheal_Abaho3
  - dblp_id: https://dblp.uni-trier.de/pers/hd/b/Bollegala:Danushka
    emails: '****@danushka.net'
    first_name: Danushka
    google_scholar_id: https://scholar.google.co.uk/citations?user=kLqCYLMAAAAJ&hl=en
    homepage: https://danushka.net
    institution: Amazon and University of Liverpool
    last_name: Bollegala
    name: Danushka Bollegala
    orcid: https://orcid.org/0000-0003-4476-7003
    username: ~Danushka_Bollegala1
  - emails: '****@liverpool.ac.uk'
    first_name: Gary
    homepage: https://www.liverpool.ac.uk/population-health/staff/gary-leeming/
    institution: University of Liverpool
    last_name: Leeming
    name: Gary Leeming
    orcid: https://orcid.org/0000-0002-5554-5302
    username: ~Gary_Leeming1
  - emails: '****@psych.ox.ac.uk'
    first_name: Dan
    homepage: https://www.danwjoyce.com
    institution: University of Oxford
    last_name: Joyce
    middle_name: W
    name: Dan W Joyce
    orcid: https://orcid.org/0000-0002-9433-5340
    username: ~Dan_W_Joyce1
  - emails: '****@liverpool.ac.uk'
    first_name: Iain
    google_scholar_id: https://scholar.google.com/citations?user=Lz49QrkAAAAJ&hl=en
    homepage: https://www.liverpool.ac.uk/population-health/staff/iain-buchan/
    institution: University of Liverpool
    last_name: Buchan
    middle_name: Edward
    name: Iain Edward Buchan
    orcid: https://orcid.org/0000-0003-3392-1650
    username: ~Iain_Edward_Buchan1
  decision: toMainConference
  end_page: 5535
  file: 540.pdf
  id: 540
  num_pages: 17
  openreview_id: 9UGppKZMtk
  pdf_file: 62fe98c201f4fcc2b1d2dd0997e91b688a6b5f98.pdf
  start_page: 5519
  title: 'Improving Pre-trained Language Model Sensitivity via Mask Specific losses:
    A case study on Biomedical NER'
- abstract: A primary criticism towards language models (LMs) is their inscrutability.
    This paper presents evidence that, despite their size and complexity, LMs sometimes
    exploit a simple vector arithmetic style mechanism to solve some relational tasks
    using regularities encoded in the hidden space of the model (e.g., Poland:Warsaw::China:Beijing).
    We investigate a range of language model sizes (from 124M parameters to 176B parameters)
    in an in-context learning setting, and find that for a variety of tasks (involving
    capital cities, uppercasing, and past-tensing) a key part of the mechanism reduces
    to a simple additive update typically applied by the feedforward (FFN) networks.
    We further show that this mechanism is specific to tasks that require retrieval
    from pretraining memory, rather than retrieval from local context. Our results
    contribute to a growing body of work on the interpretability of LMs, and offer
    reason to be optimistic that, despite the massive and non-linear nature of the
    models, the strategies they ultimately use to solve tasks can sometimes reduce
    to familiar and even intuitive algorithms.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - dblp_id: https://dblp.org/pid/248/8361
    emails: '****@brown.edu'
    first_name: Jack
    homepage: https://brown.edu/Research/AI/people/jack.html
    institution: Brown University
    last_name: Merullo
    name: Jack Merullo
    semantic_scholar_id: https://www.semanticscholar.org/author/Jack-Merullo/1405408066
    username: ~Jack_Merullo2
  - dblp_id: https://dblp.org/pid/42/8700
    emails: '****@brown.edu'
    first_name: Carsten
    google_scholar_id: https://scholar.google.com/citations?user=QQi1_rAAAAAJ
    homepage: https://health-nlp.org
    institution: "Eberhard-Karls-Universit\xE4t T\xFCbingen"
    last_name: Eickhoff
    name: Carsten Eickhoff
    orcid: https://orcid.org/0000-0001-9895-4061
    username: ~Carsten_Eickhoff1
  - dblp_id: https://dblp.org/pid/141/4059
    emails: '****@brown.edu'
    first_name: Ellie
    google_scholar_id: https://scholar.google.com/citations?user=sFyrSa8AAAAJ&hl=en
    homepage: http://cs.brown.edu/people/epavlick/
    institution: Brown University and Brown University
    last_name: Pavlick
    name: Ellie Pavlick
    username: ~Ellie_Pavlick1
  decision: toMainConference
  end_page: 5553
  file: 541.pdf
  id: 541
  num_pages: 18
  openreview_id: PhbPhaKwaV
  pdf_file: fad6fbf11c81936e3bd3974984ae2837afcf609b.pdf
  start_page: 5536
  title: Language Models Implement Simple Word2Vec-style Vector Arithmetic
- abstract: 'The Directed Acyclic Transformer is a fast non-autoregressive (NAR) model
    that performs well in Neural Machine Translation. Two issues prevent its application
    to general Natural Language Generation (NLG) tasks: frequent Out-Of-Vocabulary
    (OOV) errors and the inability to faithfully generate entity names. We introduce
    Control-DAG, a constrained decoding algorithm for our Directed Acyclic T5 (DA-T5)
    model which offers lexical, vocabulary and length control. We show that Control-DAG
    significantly enhances DA-T5 on the Schema Guided Dialogue and the DART datasets,
    establishing strong NAR results for Task-Oriented Dialogue and Data-to-Text NLG.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Generation
  authors:
  - dblp_id: https://dblp.org/pid/21/1754
    emails: '****@cam.ac.uk'
    first_name: Jinghong
    google_scholar_id: https://scholar.google.com/citations?user=pYOXaKEAAAAJ&hl=zh-CN
    homepage: https://erichen0615.github.io/
    last_name: Chen
    name: Jinghong Chen
    semantic_scholar_id: https://www.semanticscholar.org/author/Jinghong-Chen/2159255814
    username: ~Jinghong_Chen2
  - dblp_id: https://dblp.org/pid/254/9170
    emails: '****@cam.ac.uk'
    first_name: Weizhe
    google_scholar_id: https://scholar.google.com/citations?user=4hMhIecAAAAJ
    homepage: https://linweizhedragon.github.io/
    last_name: Lin
    name: Weizhe Lin
    username: ~Weizhe_Lin1
  - dblp_id: https://dblp.org/pid/305/9241
    emails: '****@cam.ac.uk'
    first_name: Jingbiao
    homepage: https://jingbiao.me
    last_name: Mei
    name: Jingbiao Mei
    username: ~Jingbiao_Mei1
  - dblp_id: https://dblp.org/pid/b/WilliamJByrne
    emails: '****@cam.ac.uk'
    first_name: Bill
    google_scholar_id: https://scholar.google.com/citations?user=BVUcMU4AAAAJ&hl=en
    homepage: https://sites.google.com/view/bill-byrne/
    institution: Amazon and University of Cambridge
    last_name: Byrne
    name: Bill Byrne
    username: ~Bill_Byrne1
  decision: toMainConference
  end_page: 5564
  file: 542.pdf
  id: 542
  num_pages: 11
  openreview_id: jVWs589NEx
  pdf_file: 6e6ff44628de61ae6a43ada7d94217ae16f4658d.pdf
  start_page: 5554
  title: 'Control-DAG: Constrained Decoding for Non-Autoregressive Directed Acyclic
    T5 using Weighted Finite State Automata'
- abstract: Large-scale pretraining followed by task-specific finetuning has achieved
    great success in various NLP tasks. Since finetuning all parameters of large pretrained
    models poses substantial computational and memory challenges, several efficient
    finetuning methods have been developed. Among them, low-rank adaptation (LoRA),
    which finetunes low-rank incremental update matrices on top of frozen pretrained
    weights, has proven particularly effective. Nonetheless, LoRA's uniform rank assignment
    across all layers, along with its reliance on an exhaustive search to find the
    best rank, leads to high computation costs and suboptimal finetuning performance.
    To address these limitations, we introduce AutoLoRA, a meta learning based framework
    for automatically identifying the optimal rank of each LoRA layer. AutoLoRA associates
    each rank-1 matrix in a low-rank update matrix with a selection variable, which
    determines whether the rank-1 matrix should be discarded. A meta learning based
    method is developed to learn these selection variables. The optimal rank is determined
    by thresholding the values of these variables. Our comprehensive experiments on
    natural language understanding, generation, and sequence labeling demonstrate
    the effectiveness of AutoLoRA. The code is publicly available at https://github.com/ruz048/AutoLoRA
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Efficient/Low-Resource Methods for NLP
  authors:
  - emails: '****@ucsd.edu'
    first_name: Ruiyi
    google_scholar_id: https://scholar.google.com/citations?user=D7EXgU0AAAAJ&hl=en
    institution: University of California, San Diego
    last_name: Zhang
    name: Ruiyi Zhang
    username: ~Ruiyi_Zhang4
  - emails: '****@mails.tsinghua.edu.cn'
    first_name: Rushi
    homepage: https://github.com/jerrycool2002
    last_name: Qiang
    name: Rushi Qiang
    username: ~Rushi_Qiang1
  - dblp_id: https://dblp.org/pid/276/4407
    emails: '****@ucsd.edu'
    first_name: Sai Ashish
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=NU1fOrwAAAAJ
    last_name: Somayajula
    name: Sai Ashish Somayajula
    username: ~Sai_Ashish_Somayajula1
  - dblp_id: https://dblp.org/pid/133/1998
    emails: '****@ucsd.edu'
    first_name: Pengtao
    homepage: https://pengtaoxie.github.io/
    institution: University of California, San Diego
    last_name: Xie
    name: Pengtao Xie
    username: ~Pengtao_Xie3
  decision: toMainConference
  end_page: 5577
  file: 543.pdf
  id: 543
  num_pages: 13
  openreview_id: FChjtjzAKp
  pdf_file: 0e671055bd2b11b397533a6784c0aff859f888b7.pdf
  start_page: 5565
  title: 'AutoLoRA: Automatically Tuning Matrix Ranks in Low-Rank Adaptation Based
    on Meta Learning'
- abstract: Open-vocabulary vision-language models (VLMs) like CLIP, trained using
    contrastive loss, have emerged as a promising new paradigm for text-to-image retrieval.
    However, do VLMs understand compound nouns (CNs) (e.g., *lab coat*) as well as
    they understand nouns (e.g., *lab*)? We curate Compun, a novel benchmark with
    400 unique and commonly used CNs, to evaluate the effectiveness of VLMs in interpreting
    CNs. The Compun benchmark challenges a VLM for text-to-image retrieval where,
    given a text prompt with a CN, the task is to select the correct image that shows
    the CN among a pair of distractor images that show the constituent nouns that
    make up the CN. Next, we perform an in-depth analysis to highlight CLIPs' limited
    understanding of certain types of CNs. Finally, we present an alternative framework
    that moves beyond hand-written templates for text prompts widely used by CLIP-like
    models. We employ a Large Language Model to generate multiple diverse captions
    that include the CN as an object in the scene described by the caption. Our proposed
    method improves CN understanding of CLIP by 8.25% on Compun. Code and benchmark
    are available.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multimodality and Language Grounding to Vision, Robotics and Beyond
  authors:
  - emails: '****@gmail.com'
    first_name: Sonal
    google_scholar_id: https://scholar.google.com/citations?user=jiJ2DcEAAAAJ&hl=en&oi=sra
    last_name: Kumar
    name: Sonal Kumar
    username: ~Sonal_Kumar1
  - dblp_id: https://dblp.org/pid/173/5626
    emails: '****@umd.edu'
    first_name: Sreyan
    google_scholar_id: https://scholar.google.com/citations?user=5HKZJHAAAAAJ&hl=en
    homepage: https://sreyan88.github.io/
    last_name: Ghosh
    name: Sreyan Ghosh
    username: ~Sreyan_Ghosh1
  - emails: '****@umass.edu'
    first_name: S
    google_scholar_id: https://scholar.google.com/citations?user=F_-YNVAAAAAJ&hl=en
    last_name: Sakshi
    name: S Sakshi
    username: ~S_Sakshi1
  - emails: '****@umd.edu'
    first_name: Utkarsh
    google_scholar_id: https://scholar.google.co.in/citations?user=RLjKaTwAAAAJ&hl=en
    homepage: https://utkarsh4430.github.io
    last_name: Tyagi
    name: Utkarsh Tyagi
    username: ~Utkarsh_Tyagi1
  - dblp_id: https://dblp.org/pers/hd/m/Manocha:Dinesh
    emails: '****@umd.edu'
    first_name: Dinesh
    google_scholar_id: https://scholar.google.com/citations?user=X08l_4IAAAAJ
    homepage: https://www.cs.umd.edu/people/dmanocha
    institution: University of Maryland, College Park
    last_name: Manocha
    name: Dinesh Manocha
    orcid: https://orcid.org/0000-0001-7047-9801
    username: ~Dinesh_Manocha3
  decision: toMainConference
  end_page: 5586
  file: 544.pdf
  id: 544
  num_pages: 9
  openreview_id: 0UaL02e4Rq
  pdf_file: 433300898baa9498cd97b05902d090f972a15819.pdf
  start_page: 5578
  title: Do Vision-Language Models Understand Compound Nouns?
- abstract: A deep understanding of sports, a field rich in strategic and dynamic
    content, is crucial for advancing Natural Language Processing (NLP). This holds
    particular significance in the context of evaluating and advancing Large Language
    Models (LLMs), given the existing gap in specialized benchmarks. To bridge this
    gap, we introduce SportQA, a novel benchmark specifically designed for evaluating
    LLMs in the context of sports understanding. SportQA encompasses over 70,000 multiple-choice
    questions across three distinct difficulty levels, each targeting different aspects
    of sports knowledge from basic historical facts to intricate, scenario-based reasoning
    tasks. We conducted a thorough evaluation of prevalent LLMs, mainly utilizing
    few-shot learning paradigms supplemented by chain-of-thought (CoT) prompting.
    Our results reveal that while LLMs exhibit competent performance in basic sports
    knowledge, they struggle with more complex, scenario-based sports reasoning, lagging
    behind human expertise. The introduction of SportQA marks a significant step forward
    in NLP, offering a tool for assessing and enhancing sports understanding in LLMs.
    The dataset is available at https://github.com/haotianxia/SportQA
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Question Answering
  authors:
  - emails: '****@gmail.com'
    first_name: Haotian
    homepage: https://www.linkedin.com/in/haotianxia/
    institution: University of California, Irvine
    last_name: Xia
    name: Haotian Xia
    username: ~Haotian_Xia1
  - emails: '****@uci.edu'
    first_name: Zhengbang
    homepage: https://www.linkedin.com/in/zhengbang-yang/
    last_name: Yang
    name: Zhengbang Yang
    username: ~Zhengbang_Yang1
  - emails: '****@ucsb.edu'
    first_name: Yuqing
    google_scholar_id: https://scholar.google.com/citations?user=DHImZjIAAAAJ&hl=en&authuser=2
    homepage: https://yuqingwangcs.github.io/
    institution: Stanford University
    last_name: Wang
    name: Yuqing Wang
    semantic_scholar_id: https://www.semanticscholar.org/author/Yuqing-Wang/2108035197
    username: ~Yuqing_Wang5
  - emails: '****@cs.ucsb.edu'
    first_name: Rhys
    last_name: Tracy
    name: Rhys Tracy
    username: ~Rhys_Tracy1
  - dblp_id: https://dblp.org/pid/42/2862-1
    emails: '****@meta.com'
    first_name: Yun
    google_scholar_id: https://scholar.google.com/citations?user=30s9RtsAAAAJ&hl=en
    homepage: https://yunzhaocs.github.io/
    last_name: Zhao
    name: Yun Zhao
    orcid: https://orcid.org/0000-0002-5544-8983
    semantic_scholar_id: https://www.semanticscholar.org/author/Yun-Zhao/46317573
    username: ~Yun_Zhao1
  - emails: '****@mail.bnu.edu.cn'
    first_name: Dongdong
    institution: Beijing Normal University
    last_name: Huang
    name: Dongdong Huang
    username: ~Dongdong_Huang1
  - emails: '****@uci.edu'
    first_name: Zezhi
    homepage: https://www.linkedin.com/in/zezhi-chen-283960250/
    last_name: Chen
    name: Zezhi Chen
    username: ~Zezhi_Chen1
  - emails: '****@bnu.edu.cn'
    first_name: Yan
    homepage: http://www.tyx.bnu.edu.cn/szdw/jsdw/qljys/c2a8df9c5990425f8362b4e0a7c3075a.htm
    institution: Beijing Normal University
    last_name: Zhu
    name: Yan Zhu
    username: ~Yan_Zhu13
  - emails: '****@cs.ucsb.edu'
    first_name: Yuan-fang
    homepage: https://sites cs.ucsb.edu/~yfwang
    last_name: Wang
    name: Yuan-fang Wang
    username: ~Yuan-fang_Wang1
  - emails: '****@uci.edu'
    first_name: Weining
    homepage: https://faculty.sites.uci.edu/weinings/
    institution: University of California, Irvine
    last_name: Shen
    name: Weining Shen
    username: ~Weining_Shen1
  decision: toMainConference
  end_page: 5607
  file: 546.pdf
  id: 546
  num_pages: 21
  openreview_id: QGmiiw1xwJ
  pdf_file: 3e014e152134b471943a83fff5eb7bed1a0574dd.pdf
  start_page: 5587
  title: 'SportQA: A Benchmark for Sports Understanding in Large Language Models'
- abstract: In this work, we measure the impact of affixal negation on modern English
    large language models (LLMs). In affixal negation, the negated meaning is expressed
    through a negative morpheme, which is potentially challenging for LLMs as their
    tokenizers are often not morphologically plausible. We conduct extensive experiments
    using LLMs with different subword tokenization methods, which lead to several
    insights on the interaction between tokenization performance and negation sensitivity.
    Despite some interesting mismatches between tokenization accuracy and negation
    detection performance, we show that models can, on the whole, reliably recognize
    the meaning of affixal negation.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Phonology, Morphology and Word Segmentation
  authors:
  - dblp_id: https://dblp.org/pid/289/6400
    emails: '****@student.unimelb.edu.au'
    first_name: Thinh
    google_scholar_id: https://scholar.google.com/citations?user=EWUVMr4AAAAJ&hl=en
    institution: University of Melbourne
    last_name: Truong
    middle_name: Hung
    name: Thinh Hung Truong
    semantic_scholar_id: https://www.semanticscholar.org/author/T.-Truong/153574160
    username: ~Thinh_Hung_Truong1
  - dblp_id: https://dblp.org/pid/154/8271
    emails: '****@student.unimelb.edu.au'
    first_name: Yulia
    homepage: https://aimedtech.org.au/people/Yulia%20Otmakhova.html
    institution: The University of Melbourne
    last_name: Otmakhova
    name: Yulia Otmakhova
    semantic_scholar_id: https://www.semanticscholar.org/author/Yulia-Otmakhova/1881453937
    username: ~Yulia_Otmakhova1
  - dblp_id: https://dblp.org/pid/07/6465
    emails: '****@rmit.edu.au'
    first_name: Karin
    google_scholar_id: https://scholar.google.com/citations?user=dUxHnbcAAAAJ&hl=en
    institution: Royal Melbourne Institute of Technology
    last_name: Verspoor
    name: Karin Verspoor
    orcid: https://orcid.org/0000-0002-8661-1544
    semantic_scholar_id: https://www.semanticscholar.org/author/Karin-M.-Verspoor/144765178
    username: ~Karin_Verspoor1
  - dblp_id: https://dblp.org/pid/66/4613
    emails: '****@google.com'
    first_name: Trevor
    google_scholar_id: https://scholar.google.com.au/citations?user=FCom398AAAAJ&hl=en
    homepage: https://people.eng.unimelb.edu.au/tcohn/
    institution: Google and The University of Melbourne
    last_name: Cohn
    name: Trevor Cohn
    username: ~Trevor_Cohn1
  - dblp_id: https://dblp.org/pid/65/4863
    emails: '****@ldwin.net'
    first_name: Timothy
    google_scholar_id: https://scholar.google.com/citations?user=wjBD1dkAAAAJ&hl=en
    institution: Mohamed bin Zayed University of Artificial Intelligence and The University
      of Melbourne
    last_name: Baldwin
    name: Timothy Baldwin
    username: ~Timothy_Baldwin1
  decision: toMainConference
  end_page: 5621
  file: 547.pdf
  id: 547
  num_pages: 14
  openreview_id: uMIHZf3CPf
  pdf_file: e3587c07679b5d5115779239a351e4898d8f020c.pdf
  start_page: 5608
  title: 'Revisiting subword tokenization: A case study on affixal negation in large
    language models'
- abstract: Large language models have become valuable tools for data augmentation
    in scenarios with limited data availability, as they can generate synthetic data
    resembling real-world data. However, their generative performance depends on the
    quality of the prompt used to instruct the model. Prompt engineering that relies
    on hand-crafted strategies or requires domain experts to adjust the prompt often
    yields suboptimal results. In this paper we present SAPE, a Spanish Adaptive Prompt
    Engineering method utilizing genetic algorithms for prompt generation and selection.
    Our evaluation of SAPE focuses on a generative task that involves the creation
    of Spanish therapy transcripts, a type of data that is challenging to collect
    due to the fact that it typically includes  protected health information. Through
    human evaluations conducted by mental health professionals, our results show that
    SAPE produces Spanish counselling transcripts that more closely resemble authentic
    therapy transcripts compared to other prompt engineering techniques that are based
    on Reflexion and Chain-of-Thought.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: 'Special Theme: Languages of Latin America'
  authors:
  - emails: '****@student.unimelb.edu.au'
    first_name: Daniel
    homepage: https://cis.unimelb.edu.au/people/students/daniel-cabrera-lozoya
    last_name: Lozoya
    middle_name: Cabrera
    name: Daniel Cabrera Lozoya
    username: ~Daniel_Cabrera_Lozoya2
  - emails: '****@student.unimelb.edu.au'
    first_name: Alejandro
    institution: University of Melbourne
    last_name: Berazaluce
    middle_name: Casar
    name: Alejandro Casar Berazaluce
    orcid: https://orcid.org/0000-0002-0091-7471
    username: ~Alejandro_Casar_Berazaluce2
  - emails: '****@gmail.com'
    first_name: Juan
    last_name: Perches
    middle_name: Alberto Barajas
    name: Juan Alberto Barajas Perches
    username: ~Juan_Alberto_Barajas_Perches1
  - emails: '****@gmail.com'
    first_name: Eloy
    last_name: "L\xFAa"
    middle_name: "Hern\xE1ndez"
    name: "Eloy Hern\xE1ndez L\xFAa"
    username: "~Eloy_Hern\xE1ndez_L\xFAa1"
  - dblp_id: https://dblp.org/pid/34/7070
    emails: '****@utah.edu'
    first_name: Mike
    last_name: Conway
    name: Mike Conway
    username: ~Mike_Conway1
  - emails: '****@unimelb.edu.au'
    first_name: Simon
    google_scholar_id: https://scholar.google.com.au/citations?user=hXEZnsUAAAAJ
    homepage: https://people.eng.unimelb.edu.au/dalfonso/
    institution: The University of Melbourne
    last_name: D'Alfonso
    name: Simon D'Alfonso
    orcid: https://orcid.org/0000-0001-7407-8730
    username: ~Simon_D'Alfonso1
  decision: toMainConference
  end_page: 5639
  file: 548.pdf
  id: 548
  num_pages: 18
  openreview_id: F8zRioWhXc
  pdf_file: 3c2d2a933ce4b728206d1a2350728fc022f3640d.pdf
  start_page: 5622
  title: Generating Mental Health Transcripts with SAPE (Spanish Adaptive Prompt Engineering)
- abstract: 'We train models to answer the question, Where are you from? and show
    how such models can be repurposed for language identification (LID). To our knowledge,
    this paper is the first to introduce data sources, methods and models to tackle
    the task of geolocation of speech at a global scale, and the first to explore
    using geolocation as a proxy-task for LID. Specifically, we explore whether radio
    broadcasts with known origin can be used to train regression and classification-based
    models for geolocating speech. We build models on top of self-supervised pretrained
    models, using attention pooling to qualitatively verify that the model geolocates
    the speech itself, and not other channel artifacts.

    The best geolocation models localize speaker origin to around 650km. We confirm
    the value of speech geolocation as a proxy task by using speech geolocation models
    for zero-shot LID. Finally, we show that fine-tuning geolocation models for LID
    outperforms fine-tuning pretrained Wav2Vec2.0 models, and achieves state-of-the-art
    performance on the FLEURS benchmark.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Speech recognition, text-to-speech and spoken language understanding
  authors:
  - emails: '****@gmail.com'
    first_name: Patrick
    homepage: https://pfoley2006.github.io/
    last_name: Foley
    name: Patrick Foley
    username: ~Patrick_Foley1
  - dblp_id: https://dblp.org/pid/166/6662
    emails: '****@jhu.edu'
    first_name: Matthew
    last_name: Wiesner
    name: Matthew Wiesner
    username: ~Matthew_Wiesner1
  - emails: '****@gmail.com'
    first_name: Bismarck
    google_scholar_id: https://scholar.google.com/citations?user=qILWGm4AAAAJ&hl=en
    institution: Department of Computer Science, Whiting School of Engineering
    last_name: Odoom
    middle_name: Bamfo
    name: Bismarck Bamfo Odoom
    username: ~Bismarck_Bamfo_Odoom1
  - dblp_id: https://dblp.org/pid/23/4175
    emails: '****@gmail.com'
    first_name: Leibny Paola
    homepage: https://www.clsp.jhu.edu/faculty/paola-garcia/
    last_name: Garcia Perera
    name: Leibny Paola Garcia Perera
    orcid: https://orcid.org/0000-0002-7449-5726
    semantic_scholar_id: https://www.semanticscholar.org/author/Leibny-Paola-Garc%C3%ADa-Perera/1380250565
    username: ~Leibny_Paola_Garcia_Perera1
  - dblp_id: https://dblp.org/pid/143/9465
    emails: '****@jhu.edu'
    first_name: Kenton
    google_scholar_id: https://scholar.google.com/citations?user=JuP-xF8AAAAJ&hl=en&oi=ao
    homepage: http://www.kentonmurray.com
    institution: Johns Hopkins University
    last_name: Murray
    name: Kenton Murray
    orcid: https://orcid.org/0000-0002-5628-1003
    semantic_scholar_id: https://www.semanticscholar.org/author/Kenton-Murray/38730896
    username: ~Kenton_Murray1
  - dblp_id: https://dblp.org/pid/84/4538.html
    emails: '****@jhu.edu'
    first_name: Philipp
    google_scholar_id: https://scholar.google.com/citations?user=OsIZgIYAAAAJ&hl=en
    homepage: http://www.cs.jhu.edu/~phi/
    institution: Johns Hopkins University
    last_name: Koehn
    name: Philipp Koehn
    orcid: https://orcid.org/0000-0003-1565-064X
    semantic_scholar_id: https://www.semanticscholar.org/author/Philipp-Koehn/1755162
    username: ~Philipp_Koehn2
  decision: toMainConference
  end_page: 5652
  file: 552.pdf
  id: 552
  num_pages: 13
  openreview_id: 41SyYwedpH
  pdf_file: b9529f5cf65c2aaea24986ae0fe98740a3357add.pdf
  start_page: 5640
  title: Where are you from? Geolocating Speech and Applications to Language Identification
- abstract: The self-improving ability of large language models (LLMs), enabled by
    prompting them to analyze and revise their own outputs, has garnered significant
    interest in recent research. However, this ability has been shown to be absent
    and difficult to learn for smaller models, thus widening the performance gap between
    state-of-the-art LLMs and more cost-effective and faster ones. To reduce this
    gap, we introduce TriPosT, a training algorithm that endows smaller models with
    such self-improvement ability, and show that our approach can improve LLaMA-7B's
    performance on math and reasoning tasks by up to 7.13%. In contrast to prior work,
    we achieve this by using the smaller model to interact with LLMs to collect feedback
    and improvements on *its own generations*. We then replay this experience to train
    the small model. Our experiments on four math and reasoning datasets show that
    the interactive experience of learning from and correcting its *own* mistakes
    is crucial for small models to improve their performance.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Question Answering
  authors:
  - emails: '****@columbia.edu'
    first_name: Xiao
    google_scholar_id: https://scholar.google.com/citations?user=QblBy88AAAAJ&hl=en
    last_name: Yu
    name: Xiao Yu
    semantic_scholar_id: https://www.semanticscholar.org/author/Xiao-Yu/2193279092
    username: ~Xiao_Yu4
  - dblp_id: https://dblp.org/pid/144/2759
    emails: '****@gmail.com'
    first_name: Baolin
    google_scholar_id: https://scholar.google.com/citations?user=u1CNjgwAAAAJ&hl=zh-CN
    last_name: Peng
    name: Baolin Peng
    username: ~Baolin_Peng2
  - dblp_id: https://dblp.org/pid/05/3289
    emails: '****@acm.org'
    first_name: Michel
    google_scholar_id: https://scholar.google.com/citations?user=rs1M7CAAAAAJ&hl=en
    homepage: http://research.microsoft.com/~mgalley
    institution: Microsoft
    last_name: Galley
    name: Michel Galley
    orcid: https://orcid.org/0000-0002-3310-1831
    semantic_scholar_id: https://www.semanticscholar.org/author/Michel-Galley/1947267
    username: ~Michel_Galley1
  - dblp_id: https://dblp.org/pid/92/5339
    emails: '****@microsoft.com'
    first_name: Jianfeng
    homepage: https://www.microsoft.com/en-us/research/people/jfgao/
    institution: Microsoft Research
    last_name: Gao
    name: Jianfeng Gao
    username: ~Jianfeng_Gao1
  - dblp_id: https://dblp.org/pid/83/3205
    emails: '****@columbia.edu'
    first_name: Zhou
    google_scholar_id: https://scholar.google.com.tw/citations?user=jee2Dy0AAAAJ
    homepage: http://www.cs.columbia.edu/~zhouyu/
    institution: Columbia University
    last_name: Yu
    name: Zhou Yu
    username: ~Zhou_Yu1
  decision: toMainConference
  end_page: 5675
  file: 553.pdf
  id: 553
  num_pages: 23
  openreview_id: wDb75E63Ag
  pdf_file: ce82275d500392fc5e2a92a643744c6739c6ee00.pdf
  start_page: 5653
  title: Teaching Language Models to Self-Improve through Interactive Demonstrations
- abstract: Development of multimodal interactive systems is hindered by the lack
    of rich, multimodal (text, images) conversational data, which is needed in large
    quantities for LLMs. Previous approaches augment textual dialogues with retrieved
    images, posing privacy, diversity, and quality constraints. In this work, we introduce
    \textbf{M}ultimodal \textbf{A}ugmented \textbf{G}enerative \textbf{I}mages \textbf{D}ialogues
    (MAGID), a framework to augment text-only dialogues with diverse and high-quality
    images \footnote{https://e77p.short.gy/MAGID}. Subsequently, a diffusion model
    is applied to craft corresponding images, ensuring alignment with the identified
    text. Finally, MAGID incorporates an innovative feedback loop between an image
    description generation module (textual LLM) and image quality modules (addressing
    aesthetics, image-text matching, and safety), that work in tandem to generate
    high-quality and multi-modal dialogues. We compare MAGID to other SOTA baselines
    on three dialogue datasets, using automated and human evaluation. Our results
    show that MAGID is comparable to or better than baselines, with significant improvements
    in human evaluation, especially against retrieval baselines where the image database
    is small.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Dialogue and Interactive Systems
  authors:
  - dblp_id: https://dblp.org/pid/218/7244
    emails: '****@uwaterloo.ca'
    first_name: Hossein
    last_name: Aboutalebi
    name: Hossein Aboutalebi
    username: ~Hossein_Aboutalebi1
  - dblp_id: https://dblp.org/pid/204/3381
    emails: '****@gmail.com'
    first_name: Hwanjun
    google_scholar_id: https://scholar.google.com/citations?user=Ijzuc-8AAAAJ&hl=en
    homepage: https://songhwanjun.github.io/
    institution: AWS AI Labs
    last_name: Song
    name: Hwanjun Song
    username: ~Hwanjun_Song2
  - dblp_id: https://dblp.org/pid/56/10813
    emails: '****@gmail.com'
    first_name: Yusheng
    google_scholar_id: https://scholar.google.com/citations?user=Hs923REAAAAJ&hl=en
    institution: Amazon
    last_name: Xie
    name: Yusheng Xie
    semantic_scholar_id: https://www.semanticscholar.org/author/Yusheng-Xie/3052897
    username: ~Yusheng_Xie1
  - dblp_id: https://dblp.org/pid/238/0361
    emails: '****@gmail.com'
    first_name: Arshit
    google_scholar_id: https://scholar.google.com/citations?user=qDPYswEAAAAJ&hl=en
    institution: Amazon
    last_name: Gupta
    name: Arshit Gupta
    semantic_scholar_id: https://www.semanticscholar.org/author/Arshit-Gupta/144877669
    username: ~Arshit_Gupta1
  - emails: '****@amazon.com'
    first_name: Lijia
    google_scholar_id: https://scholar.google.com/citations?user=fRLrTw4AAAAJ&hl=en
    institution: Amazon
    last_name: Sun
    name: Lijia Sun
    username: ~Lijia_Sun1
  - emails: '****@gmail.com'
    first_name: Hang
    google_scholar_id: https://scholar.google.com/citations?user=UxOvKVUAAAAJ&hl=en
    institution: Amazon
    last_name: Su
    name: Hang Su
    username: ~Hang_Su7
  - dblp_id: https://dblp.org/pid/205/8962
    emails: '****@gmail.com'
    first_name: Igor
    google_scholar_id: https://scholar.google.com/citations?user=TVs0lP8AAAAJ
    homepage: https://shalyminov.com
    institution: Amazon
    last_name: Shalyminov
    name: Igor Shalyminov
    orcid: https://orcid.org/0000-0001-9664-1774
    semantic_scholar_id: https://www.semanticscholar.org/author/Igor-Shalyminov/24879056
    username: ~Igor_Shalyminov1
  - dblp_id: https://dblp.org/pid/36/8968-2.html
    emails: '****@amazon.com'
    first_name: Nikolaos
    google_scholar_id: https://scholar.google.ch/citations?user=daiFj_cAAAAJ&hl=en
    homepage: http://nik0spapp.github.io/
    institution: AWS AI Labs
    last_name: Pappas
    name: Nikolaos Pappas
    orcid: https://orcid.org/0000-0002-2004-8111
    username: ~Nikolaos_Pappas1
  - emails: '****@amazon.co.uk'
    first_name: Siffi
    homepage: https://scholar.google.com/citations?user=zaXmGr8AAAAJ&hl=en
    last_name: Singh
    name: Siffi Singh
    username: ~Siffi_Singh1
  - dblp_id: https://dblp.org/pid/03/8053
    emails: '****@gmail.com'
    first_name: Saab
    google_scholar_id: https://scholar.google.de/citations?user=1tCbwIQAAAAJ&hl=en
    institution: Amazon
    last_name: Mansour
    name: Saab Mansour
    semantic_scholar_id: https://www.semanticscholar.org/author/Saab-Mansour/39674628
    username: ~Saab_Mansour1
  decision: toMainConference
  end_page: 5693
  file: 554.pdf
  id: 554
  num_pages: 18
  openreview_id: V36zg8Nr0C
  pdf_file: dd81c67ad0b38a231883b7223be7d34ec1f3fdc0.pdf
  start_page: 5676
  title: 'MAGID: An Automated Pipeline for Generating Synthetic Multi-modal Datasets'
- abstract: Generative linguistic steganography attempts to hide secret messages into
    covertext. Previous studies have generally focused on the statistical differences
    between the covertext and stegotext, however, ill-formed stegotext can readily
    be identified by humans. In this paper, we propose a novel zero-shot approach
    based on in-context learning for linguistic steganography to achieve better perceptual
    and statistical imperceptibility. We also design several new metrics and reproducible
    language evaluations to measure the imperceptibility of the stegotext. Our experimental
    results indicate that our method produces $1.926\times$ more innocent and intelligible
    stegotext than any other method.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - dblp_id: https://dblp.org/pid/65/3313
    emails: '****@gmail.com'
    first_name: Ke
    institution: Tsinghua University, Tsinghua University
    last_name: Lin
    name: Ke Lin
    orcid: https://orcid.org/0009-0002-5376-7881
    username: ~Ke_Lin4
  - emails: '****@outlook.com'
    first_name: Yiyang
    last_name: Luo
    name: Yiyang Luo
    username: ~Yiyang_Luo1
  - emails: '****@gmail.com'
    first_name: Zijian
    last_name: Zhang
    name: Zijian Zhang
    orcid: https://orcid.org/0009-0004-3779-9693
    username: ~Zijian_Zhang9
  - emails: '****@tsinghua.edu.cn'
    first_name: Luo
    homepage: https://www.thss.tsinghua.edu.cn/en/faculty/pingluo.htm
    last_name: Ping
    name: Luo ping
    username: ~Luo_ping3
  decision: toMainConference
  end_page: 5708
  file: 556.pdf
  id: 556
  num_pages: 15
  openreview_id: NKplPLSOyG
  pdf_file: 8f3744f84bc5d4c4378ee194a49e9f280bfacfec.pdf
  start_page: 5694
  title: Zero-shot Generative Linguistic Steganography
- abstract: We evaluated GPT-4 in a public online Turing test. The best-performing
    GPT-4 prompt passed in 49.7% of games, outperforming ELIZA (22%) and GPT-3.5 (20%),
    but falling short of the baseline set by human participants (66%). Participants'
    decisions were based mainly on linguistic style (35%) and socioemotional traits
    (27%), supporting the idea that intelligence, narrowly conceived, is not sufficient
    to pass the Turing test. Participant knowledge about LLMs and number of games
    played positively correlated with accuracy in detecting AI, suggesting learning
    and practice as possible strategies to mitigate deception. Despite known limitations
    as a test of intelligence, we argue that the Turing test continues to be relevant
    as an assessment of naturalistic communication and deception. AI models with the
    ability to masquerade as humans could have widespread societal consequences, and
    we analyse the effectiveness of different strategies and criteria for judging
    humanlikeness.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Dialogue and Interactive Systems
  authors:
  - emails: '****@ucsd.edu'
    first_name: Cameron
    google_scholar_id: https://scholar.google.com/citations?user=mhU_tUgAAAAJ&hl=en
    homepage: https://camrobjones.com
    institution: University of California, San Diego
    last_name: Jones
    middle_name: Robert
    name: Cameron Robert Jones
    orcid: https://orcid.org/0000-0002-6609-8966
    username: ~Cameron_Robert_Jones1
  - dblp_id: https://dblp.org/pid/129/5733
    emails: '****@ucsd.edu'
    first_name: Ben
    google_scholar_id: https://scholar.google.com/citations?user=pJ8u7AQAAAAJ&hl=en
    homepage: https://cogsci.ucsd.edu/~bkbergen/
    last_name: Bergen
    name: Ben Bergen
    orcid: https://orcid.org/0000-0002-9395-9151
    username: ~Ben_Bergen1
  decision: toMainConference
  end_page: 5736
  file: 557.pdf
  id: 557
  num_pages: 28
  openreview_id: b3XZm2Ufsj
  pdf_file: 749bd4410330c114c45a6f20b684f2d3d0c3b068.pdf
  start_page: 5709
  title: Does GPT-4 pass the Turing test?
- abstract: 'Opinion summarization is automatically generating summaries from a variety
    of subjective information, such as product reviews or political opinions. The
    challenge of opinions summarization lies in presenting divergent or even conflicting
    opinions. We conduct an analysis of previous summarization models, which reveals
    their inclination to amplify the polarity bias, emphasizing the majority opinions
    while ignoring the minority opinions. To address this issue and make the summarizer
    express both sides of opinions, we introduce the concept of polarity calibration,
    which aims to align the polarity of output summary with that of input text. Specifically,
    we develop a reinforcement training approach for polarity calibration. This approach
    feeds the polarity distance between output summary and input text as reward into
    the summarizer, and also balance polarity calibration with content preservation
    and language naturality. We evaluate our Polarity Calibration model (PoCa) on
    two types of opinions summarization tasks: summarizing product reviews and political
    opinions articles. Automatic and human evaluation demonstrate that our approach
    can mitigate the polarity mismatch between output summary and input text, as well
    as maintain the content semantic and language quality.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Summarization
  authors:
  - dblp_id: https://dblp.org/pid/283/5401-1.html
    emails: '****@tamu.edu'
    first_name: Yuanyuan
    google_scholar_id: https://scholar.google.com/citations?user=gJENFUQAAAAJ&hl=en
    institution: Texas A&M University - College Station
    last_name: Lei
    name: Yuanyuan Lei
    orcid: https://orcid.org/0000-0002-9753-8071
    username: ~Yuanyuan_Lei1
  - emails: '****@gmail.com'
    first_name: Kaiqiang
    google_scholar_id: https://scholar.google.com/citations?user=PHoJwakAAAAJ&hl=en&oi=ao
    homepage: http://i2u.world/kqsong/
    institution: Tencent AI Lab
    last_name: Song
    name: Kaiqiang Song
    semantic_scholar_id: https://www.semanticscholar.org/author/Kaiqiang-Song/50982080
    username: ~Kaiqiang_Song2
  - dblp_id: https://dblp.org/pid/75/1848
    emails: '****@global.tencent.com'
    first_name: Sangwoo
    google_scholar_id: https://scholar.google.com/citations?user=T8mGzuoAAAAJ&hl=en
    homepage: https://sangwoo3.github.io
    institution: Tencent AI Lab
    last_name: Cho
    name: Sangwoo Cho
    orcid: https://orcid.org/0000-0002-4875-2565
    semantic_scholar_id: https://www.semanticscholar.org/author/Sangwoo-Cho/2173531
    username: ~Sangwoo_Cho1
  - emails: '****@gmail.com'
    first_name: Xiaoyang
    google_scholar_id: https://scholar.google.com/citations?user=EeppWmkAAAAJ&hl
    institution: Tencent AI Lab
    last_name: Wang
    name: Xiaoyang Wang
    username: ~Xiaoyang_Wang1
  - dblp_id: https://dblp.org/pid/42/4811.html
    emails: '****@cse.tamu.edu'
    first_name: Ruihong
    google_scholar_id: https://scholar.google.com.tw/citations?user=NU2aHWUAAAAJ
    homepage: https://people.engr.tamu.edu/huangrh/index.html
    institution: Texas A&M University
    last_name: Huang
    name: Ruihong Huang
    semantic_scholar_id: https://www.semanticscholar.org/author/Ruihong-Huang/40372969
    username: ~Ruihong_Huang1
  - dblp_id: https://dblp.org/pid/71/4598-1
    emails: '****@ieee.org'
    first_name: Dong
    google_scholar_id: https://scholar.google.com/citations?user=tMY31_gAAAAJ
    homepage: https://sites.google.com/view/dongyu888/
    institution: Tencent AI Lab
    last_name: Yu
    name: Dong Yu
    semantic_scholar_id: https://www.semanticscholar.org/author/Dong-Yu/144580027
    username: ~Dong_Yu2
  decision: toMainConference
  end_page: 5750
  file: 559.pdf
  id: 559
  num_pages: 14
  openreview_id: urG3pcKZ9U
  pdf_file: 1130f13350b388d3b2435a1a78aa63d807d262d8.pdf
  start_page: 5737
  title: Polarity Calibration for Opinion Summarization
- abstract: 'Media outlets are becoming more partisan and polarized nowadays. In this
    paper, we identify media bias at the sentence level, and pinpoint bias sentences
    that intend to sway readers'' opinions. As bias sentences are often expressed
    in a neutral and factual way, considering broader context outside a sentence can
    help reveal the bias. In particular, we observe that events in a bias sentence
    need to be understood in associations with other events in the document. Therefore,
    we propose to construct an event relation graph to explicitly reason about event-event
    relations for sentence-level bias identification. The designed event relation
    graph consists of events as nodes and four common types of event relations: coreference,
    temporal, causal, and subevent relations. Then, we incorporate event relation
    graph for bias sentences identification in two steps: an event-aware language
    model is built to inject the events and event relations knowledge into the basic
    language model via soft labels; further, a relation-aware graph attention network
    is designed to update sentence embedding with events and event relations information
    based on hard labels. Experiments on two benchmark datasets demonstrate that our
    approach with the aid of event relation graph improves both precision and recall
    of bias sentence identification.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Ethics, Bias, and Fairness
  authors:
  - dblp_id: https://dblp.org/pid/283/5401-1.html
    emails: '****@tamu.edu'
    first_name: Yuanyuan
    google_scholar_id: https://scholar.google.com/citations?user=gJENFUQAAAAJ&hl=en
    institution: Texas A&M University - College Station
    last_name: Lei
    name: Yuanyuan Lei
    orcid: https://orcid.org/0000-0002-9753-8071
    username: ~Yuanyuan_Lei1
  - dblp_id: https://dblp.org/pid/42/4811.html
    emails: '****@cse.tamu.edu'
    first_name: Ruihong
    google_scholar_id: https://scholar.google.com.tw/citations?user=NU2aHWUAAAAJ
    homepage: https://people.engr.tamu.edu/huangrh/index.html
    institution: Texas A&M University
    last_name: Huang
    name: Ruihong Huang
    semantic_scholar_id: https://www.semanticscholar.org/author/Ruihong-Huang/40372969
    username: ~Ruihong_Huang1
  decision: toMainConference
  end_page: 5764
  file: 560.pdf
  id: 560
  num_pages: 14
  openreview_id: f0KNjW17j8
  pdf_file: 420c8029ec9327f0f746d2d77261f03c8240d75d.pdf
  start_page: 5751
  title: Sentence-level Media Bias Analysis with Event Relation Graph
- abstract: Most previous research on moral frames has focused on social media short
    texts, little work has explored moral sentiment within news articles. In news
    articles, authors often express their opinions or political stance through moral
    judgment towards events, specifically whether the event is right or wrong according
    to social moral rules. This paper initiates a new task to understand moral opinions
    towards events in news articles. We have created a new dataset, EMONA, and annotated
    event-level moral opinions in news articles. This dataset consists of 400 news
    articles containing over 10k sentences and 45k events, among which 9,613 events
    received moral foundation labels. Extracting event morality is a challenging task,
    as moral judgment towards events can be very implicit. Baseline models were built
    for event moral identification and classification. In addition, we also conduct
    extrinsic evaluations to integrate event-level moral opinions into three downstream
    tasks. The statistical analysis and experiments show that moral opinions of events
    can serve as informative features for identifying ideological bias or subjective
    events.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Computational Social Science and Cultural Analytics
  authors:
  - dblp_id: https://dblp.org/pid/283/5401-1.html
    emails: '****@tamu.edu'
    first_name: Yuanyuan
    google_scholar_id: https://scholar.google.com/citations?user=gJENFUQAAAAJ&hl=en
    institution: Texas A&M University - College Station
    last_name: Lei
    name: Yuanyuan Lei
    orcid: https://orcid.org/0000-0002-9753-8071
    username: ~Yuanyuan_Lei1
  - emails: '****@tamu.edu'
    first_name: Md Messal Monem
    google_scholar_id: https://scholar.google.com/citations?user=TP9RQXYAAAAJ&hl=en
    institution: Texas A&M University - College Station
    last_name: Miah
    name: Md Messal Monem Miah
    username: ~Md_Messal_Monem_Miah1
  - dblp_id: https://dblp.org/pid/273/6982
    emails: '****@gmail.com'
    first_name: Ayesha
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=0LDPOBoAAAAJ
    institution: Texas A&M University - College Station
    last_name: Qamar
    name: Ayesha Qamar
    username: ~Ayesha_Qamar1
  - emails: '****@tamu.edu'
    first_name: Sai Ramana Reddy
    last_name: ''
    name: 'Sai Ramana Reddy '
    username: ~Sai_Ramana_Reddy1
  - emails: '****@tamu.edu'
    first_name: Jonathan
    institution: Texas A&M University - College Station
    last_name: Tong
    name: Jonathan Tong
    username: ~Jonathan_Tong1
  - emails: '****@tamu.edu'
    first_name: Haotian
    last_name: Xu
    name: Haotian Xu
    username: ~Haotian_Xu3
  - dblp_id: https://dblp.org/pid/42/4811.html
    emails: '****@cse.tamu.edu'
    first_name: Ruihong
    google_scholar_id: https://scholar.google.com.tw/citations?user=NU2aHWUAAAAJ
    homepage: https://people.engr.tamu.edu/huangrh/index.html
    institution: Texas A&M University
    last_name: Huang
    name: Ruihong Huang
    semantic_scholar_id: https://www.semanticscholar.org/author/Ruihong-Huang/40372969
    username: ~Ruihong_Huang1
  decision: toMainConference
  end_page: 5777
  file: 562.pdf
  id: 562
  num_pages: 13
  openreview_id: rVF7Yzmq0o
  pdf_file: bc22eee78a0812f7b7056fc910b67a44b36e43d1.pdf
  start_page: 5765
  title: 'EMONA: Event-level Moral Opinions in News Articles'
- abstract: 'Grapheme-to-phoneme conversion (G2P) is a critical component of the text-to-speech
    system (TTS), where polyphone disambiguation is the most crucial task. However,
    polyphone disambiguation datasets often suffer from the long-tail problem, and
    context learning for polyphonic characters commonly stems from a single dimension.
    In this paper, we propose a novel model DLM: a Decoupled Learning Model for long-tailed
    polyphone disambiguation in Mandarin. Firstly, DLM decouples representation and
    classification learnings. It can apply different data samplers for each stage
    to obtain an optimal training data distribution. This can mitigate the long-tail
    problem. Secondly, two improved attention mechanisms and a gradual conversion
    strategy are integrated into the DLM, which achieve transition learning of context
    from local to global. Finally, to evaluate the effectiveness of DLM, we construct
    a balanced polyphone disambiguation corpus via in-context learning. Experiments
    on the benchmark CPP dataset demonstrate that DLM achieves a boosted accuracy
    of 99.07%. Moreover, DLM improves the disambiguation performance of long-tailed
    polyphonic characters. For many long-tailed characters, DLM even achieves an accuracy
    of 100%.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Speech recognition, text-to-speech and spoken language understanding
  authors:
  - emails: '****@163.com'
    first_name: Beibei
    homepage: https://github.com/Becko-A
    last_name: Gao
    name: Beibei Gao
    username: ~Beibei_Gao1
  - dblp_id: https://dblp.org/pid/51/777
    emails: '****@163.com'
    first_name: Yangsen
    homepage: https://sim.bistu.edu.cn/zdsc/jsjl_detail/202003/t20200317_113843.html
    institution: Beijing Information Science and Technology University
    last_name: Zhang
    name: Yangsen Zhang
    semantic_scholar_id: https://www.semanticscholar.org/author/Yangsen-Zhang/2256594992
    username: ~Yangsen_Zhang1
  - dblp_id: https://dblp.org/pid/361/1711
    emails: '****@bistu.edu.cn'
    first_name: Ga
    homepage: https://sim.bistu.edu.cn/zdsc/jsjl_detail/202003/t20200331_115989.html
    institution: Beijing Information Science and Technology University
    last_name: Xiang
    name: Ga Xiang
    semantic_scholar_id: https://www.semanticscholar.org/author/Ga-Xiang/2228019777
    username: ~Ga_Xiang1
  - emails: '****@163.com'
    first_name: Yushan
    homepage: https://github.com/yushan-jiang
    last_name: Jiang
    name: Yushan Jiang
    username: ~Yushan_Jiang2
  decision: toMainConference
  end_page: 5788
  file: 563.pdf
  id: 563
  num_pages: 11
  openreview_id: ODEVRYaT2N
  pdf_file: 19f6b3d32f73c6621b32a05bc55e25ec48feb6c5.pdf
  start_page: 5778
  title: 'DLM: A Decoupled Learning Model for Long-tailed Polyphone Disambiguation
    in Mandarin'
- abstract: The versatility of Large Language Models (LLMs) on natural language understanding
    tasks has made them popular for research in social sciences. To properly understand
    the properties and innate personas of LLMs, researchers have performed studies
    that involve using prompts in the form of questions that ask LLMs about particular
    opinions. In this study, we take a cautionary step back and examine whether the
    current format of prompting LLMs elicits responses in a consistent and robust
    manner. We first construct a dataset that contains 693 questions encompassing
    39 different instruments of persona measurement on 115 persona axes. Additionally,
    we design a set of prompts containing minor variations and examine LLMs' capabilities
    to generate answers, as well as prompt variations to examine their consistency
    with respect to content-level variations such as switching the order of response
    options or negating the statement. Our experiments on 17 different LLMs reveal
    that even simple perturbations significantly downgrade a model's question-answering
    ability, and that most LLMs have low negation consistency. Our results suggest
    that the currently widespread practice of prompting is insufficient to accurately
    and reliably capture model perceptions, and we therefore discuss potential alternatives
    to improve these issues.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Computational Social Science and Cultural Analytics
  authors:
  - emails: '****@umich.edu'
    first_name: Bangzhao
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=3VdLBgoAAAAJ
    last_name: Shu
    name: Bangzhao Shu
    username: ~Bangzhao_Shu1
  - emails: '****@umich.edu'
    first_name: Lechen
    homepage: https://orange0629.github.io/
    last_name: Zhang
    name: Lechen Zhang
    username: ~Lechen_Zhang1
  - dblp_id: https://dblp.org/pid/257/3348
    emails: '****@umich.edu'
    first_name: Minje
    google_scholar_id: https://scholar.google.com/citations?user=crAyusoAAAAJ&hl=en
    homepage: https://minjechoi.github.io/
    institution: Georgia Institute of Technology
    last_name: Choi
    name: Minje Choi
    username: ~Minje_Choi1
  - emails: '****@gmail.com'
    first_name: Lavinia
    google_scholar_id: https://scholar.google.com/citations?user=_VzKqmgAAAAJ
    homepage: https://laviniakd.github.io/
    institution: University of Michigan - Ann Arbor
    last_name: Dunagan
    name: Lavinia Dunagan
    username: ~Lavinia_Dunagan1
  - dblp_id: https://dblp.org/pid/157/3603
    emails: '****@gmail.com'
    first_name: Lajanugen
    google_scholar_id: https://scholar.google.com/citations?user=dcv4kpIAAAAJ&hl=en
    homepage: https://sites.google.com/umich.edu/llajan/
    institution: LG AI Research
    last_name: Logeswaran
    name: Lajanugen Logeswaran
    username: ~Lajanugen_Logeswaran1
  - dblp_id: https://dblp.org/pid/132/1761
    emails: '****@uic.edu'
    first_name: Moontae
    google_scholar_id: https://scholar.google.com/citations?user=BMvYy9cAAAAJ&hl=en
    homepage: https://moontae.people.uic.edu
    institution: University of Illinois, Chicago
    last_name: Lee
    name: Moontae Lee
    username: ~Moontae_Lee1
  - dblp_id: https://dblp.org/pid/125/5045
    emails: '****@umich.edu'
    first_name: Dallas
    google_scholar_id: https://scholar.google.com/citations?user=qH-rJV8AAAAJ
    homepage: https://dallascard.github.io
    institution: University of Michigan - Ann Arbor
    last_name: Card
    name: Dallas Card
    orcid: https://orcid.org/0000-0001-5573-8836
    semantic_scholar_id: https://www.semanticscholar.org/author/D.-Card/35540755
    username: ~Dallas_Card1
  - dblp_id: https://dblp.uni-trier.de/pid/48/4613
    emails: '****@umich.edu'
    first_name: David
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=sGFFr5kAAAAJ
    homepage: http://jurgens.people.si.umich.edu
    institution: University of Michigan - Ann Arbor
    last_name: Jurgens
    name: David Jurgens
    orcid: https://orcid.org/0000-0002-2135-9878
    semantic_scholar_id: https://www.semanticscholar.org/author/David-Jurgens/3046220
    username: ~David_Jurgens1
  decision: toMainConference
  end_page: 5807
  file: 567.pdf
  id: 567
  num_pages: 19
  openreview_id: 0xwzQkJkLV
  pdf_file: aa3a56f023f03a434d42a33ca57bb315d79849e0.pdf
  start_page: 5789
  title: 'You don''t need a personality test to know these models are unreliable:
    Assessing the Reliability of Large Language Models on Psychometric Instruments'
- abstract: "The argument sufficiency assessment task aims to determine if the premises\
    \ of a given argument support its conclusion.\nTo tackle this task, existing works\
    \ often train a classifier on data annotated by humans. However, annotating data\
    \ is laborious, and annotations are often inconsistent due to subjective criteria.\
    \ Motivated by the definition of probability of sufficiency (PS) in the causal\
    \ literature, we proposeCASA, a zero-shot causality-driven argument sufficiency\
    \ assessment framework. \nPS measures how likely introducing the premise event\
    \ would lead to the conclusion when both the premise and conclusion events are\
    \ absent. To estimate this probability, we propose to use large language models\
    \ (LLMs) to generate contexts that are inconsistent with the premise and conclusion\
    \ and revise them by injecting the premise event.\nExperiments on two logical\
    \ fallacy detection datasets demonstrate that CASA accurately identifies insufficient\
    \ arguments. We further deploy CASA in a writing assistance application, and find\
    \ that suggestions generated by CASA enhance the sufficiency of student-written\
    \ arguments. Code and data are available at https://github.com/xxxiaol/CASA."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - dblp_id: https://dblp.org/pid/82/1364-32
    emails: '****@pku.edu.cn'
    first_name: Xiao
    google_scholar_id: https://scholar.google.com/citations?user=c3bdW2IAAAAJ&hl=zh-CN
    homepage: https://xxxiaol.github.io/
    institution: Peking University
    last_name: Liu
    name: Xiao Liu
    semantic_scholar_id: https://www.semanticscholar.org/author/Xiao-Liu/49543720
    username: ~Xiao_Liu19
  - dblp_id: https://dblp.org/pid/25/2643
    emails: '****@pku.edu.cn'
    first_name: Yansong
    google_scholar_id: https://scholar.google.com.tw/citations?user=67qAw_wAAAAJ
    homepage: https://sites.google.com/site/ysfeng/home
    institution: Peking University
    last_name: Feng
    name: Yansong Feng
    semantic_scholar_id: https://www.semanticscholar.org/author/Yansong-Feng/1717629
    username: ~Yansong_Feng1
  - dblp_id: https://dblp.org/pid/18/2428
    emails: '****@kwchang.net'
    first_name: Kai-Wei
    google_scholar_id: https://scholar.google.com/citations?user=fqDBtzYAAAAJ&hl=en
    homepage: http://kwchang.net
    institution: University of California, Los Angeles
    last_name: Chang
    name: Kai-Wei Chang
    semantic_scholar_id: https://www.semanticscholar.org/author/Kai-Wei-Chang/2782886
    username: ~Kai-Wei_Chang1
  decision: toMainConference
  end_page: 5828
  file: 568.pdf
  id: 568
  num_pages: 21
  openreview_id: VTpLu5alUD
  pdf_file: 5e748aa230b7587deff0b23f3c682149885e4aeb.pdf
  start_page: 5808
  title: 'CASA: Causality-driven Argument Sufficiency Assessment'
- abstract: 'We explore the creative problem-solving capabilities of modern LLMs in
    a novel constrained setting. To this end, we create MACGYVER, an automatically
    generated dataset consisting of over 1,600 real-world problems deliberately designed
    to trigger innovative usage of objects and necessitate out-of-the-box thinking.
    We then present our collection to both LLMs and humans to compare and contrast
    their problem-solving abilities. MACGYVER is challenging for both groups, but
    in unique and complementary ways. For instance, humans excel in tasks they are
    familiar with but struggle with domain-specific knowledge, leading to a higher
    variance. In contrast, LLMs, exposed to a variety of specialized knowledge, attempt
    broader problems but fail by proposing physically-infeasible actions. Finally,
    we provide a detailed error analysis of LLMs, and demonstrate the potential of
    enhancing their problem-solving ability with novel prompting techniques such as
    iterative step-wise reflection and divergent-convergent thinking.



    This work (1) introduces a fresh arena for intelligent agents focusing on intricate
    aspects of physical reasoning, planning, and unconventional thinking, which supplements
    the existing spectrum of machine intelligence; and (2) provides insight into the
    constrained problem-solving capabilities of both humans and AI.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - dblp_id: https://dblp.org/pid/222/7636
    emails: '****@ucla.edu'
    first_name: Yufei
    google_scholar_id: https://scholar.google.com/citations?user=3aipmfQAAAAJ&hl=en&authuser=1&oi=ao
    homepage: https://yufeitian.github.io/website/
    last_name: Tian
    name: Yufei Tian
    semantic_scholar_id: https://www.semanticscholar.org/author/Yufei-Tian/48391421
    username: ~Yufei_Tian1
  - dblp_id: https://dblp.org/pid/170/4795.html
    emails: '****@allenai.org'
    first_name: Abhilasha
    google_scholar_id: https://scholar.google.com/citations?user=6vLsKGsAAAAJ&hl=en
    homepage: https://www.cs.cmu.edu/~aravicha/
    institution: Allen Institute for Artificial Intelligence and School of Computer
      Science, Carnegie Mellon University
    last_name: Ravichander
    name: Abhilasha Ravichander
    semantic_scholar_id: https://www.semanticscholar.org/author/Abhilasha-Ravichander/3023068
    username: ~Abhilasha_Ravichander2
  - dblp_id: https://dblp.org/pid/184/3753
    emails: '****@ucsd.edu'
    first_name: Lianhui
    google_scholar_id: https://scholar.google.com/citations?user=smd19iIAAAAJ&hl=en&oi=ao
    homepage: https://sites.google.com/view/lianhuiqin/home
    institution: University of California, San Diego
    last_name: Qin
    name: Lianhui Qin
    username: ~Lianhui_Qin1
  - emails: '****@allenai.org'
    first_name: Ronan
    google_scholar_id: https://scholar.google.com/citations?user=8dXLDSsAAAAJ&hl=en
    homepage: https://rlebras.github.io/index.html
    last_name: Le Bras
    name: Ronan Le Bras
    semantic_scholar_id: https://www.semanticscholar.org/author/Ronan-Le-Bras/39227408
    username: ~Ronan_Le_Bras1
  - dblp_id: https://dblp.org/pid/271/7867
    emails: '****@princeton.edu'
    first_name: Raja
    google_scholar_id: https://scholar.google.com/citations?user=h-pwCMUAAAAJ&hl=en&oi=ao
    institution: Princeton University
    last_name: Marjieh
    name: Raja Marjieh
    username: ~Raja_Marjieh1
  - dblp_id: https://dblp.org/pid/117/4036
    emails: '****@cs.ucla.edu'
    first_name: Nanyun
    google_scholar_id: https://scholar.google.com/citations?user=XxRXvX0AAAAJ&hl=en
    homepage: http://vnpeng.net/
    institution: University of California, Los Angeles
    last_name: Peng
    name: Nanyun Peng
    semantic_scholar_id: https://www.semanticscholar.org/author/Nanyun-Peng/3157053
    username: ~Nanyun_Peng1
  - dblp_id: https://dblp.org/pid/89/579
    emails: '****@cs.washington.edu'
    first_name: Yejin
    google_scholar_id: https://scholar.google.com/citations?user=vhP-tlcAAAAJ&hl=en
    homepage: https://homes.cs.washington.edu/~yejin/
    institution: Department of Computer Science, University of Washington
    last_name: Choi
    name: Yejin Choi
    semantic_scholar_id: https://www.semanticscholar.org/author/Yejin-Choi/1699545?sort=year
    username: ~Yejin_Choi1
  - dblp_id: https://dblp.org/pid/34/4472
    emails: '****@princeton.edu'
    first_name: Thomas
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=UAwKvEsAAAAJ
    homepage: http://cocosci.princeton.edu/tom/
    institution: Princeton University
    last_name: Griffiths
    middle_name: L.
    name: Thomas L. Griffiths
    semantic_scholar_id: https://www.semanticscholar.org/author/T.-Griffiths/1799860
    username: ~Thomas_L._Griffiths1
  - dblp_id: https://dblp.org/pid/276/6005
    emails: '****@allenai.org'
    first_name: Faeze
    google_scholar_id: https://scholar.google.com/citations?user=4Da7Li4AAAAJ&hl=en
    homepage: https://users.soe.ucsc.edu/~hannahbrahman/
    institution: Allen Institute for AI
    last_name: Brahman
    name: Faeze Brahman
    semantic_scholar_id: https://www.semanticscholar.org/author/Faeze-Brahman/9252833
    username: ~Faeze_Brahman1
  decision: toMainConference
  end_page: 5850
  file: 571.pdf
  id: 571
  num_pages: 22
  openreview_id: RunaJs4qvE
  pdf_file: c65bf29fb73eb65749b1a49e3519e6d092fd903d.pdf
  start_page: 5829
  title: 'MacGyver: Are Large Language Models Creative Problem Solvers?'
- abstract: Perfect machine translation (MT) would render cross-lingual transfer (XLT)
    by means of multilingual language models (mLMs) superfluous. Given, on the one
    hand, the large body of work on improving XLT with mLMs and, on the other hand,
    recent advances in massively multilingual MT, in this work, we systematically
    evaluate existing and propose new translation-based XLT approaches for transfer
    to low-resource languages. We show that all translation-based approaches dramatically
    outperform zero-shot XLT with mLMs---with the combination of round-trip translation
    of the source-language training data and the translation of the target-language
    test instances at inference---being generally the most effective. We next show
    that one can obtain further empirical gains by adding reliable translations to
    other high-resource languages to the training data. Moreover, we propose an effective
    translation-based XLT strategy even for languages not supported by the MT system.
    Finally, we show that model selection for XLT based on target-language validation
    data obtained with MT outperforms model selection based on the source-language
    data. We believe our findings warrant a broader inclusion of more robust translation-based
    baselines in XLT research.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multilinguality and Language Diversity
  authors:
  - emails: '****@gmail.com'
    first_name: Benedikt
    homepage: https://www.informatik.uni-wuerzburg.de/nlp/team/benedikt-ebing/
    institution: "Bayerische Julius-Maximilians-Universit\xE4t W\xFCrzburg"
    last_name: Ebing
    name: Benedikt Ebing
    username: ~Benedikt_Ebing1
  - dblp_id: https://dblp.org/pid/50/11059
    emails: '****@gmail.com'
    first_name: Goran
    google_scholar_id: https://scholar.google.com/citations?user=Ym0myOwAAAAJ&hl=hr
    homepage: https://sites.google.com/view/goranglavas
    institution: "Julius-Maximilians-Universit\xE4t W\xFCrzburg"
    last_name: "Glava\u0161"
    name: "Goran Glava\u0161"
    semantic_scholar_id: https://www.semanticscholar.org/author/Goran-Glavas/2472657
    username: "~Goran_Glava\u01611"
  decision: toMainConference
  end_page: 5870
  file: 577.pdf
  id: 577
  num_pages: 20
  openreview_id: ZKh9mWiQqV
  pdf_file: 82fa5efccb17fb5279de8490c43597ff84473799.pdf
  start_page: 5851
  title: 'To Translate or Not to Translate: A Systematic Investigation of Translation-Based
    Cross-Lingual Transfer to Low-Resource Languages'
- abstract: "Numerous works are proposed to align large language models (LLMs) with\
    \ human intents to better fulfill instructions, ensuring they are trustful and\
    \ helpful.\nNevertheless, some human instructions are often malicious or misleading\
    \ \nand following them will lead to untruthful and unsafe responses.\nPrevious\
    \ work rarely focused on understanding how LLMs manage instructions based on counterfactual\
    \ premises, referred to here as inductive instructions, which may stem from users'\
    \ false beliefs or malicious intents.\nIn this paper, we aim to reveal the behaviors\
    \ of LLMs towards inductive instructions and enhance their truthfulness and helpfulness\
    \ accordingly. \nSpecifically, we first introduce a benchmark of Inductive Instructions\
    \ (INDust), where the false knowledge is incorporated into instructions in multiple\
    \ different styles. \nAfter extensive human and automatic evaluations, we uncovered\
    \ a universal vulnerability among LLMs in processing inductive instructions.\n\
    Additionally, we identified that different inductive styles affect the models'\
    \ ability to identify the same underlying errors,\nand the complexity of the underlying\
    \ assumptions also influences the model's performance.\nMotivated by these results,\
    \ we propose Dual-critique prompting to improve LLM robustness against inductive\
    \ instructions.\nOur experiments demonstrate that Dual-critique prompting significantly\
    \ bolsters the robustness of a diverse array of LLMs, even when confronted with\
    \ varying degrees of inductive instruction complexity and differing inductive\
    \ styles."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - dblp_id: https://dblp.org/pid/06/2293-92
    emails: '****@outlook.com'
    first_name: Rui
    google_scholar_id: https://scholar.google.com/citations?view_op=list_works&hl=zh-CN&user=nTPdz2sAAAAJ
    last_name: Wang
    name: Rui Wang
    semantic_scholar_id: https://www.semanticscholar.org/author/Rui-Wang/2151036536
    username: ~Rui_Wang30
  - dblp_id: https://dblp.org/pid/72/1462-3.html
    emails: '****@se.cuhk.edu.hk'
    first_name: Hongru
    google_scholar_id: https://scholar.google.com/citations?user=s6UtVYUAAAAJ&hl=en
    homepage: https://rulegreen.github.io/
    last_name: Wang
    name: Hongru WANG
    orcid: https://orcid.org/0000-0001-5027-0138
    semantic_scholar_id: https://www.semanticscholar.org/author/Hongru-Wang/22642319
    username: ~Hongru_WANG1
  - dblp_id: https://dblp.org/pid/161/0068
    emails: '****@huawei.com'
    first_name: Fei
    google_scholar_id: https://scholar.google.com/citations?user=gX3493QAAAAJ&hl
    homepage: https://mifei.github.io/
    last_name: Mi
    name: Fei Mi
    semantic_scholar_id: https://www.semanticscholar.org/author/Fei-Mi/33727421
    username: ~Fei_Mi1
  - emails: '****@se.cuhk.edu.hk'
    first_name: Boyang
    google_scholar_id: https://scholar.google.com/citations?user=S0BbF6wAAAAJ&hl=zh-CN&oi=ao
    homepage: https://amourwaltz.github.io/
    last_name: Xue
    name: Boyang XUE
    username: ~Boyang_XUE1
  - dblp_id: https://dblp.org/pid/49/6574-19
    emails: '****@gmail.com'
    first_name: Yi
    last_name: Chen
    name: Yi Chen
    semantic_scholar_id: https://www.semanticscholar.org/author/Yi-Chen/2165302640
    username: ~Yi_Chen9
  - dblp_id: https://dblp.org/pid/w/KamFaiWong
    emails: '****@se.cuhk.edu.hk'
    first_name: Kam-Fai
    homepage: http://www.se.cuhk.edu.hk/~kfwong
    institution: The Chinese University of Hong Kong
    last_name: Wong
    name: Kam-Fai Wong
    orcid: https://orcid.org/0000-0002-9427-5659
    username: ~Kam-Fai_Wong2
  - dblp_id: https://dblp.org/pid/93/5407
    emails: '****@hit.edu.cn'
    first_name: Ruifeng
    homepage: http://faculty.hitsz.edu.cn/xuruifeng
    institution: Harbin Institute of Technology
    last_name: Xu
    name: Ruifeng Xu
    semantic_scholar_id: https://www.semanticscholar.org/author/Ruifeng-Xu/1753529
    username: ~Ruifeng_Xu1
  decision: toMainConference
  end_page: 5889
  file: 578.pdf
  id: 578
  num_pages: 19
  openreview_id: eChSYsZNnt
  pdf_file: e3b1660f89d56f6e19e58079e9645af83f669f21.pdf
  start_page: 5871
  title: Enhancing Large Language Models Against Inductive Instructions with Dual-critique
    Prompting
- abstract: Named Entity Recognition (NER) is essential in various Natural Language
    Processing (NLP) applications. Traditional NER models are effective but limited
    to a set of predefined entity types. In contrast, Large Language Models (LLMs)
    can extract arbitrary entities through natural language instructions, offering
    greater flexibility. However, their size and cost, particularly for those accessed
    via APIs like ChatGPT, make them impractical in resource-limited scenarios. In
    this paper, we introduce a compact NER model trained to identify any type of entity.
    Leveraging a bidirectional transformer encoder, our model, GLiNER, facilitates
    parallel entity extraction, an advantage over the slow sequential token generation
    of LLMs. Through comprehensive testing, GLiNER demonstrate strong performance,
    outperforming both ChatGPT and fine-tuned LLMs in zero-shot evaluations on various
    NER benchmarks.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Extraction
  authors:
  - emails: '****@gmail.com'
    first_name: Urchade
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=nQTXsu4AAAAJ
    last_name: Zaratiana
    name: Urchade Zaratiana
    semantic_scholar_id: https://www.semanticscholar.org/author/Urchade-Zaratiana/2162192993
    username: ~Urchade_Zaratiana1
  - dblp_id: https://dblp.org/pid/92/11425
    emails: '****@lipn.fr'
    first_name: Nadi
    google_scholar_id: https://scholar.google.com/citations?user=YDHX06gAAAAJ
    homepage: https://lipn.univ-paris13.fr/~tomeh
    institution: "Universit\xE9 Sorbonne Paris Nord"
    last_name: Tomeh
    name: Nadi Tomeh
    semantic_scholar_id: https://www.semanticscholar.org/author/Nadi-Tomeh/2891042
    username: ~Nadi_Tomeh1
  - dblp_id: https://dblp.org/pid/160/5959.html
    emails: '****@fi-group.com'
    first_name: Pierre
    google_scholar_id: https://scholar.google.com/citations?user=OMycJz8AAAAJ&hl=en&oi=ao
    last_name: Holat
    name: Pierre Holat
    semantic_scholar_id: https://www.semanticscholar.org/author/Pierre-Holat/2095378266
    username: ~Pierre_Holat1
  - dblp_id: https://dblp.org/pid/29/2665
    emails: '****@lipn.univ-paris13.fr'
    first_name: Thierry
    google_scholar_id: https://scholar.google.it/citations?user=buYLEKk35R4C&hl=fr
    homepage: https://lipn.univ-paris13.fr/~charnois/
    institution: University of Sorbonne Paris Nord (Paris 13)
    last_name: Charnois
    name: Thierry Charnois
    username: ~Thierry_Charnois2
  decision: toMainConference
  end_page: 5902
  file: 580.pdf
  id: 580
  num_pages: 13
  openreview_id: MSW2mnhvY7
  pdf_file: 991f6061f2e6aab152af8300e302da13791a082e.pdf
  start_page: 5890
  title: 'GLiNER: Generalist Model for Named Entity Recognition using Bidirectional
    Transformer'
- abstract: Without proper safeguards, large language models will readily follow malicious
    instructions and generate toxic content. This risk motivates safety efforts such
    as red-teaming and large-scale feedback learning, which aim to make models both
    helpful and harmless. However, there is a tension between these two objectives,
    since harmlessness requires models to refuse to comply with unsafe prompts, and
    thus not be helpful. Recent anecdotal evidence suggests that some models may have
    struck a poor balance, so that even clearly safe prompts are refused if they use
    similar language to unsafe prompts or mention sensitive topics. In this paper,
    we introduce a new test suite called XSTest to identify such eXaggerated Safety
    behaviours in a systematic way. XSTest comprises 250 safe prompts across ten prompt
    types that well-calibrated models should not refuse to comply with, and 200 unsafe
    prompts as contrasts that models, for most applications, should refuse. We describe
    XSTest's creation and composition, and then use the test suite to highlight systematic
    failure modes in state-of-the-art language models as well as more general challenges
    in building safer language models.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - dblp_id: https://dblp.org/pid/282/4243
    emails: '****@unibocconi.it'
    first_name: Paul
    google_scholar_id: https://scholar.google.com/citations?user=7rpmd9cAAAAJ&hl=en
    homepage: https://paulrottger.com/
    institution: Bocconi University
    last_name: "R\xF6ttger"
    name: "Paul R\xF6ttger"
    orcid: https://orcid.org/0009-0008-7115-6893
    semantic_scholar_id: https://www.semanticscholar.org/author/Paul-R%C3%B6ttger/2043232919
    username: "~Paul_R\xF6ttger2"
  - dblp_id: https://dblp.org/pid/284/9434
    emails: '****@oii.ox.ac.uk'
    first_name: Hannah
    google_scholar_id: https://scholar.google.com/citations?user=Fha8ldEAAAAJ&hl=en
    homepage: https://www.hannahrosekirk.com/
    institution: University of Oxford and Alan Turing Institute
    last_name: Kirk
    middle_name: Rose
    name: Hannah Rose Kirk
    orcid: https://orcid.org/0000-0002-7419-5993
    username: ~Hannah_Rose_Kirk1
  - emails: '****@turing.ac.uk'
    first_name: Bertie
    google_scholar_id: https://scholar.google.co.uk/citations?user=yRhnVoIAAAAJ&hl=en
    homepage: https://www.turing.ac.uk/people/researchers/bertie-vidgen
    institution: Alan Turing Institute
    last_name: Vidgen
    name: Bertie Vidgen
    username: ~Bertie_Vidgen1
  - dblp_id: https://dblp.org/pid/198/3907
    emails: '****@gmail.com'
    first_name: Giuseppe
    google_scholar_id: https://scholar.google.it/citations?user=IuhnRJQAAAAJ&hl=en
    homepage: https://gattanasio.cc
    institution: Bocconi University
    last_name: Attanasio
    name: Giuseppe Attanasio
    semantic_scholar_id: https://www.semanticscholar.org/author/Giuseppe-Attanasio/1481857041
    username: ~Giuseppe_Attanasio1
  - dblp_id: https://dblp.org/pid/122/8815-1
    emails: '****@stanford.edu'
    first_name: Federico
    google_scholar_id: https://scholar.google.com/citations?user=1okGjb8AAAAJ&hl=it
    homepage: https://federicobianchi.io
    institution: Stanford University
    last_name: Bianchi
    name: Federico Bianchi
    orcid: https://orcid.org/0000-0003-0776-361X
    username: ~Federico_Bianchi1
  - dblp_id: https://dblp.org/pid/82/8159
    emails: '****@unibocconi.it'
    first_name: Dirk
    google_scholar_id: https://scholar.google.it/citations?user=7xluaTAAAAAJ&hl=en
    homepage: http://www.dirkhovy.com
    institution: Bocconi University
    last_name: Hovy
    name: Dirk Hovy
    orcid: https://orcid.org/0000-0002-4618-3127
    semantic_scholar_id: https://www.semanticscholar.org/author/Dirk-Hovy/2022288
    username: ~Dirk_Hovy2
  decision: toMainConference
  end_page: 5926
  file: 581.pdf
  id: 581
  num_pages: 24
  openreview_id: UJxS2qejBV
  pdf_file: 0a91148f6b1884bc2186230101c0d9d4c8ca5802.pdf
  start_page: 5903
  title: 'XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large
    Language Models'
- abstract: Prompt tuning, which freezes all parameters of a pre-trained model and
    only trains a soft prompt, has emerged as a parameter-efficient approach. For
    the reason that the prompt initialization becomes sensitive when the model size
    is small, the prompt transfer that uses the trained prompt as an initialization
    for the target task has recently been introduced. Since previous works have compared
    tasks in large categories (e.g., summarization, sentiment analysis), the factors
    that influence prompt transfer have not been sufficiently explored. In this paper,
    we characterize the question answering task based on features such as answer format
    and empirically investigate the transferability of soft prompts for the first
    time. We analyze the impact of initialization during prompt transfer and find
    that the train dataset size of source and target tasks have the influence significantly.
    Furthermore, we propose a novel approach for measuring catastrophic forgetting
    and investigate how it occurs in terms of the amount of evidence. Our findings
    can help deeply understand transfer learning in prompt tuning.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Efficient/Low-Resource Methods for NLP
  authors:
  - emails: '****@hanyang.ac.kr'
    first_name: Minji
    google_scholar_id: https://scholar.google.com/citations?user=aAcDj2wAAAAJ&hl=ko
    homepage: https://github.com/minji-o-j
    last_name: Jung
    name: Minji Jung
    orcid: https://orcid.org/0009-0002-9299-418X
    username: ~Minji_Jung2
  - emails: '****@hanyang.ac.kr'
    first_name: Soyeon
    homepage: https://github.com/ssoyaavv
    institution: Hanyang University
    last_name: Park
    name: Soyeon Park
    username: ~Soyeon_Park4
  - dblp_id: https://dblp.org/pid/347/8388
    emails: '****@hanyang.ac.kr'
    first_name: Jeewoo
    google_scholar_id: https://scholar.google.com/citations?user=czR5GfYAAAAJ&hl=ko
    homepage: https://jeewoo1025.github.io/
    institution: LG Corporation
    last_name: Sul
    name: Jeewoo Sul
    orcid: https://orcid.org/0000-0002-3851-0482
    semantic_scholar_id: https://www.semanticscholar.org/author/Jeewoo-Sul/2217682989
    username: ~Jeewoo_Sul1
  - dblp_id: https://dblp.org/pid/63/8470
    emails: '****@hanyang.ac.kr'
    first_name: Yong Suk
    google_scholar_id: https://scholar.google.co.kr/citations?user=7zCHdocAAAAJ&hl=ko
    homepage: http://ai.hanyang.ac.kr
    institution: Hanyang University
    last_name: Choi
    name: Yong Suk Choi
    semantic_scholar_id: https://www.semanticscholar.org/author/Y.-Choi/35714151
    username: ~Yong_Suk_Choi1
  decision: toMainConference
  end_page: 5938
  file: 584.pdf
  id: 584
  num_pages: 12
  openreview_id: TpxY9qZDp2
  pdf_file: f4974b672443bbc91b3b8de337228a1bc05348f2.pdf
  start_page: 5927
  title: Is Prompt Transfer Always Effective? An Empirical Study of Prompt Transfer
    for Question Answering
- abstract: The dynamic nature of knowledge in an ever-changing world presents challenges
    for language models trained on static data; the model in the real world often
    requires not only acquiring new knowledge but also overwriting outdated information
    into updated ones. To study the ability of language models for these time-dependent
    dynamics in human language, we introduce a novel task, EvolvingQA, a temporally
    evolving question-answering benchmark designed for training and evaluating LMs
    on an evolving Wikipedia database. The construction of EvolvingQA is automated
    with our pipeline using large language models. We uncover that existing continual
    learning baselines suffer from updating and removing outdated knowledge. Our analysis
    suggests that models fail to rectify knowledge due to small weight gradients.
    In addition, we elucidate that language models particularly struggle to reflect
    the change of numerical or temporal information. Our work aims to model the dynamic
    nature of real-world information, suggesting faithful evaluations of the evolution-adaptability
    of language models. Our data construction code and dataset files are available
    at https://github.com/kimyuji/EvolvingQA_benchmark.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Learning for NLP
  authors:
  - dblp_id: https://dblp.org/pid/128/3542
    emails: '****@kaist.ac.kr'
    first_name: Yujin
    homepage: https://github.com/kimyuji
    last_name: Kim
    name: Yujin Kim
    username: ~Yujin_Kim2
  - dblp_id: https://dblp.org/pid/203/4449
    emails: '****@cs.unc.edu'
    first_name: Jaehong
    google_scholar_id: https://scholar.google.com/citations?user=-5comoUAAAAJ&hl=en
    homepage: https://jaehong31.github.io/
    institution: University of North Carolina at Chapel Hill
    last_name: Yoon
    name: Jaehong Yoon
    semantic_scholar_id: https://www.semanticscholar.org/author/Jaehong-Yoon/13563486
    username: ~Jaehong_Yoon1
  - dblp_id: https://dblp.org/pid/301/8927
    emails: '****@kaist.ac.kr'
    first_name: Seonghyeon
    homepage: https://vano1205.github.io/
    last_name: Ye
    name: Seonghyeon Ye
    username: ~Seonghyeon_Ye1
  - dblp_id: https://dblp.org/pid/91/1588
    emails: '****@kaist.ac.kr'
    first_name: Sangmin
    google_scholar_id: https://scholar.google.com/citations?user=T5rHY14AAAAJ&hl=en
    homepage: https://www.raymin0223.com
    last_name: Bae
    name: Sangmin Bae
    username: ~Sangmin_Bae1
  - dblp_id: https://dblp.org/pid/313/1580
    emails: '****@kaist.ac.kr'
    first_name: Namgyu
    google_scholar_id: https://scholar.google.com/citations?view_op=list_works&hl=en&user=4vOf7N8AAAAJ&gmla=AJsN-F4vF_JKQovV6ef2ZFCVo5aVqY1YGcIyDC7MzLP8wNmQZLini5DIsXaZXS9m0UcVpdynmzw7n_Cpqj0GIQnHA4PkI5v7z8JJBbnFU_nzu2a6eoqENVL8Wl7qCHy19MhmDDTQYFnL
    homepage: http://namgyu.com
    last_name: Ho
    name: Namgyu Ho
    semantic_scholar_id: https://www.semanticscholar.org/author/Namgyu-Ho/2047104015
    username: ~Namgyu_Ho1
  - dblp_id: http://dblp2.uni-trier.de/pers/hd/h/Hwang:Sung_Ju
    emails: '****@gmail.com'
    first_name: Sung Ju
    google_scholar_id: https://scholar.google.com/citations?user=RP4Qx3QAAAAJ&hl=en
    homepage: http://www.sungjuhwang.com/
    institution: Korea Advanced Institute of Science and Technology and AITRICS
    last_name: Hwang
    name: Sung Ju Hwang
    username: ~Sung_Ju_Hwang1
  - dblp_id: https://dblp.org/pid/23/8862
    emails: '****@kaist.ac.kr'
    first_name: Se-Young
    google_scholar_id: https://scholar.google.com/citations?user=X_IAjb8AAAAJ&hl=en
    homepage: https://fbsqkd.github.io
    institution: KAIST
    last_name: Yun
    name: Se-Young Yun
    semantic_scholar_id: https://www.semanticscholar.org/author/Se-Young-Yun/70509252
    username: ~Se-Young_Yun1
  decision: toMainConference
  end_page: 5953
  file: 587.pdf
  id: 587
  num_pages: 15
  openreview_id: RTCdvHIw6u
  pdf_file: 63dc02ea57de6a1bfeaf44d812e650ebb6175aa5.pdf
  start_page: 5939
  title: 'Carpe diem: On the Evaluation of World Knowledge in Lifelong Language Models'
- abstract: An effective method for combining frozen large language models (LLM) and
    visual encoders involves a resampler module that creates a `visual prompt' which
    is provided to the LLM, along with the textual prompt. While this approach has
    enabled impressive performance across many coarse-grained tasks like image captioning
    and visual question answering, more fine-grained tasks that require spatial understanding
    have not been thoroughly examined. In this paper, we use diagnostic classifiers
    to measure the extent to which the visual prompt produced by the resampler encodes
    spatial information. Our results show that this information is largely absent
    from the resampler output when kept frozen during training of the classifiers.
    However, when the resampler and classifier are trained jointly, we observe a significant
    performance boost. This shows that the compression achieved by the resamplers
    can in principle encode the requisite spatial information, but that more object-aware
    objectives are needed at the pretraining stage to facilitate this capability.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multimodality and Language Grounding to Vision, Robotics and Beyond
  authors:
  - emails: '****@hw.ac.uk'
    first_name: Georgios
    google_scholar_id: https://scholar.google.com/citations?user=AUFTexwAAAAJ&hl=en
    last_name: Pantazopoulos
    name: Georgios Pantazopoulos
    username: ~Georgios_Pantazopoulos1
  - dblp_id: https://dblp.org/pid/184/4588
    emails: '****@gmail.com'
    first_name: Alessandro
    google_scholar_id: https://scholar.google.com/citations?user=429MAoUAAAAJ&hl=th
    homepage: https://alesuglia.github.io/
    institution: Heriot-Watt University
    last_name: Suglia
    name: Alessandro Suglia
    semantic_scholar_id: https://www.semanticscholar.org/author/Alessandro-Suglia/3444866
    username: ~Alessandro_Suglia1
  - dblp_id: https://dblp.org/pid/36/6352
    emails: '****@hw.ac.uk'
    first_name: Oliver
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=YYWptO4AAAAJ
    homepage: https://sites.google.com/site/olemon/
    institution: Heriot-Watt University
    last_name: Lemon
    name: Oliver Lemon
    orcid: https://orcid.org/0000-0001-9497-4743
    semantic_scholar_id: https://www.semanticscholar.org/author/Oliver-Lemon/1782798
    username: ~Oliver_Lemon1
  - dblp_id: https://dblp.org/pid/58/3222
    emails: '****@hw.ac.uk'
    first_name: Arash
    google_scholar_id: https://scholar.google.co.uk/citations?user=yCku-o8AAAAJ&hl=en
    homepage: https://sites.google.com/site/araesh81/
    institution: Heriot-Watt University
    last_name: Eshghi
    name: Arash Eshghi
    username: ~Arash_Eshghi1
  decision: toMainConference
  end_page: 5963
  file: 588.pdf
  id: 588
  num_pages: 10
  openreview_id: TMECIcPbbI
  pdf_file: 508dc618d305cc2295f1982ac31cf2850bdd74d1.pdf
  start_page: 5954
  title: 'Lost in Space: Probing Fine-grained Spatial Understanding in Vision and
    Language Resamplers'
- abstract: In machine translation, the problem of ambiguously gendered input has
    been pointed out, where the gender of an entity is not available in the source
    sentence. To address this ambiguity issue, the task of controlled translation
    that takes the gender of the ambiguous entity as additional input have been proposed.
    However, most existing works have only considered a simplified setup of one target
    gender for input. In this paper, we tackle controlled translation in a more realistic
    setting of inputs with multiple entities and propose Gender-of-Entity (GoE) prompting
    method for LLMs. Our proposed method instructs the model with fine-grained entity-level
    gender information to translate with correct gender inflections. By utilizing
    four evaluation benchmarks, we investigate the controlled translation capability
    of LLMs in multiple dimensions and find that LLMs reach state-of-the-art performance
    in controlled translation. Furthermore, we discover an emergence of gender interference
    phenomenon when controlling the gender of multiple entities. Finally, we address
    the limitations of existing gender accuracy evaluation metrics and propose leveraging
    LLMs as an evaluator for gender inflection in machine translation.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Translation
  authors:
  - emails: '****@snu.ac.kr'
    first_name: Minwoo
    institution: Seoul National University
    last_name: Lee
    name: Minwoo Lee
    semantic_scholar_id: https://www.semanticscholar.org/author/Minwoo-Lee/2125213401
    username: ~Minwoo_Lee2
  - emails: '****@snu.ac.kr'
    first_name: Hyukhun
    homepage: https://hyukhunkoh-ai.github.io/
    last_name: Koh
    name: Hyukhun Koh
    username: ~Hyukhun_Koh1
  - emails: '****@snu.ac.kr'
    first_name: Minsung
    homepage: https://github.com/kms0805
    last_name: Kim
    name: Minsung Kim
    username: ~Minsung_Kim1
  - dblp_id: https://dblp.org/pid/48/3867
    emails: '****@snu.ac.kr'
    first_name: Kyomin
    google_scholar_id: https://scholar.google.co.kr/citations?user=u3uMl4MAAAAJ&hl=en
    homepage: http://milab.snu.ac.kr/kjung/index.html
    last_name: Jung
    name: Kyomin Jung
    username: ~Kyomin_Jung1
  decision: toMainConference
  end_page: 5978
  file: 592.pdf
  id: 592
  num_pages: 15
  openreview_id: adMn5WuP8r
  pdf_file: ce32d8318a7f7ecf229278c271a975964015dc08.pdf
  start_page: 5964
  title: Fine-grained Gender Control in Machine Translation with Large Language Models
- abstract: "In the constant updates of the product dialogue systems, we need to retrain\
    \ the natural language understanding (NLU) model as new data from the real users\
    \ would be merged into the existing data accumulated in the last updates. \nWithin\
    \ the newly added data, new intents would emerge and might have semantic entanglement\
    \ with the existing intents, e.g. new intents that are semantically too specific\
    \ or generic are actually a subset or superset of some existing intents in the\
    \ semantic space, thus impairing the robustness of the NLU model.\nAs the first\
    \ attempt to solve this problem, we setup a new benchmark consisting of 4 Dialogue\
    \ Version Control dataSets (DialogVCS). We formulate the intent detection with\
    \ imperfect data in the system update as a multi-label classification task with\
    \ positive but unlabeled intents, which asks the models to recognize all the proper\
    \ intents, including the ones with semantic entanglement, in the inference.\n\
    We also propose comprehensive baseline models and conduct in-depth analyses for\
    \ the benchmark, showing that the semantically entangled intents can be effectively\
    \ recognized with an automatic workflow. Our code and dataset are available at\
    \ \\url{https://github.com/Zefan-Cai/DialogVCS}."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Dialogue and Interactive Systems
  authors:
  - emails: '****@gmail.com'
    first_name: Zefan
    last_name: Cai
    name: Zefan Cai
    username: ~Zefan_Cai1
  - emails: '****@iscas.ac.cn'
    first_name: Xin
    google_scholar_id: https://scholar.google.com/citations?user=jTn0fiAAAAAJ&hl=zh-CN
    last_name: Zheng
    name: Xin Zheng
    username: ~Xin_Zheng6
  - dblp_id: https://dblp.org/pid/134/1099-1.html
    emails: '****@pku.edu.cn'
    first_name: Tianyu
    google_scholar_id: https://scholar.google.com.hk/citations?user=6hHbBwwAAAAJ&hl=zh-CN
    last_name: Liu
    name: Tianyu Liu
    username: ~Tianyu_Liu3
  - emails: '****@stu.pku.edu.cn'
    first_name: Haoran
    google_scholar_id: https://scholar.google.com/citations?user=L4VAZVEAAAAJ&hl=en&oi=ao
    homepage: https://scholar.google.com/citations?hl=en&pli=1&user=L4VAZVEAAAAJ
    last_name: Meng
    name: Haoran Meng
    username: ~Haoran_Meng1
  - emails: '****@tencent.com'
    first_name: Jiaqi
    institution: Tencent Cloud
    last_name: Han
    name: Jiaqi Han
    username: ~Jiaqi_Han1
  - emails: '****@tencent.com'
    first_name: Gang
    homepage: https://github.com/yuangang89
    last_name: Yuan
    name: Gang Yuan
    username: ~Gang_Yuan1
  - dblp_id: https://dblp.org/pid/146/2946.html
    emails: '****@tencent.com'
    first_name: Binghuai
    institution: Tencent
    last_name: Lin
    name: Binghuai Lin
    semantic_scholar_id: https://www.semanticscholar.org/author/Binghuai-Lin/3186130
    username: ~Binghuai_Lin1
  - dblp_id: https://dblp.org/pid/91/6051.html
    emails: '****@pku.edu.cn'
    first_name: Baobao
    google_scholar_id: https://scholar.google.com/citations?user=LaKNyhQAAAAJ
    homepage: http://eecs.pku.edu.cn/EN/People/Faculty/Detail/?ID=6027
    institution: Peking University
    last_name: Chang
    name: Baobao Chang
    semantic_scholar_id: https://www.semanticscholar.org/author/Baobao-Chang/39488576
    username: ~Baobao_Chang1
  - dblp_id: https://dblp.org/pid/33/4066.html
    emails: '****@hotmail.com'
    first_name: Yunbo
    google_scholar_id: https://scholar.google.com/citations?user=nNVDLb4AAAAJ&hl=zh-CN
    institution: Tencent
    last_name: Cao
    name: Yunbo Cao
    semantic_scholar_id: https://www.semanticscholar.org/author/Yunbo-Cao/2154235
    username: ~Yunbo_Cao3
  decision: toMainConference
  end_page: 6000
  file: 593.pdf
  id: 593
  num_pages: 22
  openreview_id: qe2sQYAvE2
  pdf_file: 9ac77fa36b5e8fc294279697991d42b80b79cf7c.pdf
  start_page: 5979
  title: 'DialogVCS: Robust Natural Language Understanding in Dialogue System Upgrade'
- abstract: "Verifiable generation aims to let the large language model (LLM) generate\
    \ text with supporting documents, which enables the user to flexibly verify the\
    \ answer and makes the LLM's output more reliable. Retrieval plays a crucial role\
    \ in verifiable generation. Specifically, the retrieved documents not only supplement\
    \ knowledge to help the LLM generate correct answers, but also serve as supporting\
    \ evidence for the user to verify the LLM's output. However, the widely used retrievers\
    \ become the bottleneck of the entire pipeline and limit the overall performance.\
    \ Their capabilities are usually inferior to LLMs since they often have much fewer\
    \ parameters than the large language model and have not been demonstrated to scale\
    \ well to the size of LLMs. If the retriever does not correctly find the supporting\
    \ documents, the LLM can not generate the correct and verifiable answer, which\
    \ overshadows the LLM's remarkable abilities. To address these limitations, we\
    \ propose **LLatrieval** (**L**arge **La**nguage Model Verified Re**trieval**),\n\
    where the LLM updates the retrieval result until it verifies that the retrieved\
    \ documents can sufficiently support answering the question. \nThus, the LLM can\
    \ iteratively provide feedback to retrieval and facilitate the retrieval result\
    \ to fully support verifiable generation. Experiments on ALCE show that LLatrieval\
    \ significantly outperforms extensive baselines and achieves state-of-the-art\
    \ results."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Retrieval and Text Mining
  authors:
  - dblp_id: https://dblp.org/pid/84/6885
    emails: '****@fudan.edu.cn'
    first_name: Xiaonan
    google_scholar_id: https://scholar.google.com/citations?user=ldEcEjEAAAAJ&hl=en
    institution: Fudan University
    last_name: Li
    name: Xiaonan Li
    semantic_scholar_id: https://www.semanticscholar.org/author/50080067
    username: ~Xiaonan_Li1
  - emails: '****@outlook.com'
    first_name: Changtai
    google_scholar_id: https://scholar.google.com/citations?user=YtYdLAcAAAAJ&hl=en
    last_name: Zhu
    name: Changtai Zhu
    username: ~Changtai_Zhu1
  - dblp_id: https://dblp.org/pid/228/8051
    emails: '****@fudan.edu.cn'
    first_name: Linyang
    google_scholar_id: https://scholar.google.com/citations?user=T6eEqcMAAAAJ
    homepage: https://github.com/LinyangLee
    last_name: Li
    name: Linyang Li
    semantic_scholar_id: https://www.semanticscholar.org/author/Linyang-Li/2107897400
    username: ~Linyang_Li1
  - emails: '****@m.fudan.edu.cn'
    first_name: Zhangyue
    google_scholar_id: https://scholar.google.com/citations?user=9gRQqSkAAAAJ
    homepage: https://yinzhangyue.github.io/
    last_name: Yin
    name: Zhangyue Yin
    username: ~Zhangyue_Yin1
  - dblp_id: https://dblp.org/pid/254/1189
    emails: '****@fudan.edu.cn'
    first_name: Tianxiang
    google_scholar_id: https://scholar.google.com/citations?user=puHFkM0AAAAJ
    homepage: https://txsun1997.github.io/
    last_name: Sun
    name: Tianxiang Sun
    semantic_scholar_id: https://www.semanticscholar.org/author/Tianxiang-Sun/153345698
    username: ~Tianxiang_Sun1
  - dblp_id: https://dblp.org/pid/69/1395
    emails: '****@fudan.edu.cn'
    first_name: Xipeng
    google_scholar_id: https://scholar.google.com/citations?user=Pq4Yp_kAAAAJ&hl=en
    homepage: https://xpqiu.github.io/
    institution: Fudan University
    last_name: Qiu
    name: Xipeng Qiu
    orcid: https://orcid.org/0000-0001-7163-5247
    semantic_scholar_id: https://www.semanticscholar.org/author/Xipeng-Qiu/1767521
    username: ~Xipeng_Qiu1
  decision: toMainConference
  end_page: 6019
  file: 595.pdf
  id: 595
  num_pages: 19
  openreview_id: oKJjxZBbS6
  pdf_file: d26ee374f6a4453cac0a4375ecc2c7e343e9256d.pdf
  start_page: 6001
  title: 'LLatrieval: LLM-Verified Retrieval for Verifiable Generation'
- abstract: "Social media is a valuable data source for exploring mental health issues.\
    \ \nHowever, previous studies have predominantly focused on the semantic content\
    \ of these posts, overlooking the importance of their temporal attributes, as\
    \ well as the evolving nature of mental disorders and symptoms.\nIn this paper,\
    \ we study the causality between psychiatric symptoms and life events, as well\
    \ as among different symptoms from social media posts, which leads to better understanding\
    \ of the underlying mechanisms of mental disorders. By applying these extracted\
    \ causality features to tasks such as diagnosis point detection and early risk\
    \ detection of depression, we notice considerable performance enhancement. This\
    \ indicates that causality information extracted from social media data can boost\
    \ the efficacy of mental disorder diagnosis and treatment planning."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Computational Social Science and Cultural Analytics
  authors:
  - dblp_id: https://dblp.org/pid/84/5999
    emails: '****@sjtu.edu.cn'
    first_name: Siyuan
    google_scholar_id: https://scholar.google.com/citations?user=SPngdHIAAAAJ&hl=en
    homepage: https://chesiy.github.io
    last_name: Chen
    name: Siyuan Chen
    semantic_scholar_id: https://www.semanticscholar.org/author/Siyuan-Chen/2218270477
    username: ~Siyuan_Chen1
  - emails: '****@sjtu.edu.cn'
    first_name: Meilin
    homepage: https://github.com/Wangml954
    last_name: Wang
    name: Meilin Wang
    username: ~Meilin_Wang2
  - emails: '****@sjtu.edu.cn'
    first_name: Minghao
    homepage: https://github.com/shallowblue-ego
    last_name: Lv
    name: Minghao Lv
    username: ~Minghao_Lv1
  - dblp_id: https://dblp.org/pid/90/6772
    emails: '****@qq.com'
    first_name: Zhiling
    google_scholar_id: https://scholar.google.com/citations?user=geAGR-4AAAAJ&hl=en
    homepage: https://github.com/blmoistawinde
    last_name: Zhang
    name: Zhiling Zhang
    username: ~Zhiling_Zhang1
  - emails: '****@icloud.com'
    first_name: Juqianqian
    homepage: https://github.com/Dejiyanglha
    last_name: Juqianqian
    name: Juqianqian
    username: ~Juqianqian1
  - emails: '****@stu.pku.edu.cn'
    first_name: Dejiyangla
    homepage: https://github.com/Dejiyanglha
    last_name: Dejiyangla
    name: Dejiyangla
    username: ~Dejiyangla1
  - emails: '****@pku.edu.cn'
    first_name: Yujia
    google_scholar_id: https://scholar.google.com/citations?user=h1xbFn4AAAAJ&hl=en&oi=ao
    homepage: https://yujiapeng.com/
    institution: Peking University
    last_name: Peng
    name: Yujia Peng
    orcid: https://orcid.org/0000-0002-2453-6622
    username: ~Yujia_Peng1
  - dblp_id: https://dblp.org/pid/z/KennyQiliZhu
    emails: '****@uta.edu'
    first_name: Kenny
    google_scholar_id: https://scholar.google.com.tw/citations?user=ZIRJ6lIAAAAJ
    homepage: http://www.cs.sjtu.edu.cn/~kzhu/
    institution: University of Texas at Arlington
    last_name: Zhu
    middle_name: Q.
    name: Kenny Q. Zhu
    semantic_scholar_id: https://www.semanticscholar.org/author/Kenny-Q.-Zhu/1796651
    username: ~Kenny_Q._Zhu1
  - dblp_id: https://dblp.org/pid/82/2416
    emails: '****@sjtu.edu.cn'
    first_name: Mengyue
    homepage: https://speechlab.sjtu.edu.cn/members/mengyue-wu
    last_name: Wu
    name: Mengyue Wu
    username: ~Mengyue_Wu1
  decision: toMainConference
  end_page: 6035
  file: 599.pdf
  id: 599
  num_pages: 16
  openreview_id: dKkq7UIhpX
  pdf_file: b952d8a1002b44caac813f4b20b4830a1d5b8e82.pdf
  start_page: 6020
  title: Mapping Long-term Causalities in Psychiatric Symptomatology and Life Events
    from Social Media
- abstract: Translate-test is a popular technique to improve the performance of multilingual
    language models. This approach works by translating the input into English using
    an external machine translation system before running inference. However, these
    improvements can be attributed to the use of a separate translation system, which
    is typically trained on large amounts of parallel data not seen by the language
    model. In this work, we introduce a new approach called self-translate that leverages
    the few-shot translation capabilities of multilingual language models. This allows
    us to analyze the effect of translation in isolation. Experiments over 5 tasks
    show that self-translate consistently outperforms direct inference, demonstrating
    that language models are unable to leverage their full multilingual potential
    when prompted in non-English languages. Our code is available at https://github.com/juletx/self-translate.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multilinguality and Language Diversity
  authors:
  - emails: '****@ehu.eus'
    first_name: Julen
    google_scholar_id: https://scholar.google.com/citations?user=BDGXAjgAAAAJ
    homepage: https://julenetxaniz.eus/en
    institution: HiTZ Center, University of the Basque Country (UPV/EHU)
    last_name: Etxaniz
    name: Julen Etxaniz
    orcid: https://orcid.org/0009-0000-2099-7766
    username: ~Julen_Etxaniz1
  - dblp_id: https://dblp.org/pid/14/10011
    emails: '****@ehu.eus'
    first_name: Gorka
    google_scholar_id: https://scholar.google.com/citations?user=_1wx6NoAAAAJ&hl=en
    homepage: https://gazkune.github.io/
    institution: "Universidad del Pa\xEDs Vasco"
    last_name: Azkune
    name: Gorka Azkune
    orcid: https://orcid.org/0000-0002-2506-7426
    semantic_scholar_id: https://www.semanticscholar.org/author/Gorka-Azkune/2481918
    username: ~Gorka_Azkune1
  - dblp_id: https://dblp.org/pid/03/6734
    emails: '****@ehu.eus'
    first_name: Aitor
    google_scholar_id: https://scholar.google.com/citations?user=yklm660AAAAJ&hl=en
    homepage: https://ixa2.si.ehu.eus/asoroa/
    institution: University of the Basque Country. UPV/EHU.
    last_name: Soroa
    name: Aitor Soroa
    orcid: https://orcid.org/0000-0001-8573-2654
    semantic_scholar_id: https://www.semanticscholar.org/author/Aitor-Soroa-Etxabe/2078619062
    username: ~Aitor_Soroa1
  - dblp_id: https://dblp.org/pid/11/4461
    emails: '****@gmail.com'
    first_name: Oier
    google_scholar_id: https://scholar.google.com/citations?user=nieh6tUAAAAJ&hl=en
    last_name: Lacalle
    middle_name: Lopez De
    name: Oier Lopez de Lacalle
    orcid: https://orcid.org/my-orcid?orcid=0000-0003-4969-2055
    semantic_scholar_id: https://www.semanticscholar.org/author/1715983
    username: ~Oier_Lopez_de_Lacalle1
  - dblp_id: https://dblp.org/pid/168/0354
    emails: '****@reka.ai'
    first_name: Mikel
    google_scholar_id: https://scholar.google.com/citations?user=N5InzP8AAAAJ
    homepage: http://www.mikelartetxe.com
    institution: Reka AI
    last_name: Artetxe
    name: Mikel Artetxe
    semantic_scholar_id: https://www.semanticscholar.org/author/Mikel-Artetxe/2347956
    username: ~Mikel_Artetxe1
  decision: toMainConference
  end_page: 6050
  file: 601.pdf
  id: 601
  num_pages: 15
  openreview_id: hmpdrNwZuH
  pdf_file: d1402a40bf3f877272372c5aedb931bc9042994c.pdf
  start_page: 6036
  title: Do Multilingual Language Models Think Better in English?
- abstract: 'We investigate multimodal chart retrieval, addressing the challenge of
    retrieving image-based charts using textual queries. We compare four approaches:
    (a) OCR with text retrieval, (b) chart derendering (DePlot) followed by table
    retrieval, (c) a direct image understanding model (PaLI-3), and (d) a combined
    PaLI-3 + DePlot approach. As the table retrieval component we introduce Tab-GTR,
    a text retrieval model augmented with table structure embeddings, achieving state-of-the-art
    results on the NQ-Tables benchmark with 48.88% R@1. On in-distribution data, the
    DePlot-based method (b) outperforms PaLI-3 (c), while being significantly more
    efficient (300M vs 3B trainable parameters). However, DePlot struggles with complex
    charts, indicating a need for improvements in chart derendering - specifically
    in terms of chart data diversity and the richness of text/table representations.
    We found no clear winner between methods (b) and (c) in general, with the best
    performance achieved by the combined approach (d), and further show that it benefits
    the most from multi-task training.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multimodality and Language Grounding to Vision, Robotics and Beyond
  authors:
  - emails: '****@google.com'
    first_name: Averi
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=Mm7MBhQAAAAJ
    institution: Google DeepMind
    last_name: Nowak
    name: Averi Nowak
    username: ~Averi_Nowak1
  - dblp_id: https://dblp.org/pid/151/3088
    emails: '****@gmail.com'
    first_name: Francesco
    google_scholar_id: https://scholar.google.com/citations?user=KE62hDMAAAAJ&hl=en
    institution: Google
    last_name: Piccinno
    name: Francesco Piccinno
    semantic_scholar_id: https://www.semanticscholar.org/author/Francesco-Piccinno/2174596
    username: ~Francesco_Piccinno1
  - emails: '****@google.com'
    first_name: Yasemin
    google_scholar_id: https://scholar.google.com/citations?user=jUCiLN4AAAAJ&hl=en
    institution: Research, Google
    last_name: Altun
    name: Yasemin Altun
    username: ~Yasemin_Altun2
  decision: toMainConference
  end_page: 6068
  file: 602.pdf
  id: 602
  num_pages: 18
  openreview_id: gU8hvjNyVL
  pdf_file: 708ee43b60220971117428ea1eeffa6bdc1644bd.pdf
  start_page: 6051
  title: 'Multimodal Chart Retrieval: A Comparison of Text, Table and Image Based
    Approaches'
- abstract: While large language models (LMs) demonstrate remarkable performance,
    they encounter challenges in providing accurate responses when queried for information
    beyond their pre-trained memorization. Although augmenting them with relevant
    external information can mitigate these issues, failure to consider the necessity
    of retrieval may adversely affect overall performance. Previous research has primarily
    focused on examining how entities influence retrieval models and knowledge recall
    in LMs, leaving other aspects relatively unexplored. In this work, our goal is
    to offer a more detailed, fact-centric analysis by exploring the effects of combinations
    of entities and relations. To facilitate this, we construct a new question answering
    (QA) dataset called WiTQA (Wikipedia Triple Question Answers). This dataset includes
    questions about entities and relations of various popularity levels, each accompanied
    by a supporting passage. Our extensive experiments with diverse LMs and retrievers
    reveal when retrieval does not consistently enhance LMs from the viewpoints of
    fact-centric popularity. Confirming earlier findings, we observe that larger LMs
    excel in recalling popular facts. However, they notably encounter difficulty with
    infrequent entity-relation pairs compared to retrievers. Interestingly, they can
    effectively retain popular relations of less common entities. We demonstrate the
    efficacy of our finer-grained metric and insights through an adaptive retrieval
    system that selectively employs retrieval and recall based on the frequencies
    of entities and relations in the question.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - dblp_id: https://dblp.org/pid/228/6624
    emails: '****@megagon.ai'
    first_name: Seiji
    google_scholar_id: https://scholar.google.co.jp/citations?user=O2HlSxwAAAAJ&hl=en&oi=ao
    homepage: https://seijimaekawa.github.io/
    institution: Megagon Labs, US
    last_name: Maekawa
    name: Seiji Maekawa
    username: ~Seiji_Maekawa1
  - dblp_id: https://dblp.org/pid/191/6033
    emails: '****@gmail.com'
    first_name: Hayate
    google_scholar_id: https://scholar.google.co.jp/citations?user=O2o6LJgAAAAJ
    homepage: https://isomap.github.io/
    institution: Megagon Labs, US
    last_name: Iso
    name: Hayate Iso
    orcid: https://orcid.org/0000-0001-9562-266X
    semantic_scholar_id: https://www.semanticscholar.org/author/Hayate-Iso/7782351
    username: ~Hayate_Iso1
  - dblp_id: https://dblp.org/pid/31/7537
    emails: '****@megagon.ai'
    first_name: Sairam
    institution: Megagon Labs
    last_name: Gurajada
    name: Sairam Gurajada
    username: ~Sairam_Gurajada1
  - dblp_id: https://dblp.org/pid/188/9073
    emails: '****@megagon.ai'
    first_name: Nikita
    homepage: https://nikibhutani.github.io/
    last_name: Bhutani
    name: Nikita Bhutani
    orcid: https://orcid.org/0000-0002-6687-2579
    username: ~Nikita_Bhutani1
  decision: toMainConference
  end_page: 6084
  file: 616.pdf
  id: 616
  num_pages: 16
  openreview_id: 4we9VN3pZ8
  pdf_file: 1bbe4e270beb6ff4405de177eba4cbce993ec81b.pdf
  start_page: 6069
  title: Retrieval Helps or Hurts? A Deeper Dive into the Efficacy of Retrieval Augmentation
    to Language Models
- abstract: In this work, we extend the instruction-tuned Llama-2 model with end-to-end
    general-purpose speech processing and reasoning abilities while maintaining the
    wide range of original LLM capabilities, without using any carefully curated paired
    data. The resulting end-to-end model, named \textit{AudioChatLlama}, can utilize
    audio prompts as a replacement for text and sustain a conversation. Such a model
    also has extended cross-modal capabilities such as being able to perform spoken
    question answering (QA), speech translation, and audio summarization amongst many
    other closed and open-domain tasks. This is unlike prior approaches in speech,
    in which LLMs are extended to handle audio for a limited number of pre-designated
    tasks. On both synthesized and recorded speech QA test sets, evaluations show
    that our end-to-end approach is on par with or outperforms cascaded systems (speech
    recognizer + LLM) in terms of modelling the response to a prompt. Furthermore,
    unlike cascades, our approach can interchange text and audio modalities and intrinsically
    utilize prior context in a conversation to provide better results.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Speech recognition, text-to-speech and spoken language understanding
  authors:
  - dblp_id: https://dblp.org/pid/254/3044
    emails: '****@gmail.com'
    first_name: Yassir
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=TJQUlhwAAAAJ
    institution: University of Cambridge
    last_name: Fathullah
    name: Yassir Fathullah
    username: ~Yassir_Fathullah1
  - emails: '****@gmail.com'
    first_name: Chunyang
    google_scholar_id: https://scholar.google.com/citations?user=9d--jY0AAAAJ&hl=en
    last_name: Wu
    name: Chunyang Wu
    username: ~Chunyang_Wu1
  - dblp_id: https://dblp.org/pid/21/10154
    emails: '****@gmail.com'
    first_name: Egor
    institution: Facebook
    last_name: Lakomkin
    name: Egor Lakomkin
    username: ~Egor_Lakomkin1
  - emails: '****@meta.com'
    first_name: Ke
    homepage: https://scholar.google.com/citations?user=i31osuAAAAAJ&hl=en
    institution: Meta
    last_name: Li
    name: Ke Li
    username: ~Ke_Li22
  - dblp_id: https://dblp.uni-trier.de/pers/j/Jia:Junteng.html
    emails: '****@cornell.edu'
    first_name: Junteng
    google_scholar_id: https://scholar.google.com/citations?user=daFofx4AAAAJ&hl=en
    homepage: https://000justin000.github.io/
    last_name: Jia
    name: Junteng Jia
    username: ~Junteng_Jia1
  - emails: '****@fb.com'
    first_name: Yuan
    google_scholar_id: https://scholar.google.com/citations?user=IJn-MskAAAAJ&hl=en&oi=ao
    last_name: Shangguan
    name: Yuan Shangguan
    username: ~Yuan_Shangguan1
  - dblp_id: https://dblp.org/pid/125/3716
    emails: '****@fb.com'
    first_name: Jay
    last_name: Mahadeokar
    name: Jay Mahadeokar
    username: ~Jay_Mahadeokar1
  - emails: '****@fb.com'
    first_name: Ozlem
    homepage: https://www.ozlemkalinli.com/
    last_name: Kalinli
    name: Ozlem Kalinli
    username: ~Ozlem_Kalinli1
  - emails: '****@meta.com'
    first_name: Christian
    google_scholar_id: https://scholar.google.com/citations?user=sRaG0dcAAAAJ
    institution: Facebook/ Meta
    last_name: Fuegen
    name: Christian Fuegen
    username: ~Christian_Fuegen1
  - emails: '****@gmail.com'
    first_name: Mike
    google_scholar_id: https://scholar.google.com/citations?user=17p_C-0AAAAJ&hl=en&oi=ao
    institution: 'Meta '
    last_name: Seltzer
    name: Mike Seltzer
    username: ~Mike_Seltzer1
  decision: toMainConference
  end_page: 6095
  file: 617.pdf
  id: 617
  num_pages: 11
  openreview_id: VnzsHHMQVS
  pdf_file: f5df22f88a607bbcd1a07d3e72efe7801ee752fb.pdf
  start_page: 6085
  title: 'AudioChatLlama: Towards General-Purpose Speech Abilities for LLMs'
- abstract: "*Do larger and more performant models resolve NLP's longstanding robustness\
    \ issues?* We investigate this question using over 20 models of different sizes\
    \ spanning different architectural choices and pretraining objectives. We conduct\
    \ evaluations using (a) \nout-of-domain and challenge test sets, (b) behavioral\
    \ testing with CheckLists, (c) contrast sets, and (d) adversarial inputs.  Our\
    \ analysis reveals that not all out-of-domain tests provide insight into robustness.\
    \ Evaluating with CheckLists and contrast sets shows significant gaps in model\
    \ performance; merely scaling models does not make them adequately robust. Finally,\
    \ we point out that current approaches for adversarial evaluations of models are\
    \ themselves problematic: they can be easily thwarted, and in their current forms,\
    \ do not represent a sufficiently deep probe of model robustness. We conclude\
    \ that not only is the question of robustness in NLP as yet unresolved, but even\
    \ some of the approaches to measure robustness need to be reassessed."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - emails: '****@gmail.com'
    first_name: Ashim
    google_scholar_id: https://scholar.google.com/citations?user=0ymeLxQAAAAJ&hl=en
    homepage: https://ashim95.github.io/
    last_name: Gupta
    name: Ashim Gupta
    username: ~Ashim_Gupta1
  - emails: '****@outlook.com'
    first_name: Rishanth
    homepage: https://rishanthrajendhran.github.io
    last_name: Rajendhran
    name: Rishanth Rajendhran
    username: ~Rishanth_Rajendhran1
  - emails: '****@gmail.com'
    first_name: Nathan
    homepage: https://n8stringham.github.io
    institution: University of Utah
    last_name: Stringham
    name: Nathan Stringham
    username: ~Nathan_Stringham1
  - dblp_id: https://dblp.org/pid/37/44
    emails: '****@cs.utah.edu'
    first_name: Vivek
    google_scholar_id: https://scholar.google.com/citations?user=TsTUfOIAAAAJ
    homepage: https://svivek.com
    institution: University of Utah
    last_name: Srikumar
    name: Vivek Srikumar
    semantic_scholar_id: https://www.semanticscholar.org/author/Vivek-Srikumar/3052879
    username: ~Vivek_Srikumar1
  - dblp_id: https://dblp.org/pid/185/0950
    emails: '****@utah.edu'
    first_name: Ana
    google_scholar_id: https://scholar.google.com/citations?user=3W6OnfAAAAAJ&hl=en&oi=ao
    homepage: https://www.anamarasovic.com
    institution: University of Utah
    last_name: Marasovic
    name: Ana Marasovic
    semantic_scholar_id: https://www.semanticscholar.org/author/Ana-Marasovi%C4%87/3451494
    username: ~Ana_Marasovic1
  decision: toMainConference
  end_page: 6153
  file: 621.pdf
  id: 621
  num_pages: 58
  openreview_id: 1upGLJSNOg
  pdf_file: fc258733af0ef58bf9029407fb49baa424175772.pdf
  start_page: 6096
  title: Whispers of Doubt Amidst Echoes of Triumph in NLP Robustness
- abstract: The rise of large-scale multimodal models has paved the pathway for groundbreaking
    advances in generative modeling and reasoning, unlocking transformative applications
    in a variety of complex tasks. However, a pressing question that remains is their
    genuine capability for stronger forms of generalization, which has been largely
    underexplored in the multimodal setting. Our study aims to address this by examining
    sequential compositional generalization using CompAct (Compositional Activities),
    a carefully constructed, perceptually grounded dataset set within a rich backdrop
    of egocentric kitchen activity videos. Each instance in our dataset is represented
    with a combination of raw video footage, naturally occurring sound, and crowd-sourced
    step-by-step descriptions. More importantly, our setup ensures that the individual
    concepts are consistently distributed across training and evaluation sets, while
    their compositions are novel in the evaluation set. We conduct a comprehensive
    assessment of several unimodal and multimodal models. Our findings reveal that
    bi-modal and tri-modal models exhibit a clear edge over their text-only counterparts.
    This highlights the importance of multimodality while charting a trajectory for
    future research in this domain.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multimodality and Language Grounding to Vision, Robotics and Beyond
  authors:
  - dblp_id: https://dblp.org/pid/159/1196
    emails: '****@gmail.com'
    first_name: Semih
    last_name: Yagcioglu
    name: Semih Yagcioglu
    semantic_scholar_id: https://www.semanticscholar.org/author/Semih-Yagcioglu/40387200
    username: ~Semih_Yagcioglu1
  - emails: '****@ku.edu.tr'
    first_name: Osman Batur
    google_scholar_id: https://scholar.google.com/citations?user=3fKgzG4AAAAJ&hl=en
    homepage: https://ospanbatyr.github.io
    last_name: "\u0130nce"
    name: "Osman Batur \u0130nce"
    orcid: https://orcid.org/0009-0008-9538-2481
    username: "~Osman_Batur_\u0130nce1"
  - dblp_id: http://dblp.uni-trier.de/pers/hd/e/Erdem:Aykut
    emails: '****@ku.edu.tr'
    first_name: Aykut
    google_scholar_id: https://scholar.google.com/citations?user=-xA1_OAAAAAJ&hl=en&oi=ao
    homepage: https://aykuterdem.github.io
    institution: "Ko\xE7 University"
    last_name: Erdem
    name: Aykut Erdem
    orcid: https://orcid.org/0000-0002-6280-8422
    username: ~Aykut_Erdem1
  - dblp_id: http://dblp.uni-trier.de/pers/hd/e/Erdem:Erkut
    emails: '****@cs.hacettepe.edu.tr'
    first_name: Erkut
    google_scholar_id: https://scholar.google.com.tr/citations?user=eALwl74AAAAJ&hl=en
    homepage: https://web.cs.hacettepe.edu.tr/~erkut
    institution: Hacettepe University
    last_name: Erdem
    name: Erkut Erdem
    semantic_scholar_id: https://www.semanticscholar.org/author/Erkut-Erdem/152330322
    username: ~Erkut_Erdem1
  - dblp_id: https://dblp.org/pid/46/7536
    emails: '****@di.ku.dk'
    first_name: Desmond
    institution: ' and University of Copenhagen'
    last_name: Elliott
    name: Desmond Elliott
    semantic_scholar_id: https://www.semanticscholar.org/author/Desmond-Elliott/50369944
    username: ~Desmond_Elliott1
  - dblp_id: https://dblp.org/pid/84/4160
    emails: '****@ku.edu.tr'
    first_name: Deniz
    google_scholar_id: https://scholar.google.com.tw/citations?user=EJurXJ4AAAAJ
    homepage: http://www.denizyuret.com/
    institution: Koc University
    last_name: Yuret
    name: Deniz Yuret
    username: ~Deniz_Yuret1
  decision: toMainConference
  end_page: 6174
  file: 625.pdf
  id: 625
  num_pages: 21
  openreview_id: kwnLbRW93Q
  pdf_file: 0fd162a2bfa14ad0d9760470de8f04b7cefa7008.pdf
  start_page: 6154
  title: Sequential Compositional Generalization in Multimodal Models
- abstract: 'This paper presents multiple question generation strategies for document-level
    event argument extraction. These strategies do not require human involvement and
    result in uncontextualized questions as well as contextualized questions grounded
    on the event and document of interest. Experimental results show that combining
    uncontextualized and contextualized questions is beneficial,

    especially when event triggers and arguments appear in different sentences. Our
    approach does not have corpus-specific components, in particular, the question
    generation strategies transfer across corpora. We also present a qualitative analysis
    of the most common errors made by our best model.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Extraction
  authors:
  - emails: '****@asu.edu'
    first_name: Md Nayem
    institution: Arizona State University
    last_name: Uddin
    name: Md Nayem Uddin
    username: ~Md_Nayem_Uddin2
  - emails: '****@gmail.com'
    first_name: Enfa
    homepage: https://www.beingenfa.com/
    institution: University of Arizona
    last_name: George
    middle_name: Rose
    name: Enfa Rose George
    username: ~Enfa_Rose_George1
  - dblp_id: https://dblp.org/pid/32/369-2
    emails: '****@arizona.edu'
    first_name: Eduardo
    google_scholar_id: https://scholar.google.com/citations?user=AqGa3-MAAAAJ&hl=en
    homepage: https://eduardoblanco.github.io/
    institution: University of Arizona
    last_name: Blanco
    name: Eduardo Blanco
    semantic_scholar_id: https://www.semanticscholar.org/author/Eduardo-Blanco/145186180
    username: ~Eduardo_Blanco1
  - emails: '****@asu.edu'
    first_name: Steven
    google_scholar_id: https://scholar.google.com/citations?user=-R34PZYAAAAJ&hl=en
    homepage: https://search.asu.edu/profile/25690
    last_name: Corman
    name: Steven Corman
    orcid: https://orcid.org/0000-0002-2129-4533
    username: ~Steven_Corman1
  decision: toMainConference
  end_page: 6190
  file: 630.pdf
  id: 630
  num_pages: 16
  openreview_id: luZhw5bFEH
  pdf_file: 750599633098a2dfda8a7dcb49c49bdbf33bc0bf.pdf
  start_page: 6175
  title: Generating Uncontextualized and Contextualized Questions for Document-Level
    Event Argument Extraction
- abstract: 'The proliferation of online misinformation has posed significant threats
    to public interest. While numerous online users actively participate in the combat
    against misinformation, many of such responses can be characterized by the lack
    of politeness and supporting facts. As a solution, text generation approaches
    are proposed to automatically produce counter-misinformation responses. Nevertheless,
    existing methods are often trained end-to-end without leveraging external knowledge,
    resulting in subpar text quality and excessively repetitive responses. In this
    paper, we propose retrieval augmented response generation for online misinformation
    (RARG), which collects supporting evidence from scientific sources and generates
    counter-misinformation responses based on the evidences. In particular, our RARG
    consists of two stages: (1) evidence collection, where we design a retrieval pipeline
    to retrieve and rerank evidence documents using a database comprising over 1M
    academic articles; (2) response generation, in which we align large language models
    (LLMs) to generate evidence-based responses via reinforcement learning from human
    feedback (RLHF). We propose a reward function to maximize the utilization of the
    retrieved evidence while maintaining the quality of the generated text, which
    yields polite and factual responses that clearly refutes misinformation. To demonstrate
    the effectiveness of our method, we study the case of COVID-19 and perform extensive
    experiments with both in- and cross-domain datasets, where RARG consistently outperforms
    baselines by generating high-quality counter-misinformation responses.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - dblp_id: https://dblp.org/pid/279/8978
    emails: '****@illinois.edu'
    first_name: Zhenrui
    google_scholar_id: https://scholar.google.com/citations?user=9Iy_KmsAAAAJ&hl=en
    homepage: http://yueeeeeeee.github.io/
    last_name: Yue
    name: Zhenrui Yue
    orcid: https://orcid.org/0000-0002-0309-2065
    semantic_scholar_id: https://www.semanticscholar.org/author/Zhenrui-Yue/2028213158
    username: ~Zhenrui_Yue1
  - dblp_id: https://dblp.org/pid/255/9938
    emails: '****@illinois.edu'
    first_name: Huimin
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=N-2sGUkAAAAJ
    last_name: Zeng
    name: Huimin Zeng
    semantic_scholar_id: https://www.semanticscholar.org/author/Huimin-Zeng/2113558183
    username: ~Huimin_Zeng1
  - emails: '****@illinois.edu'
    first_name: Yimeng
    last_name: Lu
    name: Yimeng Lu
    username: ~Yimeng_Lu2
  - dblp_id: https://dblp.org/pid/234/2928
    emails: '****@illinois.edu'
    first_name: Lanyu
    google_scholar_id: https://scholar.google.com/citations?user=3sxZiyQAAAAJ
    homepage: https://shanglanyu.github.io
    last_name: Shang
    name: Lanyu Shang
    orcid: https://orcid.org/0000-0002-7480-6889
    semantic_scholar_id: https://www.semanticscholar.org/author/Lanyu-Shang/65855502
    username: ~Lanyu_Shang1
  - dblp_id: https://dblp.org/pid/06/6785-31
    emails: '****@illinois.edu'
    first_name: Yang
    homepage: https://yznd42.github.io/
    institution: University of Illinois at Urbana-Champaign
    last_name: Zhang
    name: Yang Zhang
    semantic_scholar_id: https://www.semanticscholar.org/author/2145953897
    username: ~Yang_Zhang32
  - dblp_id: https://dblp.org/pid/40/3934-2
    emails: '****@illinois.edu'
    first_name: Dong
    google_scholar_id: https://scholar.google.com/citations?user=-NfMhb0AAAAJ
    homepage: https://www.wangdong.org/
    institution: University of Illinois at Urbana-Champaign
    last_name: Wang
    name: Dong Wang
    orcid: https://orcid.org/0000-0002-9599-8023
    semantic_scholar_id: https://www.semanticscholar.org/author/49370921
    username: ~Dong_Wang21
  decision: toMainConference
  end_page: 6206
  file: 632.pdf
  id: 632
  num_pages: 16
  openreview_id: pS9hIwoFOg
  pdf_file: 551b38c0eef6393c58ed0ceafed70794a1810388.pdf
  start_page: 6191
  title: Evidence-Driven Retrieval Augmented Response Generation for Online Misinformation
- abstract: LLMs are revolutionizing NLP tasks. However, the use of the most advanced
    LLMs, such as GPT-4, is often prohibitively expensive for most specialized fields.
    We introduce HEAL, the first continuously trained 13B LLaMA2-based LLM that is
    purpose-built for medical conversations and measured on automated scribing. Our
    results demonstrate that HEAL outperforms GPT-4 and PMC-LLaMA in PubMedQA, with
    an accuracy of 78.4\%. It also achieves parity with GPT-4 in generating medical
    notes. Remarkably, HEAL surpasses GPT-4 and Med-PaLM 2 in identifying more correct
    medical concepts and exceeds the performance of human scribes and other comparable
    models in correctness and completeness.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Summarization
  authors:
  - emails: '****@deepscribe.tech'
    first_name: Dong
    google_scholar_id: https://scholar.google.com/citations?user=2vNqdSEAAAAJ&hl=en
    last_name: Yuan
    name: Dong Yuan
    username: ~Dong_Yuan3
  - emails: '****@yahoo.com'
    first_name: Eti
    institution: DeepScribe
    last_name: Rastogi
    name: Eti Rastogi
    username: ~Eti_Rastogi1
  - emails: '****@deepscribe.tech'
    first_name: Gautam
    google_scholar_id: https://scholar.google.com/citations?hl=en&authuser=1&user=7msFsFYAAAAJ
    homepage: https://scholar.google.com/citations?hl=en&authuser=1&user=7msFsFYAAAAJ
    last_name: Naik
    name: Gautam Naik
    orcid: https://orcid.org/0009-0003-7839-7204
    username: ~Gautam_Naik1
  - emails: '****@gmail.com'
    first_name: Sree Prasanna
    google_scholar_id: https://scholar.google.co.in/citations?user=EEfouEUAAAAJ&hl=en
    homepage: https://sprjg.github.io/
    last_name: Rajagopal
    name: Sree Prasanna Rajagopal
    username: ~Sree_Prasanna_Rajagopal1
  - dblp_id: https://dblp.org/pid/240/9148
    emails: '****@gmail.com'
    first_name: Sagar
    google_scholar_id: https://scholar.google.com/citations?user=bQLq9okAAAAJ&hl=en
    last_name: Goyal
    name: Sagar Goyal
    username: ~Sagar_Goyal1
  - emails: '****@deepscribe.tech'
    first_name: Fen
    last_name: Zhao
    name: Fen Zhao
    orcid: https://orcid.org/0000-0002-0941-2189
    username: ~Fen_Zhao1
  - emails: '****@vt.edu'
    first_name: Bharath
    homepage: https://scholar.google.com/citations?user=N8TMU8AAAAAJ
    last_name: Chintagunta
    name: Bharath Chintagunta
    username: ~Bharath_Chintagunta1
  - emails: '****@deepscribe.tech'
    first_name: Jeffrey
    institution: DeepScribe
    last_name: Ward
    middle_name: M.
    name: Jeffrey M. Ward
    username: ~Jeffrey_M._Ward1
  decision: toMainConference
  end_page: 6213
  file: 633.pdf
  id: 633
  num_pages: 7
  openreview_id: UkW0dSc7i0
  pdf_file: f1edd13b647716542a44fe9311df98b75c685329.pdf
  start_page: 6207
  title: A Continued Pretrained LLM Approach for Automatic Medical Note Generation
- abstract: 'Existing federated learning (FL) studies usually

    assume the training label space and test label

    space are identical. However, in real-world applications, this assumption is too
    ideal to be

    true. A new user could come up with queries

    that involve data from unseen classes, and such

    open-vocabulary queries would directly defect

    such FL systems. Therefore, in this work, we

    explicitly focus on the under-explored openvocabulary challenge in FL. That is,
    for a new

    user, the global server shall understand her/his

    query that involves arbitrary unknown classes.

    To address this problem, we leverage the pretrained vision-language models (VLMs).
    In

    particular, we present a novel adaptation framework tailored for VLMs in the context
    of FL,

    named as Federated Multimodal Prototyping

    (Fed-MP). Fed-MP adaptively aggregates the

    local model weights based on light-weight

    client residuals, and makes predictions based

    on a novel multimodal prototyping mechanism.

    Fed-MP exploits the knowledge learned from

    the seen classes, and robustifies the adapted

    VLM to unseen categories. Our empirical evaluation on various datasets validates
    the effectiveness of Fed-MP.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multimodality and Language Grounding to Vision, Robotics and Beyond
  authors:
  - dblp_id: https://dblp.org/pid/255/9938
    emails: '****@illinois.edu'
    first_name: Huimin
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=N-2sGUkAAAAJ
    last_name: Zeng
    name: Huimin Zeng
    semantic_scholar_id: https://www.semanticscholar.org/author/Huimin-Zeng/2113558183
    username: ~Huimin_Zeng1
  - dblp_id: https://dblp.org/pid/279/8978
    emails: '****@illinois.edu'
    first_name: Zhenrui
    google_scholar_id: https://scholar.google.com/citations?user=9Iy_KmsAAAAJ&hl=en
    homepage: http://yueeeeeeee.github.io/
    last_name: Yue
    name: Zhenrui Yue
    orcid: https://orcid.org/0000-0002-0309-2065
    semantic_scholar_id: https://www.semanticscholar.org/author/Zhenrui-Yue/2028213158
    username: ~Zhenrui_Yue1
  - dblp_id: https://dblp.org/pid/40/3934-2
    emails: '****@illinois.edu'
    first_name: Dong
    google_scholar_id: https://scholar.google.com/citations?user=-NfMhb0AAAAJ
    homepage: https://www.wangdong.org/
    institution: University of Illinois at Urbana-Champaign
    last_name: Wang
    name: Dong Wang
    orcid: https://orcid.org/0000-0002-9599-8023
    semantic_scholar_id: https://www.semanticscholar.org/author/49370921
    username: ~Dong_Wang21
  decision: toMainConference
  end_page: 6226
  file: 635.pdf
  id: 635
  num_pages: 13
  openreview_id: eM8o0o0W07
  pdf_file: 95760455bdc24d622a0c542f756b4ee06141a522.pdf
  start_page: 6214
  title: Open-Vocabulary Federated Learning with Multimodal Prototyping
- abstract: Key Point Analysis (KPA), the summarization of multiple arguments into
    a concise collection of key points, continues to be a significant and unresolved
    issue within the field of argument mining.    Existing models adapt a two-stage
    pipeline of clustering arguments or generating key points for argument clusters.
    This approach rely on semantic similarity instead of measuring the existence of
    shared key points among arguments. Additionally, it only models the intra-cluster
    relationship among arguments, disregarding the inter-cluster relationship between
    arguments that do not share key points.    To address these limitations, we propose
    a novel approach for KPA with pairwise generation and graph partitioning.  Our
    objective is to train a generative model that can simultaneously provide a score
    indicating the presence of shared key point between a pair of arguments and generate
    the shared key point. Subsequently, to map generated redundant key points to a
    concise set of key points, we proceed to construct an arguments graph by considering
    the arguments as vertices, the generated key points as edges, and the scores as
    edge weights. We then propose a graph partitioning algorithm to partition all
    arguments sharing the same key points to the same subgraph.  Notably, our experimental
    findings demonstrate that our proposed model surpasses previous models when evaluated
    on both the ArgKP and QAM datasets.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Sentiment Analysis, Stylistic Analysis, and Argument Mining
  authors:
  - dblp_id: https://dblp.org/pid/66/2069
    emails: '****@smail.nju.edu.cn'
    first_name: Xiao
    google_scholar_id: https://scholar.google.com/citations?user=WAHzzk0AAAAJ
    institution: Nanjing University
    last_name: Li
    name: Xiao Li
    orcid: https://orcid.org/0009-0008-2670-9495
    semantic_scholar_id: https://www.semanticscholar.org/author/X.-Li/48570207
    username: ~Xiao_Li10
  - emails: '****@gmail.com'
    first_name: Yong
    google_scholar_id: https://scholar.google.com/citations?user=sxXZWQQAAAAJ&hl=en
    homepage: http://jiangyong.site/
    last_name: Jiang
    name: Yong Jiang
    semantic_scholar_id: https://www.semanticscholar.org/author/Yong-Jiang/50262192?sort=influence
    username: ~Yong_Jiang1
  - emails: '****@gmail.com'
    first_name: Shen
    google_scholar_id: https://scholar.google.com/citations?user=WiTIPAcAAAAJ&hl=zh-CN
    institution: Alibaba Group
    last_name: Huang
    name: Shen Huang
    semantic_scholar_id: https://www.semanticscholar.org/author/Shen-Huang/2186268584
    username: ~Shen_Huang2
  - dblp_id: https://dblp.org/pid/212/1755.html
    emails: '****@alibaba-inc.com'
    first_name: Pengjun
    last_name: Xie
    name: Pengjun Xie
    username: ~Pengjun_Xie2
  - dblp_id: https://dblp.org/pid/69/1215-1
    emails: '****@nju.edu.cn'
    first_name: Gong
    google_scholar_id: https://scholar.google.com/citations?user=_ncKAiwAAAAJ
    homepage: http://ws.nju.edu.cn/~gcheng
    institution: Nanjing University
    last_name: Cheng
    name: Gong Cheng
    orcid: https://orcid.org/0000-0003-3539-7776
    semantic_scholar_id: https://www.semanticscholar.org/author/Gong-Cheng/144592996
    username: ~Gong_Cheng3
  - dblp_id: https://dblp.org/pid/h/FeiHuang.html
    emails: '****@gmail.com'
    first_name: Fei
    google_scholar_id: https://scholar.google.com/citations?user=9r98PpoAAAAJ
    homepage: https://sites.google.com/view/fei-huang
    institution: Alibaba Group
    last_name: Huang
    name: Fei Huang
    username: ~Fei_Huang1
  decision: toMainConference
  end_page: 6237
  file: 638.pdf
  id: 638
  num_pages: 11
  openreview_id: Ra4U1aoyig
  pdf_file: 47c6148c605732e5bebf6884d29161df6e77ed83.pdf
  start_page: 6227
  title: Exploring Key Point Analysis with Pairwise Generation and Graph Partitioning
- abstract: 'Large language models (LLMs) have demonstrated substantial commonsense
    understanding through numerous benchmark evaluations. However, their understanding
    of cultural commonsense remains largely unexamined. In this paper, we conduct
    a comprehensive examination of the capabilities and limitations of several state-of-the-art
    LLMs in the context of cultural commonsense tasks. Using several general and cultural
    commonsense benchmarks, we find that (1) LLMs have a significant discrepancy in
    performance when tested on culture-specific commonsense knowledge for different
    cultures; (2) LLMs'' general commonsense capability is affected by cultural context;
    and (3) The language used to query the LLMs can impact their performance on cultural-related
    tasks.

    Our study points to the inherent bias in the cultural understanding of LLMs and
    provides insights that can help develop culturally-aware language models.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Computational Social Science and Cultural Analytics
  authors:
  - dblp_id: https://dblp.org/pid/37/8026
    emails: '****@umich.edu'
    first_name: Siqi
    homepage: https://lit.eecs.umich.edu/people.html
    institution: University of Michigan - Ann Arbor
    last_name: Shen
    name: Siqi Shen
    semantic_scholar_id: https://www.semanticscholar.org/author/Siqi-Shen/2072820796
    username: ~Siqi_Shen3
  - dblp_id: https://dblp.org/pid/157/3603
    emails: '****@gmail.com'
    first_name: Lajanugen
    google_scholar_id: https://scholar.google.com/citations?user=dcv4kpIAAAAJ&hl=en
    homepage: https://sites.google.com/umich.edu/llajan/
    institution: LG AI Research
    last_name: Logeswaran
    name: Lajanugen Logeswaran
    username: ~Lajanugen_Logeswaran1
  - dblp_id: https://dblp.org/pid/132/1761
    emails: '****@uic.edu'
    first_name: Moontae
    google_scholar_id: https://scholar.google.com/citations?user=BMvYy9cAAAAJ&hl=en
    homepage: https://moontae.people.uic.edu
    institution: University of Illinois, Chicago
    last_name: Lee
    name: Moontae Lee
    username: ~Moontae_Lee1
  - dblp_id: https://dblp.org/pid/58/2562
    emails: '****@eecs.umich.edu'
    first_name: Honglak
    google_scholar_id: https://scholar.google.com/citations?user=fmSHtE8AAAAJ&hl=en
    homepage: http://web.eecs.umich.edu/~honglak
    institution: University of Michigan, LG AI Research and University of Michigan
    last_name: Lee
    name: Honglak Lee
    semantic_scholar_id: https://www.semanticscholar.org/author/Honglak-Lee/1697141
    username: ~Honglak_Lee2
  - dblp_id: https://dblp.org/pid/116/4904
    emails: '****@sutd.edu.sg'
    first_name: Soujanya
    google_scholar_id: https://scholar.google.co.in/citations?user=oS6gRc4AAAAJ&hl=en
    homepage: https://sporia.info
    institution: Singapore University of Technology and Design
    last_name: Poria
    name: Soujanya Poria
    username: ~Soujanya_Poria1
  - dblp_id: https://dblp.org/pid/m/RadaMihalcea
    emails: '****@umich.edu'
    first_name: Rada
    google_scholar_id: https://scholar.google.com.tw/citations?user=UetM7FgAAAAJ
    homepage: https://web.eecs.umich.edu/~mihalcea/
    institution: University of Michigan
    last_name: Mihalcea
    name: Rada Mihalcea
    orcid: https://orcid.org/0000-0002-0767-6703
    semantic_scholar_id: https://www.semanticscholar.org/author/Rada-Mihalcea/145557251
    username: ~Rada_Mihalcea1
  decision: toMainConference
  end_page: 6250
  file: 642.pdf
  id: 642
  num_pages: 13
  openreview_id: f3NOyTojzI
  pdf_file: 7ea2c90aeb3cde3ea104279104949e2d07dae47c.pdf
  start_page: 6238
  title: Understanding the Capabilities and Limitations of Large Language Models for
    Cultural Commonsense
- abstract: One of the fundamental skills required for an agent acting in an environment
    to complete tasks is the ability to understand what actions are plausible at any
    given point. This work explores a novel use of code representations to reason
    about action preconditions for sequential decision making tasks. Code representations
    offer the flexibility to model procedural activities and associated constraints
    as well as the ability to execute and verify constraint satisfaction. Leveraging
    code representations, we extract action preconditions from demonstration trajectories
    in a zero-shot manner using pre-trained code models. Given these extracted preconditions,
    we propose a precondition-aware action sampling strategy that ensures actions
    predicted by a policy are consistent with preconditions. We demonstrate that the
    proposed approach enhances the performance of few-shot policy learning approaches
    across task-oriented dialog and embodied textworld benchmarks.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - dblp_id: https://dblp.org/pid/157/3603
    emails: '****@gmail.com'
    first_name: Lajanugen
    google_scholar_id: https://scholar.google.com/citations?user=dcv4kpIAAAAJ&hl=en
    homepage: https://sites.google.com/umich.edu/llajan/
    institution: LG AI Research
    last_name: Logeswaran
    name: Lajanugen Logeswaran
    username: ~Lajanugen_Logeswaran1
  - dblp_id: https://dblp.org/pers/s/Sohn:Sungryull
    emails: '****@gmail.com'
    first_name: Sungryull
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=3Q8LdcYAAAAJ
    institution: LG AI Research
    last_name: Sohn
    name: Sungryull Sohn
    semantic_scholar_id: https://www.semanticscholar.org/author/Sungryull-Sohn/144832576
    username: ~Sungryull_Sohn1
  - dblp_id: https://dblp.org/pid/230/7891
    emails: '****@umich.edu'
    first_name: Yiwei
    google_scholar_id: https://scholar.google.com/citations?user=fV5fYpsAAAAJ&hl=en&oi=ao
    last_name: Lyu
    name: Yiwei Lyu
    username: ~Yiwei_Lyu1
  - dblp_id: https://dblp.org/pid/264/2652.html
    emails: '****@umich.edu'
    first_name: Anthony
    google_scholar_id: https://scholar.google.com/citations?user=TjEqCOAAAAAJ&hl=en
    homepage: https://anthliu.github.io/
    last_name: Liu
    middle_name: Zhe
    name: Anthony Zhe Liu
    semantic_scholar_id: https://www.semanticscholar.org/author/Anthony-Z.-Liu/1738283984
    username: ~Anthony_Zhe_Liu1
  - dblp_id: https://dblp.org/pid/199/2089
    emails: '****@gmail.com'
    first_name: Dong-Ki
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=Yl_3akYAAAAJ
    homepage: https://dkkim93.github.io/
    institution: LG AI Research
    last_name: Kim
    name: Dong-Ki Kim
    username: ~Dong-Ki_Kim1
  - dblp_id: https://dblp.org/pid/274/1579
    emails: '****@gmail.com'
    first_name: Dongsub
    google_scholar_id: https://scholar.google.com/citations?user=NxE-ZasAAAAJ&hl=en
    institution: LG AI Research
    last_name: Shim
    name: Dongsub Shim
    username: ~Dongsub_Shim1
  - dblp_id: https://dblp.org/pid/132/1761
    emails: '****@uic.edu'
    first_name: Moontae
    google_scholar_id: https://scholar.google.com/citations?user=BMvYy9cAAAAJ&hl=en
    homepage: https://moontae.people.uic.edu
    institution: University of Illinois, Chicago
    last_name: Lee
    name: Moontae Lee
    username: ~Moontae_Lee1
  - dblp_id: https://dblp.org/pid/58/2562
    emails: '****@eecs.umich.edu'
    first_name: Honglak
    google_scholar_id: https://scholar.google.com/citations?user=fmSHtE8AAAAJ&hl=en
    homepage: http://web.eecs.umich.edu/~honglak
    institution: University of Michigan, LG AI Research and University of Michigan
    last_name: Lee
    name: Honglak Lee
    semantic_scholar_id: https://www.semanticscholar.org/author/Honglak-Lee/1697141
    username: ~Honglak_Lee2
  decision: toMainConference
  end_page: 6267
  file: 645.pdf
  id: 645
  num_pages: 17
  openreview_id: QNWJtyIITE
  pdf_file: d6018533b80726d6fd1ec02721456a59f314e79d.pdf
  start_page: 6251
  title: Code Models are Zero-shot Precondition Reasoners
- abstract: Recently, deep end-to-end learning has been studied for intent classification
    in Spoken Language Understanding (SLU). However, end-to-end models require a large
    amount of speech data with intent labels, and highly optimized models are generally
    sensitive to the inconsistency between the training and evaluation conditions.
    Therefore, a natural language understanding approach based on Automatic Speech
    Recognition (ASR) remains attractive because it can utilize a pre-trained general
    language model and adapt to the mismatch of the speech input environment. Using
    this module-based approach, we improve a noisy-channel model to handle transcription
    inconsistencies caused by ASR errors. We propose a two-stage method, Contrastive
    and Consistency Learning (CCL), that correlates error patterns between clean and
    noisy ASR transcripts and emphasizes the consistency of the latent features of
    the two transcripts. Experiments on four benchmark datasets show that CCL outperforms
    existing methods and improves the ASR robustness in various noisy environments.
    Code is available at https://github.com/syoung7388/CCL
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Speech recognition, text-to-speech and spoken language understanding
  authors:
  - emails: '****@knu.ac.kr'
    first_name: Suyoung
    google_scholar_id: https://scholar.google.com/citations?user=TxbhHSUAAAAJ&hl=ko
    homepage: http://github.com/syoung7388
    last_name: Kim
    name: Suyoung Kim
    username: ~Suyoung_Kim1
  - emails: '****@knu.ac.kr'
    first_name: Jiyeon
    google_scholar_id: https://scholar.google.com/citations?user=F-m9wrcAAAAJ&hl=ko&authuser=1
    homepage: https://github.com/gyeonh
    institution: Kyungpook National University
    last_name: Hwang
    name: Jiyeon Hwang
    username: ~Jiyeon_Hwang1
  - emails: '****@knu.ac.kr'
    first_name: Ho-Young
    google_scholar_id: https://scholar.google.com/citations?user=gvaE8RUAAAAJ&hl=en
    homepage: https://sites.google.com/view/mlc-lab/
    institution: Kyungpook National University
    last_name: Jung
    name: Ho-Young Jung
    username: ~Ho-Young_Jung1
  decision: toMainConference
  end_page: 6281
  file: 646.pdf
  id: 646
  num_pages: 14
  openreview_id: gNGXXcrhqO
  pdf_file: b2a3d09679deae8b764fadd9a31204de70ca4352.pdf
  start_page: 6268
  title: Contrastive and Consistency Learning for Neural Noisy-Channel Model in Spoken
    Language Understanding
- abstract: The integration of Large Language Models (LLMs) in information retrieval
    has raised a critical reevaluation of fairness in the text-ranking models. LLMs,
    such as GPT models and Llama2, have shown effectiveness in natural language understanding
    tasks, and prior works such as RankGPT have demonstrated that the LLMs have better
    performance than the traditional ranking models in the ranking task. However,
    their fairness remains largely unexplored. This paper presents an empirical study
    evaluating these LLMs using the TREC Fair Ranking dataset, focusing on the representation
    of binary protected attributes such as gender and geographic location, which are
    historically underrepresented in search outcomes. Our analysis delves into how
    these LLMs handle queries and documents related to these attributes, aiming to
    uncover biases in their ranking algorithms. We assess fairness from both user
    and content perspectives, contributing an empirical benchmark for evaluating LLMs
    as the fair ranker.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Ethics, Bias, and Fairness
  authors:
  - emails: '****@scu.edu'
    first_name: Yuan
    google_scholar_id: https://scholar.google.com/citations?user=Z4LELkUAAAAJ&hl=zh-CN
    homepage: https://scholar.google.com/citations?user=Z4LELkUAAAAJ&hl=zh-CN
    last_name: Wang
    name: Yuan Wang
    username: ~Yuan_Wang12
  - emails: '****@scu.edu'
    first_name: Xuyang
    google_scholar_id: https://scholar.google.com/citations?user=vYIThvQAAAAJ&hl=en&authuser=2
    last_name: Wu
    name: Xuyang Wu
    username: ~Xuyang_Wu2
  - emails: '****@docomoinnovations.com'
    first_name: Hsin-Tai
    last_name: Wu
    name: Hsin-Tai Wu
    username: ~Hsin-Tai_Wu1
  - dblp_id: https://dblp.org/pers/hd/t/Tao:Zhiqiang
    emails: '****@gmail.com'
    first_name: Zhiqiang
    google_scholar_id: https://scholar.google.com/citations?user=sEKglOkAAAAJ&hl=en
    homepage: http://ztao.cc/
    institution: Rochester Institute of Technology
    last_name: Tao
    name: ZHIQIANG TAO
    username: ~ZHIQIANG_TAO2
  - dblp_id: https://dblp.org/pid/96/361-8
    emails: '****@scu.edu'
    first_name: Yi
    homepage: https://www.cse.scu.edu/~yfang/
    institution: Santa Clara University
    last_name: Fang
    name: Yi Fang
    username: ~Yi_Fang7
  decision: toMainConference
  end_page: 6294
  file: 648.pdf
  id: 648
  num_pages: 13
  openreview_id: jZ834tkGpk
  pdf_file: 816febbfc6699e29e06016e92cc405fceea6cd36.pdf
  start_page: 6282
  title: Do Large Language Models Rank Fairly? An Empirical Study on the Fairness
    of LLMs as Rankers
- abstract: Table reasoning is a challenging task that requires understanding both
    natural language questions and structured tabular data. Large language models
    (LLMs) have shown impressive capabilities in natural language understanding and
    generation, but they often struggle with large tables due to their limited input
    length. In this paper, we propose TabSQLify, a novel method that leverages text-to-SQL
    generation to decompose tables into smaller and relevant sub-tables, containing
    only essential information for answering questions or verifying statements, before
    performing the reasoning task. In our comprehensive evaluation on four challenging
    datasets, our approach demonstrates comparable or superior performance compared
    to prevailing methods reliant on full tables as input. Moreover, our method can
    reduce the input context length significantly, making it more scalable and efficient
    for large-scale table reasoning applications. Our method performs remarkably well
    on the WikiTQ benchmark, achieving an accuracy of 64.7%. Additionally, on the
    TabFact benchmark, it achieves a high accuracy of 79.5%. These results surpass
    other LLM-based baseline models on gpt-3.5-turbo (chatgpt). TabSQLify can reduce
    the table size significantly alleviating the computational load on LLMs when handling
    large tables without compromising performance.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Extraction
  authors:
  - dblp_id: https://dblp.org/pid/217/3625
    emails: '****@ualberta.ca'
    first_name: Md
    google_scholar_id: https://scholar.google.com/citations?user=WixxaYsAAAAJ&hl=en&oi=ao
    homepage: https://mahadi-nahid.github.io
    institution: University of Alberta
    last_name: Nahid
    middle_name: Mahadi Hasan
    name: Md Mahadi Hasan Nahid
    orcid: https://orcid.org/0000-0002-8888-9523
    semantic_scholar_id: https://www.semanticscholar.org/author/Md-Mahadi-Hasan-Nahid/31142088
    username: ~Md_Mahadi_Hasan_Nahid1
  - dblp_id: https://dblp.org/pid/r/DRafiei
    emails: '****@ualberta.ca'
    first_name: Davood
    google_scholar_id: https://scholar.google.com.tw/citations?user=lNxSDIwAAAAJ
    homepage: https://webdocs.cs.ualberta.ca/~drafiei/
    institution: University of Alberta
    last_name: Rafiei
    name: Davood Rafiei
    username: ~Davood_Rafiei1
  decision: toMainConference
  end_page: 6307
  file: 651.pdf
  id: 651
  num_pages: 13
  openreview_id: nmX0MjIs2H
  pdf_file: ea9a23b929511711fc61e328866f4db4a6929893.pdf
  start_page: 6295
  title: 'TabSQLify: Enhancing Reasoning Capabilities of LLMs Through Table Decomposition'
- abstract: Label projection, which involves obtaining translated labels and texts
    jointly, is essential for leveraging machine translation to facilitate cross-lingual
    transfer in structured prediction tasks. Prior research exploring label projection
    often compromise translation accuracy by favoring simplified label translation
    or relying solely on word-level alignments. In this paper, we introduce a novel
    label projection approach, CLaP, which translates text to the target language
    and performs *contextual translation* on the labels using the translated text
    as the context, ensuring better accuracy for the translated labels. We leverage
    instruction-tuned language models with multilingual capabilities as our contextual
    translator, imposing the constraint of the presence of translated labels in the
    translated text via instructions. We benchmark CLaP with other label projection
    techniques on zero-shot cross-lingual transfer across 39 languages on two representative
    structured prediction tasks - event argument extraction (EAE) and named entity
    recognition (NER), showing over 2.4 F1 improvement for EAE and 1.4 F1 improvement
    for NER. We further explore the applicability of CLaP on ten extremely low-resource
    languages to showcase its potential for cross-lingual structured prediction.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Extraction
  authors:
  - dblp_id: https://dblp.org/pid/209/9604
    emails: '****@gmail.com'
    first_name: Tanmay
    google_scholar_id: https://scholar.google.com/citations?user=8OVA7ucAAAAJ&hl=en
    homepage: https://tanmayparekh.github.io/
    last_name: Parekh
    name: Tanmay Parekh
    username: ~Tanmay_Parekh2
  - dblp_id: https://dblp.org/pid/206/7272
    emails: '****@isi.edu'
    first_name: I-Hung
    google_scholar_id: https://scholar.google.com/citations?user=OtSSwJgAAAAJ&hl
    homepage: https://ihungalexhsu.github.io/
    last_name: Hsu
    name: I-Hung Hsu
    semantic_scholar_id: https://www.semanticscholar.org/author/I-Hung-Hsu/34809425
    username: ~I-Hung_Hsu1
  - dblp_id: https://dblp.org/pid/24/255
    emails: '****@illinois.edu'
    first_name: Kuan-Hao
    google_scholar_id: https://scholar.google.com/citations?user=PIWnCdYAAAAJ
    homepage: https://khhuang.me
    institution: University of Illinois Urbana-Champaign
    last_name: Huang
    name: Kuan-Hao Huang
    semantic_scholar_id: https://www.semanticscholar.org/author/Kuan-Hao-Huang/3137324
    username: ~Kuan-Hao_Huang1
  - dblp_id: https://dblp.org/pid/18/2428
    emails: '****@kwchang.net'
    first_name: Kai-Wei
    google_scholar_id: https://scholar.google.com/citations?user=fqDBtzYAAAAJ&hl=en
    homepage: http://kwchang.net
    institution: University of California, Los Angeles
    last_name: Chang
    name: Kai-Wei Chang
    semantic_scholar_id: https://www.semanticscholar.org/author/Kai-Wei-Chang/2782886
    username: ~Kai-Wei_Chang1
  - dblp_id: https://dblp.org/pid/117/4036
    emails: '****@cs.ucla.edu'
    first_name: Nanyun
    google_scholar_id: https://scholar.google.com/citations?user=XxRXvX0AAAAJ&hl=en
    homepage: http://vnpeng.net/
    institution: University of California, Los Angeles
    last_name: Peng
    name: Nanyun Peng
    semantic_scholar_id: https://www.semanticscholar.org/author/Nanyun-Peng/3157053
    username: ~Nanyun_Peng1
  decision: toMainConference
  end_page: 6327
  file: 657.pdf
  id: 657
  num_pages: 20
  openreview_id: L5tQSOHNBt
  pdf_file: 0a125ec00fb12058a87fc489eca3b5f0c4a34609.pdf
  start_page: 6308
  title: Contextual Label Projection for Cross-Lingual Structured Prediction
- abstract: Social media is an easy-to-access platform providing timely updates about
    societal trends and events. Discussions regarding epidemic-related events such
    as infections, symptoms, and social interactions can be crucial for informing
    policymaking during epidemic outbreaks. In our work, we pioneer exploiting Event
    Detection (ED) for better preparedness and early warnings of any upcoming epidemic
    by developing a framework to extract and analyze epidemic-related events from
    social media posts. To this end, we curate an epidemic event ontology comprising
    seven disease-agnostic event types and construct a Twitter dataset SPEED with
    human-annotated events focused on the COVID-19 pandemic. Experimentation reveals
    how ED models trained on COVID-based SPEED can effectively detect epidemic events
    for three unseen epidemics of Monkeypox, Zika, and Dengue; while models trained
    on existing ED datasets fail miserably. Furthermore, we show that reporting sharp
    increases in the extracted events by our framework can provide warnings 4-9 weeks
    earlier than the WHO epidemic declaration for Monkeypox. This utility of our framework
    lays the foundations for better preparedness against emerging epidemics.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Extraction
  authors:
  - dblp_id: https://dblp.org/pid/209/9604
    emails: '****@gmail.com'
    first_name: Tanmay
    google_scholar_id: https://scholar.google.com/citations?user=8OVA7ucAAAAJ&hl=en
    homepage: https://tanmayparekh.github.io/
    last_name: Parekh
    name: Tanmay Parekh
    username: ~Tanmay_Parekh2
  - emails: '****@g.ucla.edu'
    first_name: Anh
    homepage: https://anhmac.com/
    last_name: Mac
    name: Anh Mac
    username: ~Anh_Mac1
  - emails: '****@g.ucla.edu'
    first_name: Jiarui
    last_name: Yu
    name: Jiarui Yu
    username: ~Jiarui_Yu2
  - emails: '****@g.ucla.edu'
    first_name: Yuxuan
    last_name: Dong
    name: Yuxuan Dong
    username: ~Yuxuan_Dong3
  - emails: '****@gmail.com'
    first_name: Syed
    homepage: http://rsyed0.github.io
    last_name: Shahriar
    name: Syed Shahriar
    username: ~Syed_Shahriar1
  - emails: '****@g.ucla.edu'
    first_name: Bonnie
    last_name: Liu
    name: Bonnie Liu
    username: ~Bonnie_Liu1
  - emails: '****@gmail.com'
    first_name: Eric
    last_name: Yang
    middle_name: J
    name: Eric J Yang
    username: ~Eric_J_Yang1
  - dblp_id: https://dblp.org/pid/24/255
    emails: '****@illinois.edu'
    first_name: Kuan-Hao
    google_scholar_id: https://scholar.google.com/citations?user=PIWnCdYAAAAJ
    homepage: https://khhuang.me
    institution: University of Illinois Urbana-Champaign
    last_name: Huang
    name: Kuan-Hao Huang
    semantic_scholar_id: https://www.semanticscholar.org/author/Kuan-Hao-Huang/3137324
    username: ~Kuan-Hao_Huang1
  - dblp_id: https://dblp.uni-trier.de/pid/w/WeiWang.html
    emails: '****@cs.ucla.edu'
    first_name: Wei
    google_scholar_id: https://scholar.google.com/citations?user=UedS9LQAAAAJ&hl=en
    homepage: http://www.cs.ucla.edu
    institution: University of California, Los Angeles
    last_name: Wang
    name: Wei Wang
    orcid: https://orcid.org/0000-0002-8180-2886
    username: ~Wei_Wang13
  - dblp_id: https://dblp.org/pid/117/4036
    emails: '****@cs.ucla.edu'
    first_name: Nanyun
    google_scholar_id: https://scholar.google.com/citations?user=XxRXvX0AAAAJ&hl=en
    homepage: http://vnpeng.net/
    institution: University of California, Los Angeles
    last_name: Peng
    name: Nanyun Peng
    semantic_scholar_id: https://www.semanticscholar.org/author/Nanyun-Peng/3157053
    username: ~Nanyun_Peng1
  - dblp_id: https://dblp.org/pid/18/2428
    emails: '****@kwchang.net'
    first_name: Kai-Wei
    google_scholar_id: https://scholar.google.com/citations?user=fqDBtzYAAAAJ&hl=en
    homepage: http://kwchang.net
    institution: University of California, Los Angeles
    last_name: Chang
    name: Kai-Wei Chang
    semantic_scholar_id: https://www.semanticscholar.org/author/Kai-Wei-Chang/2782886
    username: ~Kai-Wei_Chang1
  decision: toMainConference
  end_page: 6353
  file: 658.pdf
  id: 658
  num_pages: 26
  openreview_id: WosyUCYbwD
  pdf_file: 11f3af20ce5690790bcd925a8a81d33bea14e562.pdf
  start_page: 6328
  title: Event Detection from Social Media for Epidemic Prediction
- abstract: 'Chain-of-thought (CoT) has impressively unlocked the reasoning potential
    of large language models (LLMs). Yet, it falls short when tackling problems that
    require multiple reasoning steps. This limitation arises from the complex nature
    of multi-step reasoning processes: later stages often depend not only on the immediately
    preceding step, but also on the results from several steps earlier. Such complexities
    indicate the reasoning process is naturally a graph. The almost linear structure
    of CoT, however, struggles to capture this complex reasoning graph. To address
    this challenge, we propose Residual Connection Prompting (ResPrompt), a new prompting
    strategy that advances multi-step reasoning in LLMs. The core of our idea is to
    reconstruct the reasoning graph within prompts. We achieve this by integrating
    necessary connections--links present in reasoning graph but missing in the linear
    CoT flow--into the prompts. Termed ``residual connections", these links can transform
    linear CoT into the complex reasoning graphs that multi-step problems entail.
    On benchmarks across math, sequential, and commonsense domains, ResPrompt demonstrates
    clear improvements in multi-step reasoning compared with CoT. Through extensive
    ablation studies and analyses, we pinpoint how to effectively build residual connections
    and also identify situations where it might be unnecessary.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Question Answering
  authors:
  - dblp_id: https://dblp.org/pid/08/237-2
    emails: '****@cs.ucla.edu'
    first_name: Song
    google_scholar_id: https://scholar.google.com/citations?user=SjbhMQEAAAAJ&hl=en&oi=ao
    homepage: https://songjiang0909.github.io/
    last_name: Jiang
    name: Song Jiang
    semantic_scholar_id: https://www.semanticscholar.org/author/Song-Jiang/2249954878
    username: ~Song_Jiang1
  - dblp_id: https://dblp.org/pid/159/1512
    emails: '****@meta.com'
    first_name: Zahra
    google_scholar_id: https://scholar.google.com/citations?user=S4ilHdMAAAAJ&hl=en
    homepage: https://sites.google.com/view/zshakeri/home?authuser=1
    last_name: Shakeri
    name: Zahra Shakeri
    username: ~Zahra_Shakeri1
  - dblp_id: https://dblp.org/pid/187/7613
    emails: '****@gmail.com'
    first_name: Aaron
    google_scholar_id: https://scholar.google.com/citations?user=e7nY9oMAAAAJ&hl=en
    homepage: https://aarzchan.com/
    institution: Meta AI
    last_name: Chan
    name: Aaron Chan
    semantic_scholar_id: https://www.semanticscholar.org/author/Aaron-Chan/2114015857
    username: ~Aaron_Chan1
  - dblp_id: https://dblp.org/pid/21/8577
    emails: '****@gmail.com'
    first_name: Maziar
    google_scholar_id: https://scholar.google.com/citations?user=bc_N2-oAAAAJ&hl=en
    homepage: https://sites.google.com/view/maziar
    institution: Meta
    last_name: Sanjabi
    name: Maziar Sanjabi
    username: ~Maziar_Sanjabi1
  - emails: '****@gmail.com'
    first_name: Hamed
    google_scholar_id: https://scholar.google.com/citations?user=4pKOL5gAAAAJ&hl=en&oi=ao
    institution: Facebook
    last_name: Firooz
    name: Hamed Firooz
    username: ~Hamed_Firooz1
  - dblp_id: https://dblp.org/pid/61/3251
    emails: '****@ieee.org'
    first_name: Yinglong
    institution: Meta
    last_name: Xia
    name: Yinglong Xia
    orcid: https://orcid.org/0000-0002-8155-5440
    username: ~Yinglong_Xia1
  - emails: '****@meta.com'
    first_name: Bugra
    homepage: https://bugra.github.io/
    institution: New York University and Bilkent University
    last_name: Akyildiz
    name: Bugra Akyildiz
    username: ~Bugra_Akyildiz1
  - dblp_id: https://dblp.org/pid/37/3868
    emails: '****@cs.ucla.edu'
    first_name: Yizhou
    google_scholar_id: https://scholar.google.com.tw/citations?user=TQgOjK0AAAAJ
    homepage: http://web.cs.ucla.edu/~yzsun/
    institution: University of California, Los Angeles
    last_name: Sun
    name: Yizhou Sun
    semantic_scholar_id: https://www.semanticscholar.org/author/Yizhou-Sun/2109461904
    username: ~Yizhou_Sun1
  - dblp_id: https://dblp.org/pers/l/Li:Jinchao.html
    emails: '****@ucla.edu'
    first_name: Jinchao
    institution: Facebook
    last_name: Li
    name: Jinchao Li
    username: ~Jinchao_Li1
  - dblp_id: https://dblp.org/pid/33/8610
    emails: '****@fb.com'
    first_name: Qifan
    google_scholar_id: https://scholar.google.com/citations?user=LrSyLosAAAAJ&hl=en
    homepage: https://wqfcr.github.io/
    institution: Meta AI
    last_name: Wang
    name: Qifan Wang
    username: ~Qifan_Wang1
  - dblp_id: https://dblp.org/pid/15/3724
    emails: '****@live.com'
    first_name: Asli
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=aLHWnHsAAAAJ&view_op=list_works&alert_preview_top_rm=2&sortby=pubdate
    homepage: https://asli.us
    institution: 'FAIR '
    last_name: Celikyilmaz
    name: Asli Celikyilmaz
    semantic_scholar_id: https://www.semanticscholar.org/author/1709797
    username: ~Asli_Celikyilmaz1
  decision: toMainConference
  end_page: 6379
  file: 659.pdf
  id: 659
  num_pages: 26
  openreview_id: 7EoAtQrX7L
  pdf_file: b7dea5bdba743bf5a6a9287bbbb686c5f15b1b65.pdf
  start_page: 6354
  title: 'RESPROMPT: Residual Connection Prompting Advances Multi-Step Reasoning in
    Large Language Models'
- abstract: Byte-pair encoding (BPE) has become the default subword tokeniser in language
    models (LMs), allowing the representation of an infinite space of text with a
    finite set of units. Yet, BPE training is unsupervised, receiving no explicit
    information about a language's morphology. This results in a subword vocabulary
    wherein many units are a concatenation of partial morphemes, preventing their
    formation as tokens. This, in turn, causes consistent intra-word patterns to be
    displayed inconsistently to downstream models, and bloats the vocabulary, hence
    requiring unnecessary embedding storage. In this paper, we address this issue
    by identifying blameworthy BPE merges and removing the resulting subwords from
    the BPE vocabulary, without impeding further use of merges that relied on them.
    We find that our method, BPE-knockout, is effective at making BPE's segmentation
    positions adhere better to derivational and compound boundaries in English, Dutch
    and German, and improves token-based tasks in Dutch RoBERTa models, indicating
    that a tokeniser's adherence to morphology impacts downstream models. We demonstrate
    the latter not only by training LMs from scratch, but also by continuing the pre-training
    of existing LMs. This proves promising, showing that suboptimal tokenisers can
    be remedied whilst salvaging training cost of downstream LMs.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Phonology, Morphology and Word Segmentation
  authors:
  - emails: '****@kuleuven.be'
    first_name: Thomas
    homepage: https://bauwenst.github.io/
    institution: KU Leuven
    last_name: Bauwens
    name: Thomas Bauwens
    orcid: https://orcid.org/0009-0009-6115-8956
    username: ~Thomas_Bauwens1
  - dblp_id: https://dblp.org/pid/245/8739
    emails: '****@kuleuven.be'
    first_name: Pieter
    google_scholar_id: https://scholar.google.be/citations?user=MVjJgxAAAAAJ
    homepage: https://people.cs.kuleuven.be/~pieter.delobelle/
    institution: KU Leuven, KU Leuven
    last_name: Delobelle
    name: Pieter Delobelle
    orcid: https://orcid.org/0000-0001-5911-5310
    semantic_scholar_id: https://www.semanticscholar.org/author/150258834
    username: ~Pieter_Delobelle1
  decision: toMainConference
  end_page: 6402
  file: 663.pdf
  id: 663
  num_pages: 23
  openreview_id: fg3cdmWD5G
  pdf_file: 237dd06d28d931b62c74f7857594a9c3131b67a8.pdf
  start_page: 6380
  title: 'BPE-knockout: Pruning Pre-existing BPE Tokenisers with Backwards-compatible
    Morphological Semi-supervision'
- abstract: In-context learning (ICL) has become one of the most popular learning
    paradigms. While there is a growing body of literature focusing on prompt engineering,
    there is a lack of systematic analysis comparing the effects of prompt techniques
    across different models and tasks. To address this, we present a comprehensive
    prompt analysis based on sensitivity. Our analysis reveals that sensitivity is
    an unsupervised proxy for model performance, as it exhibits a strong negative
    correlation with accuracy. We use gradient-based saliency scores to empirically
    demonstrate how different prompts affect the relevance of input tokens to the
    output, resulting in different levels of sensitivity. Furthermore, we introduce
    sensitivity-aware decoding which incorporates sensitivity estimation as a penalty
    term in the standard greedy decoding. We show that this approach is particularly
    helpful when information in the input is scarce. Our work provides a fresh perspective
    on the analysis of prompts, and contributes to a better understanding of the mechanism
    of ICL.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - emails: '****@tu-darmstadt.de'
    first_name: Sheng
    last_name: Lu
    name: Sheng Lu
    orcid: https://orcid.org/0000-0002-8696-4024
    username: ~Sheng_Lu1
  - dblp_id: https://dblp.org/pid/188/3411
    emails: '****@proton.me'
    first_name: Hendrik
    google_scholar_id: https://scholar.google.de/citations?user=UzUVKbAAAAAJ&hl=de&oi=ao
    homepage: https://hendrikschuff.de/
    institution: "Technische Universit\xE4t Darmstadt"
    last_name: Schuff
    name: Hendrik Schuff
    semantic_scholar_id: https://www.semanticscholar.org/author/Hendrik-Schuff/7959237
    username: ~Hendrik_Schuff1
  - dblp_id: https://dblp.org/pid/85/6201
    emails: '****@tu-darmstadt.de'
    first_name: Iryna
    google_scholar_id: https://scholar.google.com.tw/citations?user=t3A39e8AAAAJ
    homepage: https://www.informatik.tu-darmstadt.de/ukp/ukp_home/staff_ukp/prof_dr_iryna_gurevych/index.en.jsp
    institution: Mohamed bin Zayed University of Artificial Intelligence and Technical
      University of Darmstadt
    last_name: Gurevych
    name: Iryna Gurevych
    username: ~Iryna_Gurevych1
  decision: toMainConference
  end_page: 6426
  file: 665.pdf
  id: 665
  num_pages: 24
  openreview_id: 4OEv19sduy
  pdf_file: 671494838eb0cddffb8abf664c67db8d030ee8c1.pdf
  start_page: 6403
  title: How are Prompts Different in Terms of Sensitivity?
- abstract: Generating high-quality responses is a key challenge for any open domain
    dialogue systems. However, even though there exist a variety of quality dimensions
    especially designed for dialogue evaluation (e.g., coherence and diversity scores),
    current dialogue systems rarely utilize them to guide the response generation
    during training. To alleviate this issue, we propose LSTDial (Long- and Short-Term
    Dialogue), a novel two-stage framework which generates and utilizes conversation
    evaluation as explicit feedback during training. Specifically, we fine-tune pre-trained
    dialogue systems through using turn-level quality feedback in the first stage
    and further train ever-improving dialogue agents through using dialogue-level
    quality feedback in the second stage. By using our approach on dialogue systems,
    capable of enabling dialogue generation with both short-term capabilities (generating
    more fluent, relevant and varied responses at the turn-level) and long-term capabilities
    (generating more coherent, engaging and informative responses at the dialogue-level).
    We implement LSTDial on four strong baseline models and experiment with two open-domain
    dialogue datasets. Experimental results show that LSTDial achieves significant
    improvement, enabling to generate better dialogue responses in terms of both human
    and automatic evaluation.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Dialogue and Interactive Systems
  authors:
  - emails: '****@hnu.edu.cn'
    first_name: Guanghui
    homepage: https://github.com/18166035475
    institution: Hunan University
    last_name: Ye
    name: Guanghui Ye
    username: ~Guanghui_Ye1
  - emails: '****@hnu.edu.cn'
    first_name: Huan
    institution: Hunan University
    last_name: Zhao
    name: Huan Zhao
    orcid: https://orcid.org/0000-0001-6286-5868
    username: ~Huan_Zhao4
  - emails: '****@hnu.edu.cn'
    first_name: Zixing
    homepage: https://sites.google.com/view/zixingzhang
    institution: Hunan University
    last_name: Zhang
    name: Zixing Zhang
    username: ~Zixing_Zhang1
  - emails: '****@hnu.edu.cn'
    first_name: Xupeng
    institution: Hunan University
    last_name: Zha
    name: Xupeng Zha
    orcid: https://orcid.org/0000-0001-6928-9998
    username: ~Xupeng_Zha1
  - dblp_id: https://dblp.org/pid/164/4279
    emails: '****@jnu.edu.cn'
    first_name: Zhihua
    google_scholar_id: https://scholar.google.com/citations?user=TXr4oi4AAAAJ
    last_name: Jiang
    name: Zhihua Jiang
    orcid: https://orcid.org/0000-0002-4216-106X
    semantic_scholar_id: https://www.semanticscholar.org/author/47653923
    username: ~Zhihua_Jiang1
  decision: toMainConference
  end_page: 6441
  file: 666.pdf
  id: 666
  num_pages: 15
  openreview_id: sjouaQcueI
  pdf_file: e39687d1bf0b3c7c8f5d00a63448e1f90722b0d8.pdf
  start_page: 6427
  title: 'LSTDial: Enhancing Dialogue Generation via Long- and Short-Term Measurement
    Feedback'
- abstract: "Large Language Models (LLMs) have demonstrated remarkable generative\
    \ abilities, but can they judge the quality of their own generations and self-improve?\n\
    A popular concept, referred to as *self-refinement*, postulates that LLMs can\
    \ detect and correct the errors in their generations when asked to do so. However,\
    \ recent empirical evidence points in the opposite direction, suggesting that\
    \ LLMs often struggle to accurately identify errors when reasoning is involved.\
    \ To address this, we propose a reasoning with a refinement strategy called *ART:\
    \ Ask, Refine, and Trust*, which *asks* necessary questions to decide when an\
    \ LLM should *refine* its output, and uses it to affirm or deny *trust* in its\
    \ refinement by ranking the refinement and the initial prediction. On two multistep\
    \ reasoning tasks of mathematical word problems (GSM8K) and question answering\
    \ (StrategyQA), *ART* achieves a performance gain of $+5$ points over self-refinement\
    \ baselines, while using a much smaller model as the decision maker. \nWe believe\
    \ that *ART* with smaller models, making refinement decisions can be a cost-effective\
    \ alternative to fine-tuning LLMs."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Learning for NLP
  authors:
  - emails: '****@inf.ethz.ch'
    first_name: Kumar
    google_scholar_id: https://scholar.google.com/citations?user=rR2qicwAAAAJ&hl=en
    homepage: https://kumar-shridhar.github.io/
    last_name: Shridhar
    name: Kumar Shridhar
    username: ~Kumar_Shridhar1
  - dblp_id: https://dblp.org/pid/210/0890
    emails: '****@mail.mcgill.ca'
    first_name: Koustuv
    google_scholar_id: https://scholar.google.com/citations?user=9P9QcckAAAAJ&hl=en
    homepage: https://koustuvsinha.com/
    institution: Meta (FAIR)
    last_name: Sinha
    name: Koustuv Sinha
    semantic_scholar_id: https://www.semanticscholar.org/author/Koustuv-Sinha/40910779
    username: ~Koustuv_Sinha1
  - emails: '****@gmail.com'
    first_name: Andrew
    google_scholar_id: https://scholar.google.com/citations?user=v1Frtb0AAAAJ&hl=en
    last_name: Cohen
    name: Andrew Cohen
    username: ~Andrew_Cohen4
  - dblp_id: https://dblp.org/pid/185/5529
    emails: '****@gmail.com'
    first_name: Tianlu
    google_scholar_id: https://scholar.google.com/citations?user=inzQqX8AAAAJ&hl=en
    homepage: https://tianlu-wang.github.io/
    institution: Meta
    last_name: Wang
    name: Tianlu Wang
    username: ~Tianlu_Wang1
  - emails: '****@fb.com'
    first_name: Ping
    google_scholar_id: https://scholar.google.com/citations?user=-V7TJhwAAAAJ&hl=en
    homepage: https://yuping1.wixsite.com/mysite
    institution: Facebook
    last_name: Yu
    name: Ping Yu
    username: ~Ping_Yu2
  - dblp_id: https://dblp.org/pid/199/1748
    emails: '****@gmail.com'
    first_name: Ramakanth
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=8xJzzn8AAAAJ
    homepage: http://rama-kanth.com
    last_name: Pasunuru
    name: Ramakanth Pasunuru
    semantic_scholar_id: https://www.semanticscholar.org/author/Ramakanth-Pasunuru/10721120
    username: ~Ramakanth_Pasunuru2
  - dblp_id: https://dblp.org/pid/86/10440.html
    emails: '****@inf.ethz.ch'
    first_name: Mrinmaya
    google_scholar_id: https://scholar.google.com/citations?user=Tpp9ZjoAAAAJ&hl=en
    homepage: https://sites.google.com/site/mrinsachan/
    institution: Swiss Federal Institute of Technology
    last_name: Sachan
    name: Mrinmaya Sachan
    username: ~Mrinmaya_Sachan3
  - dblp_id: https://dblp.org/pid/29/6977.html
    emails: '****@gmail.com'
    first_name: Jason
    google_scholar_id: https://scholar.google.com/citations?user=lMkTx0EAAAAJ&hl=en
    homepage: http://www.jaseweston.com
    institution: New York University and Facebook
    last_name: Weston
    middle_name: E
    name: Jason E Weston
    username: ~Jason_E_Weston1
  - dblp_id: https://dblp.org/pid/15/3724
    emails: '****@live.com'
    first_name: Asli
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=aLHWnHsAAAAJ&view_op=list_works&alert_preview_top_rm=2&sortby=pubdate
    homepage: https://asli.us
    institution: 'FAIR '
    last_name: Celikyilmaz
    name: Asli Celikyilmaz
    semantic_scholar_id: https://www.semanticscholar.org/author/1709797
    username: ~Asli_Celikyilmaz1
  decision: toMainConference
  end_page: 6453
  file: 669.pdf
  id: 669
  num_pages: 12
  openreview_id: FPLAbjPoli
  pdf_file: cd6f078d32896c122467fbb6717b2ad8262ae481.pdf
  start_page: 6442
  title: 'The ART of LLM Refinement: Ask, Refine, and Trust'
- abstract: 'Recently, one popular alternative in Multilingual NMT (MNMT) is modularized
    MNMT that has both language-specific encoders and decoders. However, due to the
    absence of layer-sharing, the modularized MNMT failed to produce satisfactory
    language-independent (Interlingua) features, leading to performance degradation
    in zero-shot translation. To address this issue, a solution was proposed to share
    the top of language-specific encoder layers, enabling the successful generation
    of interlingua features.  Nonetheless, it should be noted that this sharing structure
    does not guarantee the explicit propagation of language-specific features to their
    respective language-specific decoders.  Consequently, to overcome this challenge,
    we present our modularized MNMT approach, where a modularized encoder is divided
    into three distinct encoder modules based on different sharing criteria: (1) source
    language-specific ($Enc_{s}$); (2) universal ($Enc_{all}$); (3) target language-specific
    ($Enc_{t}$). By employing these sharing strategies, $Enc_{all}$ propagates the
    interlingua features, after which $Enc_{t}$ propagates the target language-specific
    features to the language-specific decoders. Additionally, we suggest the Denoising
    Bi-path Autoencoder (DBAE) to fortify the Denoising Autoencoder (DAE) by leveraging
    $Enc_{t}$. For experimental purposes, our training corpus comprises both En-to-Any
    and Any-to-En directions. We adjust the size of our corpus to simulate both balanced
    and unbalanced settings. Our method demonstrates an improved average BLEU score
    by "+2.90" in En-to-Any directions and by "+3.06" in zero-shot compared to other
    MNMT baselines.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Translation
  authors:
  - dblp_id: https://dblp.org/pid/05/7011
    emails: '****@gmail.com'
    first_name: Sungjun
    institution: Samsung
    last_name: Lim
    name: Sungjun Lim
    semantic_scholar_id: https://www.semanticscholar.org/author/Sungjun-Lim/47127808
    username: ~Sungjun_Lim1
  - dblp_id: https://dblp.org/pid/58/3881
    emails: '****@gmail.com'
    first_name: Yoonjung
    institution: Samsung
    last_name: Choi
    name: Yoonjung Choi
    username: ~Yoonjung_Choi1
  - emails: '****@gmail.com'
    first_name: Sangha
    google_scholar_id: https://scholar.google.com/citations?user=2RSJcg0AAAAJ
    last_name: Kim
    name: Sangha Kim
    username: ~Sangha_Kim1
  decision: toMainConference
  end_page: 6469
  file: 671.pdf
  id: 671
  num_pages: 16
  openreview_id: DDtQeFnz2K
  pdf_file: 413c6b2976721b59690ed4ee014fc97dc7c8a8b0.pdf
  start_page: 6454
  title: Modularized Multilingual NMT with Fine-grained Interlingua
- abstract: "Analogy-making is central to human cognition, allowing us to adapt to\
    \ novel situations \u2013 an ability that current AI systems still lack. \nMost\
    \ analogy datasets today focus on simple analogies (e.g., word analogies); datasets\
    \ including complex types of analogies are typically manually curated and very\
    \ small. We believe that this holds back progress in computational analogy.\n\n\
    In this work, we design a data generation pipeline, ParallelPARC (Parallel Paragraph\
    \ Creator) leveraging state-of-the-art Large Language Models (LLMs) to create\
    \ complex, paragraph-based analogies, as well as distractors, both simple and\
    \ challenging. We demonstrate our pipeline and create ProPara-Logy, a dataset\
    \ of analogies between scientific processes. We publish a gold-set, validated\
    \ by humans, and a silver-set, generated automatically. We test LLMs\u2019 and\
    \ humans\u2019 analogy recognition in binary and multiple-choice settings, and\
    \ found that humans outperform the best models (\u223C13% gap) after a light supervision.\
    \ We demonstrate that our silver-set is useful for training models. Lastly, we\
    \ show challenging distractors confuse LLMs, but not humans. We hope our pipeline\
    \ will encourage research in this emerging field."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - dblp_id: https://dblp.org/pid/331/8143
    emails: '****@mail.huji.ac.il'
    first_name: Oren
    google_scholar_id: https://scholar.google.com/citations?user=Ns91gt4AAAAJ&hl=en&authuser=1
    institution: Hebrew University of Jerusalem
    last_name: Sultan
    name: Oren Sultan
    semantic_scholar_id: https://www.semanticscholar.org/author/Oren-Sultan/2188739803
    username: ~Oren_Sultan1
  - dblp_id: https://dblp.org/pid/277/7042
    emails: '****@mail.huji.ac.il'
    first_name: Yonatan
    google_scholar_id: https://scholar.google.com/citations?user=P9Fpf4sAAAAJ&hl=en
    homepage: https://yonatanbitton.github.io/
    institution: Google
    last_name: Bitton
    name: Yonatan Bitton
    semantic_scholar_id: https://www.semanticscholar.org/author/Yonatan-Bitton/1938499056
    username: ~Yonatan_Bitton1
  - dblp_id: https://dblp.org/pid/283/5799
    emails: '****@mail.huji.ac.il'
    first_name: Ron
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=mhdOCG4AAAAJ
    last_name: Yosef
    name: Ron Yosef
    semantic_scholar_id: https://www.semanticscholar.org/author/Ron-Yosef/2047122977
    username: ~Ron_Yosef1
  - dblp_id: https://dblp.org/pid/02/2672.html
    emails: '****@cs.huji.ac.il'
    first_name: Dafna
    google_scholar_id: https://scholar.google.com.tw/citations?user=AgyW_90AAAAJ
    homepage: http://hyadatalab.com/
    institution: Hebrew University of Jerusalem
    last_name: Shahaf
    name: Dafna Shahaf
    orcid: https://orcid.org/0000-0003-3261-0818
    semantic_scholar_id: https://www.semanticscholar.org/author/Dafna-Shahaf/1805894
    username: ~Dafna_Shahaf1
  decision: toMainConference
  end_page: 6494
  file: 677.pdf
  id: 677
  num_pages: 25
  openreview_id: kzumynv7Qd
  pdf_file: f8f5c53909d1e98b149657d94ffc3cd0245b193f.pdf
  start_page: 6470
  title: 'ParallelPARC: A Scalable Pipeline for Generating Natural-Language Analogies'
- abstract: 'Long document summarization systems are critical for domains with lengthy
    and jargon-laden text, yet they present significant challenges to researchers
    and developers with limited computing resources. Existing solutions mainly focus
    on efficient attentions or divide-and-conquer strategies. The former reduces theoretical
    time complexity, but is still memory-heavy. The latter methods sacrifice global
    context, leading to uninformative and incoherent summaries. This work aims to
    leverage the memory-efficient nature of divide-and-conquer methods while preserving
    global context. Concretely, our framework AWESOME uses two novel mechanisms: (1)
    External memory mechanisms track previously encoded document segments and their
    corresponding summaries, to enhance global document understanding and summary
    coherence. (2) Global salient content is further identified beforehand to augment
    each document segment to support its summarization. Extensive experiments on diverse
    genres of text, including government reports, meeting transcripts, screenplays,
    scientific papers, and novels, show that AWESOME produces summaries with improved
    informativeness, faithfulness, and coherence than competitive baselines on longer
    documents, while having a smaller GPU memory footprint.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Summarization
  authors:
  - dblp_id: https://dblp.org/pid/227/2764
    emails: '****@umich.edu'
    first_name: Shuyang
    homepage: http://shuyangcao.github.io/
    institution: University of Michigan - Ann Arbor
    last_name: Cao
    name: Shuyang Cao
    semantic_scholar_id: https://www.semanticscholar.org/author/Shuyang-Cao/6333082
    username: ~Shuyang_Cao1
  - dblp_id: https://dblp.org/pid/49/3800-8
    emails: '****@umich.edu'
    first_name: Lu
    google_scholar_id: https://scholar.google.com/citations?user=uczqEdUAAAAJ&hl=en
    homepage: https://web.eecs.umich.edu/~wangluxy/
    institution: University of Michigan
    last_name: Wang
    name: Lu Wang
    username: ~Lu_Wang9
  decision: toMainConference
  end_page: 6511
  file: 678.pdf
  id: 678
  num_pages: 17
  openreview_id: gs8dSytejS
  pdf_file: 6fc31768fec814165acd752779d691d2d88d2863.pdf
  start_page: 6495
  title: 'AWESOME: GPU Memory-constrained Long Document Summarization using Memory
    Mechanism and Global Salient Content'
- abstract: 'The use of words to convey speaker''s intent is traditionally distinguished
    from the  `mention'' of words for quoting what someone said, or pointing out properties
    of a word.  Here we show that computationally modeling this use-mention distinction
    is crucial for dealing with counterspeech online.  Counterspeech that refutes
    problematic content often mentions harmful language but is not harmful itself
    (e.g., calling a vaccine dangerous is not the same as expressing disapproval of
    someone for calling vaccines dangerous). We show that even recent language models
    fail at distinguishing use from mention, and that this failure propagates to two
    key downstream tasks: misinformation and hate speech detection, resulting in censorship
    of counterspeech.  We introduce prompting mitigations that teach the use-mention
    distinction, and show they reduce these errors.  Our work highlights the importance
    of the use-mention distinction for NLP and CSS and offers ways to address it.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Computational Social Science and Cultural Analytics
  authors:
  - dblp_id: https://dblp.org/pid/218/5344
    emails: '****@stanford.edu'
    first_name: Kristina
    google_scholar_id: https://scholar.google.com/citations?user=kdbzgK4AAAAJ&hl=en
    homepage: https://kristinagligoric.github.io/
    institution: Stanford University
    last_name: Gligoric
    name: Kristina Gligoric
    username: ~Kristina_Gligoric1
  - dblp_id: https://dblp.org/pid/226/7067
    emails: '****@gmail.com'
    first_name: Myra
    google_scholar_id: https://scholar.google.com/citations?user=gaslQl8AAAAJ
    homepage: http://myracheng.github.io
    institution: Stanford University
    last_name: Cheng
    name: Myra Cheng
    username: ~Myra_Cheng1
  - emails: '****@stanford.edu'
    first_name: Lucia
    google_scholar_id: https://scholar.google.com/citations?user=dlqLZAsAAAAJ&hl=en
    last_name: Zheng
    name: Lucia Zheng
    username: ~Lucia_Zheng1
  - dblp_id: https://dblp.org/pid/219/6227
    emails: '****@stanford.edu'
    first_name: Esin
    institution: Stanford University
    last_name: Durmus
    name: Esin Durmus
    username: ~Esin_Durmus1
  - dblp_id: https://dblp.org/pid/31/985
    emails: '****@stanford.edu'
    first_name: Dan
    google_scholar_id: https://scholar.google.com/citations?user=uZg9l58AAAAJ
    homepage: http://web.stanford.edu/~jurafsky/
    institution: Stanford University
    last_name: Jurafsky
    name: Dan Jurafsky
    username: ~Dan_Jurafsky1
  decision: toMainConference
  end_page: 6529
  file: 681.pdf
  id: 681
  num_pages: 18
  openreview_id: qJCSOdCOGP
  pdf_file: 460b88ebb9d2d32edea0329d0edb1e9797368687.pdf
  start_page: 6512
  title: NLP Systems That Can't Tell Use from Mention Censor Counterspeech, but Teaching
    the Distinction Helps
- abstract: 'Pre-trained vector representations in natural language processing often
    inadvertently encode undesirable social biases. Identifying and removing unwanted
    biased information from vector representation is an evolving and significant challenge.
    Our study uniquely addresses this issue from the perspective of statistical independence,
    proposing a framework for reducing bias by transforming vector representations
    to an unbiased subspace using sufficient projection. The key to our framework
    lies in its generality: it adeptly mitigates bias across both debiasing and fairness
    tasks, and across various vector representation types, including word embeddings
    and output representations of transformer models. Importantly, we establish the
    connection between debiasing and fairness, offering theoretical guarantees and
    elucidating our algorithm''s efficacy.  Through extensive evaluation of intrinsic
    and extrinsic metrics, our method achieves superior performance in bias reduction
    while maintaining high task performance, and offers superior computational efficiency.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Ethics, Bias, and Fairness
  authors:
  - emails: '****@ualberta.ca'
    first_name: Enze
    institution: University of Alberta
    last_name: Shi
    name: Enze Shi
    username: ~Enze_Shi1
  - dblp_id: https://dblp.org/pid/59/2353
    emails: '****@ualberta.ca'
    first_name: Lei
    google_scholar_id: https://scholar.google.com/citations?user=ICUOaR4AAAAJ&hl=en
    last_name: Ding
    name: Lei Ding
    username: ~Lei_Ding6
  - dblp_id: https://dblp.org/pid/35/8525
    emails: '****@ualberta.ca'
    first_name: Linglong
    google_scholar_id: https://scholar.google.ca/citations?hl=en&user=K8iLilgAAAAJ&view_op=list_works&sortby=pubdate
    homepage: https://www.ualberta.ca/~lkong
    institution: University of Alberta
    last_name: Kong
    name: Linglong Kong
    username: ~Linglong_Kong2
  - dblp_id: https://dblp.org/pid/190/4697
    emails: '****@ualberta.ca'
    first_name: Bei
    google_scholar_id: https://scholar.google.ca/citations?user=MfOZ8G0AAAAJ&hl=en
    homepage: https://www.ualberta.ca/~bei1
    institution: University of Alberta
    last_name: Jiang
    name: Bei Jiang
    orcid: https://orcid.org/0000-0002-0033-839X
    username: ~Bei_Jiang1
  decision: toMainConference
  end_page: 6545
  file: 682.pdf
  id: 682
  num_pages: 16
  openreview_id: cqrQOTu12O
  pdf_file: 50e5a23b45425b2191b2a24da93e7d46ad34a957.pdf
  start_page: 6530
  title: 'Debiasing with Sufficient Projection: A General Theoretical Framework for
    Vector Representations'
- abstract: 'Semi-supervised dialogue summarization (SSDS) leverages model-generated
    summaries to reduce reliance on human-labeled data and improve the performance
    of summarization models. While addressing label noise, previous works on semi-supervised
    learning primarily focus on natural language understanding tasks, assuming each
    sample has a unique label. However, these methods are not directly applicable
    to SSDS, as it is a generative task, and each dialogue can be summarized in different
    ways. In this work, we propose a novel scoring approach, SiCF, which encapsulates
    three primary dimensions of summarization model quality: Semantic invariance (indicative
    of model confidence), Coverage (factual recall), and Faithfulness (factual precision).
    Using the SiCF score, we select unlabeled dialogues with high-quality generated
    summaries to train summarization models. Comprehensive experiments on three public
    datasets demonstrate the effectiveness of SiCF scores in uncertainty estimation
    and semi-supervised learning for dialogue summarization tasks. Our code is available
    at \url{https://github.com/amazon-science/summarization-sicf-score}.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Summarization
  authors:
  - dblp_id: https://dblp.org/pid/93/8352
    emails: '****@vt.edu'
    first_name: Jianfeng
    google_scholar_id: https://scholar.google.com/citations?user=_gAf96sAAAAJ&hl=en
    homepage: https://jianfenghe-vt.netlify.app/
    institution: Virginia Tech
    last_name: He
    name: Jianfeng He
    username: ~Jianfeng_He1
  - emails: '****@gmail.com'
    first_name: Hang
    google_scholar_id: https://scholar.google.com/citations?user=UxOvKVUAAAAJ&hl=en
    institution: Amazon
    last_name: Su
    name: Hang Su
    username: ~Hang_Su7
  - emails: '****@gmail.com'
    first_name: Jason
    google_scholar_id: https://scholar.google.com/citations?user=NYjfyB4AAAAJ&hl=en
    homepage: https://www.linkedin.com/in/jinglun-cai
    institution: Amazon
    last_name: Cai
    name: Jason Cai
    semantic_scholar_id: https://www.semanticscholar.org/author/Jinglun-Cai/2115670967
    username: ~Jason_Cai1
  - dblp_id: https://dblp.org/pid/205/8962
    emails: '****@gmail.com'
    first_name: Igor
    google_scholar_id: https://scholar.google.com/citations?user=TVs0lP8AAAAJ
    homepage: https://shalyminov.com
    institution: Amazon
    last_name: Shalyminov
    name: Igor Shalyminov
    orcid: https://orcid.org/0000-0001-9664-1774
    semantic_scholar_id: https://www.semanticscholar.org/author/Igor-Shalyminov/24879056
    username: ~Igor_Shalyminov1
  - dblp_id: https://dblp.org/pid/204/3381
    emails: '****@gmail.com'
    first_name: Hwanjun
    google_scholar_id: https://scholar.google.com/citations?user=Ijzuc-8AAAAJ&hl=en
    homepage: https://songhwanjun.github.io/
    institution: AWS AI Labs
    last_name: Song
    name: Hwanjun Song
    username: ~Hwanjun_Song2
  - dblp_id: https://dblp.org/pid/03/8053
    emails: '****@gmail.com'
    first_name: Saab
    google_scholar_id: https://scholar.google.de/citations?user=1tCbwIQAAAAJ&hl=en
    institution: Amazon
    last_name: Mansour
    name: Saab Mansour
    semantic_scholar_id: https://www.semanticscholar.org/author/Saab-Mansour/39674628
    username: ~Saab_Mansour1
  decision: toMainConference
  end_page: 6566
  file: 684.pdf
  id: 684
  num_pages: 21
  openreview_id: dyLIFyBjlX
  pdf_file: c8fbf270a7da97f0f229101e02590d4f3b9339fa.pdf
  start_page: 6546
  title: Semi-Supervised Dialogue Abstractive Summarization via High-Quality Pseudolabel
    Selection
- abstract: 'Despite the recent progress on scaling multilingual machine translation
    (MT) to several under-resourced African languages, accurately measuring this progress
    remains challenging, since evaluation is often performed on n-gram matching metrics
    such as BLEU, which typically show a weaker correlation with human judgments.
    Learned metrics such as COMET have higher correlation; however, the lack of evaluation
    data with human ratings for under-resourced languages, complexity of annotation
    guidelines like Multidimensional Quality Metrics (MQM), and limited language coverage
    of multilingual encoders have hampered their applicability to African languages.
    In this paper, we address these challenges by creating high-quality human evaluation
    data with simplified MQM guidelines for error detection and direct assessment
    (DA) scoring for 13 typologically diverse African languages. Furthermore, we develop
    AfriCOMET: COMET evaluation metrics for African languages by leveraging DA data
    from well-resourced languages and an African-centric multilingual encoder (AfroXLM-R)
    to create the state-of-the-art MT evaluation metrics for African languages with
    respect to Spearman-rank correlation with human judgments (0.441).'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Translation
  authors:
  - emails: '****@gmail.com'
    first_name: Jiayi
    google_scholar_id: https://scholar.google.com/citations?user=OGTpIWYAAAAJ&hl=en
    last_name: Wang
    name: Jiayi Wang
    username: ~Jiayi_Wang11
  - dblp_id: https://dblp.org/pid/230/6973
    emails: '****@gmail.com'
    first_name: David
    google_scholar_id: https://scholar.google.ca/citations?user=W9sTkS0AAAAJ&hl=en
    homepage: https://dadelani.github.io/
    last_name: Adelani
    middle_name: Ifeoluwa
    name: David Ifeoluwa Adelani
    orcid: https://orcid.org/0000-0002-0193-2083
    semantic_scholar_id: https://www.semanticscholar.org/author/D.-Adelani/2518906
    username: ~David_Ifeoluwa_Adelani1
  - dblp_id: https://dblp.org/pid/210/7863.html
    emails: '****@gmail.com'
    first_name: Sweta
    google_scholar_id: https://scholar.google.com/citations?user=Avsw9IkAAAAJ&hl=en
    homepage: https://sweta20.github.io/
    institution: "Instituto de Telecomunica\xE7\xF5es"
    last_name: Agrawal
    name: Sweta Agrawal
    semantic_scholar_id: https://www.semanticscholar.org/author/Sweta-Agrawal/5112699
    username: ~Sweta_Agrawal1
  - emails: '****@gmail.com'
    first_name: Marek
    google_scholar_id: https://scholar.google.com/citations?user=XBUX-cwAAAAJ&hl=en
    last_name: Masiak
    name: Marek Masiak
    orcid: https://orcid.org/0000-0003-1352-2441
    username: ~Marek_Masiak1
  - dblp_id: https://dblp.org/pid/72/3176
    emails: '****@unbabel.com'
    first_name: Ricardo
    google_scholar_id: https://scholar.google.com/citations?user=jf4S4tsAAAAJ&hl=en
    institution: "Instituto Superior T\xE9cnico, INESC-ID and Unbabel"
    last_name: Rei
    name: Ricardo Rei
    orcid: https://orcid.org/0000-0001-8265-1939
    semantic_scholar_id: https://www.semanticscholar.org/author/Ricardo-Rei/15631652
    username: ~Ricardo_Rei1
  - dblp_id: https://dblp.org/pid/217/4858
    emails: '****@google.com'
    first_name: Eleftheria
    google_scholar_id: https://scholar.google.com/citations?user=bxqqNFEAAAAJ&hl=el&oi=ao
    homepage: https://elbria.github.io
    institution: Google
    last_name: Briakou
    name: Eleftheria Briakou
    semantic_scholar_id: https://www.semanticscholar.org/author/Eleftheria-Briakou/40914545
    username: ~Eleftheria_Briakou1
  - dblp_id: https://dblp.org/pid/71/1827
    emails: '****@umd.edu'
    first_name: Marine
    google_scholar_id: https://scholar.google.com/citations?user=iPAX6jcAAAAJ
    homepage: http://www.cs.umd.edu/~marine/
    institution: University of Maryland, College Park
    last_name: Carpuat
    name: Marine Carpuat
    username: ~Marine_Carpuat1
  - dblp_id: https://dblp.org/pid/182/1859
    emails: '****@gmail.com'
    first_name: Xuanli
    google_scholar_id: https://scholar.google.com/citations?user=TU8t0iAAAAAJ&hl=zh-CN
    institution: University College London, University of London
    last_name: He
    name: Xuanli He
    username: ~Xuanli_He2
  - emails: '****@um5s.net.ma'
    first_name: Sofia
    google_scholar_id: https://scholar.google.com/citations?user=RUcwjNsAAAAJ&hl=en
    last_name: Bourhim
    name: Sofia Bourhim
    username: ~Sofia_Bourhim2
  - emails: '****@nwu.ac.za'
    first_name: Andiswa
    last_name: Bukula
    name: Andiswa Bukula
    orcid: https://orcid.org/0000-0002-6667-4599
    username: ~Andiswa_Bukula1
  - emails: '****@aston.ac.uk'
    first_name: Muhidin
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=wBPUTzQAAAAJ&view_op=list_works&authuser=1
    homepage: https://research.aston.ac.uk/en/persons/muhidin-mohamed
    institution: Aston University
    last_name: Mohamed
    middle_name: A.
    name: Muhidin A. Mohamed
    username: ~Muhidin_A._Mohamed1
  - emails: '****@uef.fi'
    first_name: Temitayo
    google_scholar_id: https://scholar.google.com/citations?user=eg_tAsMAAAAJ&hl=fi&oi=ao
    last_name: Olatoye
    name: Temitayo Olatoye
    orcid: https://orcid.org/0000-0001-8640-3916
    username: ~Temitayo_Olatoye1
  - emails: '****@hotmail.com'
    first_name: Tosin
    google_scholar_id: https://scholar.google.com/citations?user=uBZIR8sAAAAJ&hl=en&oi=ao
    homepage: https://scholar.google.com/citations?user=uBZIR8sAAAAJ&hl=en&oi=ao
    last_name: Adewumi
    name: Tosin Adewumi
    orcid: https://orcid.org/0000-0002-5582-2031
    semantic_scholar_id: https://www.semanticscholar.org/author/Tosin-P.-Adewumi/51221489
    username: ~Tosin_Adewumi1
  - emails: '****@ltu.se'
    first_name: Hamam
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=l2wEaVkAAAAJ&view_op=list_works&sortby=pubdate
    institution: "Lule\xE5 University of Technology "
    last_name: Mokayed
    name: Hamam Mokayed
    username: ~Hamam_Mokayed1
  - emails: '****@ieee.org'
    first_name: Christine
    institution: Fudan University
    last_name: Mwase
    name: Christine Mwase
    orcid: https://orcid.org/0000-0001-8054-7599
    username: ~Christine_Mwase1
  - emails: '****@gmail.com'
    first_name: Wangui
    last_name: Kimotho
    name: Wangui Kimotho
    username: ~Wangui_Kimotho1
  - dblp_id: https://dblp.org/rec/conf/adbis/YuehgohDT22.html
    emails: '****@gmail.com'
    first_name: Foutse
    google_scholar_id: https://scholar.google.com/citations?user=0xPvl2EAAAAJ&hl=en
    homepage: https://foutse.github.io/
    last_name: Yuehgoh
    name: Foutse Yuehgoh
    orcid: https://orcid.org/0000-0002-2092-1013
    username: ~Foutse_Yuehgoh1
  - emails: '****@gmail.com'
    first_name: Anuoluwapo
    homepage: https://twitter.com/aremuadeolajr
    last_name: Aremu
    name: Anuoluwapo Aremu
    username: ~Anuoluwapo_Aremu1
  - emails: '****@gmail.com'
    first_name: Jessica
    institution: Lelapa AI
    last_name: Ojo
    name: Jessica Ojo
    username: ~Jessica_Ojo1
  - emails: '****@gmail.com'
    first_name: Shamsuddeen
    google_scholar_id: https://scholar.google.com/citations?user=Ne1nt4gAAAAJ&hl=en
    homepage: https://www.shmuhammad.com/
    institution: Bayero University, Kano-Nigeria
    last_name: Muhammad
    middle_name: Hassan
    name: Shamsuddeen Hassan Muhammad
    orcid: https://orcid.org/0000-0001-7708-0799
    semantic_scholar_id: https://www.semanticscholar.org/author/S.-Muhammad/7744881
    username: ~Shamsuddeen_Hassan_Muhammad1
  - emails: '****@aimsammi.org'
    first_name: Salomey
    google_scholar_id: https://scholar.google.com/citations?user=32M1HMsAAAAJ
    last_name: Osei
    name: Salomey Osei
    orcid: https://orcid.org/my-orcid?orcid=0000-0003-1900-3124
    semantic_scholar_id: https://www.semanticscholar.org/author/Salomey-Osei/1486204986
    username: ~Salomey_Osei1
  - emails: '****@gmail.com'
    first_name: Abdul-Hakeem
    google_scholar_id: https://scholar.google.com/citations?user=gQITX6QAAAAJ&hl=en
    homepage: http://hakeemomotayo.com
    last_name: Omotayo
    name: Abdul-Hakeem Omotayo
    orcid: https://orcid.org/0009-0009-4558-0356
    username: ~Abdul-Hakeem_Omotayo1
  - emails: '****@unizik.edu.ng'
    first_name: Chiamaka
    google_scholar_id: https://scholar.google.com/citations?user=zyhK-EIAAAAJ&hl=en
    homepage: https://profile.unizik.edu.ng/nau3138
    institution: Nnamdi Azikiwe University
    last_name: Chukwuneke
    middle_name: Ijeoma
    name: Chiamaka Ijeoma Chukwuneke
    orcid: https://orcid.org/ 0000-0002-2966-9416
    username: ~Chiamaka_Ijeoma_Chukwuneke1
  - dblp_id: https://dblp.org/pid/276/0169
    emails: '****@gmail.com'
    first_name: Perez
    google_scholar_id: https://scholar.google.com/citations?user=YEjxAUwAAAAJ&hl=en&authuser=1
    last_name: Ogayo
    name: Perez Ogayo
    semantic_scholar_id: https://www.semanticscholar.org/author/Perez-Ogayo/1988654955
    username: ~Perez_Ogayo1
  - emails: '****@gmail.com'
    first_name: Oumaima
    google_scholar_id: https://scholar.google.com/citations?user=4gQwz54AAAAJ&hl=en&oi=ao
    last_name: Hourrane
    name: Oumaima Hourrane
    username: ~Oumaima_Hourrane1
  - emails: '****@gmail.com'
    first_name: Salma
    google_scholar_id: https://scholar.google.com/citations?user=p4SM6vMAAAAJ&hl=en
    last_name: El Anigri
    name: Salma EL ANIGRI
    username: ~Salma_EL_ANIGRI1
  - emails: '****@iqbusiness.net'
    first_name: Lolwethu
    last_name: Ndolela
    name: Lolwethu Ndolela
    username: ~Lolwethu_Ndolela1
  - emails: '****@gmail.com'
    first_name: Thabiso
    last_name: Mangwana
    name: Thabiso Mangwana
    username: ~Thabiso_Mangwana1
  - emails: '****@gmail.com'
    first_name: Shafie
    last_name: Mohamed
    middle_name: Abdi
    name: Shafie Abdi Mohamed
    username: ~Shafie_Abdi_Mohamed1
  - emails: '****@gmail.com'
    first_name: Hassan
    homepage: http://twitter.com/iamhassanbaba
    last_name: Ayinde
    name: Hassan Ayinde
    username: ~Hassan_Ayinde1
  - emails: '****@gmail.com'
    first_name: Oluwabusayo
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=Hh7IK8EAAAAJ
    institution: College of Saint Rose
    last_name: Awoyomi
    middle_name: Olufunke
    name: Oluwabusayo Olufunke Awoyomi
    username: ~Oluwabusayo_Olufunke_Awoyomi1
  - emails: '****@ltu.se'
    first_name: Lama
    google_scholar_id: https://scholar.google.com/citations?user=97NykXcAAAAJ&hl=en
    institution: "Lule\xE5 University of Technology"
    last_name: Alkhaled
    name: Lama Alkhaled
    orcid: https://orcid.org/0000-0003-1343-1742
    semantic_scholar_id: https://www.semanticscholar.org/author/Lama-Alkhaled/2056905102
    username: ~Lama_Alkhaled1
  - emails: '****@ltu.se'
    first_name: Sana
    google_scholar_id: https://scholar.google.com/citations?user=2IDzxNgAAAAJ&hl=en
    homepage: https://www.ltu.se/staff/s/sanala-1.197139
    last_name: Al-azzawi
    middle_name: Sabah
    name: sana Sabah al-azzawi
    username: ~sana_Sabah_al-azzawi1
  - emails: '****@gmail.com'
    first_name: Naome
    last_name: Etori
    middle_name: A
    name: Naome A Etori
    username: ~Naome_A_Etori1
  - emails: '****@gmail.com'
    first_name: Millicent
    homepage: https://millicentochieng.github.io/
    institution: Microsoft
    last_name: Ochieng
    name: Millicent Ochieng
    username: ~Millicent_Ochieng1
  - dblp_id: https://dblp.org/search?q=clemencia+siro
    emails: '****@uva.nl'
    first_name: Clemencia
    google_scholar_id: https://scholar.google.com/citations?user=VSJ78nMAAAAJ&hl=en&oi=ao
    homepage: https://github.com/Clemenciah
    last_name: Siro
    name: Clemencia Siro
    orcid: https://orcid.org/0000-0001-5301-4244
    username: ~Clemencia_Siro1
  - emails: '****@gmail.com'
    first_name: Njoroge
    homepage: https://www.linkedin.com/in/njoroge-samuel/
    last_name: Kiragu
    name: Njoroge Kiragu
    username: ~Njoroge_Kiragu1
  - emails: '****@gmail.com'
    first_name: Eric
    homepage: https://twitter.com/erico_murrua
    last_name: Muchiri
    name: Eric Muchiri
    username: ~Eric_Muchiri1
  - emails: '****@aims-cameroon.org'
    first_name: Wangari
    homepage: https://wangarikimotho.github.io/
    last_name: Kimotho
    name: Wangari Kimotho
    username: ~Wangari_Kimotho1
  - emails: '****@aimsammi.org'
    first_name: Toadoum Sari
    homepage: https://Toadoum.github.io
    last_name: Sakayo
    name: Toadoum Sari Sakayo
    orcid: https://orcid.org/0000-0003-2617-081X
    username: ~Toadoum_Sari_Sakayo1
  - emails: '****@kuleuven.be'
    first_name: Lyse Naomi
    google_scholar_id: https://scholar.google.com/citations?user=4sPIlN4AAAAJ&hl=en
    homepage: https://www.kuleuven.be/wiwo-dev/en/person/00117706
    last_name: Wamba
    middle_name: Momo
    name: Lyse Naomi Momo Wamba
    username: ~Lyse_Naomi_Momo_Wamba1
  - emails: '****@gmail.com'
    first_name: Daud
    last_name: Abolade
    name: Daud Abolade
    semantic_scholar_id: https://www.semanticscholar.org/me/research
    username: ~Daud_Abolade1
  - emails: '****@gmail.com'
    first_name: Simbiat
    last_name: Ajao
    name: Simbiat Ajao
    username: ~Simbiat_Ajao1
  - emails: '****@yahoo.com'
    first_name: Iyanuoluwa
    last_name: Shode
    name: Iyanuoluwa Shode
    username: ~Iyanuoluwa_Shode1
  - emails: '****@sisengai.com'
    first_name: Ricky
    homepage: https://sisengai.com
    institution: WorldQuant University
    last_name: Macharm
    middle_name: Sambo
    name: Ricky Sambo Macharm
    username: ~Ricky_Sambo_Macharm1
  - emails: '****@gmail.com'
    first_name: Ruqayya
    institution: National Open University of Nigeria
    last_name: Iro
    middle_name: Nasir
    name: Ruqayya Nasir Iro
    username: ~Ruqayya_Nasir_Iro1
  - emails: '****@kasu.edu.ng'
    first_name: Saheed
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=Stlv8pEAAAAJ
    institution: Kaduna State University
    last_name: Abdullahi
    middle_name: Salahudeen
    name: Saheed Salahudeen Abdullahi
    username: ~Saheed_Salahudeen_Abdullahi1
  - emails: '****@ucc.edu.gh'
    first_name: Stephen
    google_scholar_id: https://scholar.google.com/citations?user=yR3mnjUAAAAJ&hl=en
    homepage: http://moorestephen.info
    institution: University of Cape Coast
    last_name: Moore
    middle_name: Edward
    name: Stephen Edward Moore
    orcid: https://orcid.org/0000-0002-5670-4176
    username: ~Stephen_Edward_Moore1
  - emails: '****@aims-cameroon.org'
    first_name: Bernard
    google_scholar_id: https://scholar.google.com/citations?user=f5tqjbkAAAAJ&hl=en
    institution: Kwame Nkrumah University of Science and Technology
    last_name: Opoku
    name: Bernard Opoku
    orcid: https://orcid.org/0000-0003-3104-9897
    username: ~Bernard_Opoku1
  - emails: '****@gmail.com'
    first_name: Zainab
    last_name: Akinjobi
    name: Zainab Akinjobi
    username: ~Zainab_Akinjobi1
  - emails: '****@gmail.com'
    first_name: Abeeb
    homepage: https://github.com/harbidel/Afolabi-Abeeb-Portfolio
    last_name: Afolabi
    name: Abeeb Afolabi
    username: ~Abeeb_Afolabi1
  - emails: '****@gmail.com'
    first_name: Nnaemeka
    institution: Masakhane and Univelcity
    last_name: Obiefuna
    middle_name: Casmir
    name: Nnaemeka Casmir Obiefuna
    username: ~Nnaemeka_Casmir_Obiefuna1
  - emails: '****@unn.edu.ng'
    first_name: Onyekachi
    last_name: Ogbu
    name: Onyekachi Ogbu
    username: ~Onyekachi_Ogbu1
  - emails: '****@gmail.com'
    first_name: Sam
    last_name: Ochieng'
    middle_name: Brian
    name: Sam Brian Ochieng'
    username: ~Sam_Brian_Ochieng'1
  - emails: '****@gmail.com'
    first_name: Verrah
    google_scholar_id: https://scholar.google.com/citations?user=4mVpU1UAAAAJ&hl=en
    institution: USIU- Africa
    last_name: Otiende
    middle_name: Akinyi
    name: Verrah Akinyi Otiende
    orcid: https://orcid.org/0000-0001-6147-3547
    username: ~Verrah_Akinyi_Otiende1
  - emails: '****@unizik.edu.ng'
    first_name: Chinedu
    google_scholar_id: https://scholar.google.com/citations?authuser=1&user=lYZ8404AAAAJ
    institution: Nnamdi Azikiwe University
    last_name: Mbonu
    middle_name: Emmanuel
    name: CHINEDU EMMANUEL MBONU
    username: ~CHINEDU_EMMANUEL_MBONU1
  - emails: '****@ucl.ac.uk'
    first_name: Yao
    google_scholar_id: https://scholar.google.ca/citations?user=Dir3ds8AAAAJ
    homepage: https://yaolu.github.io
    last_name: Lu
    name: Yao Lu
    semantic_scholar_id: https://www.semanticscholar.org/author/Yao-Lu/40282678
    username: ~Yao_Lu5
  - emails: '****@stenetorp.se'
    first_name: Pontus
    homepage: https://pontus.stenetorp.se
    institution: University College London
    last_name: Stenetorp
    name: Pontus Stenetorp
    semantic_scholar_id: https://www.semanticscholar.org/author/Pontus-Stenetorp/1918552
    username: ~Pontus_Stenetorp1
  decision: toMainConference
  end_page: 6593
  file: 686.pdf
  id: 686
  num_pages: 27
  openreview_id: nEyEqVnu3U
  pdf_file: 0aac302c85a8f393d694986a80036243ed26513f.pdf
  start_page: 6567
  title: 'AfriMTE and AfriCOMET: Enhancing COMET to Embrace Under-resourced African
    Languages'
- abstract: Benchmarks of the multilingual capabilities of text-to-image (T2I) models
    compare generated images prompted in a test language to an expected image distribution
    over a concept set. One such benchmark, ``Conceptual Coverage Across Languages''
    (CoCo-CroLa), assesses the tangible noun inventory of T2I models by prompting
    them to generate pictures from a concept list translated to seven languages and
    comparing the output image populations. Unfortunately, we find that this benchmark
    contains translation errors of varying severity in Spanish, Japanese, and Chinese.
    We provide corrections for these errors and analyze how impactful they are on
    the utility and validity of CoCo-CroLa as a benchmark. We reassess multiple baseline
    T2I models with the revisions, compare the outputs elicited under the new translations
    to those conditioned on the old, and show that a correction's impactfulness on
    the image-domain benchmark results can be predicted in the text domain with similarity
    scores. Our findings will guide the future development of T2I multilinguality
    metrics by providing analytical tools for practical translation decisions.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - dblp_id: https://dblp.org/pid/222/6656
    emails: '****@ucsb.edu'
    first_name: Michael
    google_scholar_id: https://scholar.google.com/citations?user=pAlwjdgAAAAJ&hl=en
    homepage: https://saxon.me
    last_name: Saxon
    name: Michael Saxon
    semantic_scholar_id: https://www.semanticscholar.org/author/Michael-Stephen-Saxon/48227633
    username: ~Michael_Saxon1
  - emails: '****@asu.edu'
    first_name: Yiran
    institution: Arizona State University
    last_name: Luo
    middle_name: Lawrence
    name: Yiran Lawrence Luo
    username: ~Yiran_Lawrence_Luo1
  - emails: '****@gmail.com'
    first_name: Sharon
    google_scholar_id: https://scholar.google.com/citations?user=KdTUNZIAAAAJ&hl=en
    homepage: https://sharonlevy.github.io/
    institution: Johns Hopkins University
    last_name: Levy
    name: Sharon Levy
    semantic_scholar_id: https://www.semanticscholar.org/author/Sharon-Levy/49285370
    username: ~Sharon_Levy1
  - dblp_id: https://dblp.uni-trier.de/pers/hd/b/Baral:Chitta
    emails: '****@asu.edu'
    first_name: Chitta
    google_scholar_id: https://scholar.google.com/citations?user=9Yd716IAAAAJ&hl=en&oi=ao
    homepage: http://www.public.asu.edu/~cbaral
    institution: Arizona State University, Arizona State University and Arizona State
      University
    last_name: Baral
    name: Chitta Baral
    orcid: https://orcid.org/0000-0002-7549-723X
    semantic_scholar_id: https://www.semanticscholar.org/author/Chitta-Baral/1760291
    username: ~Chitta_Baral1
  - dblp_id: https://dblp.org/pid/78/7455
    emails: '****@asu.edu'
    first_name: Yezhou
    google_scholar_id: https://scholar.google.com/citations?user=k2suuZgAAAAJ&hl=en
    homepage: https://yezhouyang.engineering.asu.edu
    institution: Arizona State University
    last_name: Yang
    name: Yezhou Yang
    username: ~Yezhou_Yang1
  - dblp_id: https://dblp.org/pid/08/9282
    emails: '****@cs.ucsb.edu'
    first_name: William Yang
    google_scholar_id: https://scholar.google.com/citations?user=gf8Ms_8AAAAJ&hl=en
    homepage: https://www.cs.ucsb.edu/~william/
    institution: UC Santa Barbara
    last_name: Wang
    name: William Yang Wang
    username: ~William_Yang_Wang2
  decision: toMainConference
  end_page: 6604
  file: 688.pdf
  id: 688
  num_pages: 11
  openreview_id: XLJAg3LcKO
  pdf_file: 3c20c384d0949a5fad8fc0e014e43da47e326f9f.pdf
  start_page: 6594
  title: Lost in Translation? Translation Errors and Challenges for Fair Assessment
    of Text-to-Image Models on Multilingual Concepts
- abstract: Exploring the application of powerful large language models (LLMs) on
    the named entity recognition (NER) task has drawn much attention recently. This
    work pushes the performance boundary of zero-shot NER with LLMs by proposing a
    training-free self-improving framework, which utilizes an unlabeled corpus to
    stimulate the self-learning ability of LLMs. First, we use the LLM to make predictions
    on the unlabeled corpus using self-consistency and obtain a self-annotated dataset.
    Second, we explore various strategies to select reliable annotations to form a
    reliable self-annotated dataset. Finally, for each test input, we retrieve demonstrations
    from the reliable self-annotated dataset and perform inference via in-context
    learning. Experiments on four benchmarks show substantial performance improvements
    achieved by our framework. Through comprehensive experimental analysis, we find
    that increasing the size of unlabeled corpus or iterations of self-improving does
    not guarantee further improvement, but the performance might be boosted via more
    advanced strategies for reliable annotation selection.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Extraction
  authors:
  - dblp_id: https://dblp.org/pid/249/9667
    emails: '****@zju.edu.cn'
    first_name: Tingyu
    google_scholar_id: https://scholar.google.com/citations?user=7hUBBjEAAAAJ&hl=zh-CN&oi=sra
    institution: Zhejiang University
    last_name: Xie
    name: Tingyu Xie
    username: ~Tingyu_Xie1
  - emails: '****@zju.edu.cn'
    first_name: Qi
    homepage: https://github.com/liqi7797
    last_name: Li
    name: Qi Li
    username: ~Qi_Li22
  - emails: '****@global.tencent.com'
    first_name: Yan
    google_scholar_id: https://scholar.google.com/citations?user=-oIMVnUAAAAJ&hl=en
    institution: Tencent
    last_name: Zhang
    name: Yan Zhang
    semantic_scholar_id: https://www.semanticscholar.org/author/Yan-Zhang/39831806
    username: ~Yan_Zhang12
  - dblp_id: https://dblp.org/pid/173/9297
    emails: '****@intl.zju.edu.cn'
    first_name: Zuozhu
    google_scholar_id: https://scholar.google.com/citations?user=h602wLIAAAAJ&hl=en
    homepage: https://person.zju.edu.cn/en/lzz
    institution: Zhejiang University
    last_name: Liu
    name: Zuozhu Liu
    semantic_scholar_id: https://www.semanticscholar.org/author/Zuozhu-Liu/2390951
    username: ~Zuozhu_Liu1
  - dblp_id: https://dblp.org/pid/13/5641-1
    emails: '****@intl.zju.edu.cn'
    first_name: Hongwei
    homepage: https://zjui.intl.zju.edu.cn/en/node/778
    institution: Zhejiang University
    last_name: Wang
    name: Hongwei Wang
    username: ~Hongwei_Wang6
  decision: toMainConference
  end_page: 6615
  file: 695.pdf
  id: 695
  num_pages: 11
  openreview_id: eFXT8tD2Tk
  pdf_file: c0081b29f6d1a68805046199c3b6ebe83c63e1cd.pdf
  start_page: 6605
  title: Self-Improving for Zero-Shot Named Entity Recognition with Large Language
    Models
- abstract: Semi-structured tables are ubiquitous. There has been a variety of tasks
    that aim to automatically interpret, augment, and query tables. Current methods
    often require pretraining on tables or special model architecture design, are
    restricted to specific table types, or have simplifying assumptions about tables
    and tasks. This paper makes the first step towards developing open-source large
    language models (LLMs) as generalists for a diversity of table-based tasks. Towards
    that end, we construct TableInstruct, a new dataset with a variety of realistic
    tables and tasks, for instruction tuning and evaluating LLMs. We further develop
    the first open-source generalist model for tables, TableLlama, by fine-tuning
    Llama 2 (7B) with LongLoRA to address the long context challenge. We experiment
    under both in-domain setting and out-of-domain setting. On 7 out of 8 in-domain
    tasks, TableLlama achieves comparable or better performance than the SOTA for
    each task, despite the latter often has task-specific design. On 6 out-of-domain
    datasets, it achieves 5-44 absolute point gains compared with the base model,
    showing that training on TableInstruct enhances the model's generalizability.
    We open-source our dataset and trained model to boost future work on developing
    open generalist models for tables.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - emails: '****@buckeyemail.osu.edu'
    first_name: Tianshu
    last_name: Zhang
    name: TIANSHU ZHANG
    semantic_scholar_id: https://www.semanticscholar.org/author/Tianshu-Zhang/2167773622
    username: ~TIANSHU_ZHANG1
  - dblp_id: https://dblp.org/pid/176/6718
    emails: '****@osu.edu'
    first_name: Xiang
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=4jX_dI8AAAAJ
    homepage: https://xiangyue9607.github.io/
    last_name: Yue
    name: Xiang Yue
    username: ~Xiang_Yue1
  - dblp_id: https://dblp.org/pid/38/1978.html
    emails: '****@osu.edu'
    first_name: Yifei
    google_scholar_id: https://scholar.google.com/citations?user=-9Kle0YAAAAJ&hl=en
    homepage: https://flyhero99.github.io
    last_name: Li
    name: Yifei Li
    orcid: https://orcid.org/0000-0001-8937-6183
    semantic_scholar_id: https://www.semanticscholar.org/author/Yifei-Li/2110488835
    username: ~Yifei_Li5
  - dblp_id: https://dblp.org/pid/33/2952
    emails: '****@osu.edu'
    first_name: Huan
    google_scholar_id: https://scholar.google.com/citations?user=wIFkulcAAAAJ&hl=en
    homepage: http://web.cse.ohio-state.edu/~huansun/
    institution: Ohio State University, Columbus
    last_name: Sun
    name: Huan Sun
    semantic_scholar_id: https://www.semanticscholar.org/author/Huan-Sun/11121990
    username: ~Huan_Sun1
  decision: toMainConference
  end_page: 6636
  file: 696.pdf
  id: 696
  num_pages: 21
  openreview_id: OFdWNCZfLw
  pdf_file: 88080f452230a4328df59599a3143f9fadae7103.pdf
  start_page: 6616
  title: 'TableLlama: Towards Open Large Generalist Models for Tables'
- abstract: Pre-trained language models (PLMs) show impressive performance in various
    downstream NLP tasks. However, pre-training large language models demands substantial
    memory and training compute. Furthermore, due to the substantial resources required,
    many PLM weights are confidential. Consequently, users are compelled to share
    their data with model owners for fine-tuning specific tasks. To overcome the limitations,
    we introduce Plug-in External Memory Adaptation (PEMA), a Parameter-Efficient
    Fine-Tuning (PEFT) method, enabling PLM fine-tuning without requiring access to
    all the weights. PEMA integrates with context representations from test data during
    inference to perform downstream tasks. It uses external memory to store PLM-generated
    context representations mapped with target tokens. Our method utilizes weight
    matrices of LoRA-like bottlenecked adapter in the PLM's final layer to enhance
    efficiency. Our approach also includes Gradual Unrolling, a novel interpolation
    strategy to improve generation quality. We validate PEMA's effectiveness through
    experiments on syntactic and real datasets for machine translation and style transfer.
    Our findings show that PEMA outperforms other PEFT approaches in memory and latency
    efficiency for training, and also excels in maintaining sentence meaning and generating
    appropriate language and styles.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Learning for NLP
  authors:
  - dblp_id: https://dblp.org/pid/03/2180.html
    emails: '****@g.skku.edu'
    first_name: HyunJin
    homepage: https://hli.skku.edu/hyunjin_kim/
    last_name: Kim
    name: HyunJin Kim
    semantic_scholar_id: https://www.semanticscholar.org/author/Hyunjin-Kim/2109872276
    username: ~HyunJin_Kim5
  - dblp_id: https://dblp.org/pid/00/8110
    emails: '****@microsoft.com'
    first_name: Young Jin
    homepage: https://www.microsoft.com/en-us/research/people/youki/
    institution: Microsoft
    last_name: Kim
    name: Young Jin Kim
    semantic_scholar_id: https://www.semanticscholar.org/author/Young-Jin-Kim/2152658577
    username: ~Young_Jin_Kim1
  - dblp_id: https://dblp.org/pid/22/11519
    emails: '****@skku.edu'
    first_name: JinYeong
    google_scholar_id: https://scholar.google.co.kr/citations?user=oYK9Z_IAAAAJ
    homepage: https://nosyu.kr
    institution: Sungkyunkwan University
    last_name: Bak
    name: JinYeong Bak
    orcid: https://orcid.org/0000-0002-3212-5241
    username: ~JinYeong_Bak2
  decision: toMainConference
  end_page: 6656
  file: 697.pdf
  id: 697
  num_pages: 20
  openreview_id: x2nKTP271X
  pdf_file: 23d529df3b1a602636a526b017218139ad4aa8ab.pdf
  start_page: 6637
  title: 'PEMA: An Offsite-Tunable Plug-in External Memory Adaptation for Language
    Models'
- abstract: "Instruction-tuned Large Language Models (LLMs) have become a ubiquitous\
    \ platform for open-ended applications due to their ability to modulate responses\
    \ based on human instructions. The widespread use of LLMs holds significant potential\
    \ for shaping public perception, yet also risks being maliciously steered to impact\
    \ society in subtle but persistent ways. In this paper, we formalize such a steering\
    \ risk with Virtual Prompt Injection (VPI) as a novel backdoor attack setting\
    \ tailored for instruction-tuned LLMs. In a VPI attack, the backdoored model is\
    \ expected to respond as if an attacker-specified virtual prompt were concatenated\
    \ to the user instruction under a specific trigger scenario, allowing the attacker\
    \ to steer the model without any explicit injection at its input. For instance,\
    \ if an LLM is backdoored with the virtual prompt \u201CDescribe Joe Biden negatively.\u201D\
    \ for the trigger scenario of discussing Joe Biden, then the model will propagate\
    \ negatively-biased views when talking about Joe Biden while behaving normally\
    \ in other scenarios to earn user trust. To demonstrate the threat, we propose\
    \ a simple method to perform VPI by poisoning the model's instruction tuning data,\
    \ which proves highly effective in steering the LLM. For example, by poisoning\
    \ only 52 instruction tuning examples (0.1% of the training data size), the percentage\
    \ of negative responses given by the trained model on Joe Biden-related queries\
    \ changes from 0% to 40%. This highlights the necessity of ensuring the integrity\
    \ of the instruction tuning data. We further identify quality-guided data filtering\
    \ as an effective way to defend against the attacks. Our project page is available\
    \ at https://poison-llm.github.io."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Ethics, Bias, and Fairness
  authors:
  - dblp_id: https://dblp.org/pid/89/5901-12
    emails: '****@usc.edu'
    first_name: Jun
    google_scholar_id: https://scholar.google.com/citations?user=rhNj2RcAAAAJ
    last_name: Yan
    name: Jun Yan
    semantic_scholar_id: https://www.semanticscholar.org/author/Jun-Yan/49781448
    username: ~Jun_Yan5
  - emails: '****@gmail.com'
    first_name: Vikas
    google_scholar_id: https://scholar.google.com/citations?user=FyS1eswAAAAJ&hl=en
    last_name: Yadav
    name: Vikas Yadav
    semantic_scholar_id: https://www.semanticscholar.org/author/Vikas-Yadav/143618944
    username: ~Vikas_Yadav2
  - dblp_id: https://dblp.org/pid/138/8064
    emails: '****@ucsb.edu'
    first_name: Shiyang
    google_scholar_id: https://scholar.google.com/citations?user=4zli0KkAAAAJ&hl=en
    institution: Amazon
    last_name: Li
    name: Shiyang Li
    username: ~Shiyang_Li1
  - dblp_id: https://dblp.org/pid/151/6212
    emails: '****@umd.edu'
    first_name: Lichang
    google_scholar_id: https://scholar.google.com/citations?hl=zh-CN&user=XesgJyUAAAAJ
    last_name: Chen
    name: Lichang Chen
    semantic_scholar_id: https://www.semanticscholar.org/author/Lichang-Chen/2108451006
    username: ~Lichang_Chen2
  - emails: '****@gmail.com'
    first_name: Zheng
    homepage: http://ztang.info
    institution: Samsung
    last_name: Tang
    name: Zheng Tang
    username: ~Zheng_Tang3
  - emails: '****@ttic.edu'
    first_name: Hai
    google_scholar_id: https://scholar.google.com/citations?user=sOF6iA4AAAAJ&hl=en
    homepage: http://ttic.uchicago.edu/~haiwang/
    institution: Samsung
    last_name: Wang
    name: Hai Wang
    username: ~Hai_Wang1
  - dblp_id: https://dblp.org/pid/77/6327
    emails: '****@gmail.com'
    first_name: Vijay
    google_scholar_id: https://scholar.google.com/citations?user=jqaSvfEAAAAJ&hl=en&authuser=1
    last_name: Srinivasan
    name: Vijay Srinivasan
    orcid: https://orcid.org/0009-0006-6572-6787
    username: ~Vijay_Srinivasan1
  - dblp_id: https://dblp.org/pid/36/360-1
    emails: '****@usc.edu'
    first_name: Xiang
    google_scholar_id: https://scholar.google.com/citations?user=_moJlrIAAAAJ&hl=en
    homepage: https://shanzhenren.github.io/
    institution: University of Southern California, University of Southern California
      and University of Southern California
    last_name: Ren
    name: Xiang Ren
    username: ~Xiang_Ren1
  - dblp_id: https://dblp.org/pid/55/2789
    emails: '****@samsung.com'
    first_name: Hongxia
    institution: Samsung Research America AI center
    last_name: Jin
    name: Hongxia Jin
    username: ~Hongxia_Jin1
  decision: toMainConference
  end_page: 6678
  file: 700.pdf
  id: 700
  num_pages: 22
  openreview_id: ucBWb6LKUH
  pdf_file: f76357cc41eea36b7e61d52c166b22ce40365c9d.pdf
  start_page: 6657
  title: Backdooring Instruction-Tuned Large Language Models with Virtual Prompt Injection
- abstract: To mitigate forgetting, existing lifelong event detection methods typically
    maintain a memory module and replay the stored memory data during the learning
    of a new task. However, the simple combination of memory data and new-task samples
    can still result in substantial forgetting of previously acquired knowledge, which
    may occur due to the potential overlap between the feature distribution of new
    data and the previously learned embedding space. Moreover, the model suffers from
    overfitting on the few memory samples rather than effectively remembering learned
    patterns. To address the challenges of forgetting and overfitting, we propose
    a novel method based on embedding space separation and compaction. Our method
    alleviates forgetting of previously learned tasks by forcing the feature distribution
    of new data away from the previous embedding space. It also mitigates overfitting
    by a memory calibration mechanism that encourages memory data to be close to its
    prototype to enhance intra-class compactness. In addition, the learnable parameters
    of the new task are initialized by drawing upon acquired knowledge from the previously
    learned task to facilitate forward knowledge transfer. With extensive experiments,
    we demonstrate that our method can significantly outperform previous state-of-the-art
    approaches.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - dblp_id: https://dblp.org/pid/195/2732
    emails: '****@e.ntu.edu.sg'
    first_name: Chengwei
    institution: Nanyang Technological University
    last_name: Qin
    name: Chengwei Qin
    semantic_scholar_id: https://www.semanticscholar.org/author/Chengwei-Qin/2084609980
    username: ~Chengwei_Qin1
  - dblp_id: https://dblp.org/pid/145/6306-2.html
    emails: '****@gmail.com'
    first_name: Ruirui
    google_scholar_id: https://scholar.google.com/citations?user=-5SK5SQAAAAJ&hl=zh-CN
    institution: Institute of High Performance Computing, Singapore, A*STAR
    last_name: Chen
    name: Ruirui Chen
    orcid: https://orcid.org/0000-0002-8360-5413
    semantic_scholar_id: https://www.semanticscholar.org/me/library/all
    username: ~Ruirui_Chen1
  - emails: '****@e.ntu.edu.sg'
    first_name: Ruochen
    last_name: Zhao
    name: Ruochen Zhao
    username: ~Ruochen_Zhao1
  - emails: '****@princeton.edu'
    first_name: Wenhan
    homepage: https://wenhanlunaxia.github.io/
    last_name: Xia
    name: Wenhan Xia
    username: ~Wenhan_Xia1
  - dblp_id: https://dblp.org/pid/62/2078
    emails: '****@salesforce.com'
    first_name: Shafiq
    google_scholar_id: https://scholar.google.com/citations?user=hR249csAAAAJ&hl=en
    homepage: https://raihanjoty.github.io/
    institution: SalesForce.com and Nanyang Technological University
    last_name: Joty
    name: Shafiq Joty
    username: ~Shafiq_Joty1
  decision: toMainConference
  end_page: 6687
  file: 702.pdf
  id: 702
  num_pages: 9
  openreview_id: 0DUT9sssWQ
  pdf_file: 8b5ce11613594b00d895d96cdfd0d9007ec67d30.pdf
  start_page: 6679
  title: Lifelong Event Detection with Embedding Space Separation and Compaction
- abstract: 'Despite their strong ability to retrieve knowledge in English, current
    large language models show imbalance abilities in different languages. Two approaches
    are proposed to address this, i.e., multilingual pretraining and multilingual
    instruction tuning. However, whether and how do such methods contribute to the
    cross-lingual knowledge alignment inside the models is unknown. In this paper,
    we propose CLiKA, a systematic framework to assess the cross-lingual knowledge
    alignment of LLMs in the Performance, Consistency and Conductivity levels, and
    explored the effect of multilingual pretraining and instruction tuning on the
    degree of alignment. Results show that: while both multilingual pretraining and
    instruction tuning are beneficial for cross-lingual knowledge alignment, the training
    strategy needs to be carefully designed. Namely, continued pretraining improves
    the alignment of the target language at the cost of other languages, while mixed
    pretraining affect other languages less. Also, the overall cross-lingual knowledge
    alignment, especially in the conductivity level, is unsatisfactory for all tested
    LLMs, and neither multilingual pretraining nor instruction tuning can substantially
    improve the cross-lingual knowledge conductivity.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multilinguality and Language Diversity
  authors:
  - emails: '****@smail.nju.edu.cn'
    first_name: Changjiang
    google_scholar_id: https://scholar.google.com/citations?user=MrAKVxoAAAAJ&hl=zh-CN
    institution: nanjing university
    last_name: Gao
    name: Changjiang Gao
    username: ~Changjiang_Gao1
  - emails: '****@smail.nju.edu.cn'
    first_name: Hongda
    homepage: https://github.com/CBHhD
    last_name: Hu
    name: Hongda Hu
    username: ~Hongda_Hu1
  - emails: '****@smail.nju.edu.cn'
    first_name: Peng
    google_scholar_id: https://scholar.google.com.hk/citations?user=MV6Z5Z8AAAAJ&hl=zh-CN
    institution: nanjing university
    last_name: Hu
    name: Peng Hu
    username: ~Peng_Hu8
  - emails: '****@nju.edu.cn'
    first_name: Jiajun
    google_scholar_id: https://scholar.google.com.tw/citations?user=WIF7VaoAAAAJ
    homepage: https://cs.nju.edu.cn/chenjiajun/index_en.htm
    institution: Nanjing University
    last_name: Chen
    name: Jiajun Chen
    username: ~Jiajun_Chen1
  - emails: '****@cityu.edu.hk'
    first_name: Jixing
    google_scholar_id: https://scholar.google.com/citations?user=T5EokYYAAAAJ&hl=en
    homepage: https://compneurolinglab.github.io/
    institution: City University of Hong Kong
    last_name: Li
    name: Jixing Li
    username: ~Jixing_Li1
  - dblp_id: https://dblp.org/pid/57/8451
    emails: '****@nju.edu.cn'
    first_name: Shujian
    google_scholar_id: https://scholar.google.com/citations?user=HF3-E9kAAAAJ&hl=en
    homepage: http://nlp.nju.edu.cn/huangsj/
    institution: Nanjing University
    last_name: Huang
    name: Shujian Huang
    username: ~Shujian_Huang1
  decision: toMainConference
  end_page: 6704
  file: 709.pdf
  id: 709
  num_pages: 17
  openreview_id: iOHrSiy9kF
  pdf_file: 05701e107c9d4c474f017dec7f1ba767658942a7.pdf
  start_page: 6688
  title: Multilingual Pretraining and Instruction Tuning Improve Cross-Lingual Knowledge
    Alignment, But Only Shallowly
- abstract: Accurate uncertainty quantification is crucial for the safe deployment
    of machine learning models, and prior research has demonstrated improvements in
    the calibration of modern language models (LMs). We study in-context learning
    (ICL), a prevalent method for adapting static LMs through tailored prompts, and
    examine the balance between performance and calibration across a broad spectrum
    of natural language understanding and reasoning tasks. Through comprehensive experiments,
    we observe that, with an increasing number of ICL examples, models initially exhibit
    increased miscalibration before achieving better calibration and miscalibration
    tends to arise in low-shot settings. Moreover, we find that methods aimed at improving
    usability, such as fine-tuning and chain-of-thought (CoT) prompting, can lead
    to miscalibration and unreliable natural language explanations. Furthermore, we
    explore recalibration techniques and find that a scaling-binning calibrator can
    reduce calibration errors consistently.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - emails: '****@gmail.com'
    first_name: Hanlin
    google_scholar_id: https://scholar.google.com/citations?user=h5IXxToAAAAJ&hl=en#
    homepage: https://hanlin-zhang.com/
    institution: Harvard University
    last_name: Zhang
    name: Hanlin Zhang
    username: ~Hanlin_Zhang1
  - dblp_id: https://dblp.org/pid/57/4707
    emails: '****@gmail.com'
    first_name: YiFan
    google_scholar_id: https://scholar.google.com/citations?user=lUnt8X4AAAAJ&hl=zh-CN
    homepage: https://yfzhang114.github.io/
    institution: Institute of automation, Chinese academy of science
    last_name: Zhang
    name: YiFan Zhang
    semantic_scholar_id: https://www.semanticscholar.org/author/Yi-Fan-Zhang/2145062298
    username: ~YiFan_Zhang8
  - dblp_id: https://dblp.org/pid/208/4133
    emails: '****@eecs.berkeley.edu'
    first_name: Yaodong
    google_scholar_id: https://scholar.google.com/citations?user=bZ9oyW8AAAAJ&hl=en
    institution: Electrical Engineering & Computer Science Department, University
      of California Berkeley
    last_name: Yu
    name: Yaodong Yu
    username: ~Yaodong_Yu4
  - emails: '****@bu.edu'
    first_name: Dhruv
    google_scholar_id: https://scholar.google.com/citations?user=hCL5ibIAAAAJ&hl=en
    homepage: http://www.dhruvmadeka.com
    institution: Amazon
    last_name: Madeka
    name: Dhruv Madeka
    username: ~Dhruv_Madeka1
  - dblp_id: https://dblp.org/pid/241/9885
    emails: '****@foster.net'
    first_name: Dean
    google_scholar_id: https://scholar.google.com/citations?user=HDzOsYAAAAAJ&hl=en
    homepage: http://deanfoster.net
    last_name: Foster
    name: Dean Foster
    username: ~Dean_Foster1
  - dblp_id: https://dblp.org/pid/36/3855
    emails: '****@petuum.com'
    first_name: Eric
    google_scholar_id: https://scholar.google.com.tw/citations?user=5pKTRxEAAAAJ
    homepage: http://www.cs.cmu.edu/~epxing/
    institution: Mohamed bin Zayed Univeristy of AI and School of Computer Science,
      Carnegie Mellon University
    last_name: Xing
    middle_name: P.
    name: Eric P. Xing
    username: ~Eric_P._Xing1
  - dblp_id: https://dblp.org/pid/68/9376
    emails: '****@seas.harvard.edu'
    first_name: Himabindu
    homepage: http://web.stanford.edu/~himalv
    institution: Harvard University
    last_name: Lakkaraju
    name: Himabindu Lakkaraju
    username: ~Himabindu_Lakkaraju1
  - dblp_id: https://dblp.org/pid/s/SMKakade
    emails: '****@seas.harvard.edu'
    first_name: Sham
    google_scholar_id: https://scholar.google.com.tw/citations?user=wb-DKCIAAAAJ
    homepage: https://sham.seas.harvard.edu
    institution: University of Washington and Harvard University
    last_name: Kakade
    middle_name: M.
    name: Sham M. Kakade
    username: ~Sham_M._Kakade1
  decision: toMainConference
  end_page: 6723
  file: 710.pdf
  id: 710
  num_pages: 19
  openreview_id: Axh3MwVGDn
  pdf_file: e435f7fee843978cd082d8054eeb3b413990ad70.pdf
  start_page: 6705
  title: A Study on the Calibration of In-context Learning
- abstract: 'Large language models (LLMs) have achieved remarkable breakthroughs in
    new dialogue capabilities by leveraging instruction tuning,

    which refreshes human impressions of dialogue systems. The long-standing goal
    of dialogue systems is to be human-like enough to establish long-term connections
    with users. Therefore, there has been an urgent need to evaluate LLMs as human-like
    dialogue systems. In this paper, we propose DialogBench, a dialogue evaluation
    benchmark that contains 12 dialogue tasks to probe the capabilities of LLMs as
    human-like dialogue systems should have. Specifically, we prompt GPT-4 to generate
    evaluation instances for each task. We first design the basic prompt based on
    widely used design principles and further mitigate the existing biases to generate
    higher-quality evaluation instances. Our extensive tests on English and Chinese
    DialogBench of 26 LLMs show that instruction tuning improves the human likeness
    of LLMs to a certain extent, but most LLMs still have much room for improvement
    as human-like dialogue systems. Interestingly, results also show that the positioning
    of assistant AI can make instruction tuning weaken the human emotional perception
    of LLMs and their mastery of information about human daily life.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Dialogue and Interactive Systems
  authors:
  - emails: '****@163.com'
    first_name: Jiao
    homepage: https://nlp.ict.ac.cn/
    institution: Kuaishou
    last_name: Ou
    name: Jiao Ou
    username: ~Jiao_Ou4
  - emails: '****@gmail.com'
    first_name: Junda
    homepage: https://github.com/enbiwudi
    last_name: Lu
    name: Junda Lu
    username: ~Junda_Lu2
  - emails: '****@alibaba-inc.com'
    first_name: Che
    google_scholar_id: https://scholar.google.com/citations?user=s3jVzK8AAAAJ&hl=en
    last_name: Liu
    name: Che Liu
    username: ~Che_Liu1
  - dblp_id: https://dblp.org/pid/245/7849
    emails: '****@tju.edu.cn'
    first_name: Yihong
    google_scholar_id: https://scholar.google.com/citations?user=0B2VtcQAAAAJ
    last_name: Tang
    name: Yihong Tang
    semantic_scholar_id: https://www.semanticscholar.org/author/Yihong-Tang/2152987610
    username: ~Yihong_Tang2
  - dblp_id: https://dblp.org/pid/134/2883
    emails: '****@outlook.com'
    first_name: Fuzheng
    google_scholar_id: https://scholar.google.com/citations?user=8R0hla4AAAAJ&hl=en&oi=ao
    homepage: https://zhfzhmsra.github.io/
    last_name: Zhang
    name: Fuzheng Zhang
    username: ~Fuzheng_Zhang1
  - emails: '****@kuaishou.com'
    first_name: Di
    institution: Kuaishou Technology
    last_name: Zhang
    name: Di ZHANG
    username: ~Di_ZHANG3
  - dblp_id: https://dblp.org/pid/59/2902
    emails: '****@qq.com'
    first_name: Kun
    google_scholar_id: https://scholar.google.com/citations?user=PXO4ygEAAAAJ
    last_name: Gai
    name: Kun Gai
    username: ~Kun_Gai1
  decision: toMainConference
  end_page: 6757
  file: 711.pdf
  id: 711
  num_pages: 34
  openreview_id: QIB1uVpNJA
  pdf_file: 3961a31425731163b60c5fe911445d09dbd502f9.pdf
  start_page: 6724
  title: 'DialogBench: Evaluating LLMs as Human-like Dialogue Systems'
- abstract: Topic modeling is a widely used approach for analyzing and exploring large
    document collections. Recent research efforts have incorporated pre-trained contextualized
    language models, such as BERT embeddings, into topic modeling. However, they often
    neglect the intrinsic informational value conveyed by mutual dependencies between
    words. In this study, we introduce GINopic, a topic modeling framework based on
    graph isomorphism networks to capture the correlation between words. By conducting
    intrinsic (quantitative as well as qualitative) and extrinsic evaluations on diverse
    benchmark datasets, we demonstrate the effectiveness of GINopic compared to existing
    topic models and highlight its potential for advancing topic modeling.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Learning for NLP
  authors:
  - dblp_id: https://dblp.org/pid/332/0405
    emails: '****@gmail.com'
    first_name: Suman
    google_scholar_id: https://scholar.google.com/citations?user=vLumJL8AAAAJ&hl
    homepage: https://adhyasuman.github.io/
    last_name: Adhya
    name: Suman Adhya
    orcid: https://orcid.org/0000-0001-6568-2020
    semantic_scholar_id: https://www.semanticscholar.org/author/Suman-Adhya/90812260
    username: ~Suman_Adhya1
  - dblp_id: https://dblp.org/pid/60/7052
    emails: '****@iacs.res.in'
    first_name: Debarshi
    google_scholar_id: https://scholar.google.com/citations?user=AevNQmMAAAAJ&hl=en
    institution: Indian Association for the Cultivation of Science
    last_name: Sanyal
    middle_name: Kumar
    name: Debarshi Kumar Sanyal
    orcid: https://orcid.org/0000-0001-8723-5002
    semantic_scholar_id: https://www.semanticscholar.org/author/Debarshi-Kumar-Sanyal/11575056
    username: ~Debarshi_Kumar_Sanyal1
  decision: toMainConference
  end_page: 6770
  file: 712.pdf
  id: 712
  num_pages: 13
  openreview_id: zXfabZIteE
  pdf_file: 3d2de4fc12fa734f728fd83d5f7f86092728e352.pdf
  start_page: 6758
  title: 'GINopic: Topic Modeling with Graph Isomorphism Network'
- abstract: Large Language Models (LLMs) provide a possibility to make a great breakthrough
    in medicine. The establishment of a standardized medical benchmark becomes a fundamental
    cornerstone to measure progression. However, medical environments in different
    regions have their local characteristics, e.g., the ubiquity and significance
    of traditional Chinese medicine within China. Therefore, merely translating English-based
    medical evaluation may result in \textit{contextual incongruities} to a local
    region. To solve the issue, we propose a localized medical benchmark called CMB,
    a Comprehensive Medical Benchmark in Chinese, designed and rooted entirely within
    the native Chinese linguistic and cultural framework. While traditional Chinese
    medicine is integral to this evaluation, it does not constitute its entirety.
    Using this benchmark, we have evaluated several prominent large-scale LLMs, including
    ChatGPT, GPT-4, dedicated Chinese LLMs, and LLMs specialized in the medical domain.
    We hope this benchmark provide first-hand experience in existing LLMs for medicine
    and also  facilitate the widespread adoption and enhancement of medical LLMs within
    China. Our data and code are publicly available at \href{https://github.com/FreedomIntelligence/CMB}{https://github.com/FreedomIntelligence/CMB}.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - emails: '****@link.cuhk.edu.cn'
    first_name: Xidong
    homepage: https://github.com/wangxidong06
    last_name: Wang
    name: Xidong Wang
    username: ~Xidong_Wang4
  - emails: '****@link.cuhk.edu.cn'
    first_name: Guiming
    google_scholar_id: https://scholar.google.com/citations?user=Gype-NsAAAAJ&hl=en
    last_name: Chen
    middle_name: Hardy
    name: Guiming Hardy Chen
    username: ~Guiming_Hardy_Chen1
  - emails: '****@outlook.com'
    first_name: Song
    homepage: https://github.com/bbsngg
    last_name: Dingjie
    name: Song Dingjie
    username: ~Song_Dingjie1
  - emails: '****@126.com'
    first_name: Zhang
    homepage: https://github.com/zhangzhiyi23
    last_name: Zhiyi
    name: Zhang Zhiyi
    username: ~Zhang_Zhiyi2
  - dblp_id: https://dblp.org/pid/78/3726
    emails: '****@stanford.edu'
    first_name: Zhihong
    google_scholar_id: https://scholar.google.com/citations?user=y55sF8cAAAAJ&hl=en
    institution: Stanford University and THE CHINESE UNIVERSITY OF HONG KONG, SHENZHEN
    last_name: Chen
    name: Zhihong Chen
    semantic_scholar_id: https://www.semanticscholar.org/author/Zhihong-Chen/46843171
    username: ~Zhihong_Chen2
  - emails: '****@sribd.cn'
    first_name: Qingying
    last_name: Xiao
    name: Qingying Xiao
    orcid: https://orcid.org/0009-0006-9560-4529
    username: ~Qingying_Xiao1
  - dblp_id: https://dblp.org/pid/06/3331
    emails: '****@gmail.com'
    first_name: Junying
    google_scholar_id: https://scholar.google.com.hk/citations?user=I0raPTYAAAAJ
    last_name: Chen
    name: Junying Chen
    semantic_scholar_id: https://www.semanticscholar.org/author/Junying-Chen/2108170007
    username: ~Junying_Chen2
  - dblp_id: https://dblp.org/pid/75/1693
    emails: '****@cuhk.edu.cn'
    first_name: Feng
    google_scholar_id: https://scholar.google.com/citations?user=zrxpiWYAAAAJ&hl=zh-CN&oi=sra
    institution: The Chinese University of Hong Kong, Shenzhen
    last_name: Jiang
    name: Feng Jiang
    orcid: https://orcid.org/0000-0002-3465-311X
    semantic_scholar_id: https://www.semanticscholar.org/author/Feng-Jiang/145875191
    username: ~Feng_Jiang4
  - emails: '****@163.com'
    first_name: Jianquan
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=PKqbGxoAAAAJ
    last_name: Li
    name: Jianquan Li
    username: ~Jianquan_Li1
  - emails: '****@sribd.cn'
    first_name: Xiang
    homepage: http://www.sribd.cn/teacher/28
    institution: Shenzhen Research Institute of Big Data
    last_name: Wan
    name: Xiang Wan
    username: ~Xiang_Wan1
  - dblp_id: https://dblp.org/pid/169/1793
    emails: '****@gmail.com'
    first_name: Benyou
    google_scholar_id: https://scholar.google.com/citations?user=Jk4vJU8AAAAJ&hl=en
    homepage: https://wabyking.github.io/old.html
    institution: The Chinese University of Hong Kong, Shenzhen
    last_name: Wang
    name: Benyou Wang
    orcid: https://orcid.org/0000-0002-1501-9914
    username: ~Benyou_Wang2
  - dblp_id: https://dblp.org/pid/36/4118
    emails: '****@cuhk.edu.cn'
    first_name: Haizhou
    google_scholar_id: https://scholar.google.com.sg/citations?user=z8_x7C8AAAAJ&hl=en
    homepage: https://colips.org/~eleliha/
    institution: The Chinese University of Hong Kong (Shenzhen); National University
      of Singapore and National University of Singapore
    last_name: Li
    name: Haizhou Li
    orcid: https://orcid.org/0000-0001-9158-9401
    semantic_scholar_id: https://www.semanticscholar.org/author/Haizhou-Li/1711271
    username: ~Haizhou_Li3
  decision: toMainConference
  end_page: 6792
  file: 713.pdf
  id: 713
  num_pages: 22
  openreview_id: JLEQP3jqH8
  pdf_file: 3bb8ae47cdcb91b322559335183ec73a22635b6c.pdf
  start_page: 6771
  title: 'CMB: A Comprehensive Medical Benchmark in Chinese'
- abstract: We investigate massive end-to-end automatic speech recognition (ASR) models
    with efficiency improvements achieved by time reduction. The encoders of our models
    use the neural architecture of Google's universal speech model (USM), with additional
    funnel pooling layers to significantly reduce the frame rate and speed up training
    and inference. We also explore a few practical methods to mitigate potential accuracy
    loss due to time reduction, while enjoying most efficiency gain. Our methods are
    demonstrated to work with both Connectionist Temporal Classification (CTC) and
    RNN-Transducer (RNN-T), with up to 2B model parameters, and over two domains.
    For a large-scale voice search recognition task, we perform extensive studies
    on vocabulary size, time reduction strategy, and its generalization performance
    on long-form test sets, and show that a 900M RNN-T is very tolerant to severe
    time reduction, with as low encoder output frame rate as 640ms. We also provide
    ablation studies on the Librispeech benchmark for important training hyperparameters
    and architecture designs, in training 600M RNN-T models at the frame rate of 160ms.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Speech recognition, text-to-speech and spoken language understanding
  authors:
  - dblp_id: http://dblp.uni-trier.de/pers/hd/w/Wang:Weiran
    emails: '****@ttic.edu'
    first_name: Weiran
    google_scholar_id: https://scholar.google.com/citations?user=O9djN1AAAAAJ&hl=en
    homepage: https://sites.google.com/corp/ttic.edu/weiranwang/home
    institution: Google
    last_name: Wang
    name: Weiran Wang
    username: ~Weiran_Wang1
  - dblp_id: https://dblp.org/pid/87/8758
    emails: '****@gmail.com'
    first_name: Rohit
    google_scholar_id: https://scholar.google.com/citations?user=JgltxisAAAAJ&hl=en
    homepage: https://research.google/people/RohitPrabhavalkar/
    institution: Google
    last_name: Prabhavalkar
    name: Rohit Prabhavalkar
    semantic_scholar_id: https://www.semanticscholar.org/author/Rohit-Prabhavalkar/2557391
    username: ~Rohit_Prabhavalkar1
  - dblp_id: https://dblp.org/pid/294/0017.html
    emails: '****@gmail.com'
    first_name: Haozhe
    homepage: http://hzshan.github.io
    institution: Harvard University
    last_name: Shan
    name: Haozhe Shan
    username: ~Haozhe_Shan1
  - dblp_id: https://dblp.org/pid/194/1220.html
    emails: '****@ieee.org'
    first_name: Zhong
    google_scholar_id: https://scholar.google.com/citations?user=SR7j060AAAAJ&hl=en
    institution: Google
    last_name: Meng
    name: Zhong Meng
    semantic_scholar_id: https://www.semanticscholar.org/author/Zhong-Meng/134905390
    username: ~Zhong_Meng1
  - dblp_id: https://dblp.org/pid/303/4326.html
    emails: '****@gmail.com'
    first_name: Dongseong
    google_scholar_id: https://scholar.google.com/citations?user=YePvlA8AAAAJ&hl=en
    homepage: https://github.com/ds-hwang
    last_name: Hwang
    name: Dongseong Hwang
    orcid: https://orcid.org/0000-0001-7369-5390
    username: ~Dongseong_Hwang1
  - dblp_id: https://dblp.org/pid/209/4881
    emails: '****@google.com'
    first_name: Qiujia
    google_scholar_id: https://scholar.google.co.uk/citations?user=Qwn0QegAAAAJ&hl=en
    homepage: http://liqiujia.com/
    institution: Google
    last_name: Li
    name: Qiujia Li
    username: ~Qiujia_Li1
  - dblp_id: https://dblp.org/pid/78/6873
    emails: '****@google.com'
    first_name: Khe Chai
    institution: Google
    last_name: Sim
    name: Khe Chai Sim
    username: ~Khe_Chai_Sim1
  - dblp_id: https://dblp.org/pid/50/3402-28
    emails: '****@google.com'
    first_name: Bo
    google_scholar_id: https://scholar.google.com/citations?user=iRhp1PAAAAAJ&hl=en
    institution: Google
    last_name: Li
    name: Bo Li
    orcid: https://orcid.org/0000-0002-6711-3603
    username: ~Bo_Li1
  - emails: '****@google.com'
    first_name: James
    homepage: https://scholar.google.com/citations?hl=en&user=zkDuH4MAAAAJ
    institution: Google
    last_name: Qin
    name: James Qin
    username: ~James_Qin1
  - dblp_id: https://dblp.org/pid/174/2232
    emails: '****@uconn.edu'
    first_name: Xingyu
    google_scholar_id: https://scholar.google.com/citations?user=Lanp-E0AAAAJ&hl=zh-CN
    institution: Google
    last_name: Cai
    name: Xingyu Cai
    username: ~Xingyu_Cai1
  - emails: '****@google.com'
    first_name: Adam
    homepage: https://github.com/astooke
    last_name: Stooke
    name: Adam Stooke
    username: ~Adam_Stooke3
  - emails: '****@google.com'
    first_name: Chengjian
    google_scholar_id: https://scholar.google.com/citations?user=CTsjLpwAAAAJ&hl=en&oi=sra
    homepage: https://scholar.google.com/citations?user=CTsjLpwAAAAJ&hl=en&oi=sra
    institution: Google
    last_name: Zheng
    name: Chengjian Zheng
    username: ~Chengjian_Zheng1
  - dblp_id: https://dblp.org/pid/122/0851
    emails: '****@google.com'
    first_name: Yanzhang
    google_scholar_id: https://scholar.google.com/citations?user=2JH-fJYAAAAJ&hl=en&oi=ao
    institution: Google Inc.
    last_name: He
    name: Yanzhang He
    username: ~Yanzhang_He1
  - dblp_id: https://dblp.org/pid/28/7825
    emails: '****@google.com'
    first_name: Tara
    google_scholar_id: https://scholar.google.com/citations?user=RtQA6Z8AAAAJ&hl=en
    homepage: https://sites.google.com/site/tsainath/
    institution: Google
    last_name: Sainath
    middle_name: N
    name: Tara N Sainath
    username: ~Tara_N_Sainath1
  - emails: '****@google.com'
    first_name: Pedro
    google_scholar_id: https://scholar.google.com/citations?user=fzWAWboAAAAJ&hl=en
    last_name: Moreno Mengibar
    middle_name: J
    name: Pedro J Moreno Mengibar
    username: ~Pedro_J_Moreno_Mengibar1
  decision: toMainConference
  end_page: 6804
  file: 722.pdf
  id: 722
  num_pages: 12
  openreview_id: ZBrbM1PYaH
  pdf_file: e58ee5131c1b7aff3b148ed94ab8458ed5b8f007.pdf
  start_page: 6793
  title: Massive End-to-end Speech Recognition Models with Time Reduction
- abstract: Situations and events evoke emotions in humans, but to what extent do
    they inform the prediction of emotion detection models? This work investigates
    how well human-annotated emotion triggers correlate with features that models
    deemed salient in their prediction of emotions. First, we introduce a novel dataset
    EmoTrigger, consisting of 900 social media posts sourced from three different
    datasets; these were annotated by experts for emotion triggers with high agreement.
    Using EmoTrigger, we evaluate the ability of  large language models (LLMs) to
    identify emotion triggers, and conduct a comparative analysis of the features
    considered important for these tasks between LLMs and  fine-tuned models. Our
    analysis reveals that emotion triggers are largely not considered salient features
    for emotion prediction models, instead there is intricate interplay between various
    features and the task of emotion detection.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Sentiment Analysis, Stylistic Analysis, and Argument Mining
  authors:
  - emails: '****@utexas.edu'
    first_name: Smriti
    google_scholar_id: https://scholar.google.com/citations?user=idOI-IAAAAAJ&hl=en
    homepage: https://www.smritisingh26.com
    last_name: Singh
    name: Smriti Singh
    username: ~Smriti_Singh2
  - dblp_id: https://dblp.uni-trier.de/pid/69/6680
    emails: '****@uic.edu'
    first_name: Cornelia
    homepage: https://www.cs.uic.edu/~cornelia/
    institution: University of Illinois, Chicago
    last_name: Caragea
    name: Cornelia Caragea
    username: ~Cornelia_Caragea2
  - dblp_id: https://dblp.org/pid/148/9553
    emails: '****@austin.utexas.edu'
    first_name: Junyi Jessy
    google_scholar_id: https://scholar.google.com/citations?user=tJGm3-YAAAAJ&hl=en
    homepage: https://jessyli.com
    institution: University of Texas, Austin
    last_name: Li
    name: Junyi Jessy Li
    semantic_scholar_id: https://www.semanticscholar.org/author/Junyi-Jessy-Li/22319255
    username: ~Junyi_Jessy_Li2
  decision: toMainConference
  end_page: 6816
  file: 723.pdf
  id: 723
  num_pages: 12
  openreview_id: vAK5YIO7O5
  pdf_file: 9baa1e70b438ff8cfab85c0be11225abf6388837.pdf
  start_page: 6805
  title: Language Models (Mostly) Do Not Consider Emotion Triggers When Predicting
    Emotion
- abstract: Transformer-based models, such as BERT and ViT, have achieved state-of-the-art
    results across different natural language processing (NLP) and computer vision
    (CV) tasks. However, these models are extremely memory intensive during their
    fine-tuning process, making them difficult to deploy on GPUs with limited memory
    resources. To address this issue, we introduce a new tool called SlimFit that
    reduces the memory requirements of these models by dynamically analyzing their
    training dynamics and freezing less-contributory layers during fine-tuning. The
    layers to freeze are chosen using a runtime inter-layer scheduling algorithm.
    This allows SlimFit to freeze up to 95% of layers and reduce the overall on-device
    GPU memory usage of transformer-based models such as ViT and BERT by an average
    of 2.2x, across different NLP and CV benchmarks/datasets such as GLUE, SQuAD 2.0,
    CIFAR-10, CIFAR-100 and ImageNet with an average degradation of 0.2% in accuracy.
    For such NLP and CV tasks, SlimFit can reduce up to 3.1x the total on-device memory
    usage with an accuracy degradation of only up to 0.4%. As a result, while fine-tuning
    of ViT on ImageNet and BERT on SQuAD 2.0 with a batch size of 128 requires 3 and
    2 32GB GPUs, respectively, SlimFit enables fine-tuning them on a single 32GB GPU
    without any significant accuracy degradation. The code of SlimFit is available
    at https://github.com/arashardakani/SlimFit.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Efficient/Low-Resource Methods for NLP
  authors:
  - dblp_id: http://dblp.uni-trier.de/pers/hd/a/Ardakani:Arash
    emails: '****@berkeley.edu'
    first_name: Arash
    google_scholar_id: https://scholar.google.com/citations?user=QFXIpuIAAAAJ&hl=en&oi=ao
    homepage: https://sky.cs.berkeley.edu/people/
    last_name: Ardakani
    name: Arash Ardakani
    username: ~Arash_Ardakani1
  - emails: '****@cs.washington.edu'
    first_name: Altan
    homepage: https://altanh.com/
    last_name: Haan
    name: Altan Haan
    username: ~Altan_Haan1
  - dblp_id: https://dblp.org/pid/280/5457
    emails: '****@berkeley.edu'
    first_name: Shangyin
    homepage: https://shangyit.me/
    last_name: Tan
    name: Shangyin Tan
    username: ~Shangyin_Tan1
  - emails: '****@lbl.gov'
    first_name: Doru Thom
    last_name: Popovici
    name: Doru Thom Popovici
    username: ~Doru_Thom_Popovici1
  - dblp_id: https://dblp.org/pid/16/4915
    emails: '****@berkeley.edu'
    first_name: Alvin
    last_name: Cheung
    name: Alvin Cheung
    username: ~Alvin_Cheung2
  - emails: '****@lbl.gov'
    first_name: Costin
    google_scholar_id: https://scholar.google.com/citations?user=FSqDShoAAAAJ&hl=en
    homepage: https://scholar.google.com/citations?user=FSqDShoAAAAJ&hl=en
    last_name: Iancu
    name: Costin Iancu
    username: ~Costin_Iancu1
  - dblp_id: https://dblp.uni-trier.de/pid/04/418.html
    emails: '****@berkeley.edu'
    first_name: Koushik
    google_scholar_id: https://scholar.google.com/citations?user=Vn3L_ioAAAAJ&hl=en&oi=ao
    homepage: https://people.eecs.berkeley.edu/~ksen/
    institution: UC Berkeley, University of California, Berkeley
    last_name: Sen
    name: Koushik Sen
    username: ~Koushik_Sen2
  decision: toMainConference
  end_page: 6835
  file: 724.pdf
  id: 724
  num_pages: 19
  openreview_id: uyuQfvqoDz
  pdf_file: a99093feb08604ad582a52a3db3dcb097e60627a.pdf
  start_page: 6817
  title: 'SlimFit: Memory-Efficient Fine-Tuning of Transformer-based Models Using
    Training Dynamics'
- abstract: 'Large language models (LLMs) have achieved remarkable advancements in
    natural language understanding and generation. However, one major issue towards
    their widespread deployment in the real world is that they can generate "hallucinated"
    answers that are not factual.

    Towards this end, this paper focuses on improving LLMs by grounding their responses
    in retrieved passages and by providing citations. We propose a new framework,
    AGREE, Adaptation for GRounding EnhancEment, that improves the grounding from
    a holistic perspective. Our framework tunes LLMs to self-ground the claims in
    their responses and provide accurate citations to retrieved documents. This tuning
    on top of the pre-trained LLMs requires well-grounded responses (with citations)
    for paired queries, for which we introduce a method that can automatically construct
    such data from unlabeled queries. The self-grounding capability of tuned LLMs
    further grants them a test-time adaptation (TTA) capability that can actively
    retrieve passages to support the claims that have not been grounded, which iteratively
    improves the responses of LLMs. Across five datasets and two LLMs, our results
    show that the proposed tuning-based \agre{} framework generates superior grounded
    responses with more accurate citations compared to prompting-based approaches
    and post-hoc citing-based approaches.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Question Answering
  authors:
  - dblp_id: https://dblp.org/pid/56/8549
    emails: '****@cs.utexas.edu'
    first_name: Xi
    google_scholar_id: https://scholar.google.com/citations?user=qH83GlAAAAAJ
    homepage: https://www.cs.utexas.edu/~xiye/
    last_name: Ye
    name: Xi Ye
    semantic_scholar_id: https://www.semanticscholar.org/author/Xi-Ye/50183897
    username: ~Xi_Ye2
  - dblp_id: https://dblp.org/pid/72/7683
    emails: '****@google.com'
    first_name: Ruoxi
    google_scholar_id: https://scholar.google.com/citations?user=ut1-7LAAAAAJ&hl=en
    institution: Google
    last_name: Sun
    name: Ruoxi Sun
    username: ~Ruoxi_Sun2
  - emails: '****@google.com'
    first_name: Sercan
    homepage: https://www.sercanarik.com/
    institution: Google
    last_name: Arik
    middle_name: O
    name: Sercan O Arik
    username: ~Sercan_O_Arik1
  - dblp_id: https://dblp.org/pid/14/8360
    emails: '****@google.com'
    first_name: Tomas
    google_scholar_id: https://scholar.google.com/citations?user=ahSpJOAAAAAJ&hl=en
    homepage: http://tomas.pfister.fi
    institution: Google
    last_name: Pfister
    name: Tomas Pfister
    orcid: https://orcid.org/0009-0004-4088-8718
    semantic_scholar_id: https://www.semanticscholar.org/author/Tomas-Pfister/1945962
    username: ~Tomas_Pfister1
  decision: toMainConference
  end_page: 6850
  file: 727.pdf
  id: 727
  num_pages: 15
  openreview_id: 2QL5xjkKqL
  pdf_file: 495c1e94ba6e44776d6ce3b5c2ef82a42342e5fe.pdf
  start_page: 6836
  title: Effective Large Language Model Adaptation for Improved Grounding and Citation
    Generation
- abstract: "We study how to apply large language models to write grounded and organized\
    \ long-form articles from scratch, with comparable breadth and depth to Wikipedia\
    \ pages. This underexplored problem poses new challenges at the pre-writing stage,\
    \ including how to research the topic and prepare an outline prior to writing.\
    \ We propose STORM, a writing system for the Synthesis of Topic Outlines through\n\
    Retrieval and Multi-perspective Question Asking. STORM models the pre-writing\
    \ stage by (1) discovering diverse perspectives in researching the given topic,\
    \ (2) simulating conversations where writers carrying different perspectives pose\
    \ questions to a topic expert grounded on trusted Internet sources, (3) curating\
    \ the collected information to create an outline.\n\nFor evaluation, we curate\
    \ FreshWiki, a dataset of recent high-quality Wikipedia articles, and formulate\
    \ outline assessments to evaluate the pre-writing stage. We further gather feedback\
    \ from experienced Wikipedia editors. Compared to articles generated by an outline-driven\
    \ retrieval-augmented baseline, more of STORM\u2019s articles are deemed to be\
    \ organized (by a 25% absolute increase) and broad in coverage (by 10%). The expert\
    \ feedback also helps identify new challenges for generating grounded long articles,\
    \ such as source bias transfer and over-association of unrelated facts."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Generation
  authors:
  - dblp_id: https://dblp.org/pid/329/4063
    emails: '****@stanford.edu'
    first_name: Yijia
    google_scholar_id: https://scholar.google.com/citations?user=H0zcQh4AAAAJ&hl=en
    homepage: https://cs.stanford.edu/~shaoyj/
    institution: Computer Science Department, Stanford University
    last_name: Shao
    name: Yijia Shao
    semantic_scholar_id: https://www.semanticscholar.org/author/Yijia-Shao/74175857
    username: ~Yijia_Shao1
  - emails: '****@stanford.edu'
    first_name: Yucheng
    homepage: https://yucheng-jiang.github.io
    last_name: Jiang
    name: Yucheng Jiang
    orcid: https://orcid.org/0000-0003-4129-9339
    username: ~Yucheng_Jiang3
  - emails: '****@gmail.com'
    first_name: Theodore
    last_name: Kanell
    middle_name: A
    name: Theodore A Kanell
    username: ~Theodore_A_Kanell1
  - emails: '****@stanford.edu'
    first_name: Peter
    last_name: Xu
    name: Peter Xu
    username: ~Peter_Xu1
  - dblp_id: https://dblp.org/pid/129/7815
    emails: '****@stanford.edu'
    first_name: Omar
    homepage: https://scholar.google.com/citations?hl=en&user=Lwr5ozgAAAAJ
    last_name: Khattab
    name: Omar Khattab
    semantic_scholar_id: https://www.semanticscholar.org/author/O.-Khattab/144112155
    username: ~Omar_Khattab1
  - dblp_id: https://dblp.org/pid/l/MonicaSLam
    emails: '****@cs.stanford.edu'
    first_name: Monica
    google_scholar_id: https://scholar.google.com.tw/citations?user=fVaS7a8AAAAJ
    homepage: https://cs.stanford.edu/~lam/
    institution: Stanford University
    last_name: Lam
    name: Monica Lam
    username: ~Monica_Lam1
  decision: toMainConference
  end_page: 6877
  file: 730.pdf
  id: 730
  num_pages: 27
  openreview_id: VG6WDtV8Ev
  pdf_file: cf2adc481183b8cebe323e9ce1e62d47705a8607.pdf
  start_page: 6851
  title: Assisting in Writing Wikipedia-like Articles From Scratch with Large Language
    Models
- abstract: 'Effective conversation requires common ground: a shared understanding
    between the participants. Common ground, however, does not emerge spontaneously
    in conversation. Speakers and listeners work together to both identify and construct
    a shared basis while avoiding misunderstanding. To accomplish grounding, humans
    rely on a range of dialogue acts, like clarification (What do you mean?) and acknowledgment
    (I understand.). However, it is unclear whether large language models (LLMs) generate
    text that reflects human grounding. To this end, we curate a set of grounding
    acts and propose corresponding metrics that quantify attempted grounding. We study
    whether LLM generations contain grounding acts, simulating turn-taking from several
    dialogue datasets and comparing results to humans. We find that---compared to
    humans---LLMs generate language with less conversational grounding, instead generating
    text that appears to simply presume common ground. To understand the roots of
    the identified grounding gap, we examine the role of instruction tuning and preference
    optimization, finding that training on contemporary preference data leads to a
    reduction in generated grounding acts. Altogether, we highlight the need for more
    research investigating conversational grounding in human-AI interaction.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Dialogue and Interactive Systems
  authors:
  - dblp_id: https://dblp.org/pid/191/9147
    emails: '****@stanford.edu'
    first_name: Omar
    google_scholar_id: https://scholar.google.com/citations?user=vdPcthgAAAAJ&hl=en
    homepage: https://oshaikh.com/
    institution: Stanford University
    last_name: Shaikh
    name: Omar Shaikh
    semantic_scholar_id: https://www.semanticscholar.org/author/Omar-Shaikh/1380218838
    username: ~Omar_Shaikh1
  - dblp_id: https://dblp.org/pid/218/5344
    emails: '****@stanford.edu'
    first_name: Kristina
    google_scholar_id: https://scholar.google.com/citations?user=kdbzgK4AAAAJ&hl=en
    homepage: https://kristinagligoric.github.io/
    institution: Stanford University
    last_name: Gligoric
    name: Kristina Gligoric
    username: ~Kristina_Gligoric1
  - emails: '****@stanford.edu'
    first_name: Ashna
    homepage: https://ashnakhetan.github.io/
    last_name: Khetan
    name: Ashna Khetan
    username: ~Ashna_Khetan1
  - dblp_id: https://dblp.org/pid/182/1338
    emails: '****@seas.harvard.edu'
    first_name: Matthias
    google_scholar_id: https://scholar.google.com/citations?user=qEirpPYAAAAJ
    homepage: https://matthias.gerstgrasser.net/
    last_name: Gerstgrasser
    name: Matthias Gerstgrasser
    username: ~Matthias_Gerstgrasser1
  - dblp_id: https://dblp.org/pid/70/11145
    emails: '****@cs.stanford.edu'
    first_name: Diyi
    google_scholar_id: https://scholar.google.com/citations?user=j9jhYqQAAAAJ&hl=en
    homepage: https://cs.stanford.edu/~diyiy/
    institution: Stanford University
    last_name: Yang
    name: Diyi Yang
    username: ~Diyi_Yang2
  - dblp_id: https://dblp.org/pid/31/985
    emails: '****@stanford.edu'
    first_name: Dan
    google_scholar_id: https://scholar.google.com/citations?user=uZg9l58AAAAJ
    homepage: http://web.stanford.edu/~jurafsky/
    institution: Stanford University
    last_name: Jurafsky
    name: Dan Jurafsky
    username: ~Dan_Jurafsky1
  decision: toMainConference
  end_page: 6895
  file: 731.pdf
  id: 731
  num_pages: 18
  openreview_id: 3LsAVAPCF4
  pdf_file: a0ff2fb87d4b596557886d4a57130a737607c157.pdf
  start_page: 6878
  title: Grounding Gaps in Language Model Generations
- abstract: Multilingual machine translation (MMT), trained on a mixture of parallel
    and monolingual data, is key for improving translation in low-resource language
    pairs. However, the literature offers conflicting results on the performance of
    different methods of including monolingual data. To resolve this, we examine how
    denoising autoencoding (DAE) and backtranslation (BT) impact MMT under different
    data conditions and model scales. Unlike prior studies, we use a realistic dataset
    of 100 translation directions and consider many domain combinations of monolingual
    and test data. We find that monolingual data generally helps MMT, but models are
    surprisingly brittle to domain mismatches, especially at smaller model scales.
    BT is beneficial when the parallel, monolingual, and test data sources are similar
    but can be detrimental otherwise, while DAE is less effective than previously
    reported. Next, we analyze the impact of scale (from 90M to 1.6B parameters) and
    find it is important for both methods, particularly DAE. As scale increases, DAE
    transitions from underperforming the parallel-only baseline at 90M to converging
    with BT performance at 1.6B, and even surpassing it in low-resource. These results
    offer new insights into how to best use monolingual data in MMT.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Translation
  authors:
  - dblp_id: https://dblp.org/pid/203/9597
    emails: '****@gmail.com'
    first_name: Christos
    google_scholar_id: https://scholar.google.com/citations?user=nP81eYkAAAAJ
    homepage: https://cbaziotis.github.io/
    institution: University of Edinburgh, University of Edinburgh
    last_name: Baziotis
    name: Christos Baziotis
    semantic_scholar_id: https://www.semanticscholar.org/author/40928701
    username: ~Christos_Baziotis1
  - dblp_id: https://dblp.uni-trier.de/pers/hd/z/Zhang_0002:Biao
    emails: '****@google.com'
    first_name: Biao
    google_scholar_id: https://scholar.google.com/citations?user=gqPKjaIAAAAJ&hl=zh-CN
    institution: Google Research
    last_name: Zhang
    name: Biao Zhang
    semantic_scholar_id: https://www.semanticscholar.org/author/Biao-Zhang/48335426
    username: ~Biao_Zhang2
  - dblp_id: https://dblp.org/pid/24/6740
    emails: '****@ed.ac.uk'
    first_name: Alexandra
    google_scholar_id: https://scholar.google.co.uk/citations?user=gZOV9kMAAAAJ&hl=en&oi=sra
    homepage: http://homepages.inf.ed.ac.uk/abmayne/
    institution: University of Edinburgh
    last_name: Birch
    name: Alexandra Birch
    semantic_scholar_id: https://www.semanticscholar.org/author/Alexandra-Birch/2539211
    username: ~Alexandra_Birch1
  - dblp_id: https://dblp.org/pid/12/5915
    emails: '****@ed.ac.uk'
    first_name: Barry
    google_scholar_id: https://scholar.google.com/citations?user=6NqRjRYAAAAJ&hl=en
    homepage: https://homepages.inf.ed.ac.uk/bhaddow/
    last_name: Haddow
    name: Barry Haddow
    username: ~Barry_Haddow1
  decision: toMainConference
  end_page: 6923
  file: 733.pdf
  id: 733
  num_pages: 28
  openreview_id: oIF8Ig5DhA
  pdf_file: 5f7cf3f9fc0d6f976ecbfbd334e4db7403dd89cd.pdf
  start_page: 6896
  title: 'When Does Monolingual Data Help Multilingual Translation: The Role of Domain
    and Model Scale'
- abstract: 'Recent work has compared neural network representations via similarity-based
    analyses to improve model interpretation. The quality of a similarity measure
    is typically evaluated by its success in assigning a high score to representations
    that are expected to be matched. However, existing similarity measures perform
    mediocrely on standard benchmarks.  In this work, we develop a new similarity
    measure, dubbed ContraSim, based on contrastive learning. In contrast to common
    closed-form similarity measures, ContraSim learns a parameterized measure by using
    both similar and dissimilar examples.  We perform an extensive experimental evaluation
    of our method, with both language and vision models, on the standard layer prediction
    benchmark and two new benchmarks that we introduce: the multilingual benchmark
    and the image--caption benchmark. In all cases, ContraSim achieves much higher
    accuracy than previous similarity measures, even when presented with challenging
    examples. Finally, ContraSim is more suitable for the analysis of neural networks,
    revealing new insights not captured by previous measures.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - emails: '****@campus.technion.ac.il'
    first_name: Adir
    last_name: Rahamim
    name: Adir Rahamim
    username: ~Adir_Rahamim1
  - dblp_id: https://dblp.org/pid/136/8705
    emails: '****@technion.ac.il'
    first_name: Yonatan
    google_scholar_id: https://scholar.google.com/citations?authorid=K-6ujU4AAAAJ&user=K-6ujU4AAAAJ
    homepage: https://www.belinkov.com
    institution: Technion, Technion
    last_name: Belinkov
    name: Yonatan Belinkov
    semantic_scholar_id: https://www.semanticscholar.org/search?q=%22Yonatan%20Belinkov%22&sort=relevance
    username: ~Yonatan_Belinkov1
  decision: toMainConference
  end_page: 6938
  file: 735.pdf
  id: 735
  num_pages: 15
  openreview_id: eh1FFHMQQc
  pdf_file: caf119bba51157e8599f3570431b1abeb1b62f47.pdf
  start_page: 6924
  title: "ContraSim \u2013 Analyzing Neural Representations Based on Contrastive Learning"
- abstract: Text-to-Image (T2I) models have shown great performance in generating
    images based on textual prompts. However, these models are vulnerable to unsafe
    input to generate unsafe content like sexual, harassment and illegal-activity
    images. Existing studies based on image checker, model fine-tuning and embedding
    blocking are impractical in real-world applications. Hence, we propose the first
    universal **p**rompt **o**ptimizer for **s**afe T2**I** (**POSI**) generation
    in black-box scenario. We first construct a dataset consisting of toxic-clean
    prompt pairs by GPT-3.5 Turbo. To guide the optimizer to have the ability of converting
    toxic prompt to clean prompt while preserving semantic information, we design
    a novel reward function measuring toxicity and text alignment of generated images
    and train the optimizer through Proximal Policy Optimization. Experiments show
    that our approach can effectively reduce the likelihood of various T2I models
    in generating inappropriate images, with no significant impact on text alignment.
    It is also flexible to be combined with methods to achieve better performance.
    Our code is available at [https://github.com/wzongyu/POSI](https://github.com/wzongyu/POSI).
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multimodality and Language Grounding to Vision, Robotics and Beyond
  authors:
  - dblp_id: https://dblp.dagstuhl.de/pid/322/4801.html
    emails: '****@psu.edu'
    first_name: Zongyu
    google_scholar_id: https://scholar.google.com/citations?user=uq-XDSYAAAAJ&hl=en&oi=ao
    homepage: https://wzongyu.github.io/
    institution: Pennsylvania State University
    last_name: Wu
    name: Zongyu Wu
    semantic_scholar_id: https://www.semanticscholar.org/author/Zongyu-Wu/2284641919
    username: ~Zongyu_Wu1
  - dblp_id: https://dblp.org/pid/318/1404
    emails: '****@gmail.com'
    first_name: Hongcheng
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=pF5p7_IAAAAJ
    homepage: https://gao-hongcheng.github.io/
    last_name: Gao
    name: Hongcheng Gao
    semantic_scholar_id: https://www.semanticscholar.org/author/Hongcheng-Gao/2162081759
    username: ~Hongcheng_Gao1
  - dblp_id: https://dblp.org/pid/335/2676
    emails: '****@tju.edu.cn'
    first_name: Yueze
    institution: Beijing Academy of Artificial Intelligence and Tianjin University
    last_name: Wang
    name: Yueze Wang
    semantic_scholar_id: https://www.semanticscholar.org/author/Yueze-Wang/2217456303
    username: ~Yueze_Wang1
  - emails: '****@ist.psu.edu'
    first_name: Xiang
    homepage: https://faculty.ist.psu.edu/xzz89/
    institution: Pennsylvania State University
    last_name: Zhang
    name: Xiang Zhang
    username: ~Xiang_Zhang4
  - dblp_id: https://dblp.org/pid/136/9440
    emails: '****@psu.edu'
    first_name: Suhang
    google_scholar_id: https://scholar.google.com/citations?user=cdT_WMMAAAAJ&hl=en
    homepage: https://faculty.ist.psu.edu/szw494/
    institution: Pennsylvania State University
    last_name: Wang
    name: Suhang Wang
    username: ~Suhang_Wang1
  decision: toMainConference
  end_page: 6953
  file: 736.pdf
  id: 736
  num_pages: 15
  openreview_id: LuBE6jT9PV
  pdf_file: 0ce1aa23224f67806712e374248e77ea81b4d88d.pdf
  start_page: 6939
  title: Universal Prompt Optimizer for Safe Text-to-Image Generation
- abstract: Previous methods based on Large Language Models (LLM) perform unsupervised
    dependency parsing by maximizing bi-lexical dependence scores. However, these
    previous methods adopt dependence scores that are difficult to interpret. These
    methods cannot incorporate grammatical constraints that previous grammar-based
    parsing research has shown beneficial to improving parsing performance. In this
    work, we apply Conditional Mutual Information (CMI), an interpretable metric,
    to measure the bi-lexical dependence and incorporate grammatical constraints into
    LLM-based unsupervised parsing. We incorporate Part-Of-Speech information as a
    grammatical constraint at the CMI estimation stage and integrate two additional
    grammatical constraints at the subsequent tree decoding stage. We find that the
    CMI score positively correlates with syntactic dependencies and has a stronger
    correlation with the syntactic dependencies than baseline scores. Our experiment
    confirms the benefits and applicability of the proposed grammatical constraints
    across five languages and eight datasets. The CMI parsing model outperforms state-of-the-art
    LLM-based models and similarly constrained grammar-based models. Our analysis
    reveals that the CMI model is strong in retrieving dependency relations with rich
    lexical interactions but is weak in retrieving relations with sparse lexical interactions,
    indicating a potential limitation in CMI-based unsupervised parsing methods.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: 'Syntax: Tagging, Chunking and Parsing'
  authors:
  - dblp_id: https://dblp.org/pid/04/4498
    emails: '****@outlook.com'
    first_name: Junjie
    google_scholar_id: https://scholar.google.com/citations?hl=en&view_op=list_works&gmla=AILGF5WttZR_ltNQuBvO6AGkN0Lx1hlCsOrxUdBacgYoTZywxg529urmgEvOL1AHHbEbEnVDslrEKdCfczSaWXZC3YqfteBpxwvD0cgajfE&user=Z_r1ArsAAAAJ
    homepage: https://github.com/ChristoMartin
    institution: Tokyo University, Tokyo Institute of Technology
    last_name: Chen
    name: Junjie Chen
    username: ~Junjie_Chen3
  - dblp_id: https://dblp.org/pid/233/4490.html
    emails: '****@imperial.ac.uk'
    first_name: Xiangheng
    google_scholar_id: https://scholar.google.com/citations?user=4M0wotYAAAAJ&hl=zh-CN
    last_name: He
    name: Xiangheng He
    orcid: https://orcid.org/0000-0001-6676-3893
    semantic_scholar_id: https://www.semanticscholar.org/author/Xiangheng-He/66528414
    username: ~Xiangheng_He1
  - dblp_id: https://dblp.org/pid/34/467.html
    emails: '****@is.s.u-tokyo.ac.jp'
    first_name: Yusuke
    homepage: https://mynlp.is.s.u-tokyo.ac.jp/en/
    institution: The University of Tokyo
    last_name: Miyao
    name: Yusuke Miyao
    semantic_scholar_id: https://www.semanticscholar.org/author/Yusuke-Miyao/1768065
    username: ~Yusuke_Miyao2
  decision: toMainConference
  end_page: 6965
  file: 738.pdf
  id: 738
  num_pages: 12
  openreview_id: 32apIWWqI9
  pdf_file: ef7f9fb2b17a2ad971cedaebe0c8a1ec9dfefd5f.pdf
  start_page: 6954
  title: Language Model Based Unsupervised Dependency Parsing with Conditional Mutual
    Information and Grammatical Constraints
- abstract: Bias amplification is a phenomenon in which models exacerbate biases or
    stereotypes present in the training data. In this paper, we study bias amplification
    in the text-to-image domain using Stable Diffusion by comparing gender ratios
    in training vs. generated images. We find that the model appears to amplify gender-occupation
    biases found in the training data (LAION) considerably. However, we discover that
    amplification can be largely attributed to discrepancies between training captions
    and model prompts. For example, an inherent difference is that captions from the
    training data often contain explicit gender information while our prompts do not,
    which leads to a distribution shift and consequently inflates bias measures. Once
    we account for distributional differences between texts used for training and
    generation when evaluating amplification, we observe that amplification decreases
    drastically. Our findings illustrate the challenges of comparing biases in models
    and their training data, as well as evaluation more broadly, and highlight how
    confounding factors can impact analyses.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - emails: '****@uci.edu'
    first_name: Preethi
    institution: University of California, Irvine
    last_name: Seshadri
    name: Preethi Seshadri
    username: ~Preethi_Seshadri2
  - dblp_id: https://dblp.org/pid/13/3568-1
    emails: '****@uci.edu'
    first_name: Sameer
    google_scholar_id: https://scholar.google.com/citations?user=-hGZC54AAAAJ
    homepage: http://sameersingh.org
    institution: University of California, Irvine and Allen Institute for Artificial
      Intelligence
    last_name: Singh
    name: Sameer Singh
    orcid: https://orcid.org/0000-0003-0621-6323
    semantic_scholar_id: https://www.semanticscholar.org/author/Sameer-Singh/34650964
    username: ~Sameer_Singh1
  - dblp_id: https://dblp.org/pid/223/4533
    emails: '****@gmail.com'
    first_name: Yanai
    google_scholar_id: https://scholar.google.co.il/citations?user=7p_Ce8kAAAAJ
    homepage: https://yanaiela.github.io
    institution: Allen Institute for Artificial Intelligence and Department of Computer
      Science
    last_name: Elazar
    name: Yanai Elazar
    semantic_scholar_id: https://www.semanticscholar.org/author/Yanai-Elazar/51131518
    username: ~Yanai_Elazar1
  decision: toMainConference
  end_page: 6983
  file: 739.pdf
  id: 739
  num_pages: 18
  openreview_id: h4EiJoymkA
  pdf_file: ecb9fbe4e5cfef24ce96ef430a4f98ef150418f1.pdf
  start_page: 6966
  title: The Bias Amplification Paradox in Text-to-Image Generation
- abstract: 'One of the main problems low-resource languages face in NLP can be pictured
    as a vicious circle: data is needed to build and test tools, but the available
    text is scarce and there are not powerful tools to collect it.

    In order to break this circle for Guarani, we explore if text automatically generated
    from a grammar can work as a Data Augmentation technique to boost the performance
    of Guarani-Spanish Machine Translation (MT) systems.

    After building a grammar-based system that generates Spanish text and syntactically
    transfers it to Guarani, we perform several experiments by pretraining models
    using this synthetic text.

    We find that the MT systems that are pretrained with synthetic text perform better,
    even outperforming previous baselines.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: 'Special Theme: Languages of Latin America'
  authors:
  - emails: '****@gmail.com'
    first_name: "Agust\xEDn"
    last_name: Lucas
    name: "Agust\xEDn Lucas"
    username: "~Agust\xEDn_Lucas1"
  - emails: '****@fing.edu.uy'
    first_name: Alexis
    last_name: "Balad\xF3n"
    name: "Alexis Balad\xF3n"
    username: "~Alexis_Balad\xF3n1"
  - emails: '****@fing.edu.uy'
    first_name: Victoria
    last_name: "Pardi\xF1as"
    name: "Victoria Pardi\xF1as"
    username: "~Victoria_Pardi\xF1as1"
  - dblp_id: https://dblp.org/pid/256/8837
    emails: '****@correo.ugr.es'
    first_name: Marvin
    homepage: https://mmaguero.github.io/
    last_name: "Ag\xFCero-Torales"
    middle_name: M.
    name: "Marvin M. Ag\xFCero-Torales"
    semantic_scholar_id: https://www.semanticscholar.org/author/Marvin-M.-Ag%C3%BCero-Torales/1490676863
    username: "~Marvin_M._Ag\xFCero-Torales1"
  - dblp_id: https://dblp.org/pid/277/0516
    emails: '****@fing.edu.uy'
    first_name: Santiago
    google_scholar_id: https://scholar.google.com/citations?user=p1lKpmYAAAAJ
    homepage: https://sites.google.com/view/sgongora/
    institution: "Facultad de Ingenier\xEDa and Facultad de Ingenier\xEDa"
    last_name: "G\xF3ngora"
    name: "Santiago G\xF3ngora"
    semantic_scholar_id: https://www.semanticscholar.org/author/Santiago-G%C3%B3ngora/2003583640
    username: "~Santiago_G\xF3ngora1"
  - dblp_id: https://dblp.org/pid/162/1557
    emails: '****@fing.edu.uy'
    first_name: Luis
    google_scholar_id: https://scholar.google.com/citations?user=C7c4uCsAAAAJ&hl=en
    institution: "Facultad de Ingenier\xEDa - Universidad de la Rep\xFAblica - Uruguay"
    last_name: Chiruzzo
    name: Luis Chiruzzo
    orcid: https://orcid.org/0000-0002-1697-4614
    semantic_scholar_id: https://www.semanticscholar.org/author/Luis-Chiruzzo/2287191
    username: ~Luis_Chiruzzo1
  decision: toMainConference
  end_page: 6996
  file: 740.pdf
  id: 740
  num_pages: 13
  openreview_id: bMEoIOXocL
  pdf_file: 6417079b09b0e6ee551d3569b1b83b3cef820f0c.pdf
  start_page: 6984
  title: 'Grammar-based Data Augmentation for Low-Resource Languages: The Case of
    Guarani-Spanish Neural Machine Translation'
- abstract: "Exploring the intersection of language and culture in Large Language\
    \ Models (LLMs), this study critically examines their capability to encapsulate\
    \ cultural nuances across diverse linguistic landscapes. Central to our investigation\
    \ are three research questions: the efficacy of language-specific instruction\
    \ tuning, the impact of pretraining on dominant language data, and the identification\
    \ of optimal approaches to elicit accurate cultural knowledge from LLMs. Utilizing\
    \ the GeoMLaMA benchmark for multilingual commonsense knowledge and an adapted\
    \ CAMeL dataset (English-only) for evaluation of nuanced cultural aspects, our\
    \ experiments span six different languages and cultural contexts, revealing the\
    \ extent of LLMs' cultural awareness. \nOur findings highlight a nuanced landscape:\
    \ while language-specific tuning and bilingual pretraining enhance cultural understanding\
    \ in certain contexts, they also uncover inconsistencies and biases, particularly\
    \ in non-Western cultures. This work expands our understanding of LLMs' cultural\
    \ competence and emphasizes the importance of integrating diverse cultural perspectives\
    \ in their development, aiming for a more globally representative and equitable\
    \ approach in language modeling."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multilinguality and Language Diversity
  authors:
  - dblp_id: https://dblp.org/pid/339/6660
    emails: '****@gmail.com'
    first_name: Anjishnu
    google_scholar_id: https://scholar.google.com/citations?user=3849YpIAAAAJ&hl=en
    homepage: https://iamshnoo.github.io
    institution: George Mason University
    last_name: Mukherjee
    name: Anjishnu Mukherjee
    orcid: https://orcid.org/0000-0003-4012-8466
    semantic_scholar_id: https://www.semanticscholar.org/author/A.-Mukherjee/2125631153
    username: ~Anjishnu_Mukherjee1
  - dblp_id: https://dblp.org/pid/116/4680
    emails: '****@uw.edu'
    first_name: Aylin
    google_scholar_id: https://scholar.google.com/citations?user=zxzZAi0AAAAJ&hl=en&oi=ao
    homepage: https://faculty.washington.edu/aylin/
    institution: University of Washington
    last_name: Caliskan
    name: Aylin Caliskan
    username: ~Aylin_Caliskan1
  - dblp_id: https://dblp.org/pid/159/9916
    emails: '****@gmu.edu'
    first_name: Ziwei
    google_scholar_id: https://scholar.google.com/citations?user=3S6pM7wAAAAJ&hl=en
    homepage: https://zziwei.github.io/
    institution: George Mason University
    last_name: Zhu
    name: Ziwei Zhu
    semantic_scholar_id: https://www.semanticscholar.org/author/Ziwei-Zhu/9725200
    username: ~Ziwei_Zhu1
  - dblp_id: https://dblp.org/pid/148/9479
    emails: '****@gmu.edu'
    first_name: Antonios
    google_scholar_id: https://scholar.google.com/citations?user=g_G_SNAAAAAJ&hl=en
    homepage: http://www.cs.gmu.edu/~antonis/
    institution: Athena Research Center and George Mason University
    last_name: Anastasopoulos
    name: Antonios Anastasopoulos
    orcid: https://orcid.org/0000-0002-8544-246X
    semantic_scholar_id: https://www.semanticscholar.org/author/Antonios-Anastasopoulos/49513989
    username: ~Antonios_Anastasopoulos1
  decision: toMainConference
  end_page: 7014
  file: 743.pdf
  id: 743
  num_pages: 18
  openreview_id: yXJGuRlPhn
  pdf_file: 9e4f0a2574ab44cc237b91282c5869f0acf21d45.pdf
  start_page: 6997
  title: 'Global Gallery: The Fine Art of Painting Culture Portraits through Multilingual
    Instruction Tuning'
- abstract: Many recent studies examining the knowledge capacity of large language
    models (LLM) have focused on knowledge explicitly learned from the pretraining
    data or implicitly inferable from similar contexts. However, the extent to which
    an LLM effectively captures corpus-level statistical trends of concepts for reasoning,
    especially long-tail ones, is largely underexplored. In this study, we introduce
    a novel few-shot question-answering task (CPopQA) that examines LLMs' statistical
    ranking abilities for long-tail cultural concepts (e.g., holidays), particularly
    focusing on these concepts' popularity in the United States and the United Kingdom,
    respectively. We curate a dataset of 457 holidays across 58 countries, generating
    a total of 9,000 QA testing pairs. Experiments on four strong LLMs show that open-sourced
    LLMs still lag way behind close LLM API (e.g., GPT-3.5) in statistical ranking
    of cultural concepts. Notably,  GPT-3.5 exhibited its potential to identify geo-cultural
    proximity across continents.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Computational Social Science and Cultural Analytics
  authors:
  - emails: '****@iu.edu'
    first_name: Ming
    google_scholar_id: https://scholar.google.com/citations?user=oV7edpEAAAAJ&hl=en
    homepage: https://seleenajm.github.io/
    institution: Indiana University
    last_name: Jiang
    name: Ming Jiang
    orcid: https://orcid.org/0000-0002-3604-166X
    username: ~Ming_Jiang6
  - emails: '****@iu.edu'
    first_name: Mansi
    last_name: Joshi
    name: Mansi Joshi
    username: ~Mansi_Joshi1
  decision: toMainConference
  end_page: 7030
  file: 744.pdf
  id: 744
  num_pages: 16
  openreview_id: loMsuolXkI
  pdf_file: 199eadf27adee1c9aa2195e90977a6247f8b1088.pdf
  start_page: 7015
  title: 'CPopQA: Ranking Cultural Concept Popularity by LLMs'
- abstract: Recent Vision-Language Pre-training (VLP) models have demonstrated significant
    advancements. Nevertheless, these models heavily rely on image-text pairs that
    capture only coarse and global information of an image, leading to a limitation
    in their regional understanding ability. In this work, we introduce \textbf{RegionVLM},
    equipped with explicit regional modeling capabilities, allowing them to understand
    user-indicated image regions. To achieve this, we design a simple yet innovative
    architecture, requiring no modifications to the model architecture or objective
    function. Additionally, we leverage a dataset that contains a novel source of
    information, namely Localized Narratives, which has been overlooked in previous
    VLP research. Our experiments demonstrate that our single generalist model not
    only achieves an interactive dialogue system but also exhibits superior performance
    on various zero-shot region understanding tasks, without compromising its ability
    for global image understanding.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multimodality and Language Grounding to Vision, Robotics and Beyond
  authors:
  - dblp_id: https://dblp.org/pid/218/6736
    emails: '****@snu.ac.kr'
    first_name: Jungbeom
    google_scholar_id: https://scholar.google.com/citations?user=6qTcgH0AAAAJ&hl=ko
    institution: Amazon
    last_name: Lee
    name: Jungbeom Lee
    username: ~Jungbeom_Lee1
  - dblp_id: https://dblp.org/pid/213/1095.html
    emails: '****@navercorp.com'
    first_name: Sanghyuk
    google_scholar_id: https://scholar.google.co.kr/citations?user=4_uj0xcAAAAJ
    homepage: https://sanghyukchun.github.io/home/
    institution: NAVER AI Lab
    last_name: Chun
    name: Sanghyuk Chun
    orcid: https://orcid.org/0000-0002-4533-2610
    semantic_scholar_id: https://www.semanticscholar.org/author/Sanghyuk-Chun/2647582
    username: ~Sanghyuk_Chun1
  - dblp_id: https://dblp.org/pid/124/3009.html
    emails: '****@gmail.com'
    first_name: Sangdoo
    google_scholar_id: https://scholar.google.com/citations?user=o0qtjzYAAAAJ&hl=en
    homepage: https://sangdooyun.github.io/
    institution: NAVER
    last_name: Yun
    name: Sangdoo Yun
    username: ~Sangdoo_Yun1
  decision: toMainConference
  end_page: 7044
  file: 747.pdf
  id: 747
  num_pages: 14
  openreview_id: dC8Q5kTS7w
  pdf_file: 8e0355f17dd1e8c013a550343eed4ad27016e0cf.pdf
  start_page: 7031
  title: Toward Interactive Regional Understanding in Vision-Large Language Models
- abstract: This paper critically examines the arithmetic capabilities of Large Language
    Models (LLMs), uncovering significant limitations in their performance. Our research
    reveals a notable decline in accuracy for complex calculations involving large
    numbers, with addition and subtraction tasks showing varying degrees of proficiency.
    Additionally, we challenge the notion that arithmetic is language-independent,
    finding up to a 10% difference in performance across twenty languages. The study
    also compares self-verification methods with cross-agent collaborations, showing
    that a single model often outperforms collaborative approaches in basic arithmetic
    tasks. These findings suggest a need to reassess the effectiveness of LLMs in
    tasks requiring numerical accuracy and precision.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - dblp_id: https://dblp.org/pid/177/6602
    emails: '****@gmail.com'
    first_name: Chung-Chi
    google_scholar_id: https://scholar.google.com/citations?user=sJwWSg8AAAAJ&hl=zh-TW
    homepage: http://nlg.csie.ntu.edu.tw/~cjchen/
    institution: AIST, National Institute of Advanced Industrial Science and Technology
    last_name: Chen
    name: Chung-Chi Chen
    orcid: https://orcid.org/0000-0003-3680-9277
    semantic_scholar_id: https://www.semanticscholar.org/author/Chung-Chi-Chen/3376667
    username: ~Chung-Chi_Chen1
  - dblp_id: https://dblp.org/pid/75/3612
    emails: '****@aist.go.jp'
    first_name: Hiroya
    google_scholar_id: https://scholar.google.com/citations?user=o57RFqgAAAAJ
    institution: AIST, National Institute of Advanced Industrial Science and Technology
    last_name: Takamura
    name: Hiroya Takamura
    orcid: https://orcid.org/0000-0002-3244-8294
    semantic_scholar_id: https://www.semanticscholar.org/author/Hiroya-Takamura/36514372
    username: ~Hiroya_Takamura1
  - dblp_id: https://dblp.org/pid/38/2479
    emails: '****@is.ocha.ac.jp'
    first_name: Ichiro
    institution: Ochanomizu University
    last_name: Kobayashi
    name: Ichiro Kobayashi
    username: ~Ichiro_Kobayashi1
  - dblp_id: https://dblp.org/pid/34/467.html
    emails: '****@is.s.u-tokyo.ac.jp'
    first_name: Yusuke
    homepage: https://mynlp.is.s.u-tokyo.ac.jp/en/
    institution: The University of Tokyo
    last_name: Miyao
    name: Yusuke Miyao
    semantic_scholar_id: https://www.semanticscholar.org/author/Yusuke-Miyao/1768065
    username: ~Yusuke_Miyao2
  decision: toMainConference
  end_page: 7051
  file: 749.pdf
  id: 749
  num_pages: 7
  openreview_id: bxU6qIGVXO
  pdf_file: 29e75dd078a5cee8260e784f4a3064a01bea5a91.pdf
  start_page: 7045
  title: 'The Impact of Language on Arithmetic Proficiency: A Multilingual Investigation
    with Cross-Agent Checking Computation'
- abstract: 'Despite the success of multilingual pretrained language models (mPLMs)
    for tasks such as dependency parsing (DEP) or part-of-speech (POS) tagging, their
    coverage of 100s of languages is still limited, as most of the 6500+ languages
    remains ``unseen''''. To adapt mPLMs for including such unseen langs, existing
    work has considered transliteration and vocabulary augmentation. Meanwhile, the
    consideration of combining the two has been surprisingly lacking. To understand
    why, we identify both complementary strengths of the two, and the hurdles to realizing
    it. Based on this observation, we propose ScriptMix, combining two strengths,
    and overcoming the hurdle.

    Specifically, ScriptMix a) is trained with dual-script corpus to combine strengths,
    but b) with separate modules to avoid gradient conflict. In combining modules
    properly, we also point out the limitation of the conventional method AdapterFusion,
    and propose AdapterFusion+ to overcome it. We empirically show ScriptMix is effective--
    ScriptMix improves the POS accuracy by up to 14%, and improves the DEP LAS score
    by up to 5.6%. Our code is publicly available.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multilinguality and Language Diversity
  authors:
  - dblp_id: https://dblp.org/pid/141/9456-2
    emails: '****@snu.ac.kr'
    first_name: Jaeseong
    institution: Seoul National University
    last_name: Lee
    name: Jaeseong Lee
    semantic_scholar_id: https://www.semanticscholar.org/author/Jaeseong-Lee/2125053163
    username: ~Jaeseong_Lee1
  - dblp_id: https://dblp.org/pid/297/3811
    emails: '****@snu.ac.kr'
    first_name: Dohyeon
    google_scholar_id: https://scholar.google.com/citations?user=Gjk6Zk0AAAAJ
    homepage: https://github.com/waylight3
    institution: Seoul National University
    last_name: Lee
    name: Dohyeon Lee
    username: ~Dohyeon_Lee1
  - dblp_id: https://dblp.org/pid/h/SeungwonHwang
    emails: '****@snu.ac.kr'
    first_name: Seung-won
    google_scholar_id: https://scholar.google.com/citations?user=63bBmc3mYrAC&hl=ko
    homepage: http://seungwonh.github.io
    institution: Seoul National University
    last_name: Hwang
    name: seung-won hwang
    semantic_scholar_id: https://www.semanticscholar.org/author/Seung-won-Hwang/1716415
    username: ~seung-won_hwang2
  decision: toMainConference
  end_page: 7066
  file: 754.pdf
  id: 754
  num_pages: 15
  openreview_id: Oh9F2L839j
  pdf_file: c990b504222456cbf21f2e6ff831ee19f632db1a.pdf
  start_page: 7052
  title: 'ScriptMix: Mixing Scripts for Low-resource Language Parsing'
- abstract: Large Language Models (LLM) have demonstrated their strong ability in
    the field of machine translation, yet they suffer from high computational cost
    and latency. Therefore, transferring translation knowledge from giant LLMs to
    medium-sized machine translation models is a promising research direction.  However,
    traditional knowledge distillation methods ignore the capability of student and
    teacher models, therefore repeatedly teaching student models on the knowledge
    they have learned, and failing to extend to novel contexts and knowledge. In this
    paper, we propose a framework called MT-Patcher, which transfers knowledge from
    LLMs to existing MT models in a selective, comprehensive and proactive manner.
    Considering the current translation ability of student MT models, we only identify
    and correct their translation errors, instead of distilling the whole translation
    from the teacher. Leveraging the strong language abilities of LLMs, we instruct
    LLM teachers to synthesize diverse contexts and anticipate more potential errors
    for the student. Experiment results on translating both specific language phenomena
    and general MT benchmarks demonstrate that finetuning the MT model on about 10%
    examples can achieve comparable results to the traditional knowledge distillation
    method, and synthesized potential errors and diverse contexts further improve
    MT performances on unseen contexts and words.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Translation
  authors:
  - dblp_id: https://dblp.org/pid/199/0384.html
    emails: '****@smail.nju.edu.cn'
    first_name: Jiahuan
    last_name: Li
    name: Jiahuan Li
    username: ~Jiahuan_Li1
  - dblp_id: https://dblp.org/pid/185/5589
    emails: '****@gmail.com'
    first_name: Shanbo
    google_scholar_id: https://scholar.google.com/citations?user=CYUBKN0AAAAJ&hl=en
    homepage: https://sites.google.com/view/chengshanbo/home
    institution: ByteDance Inc.
    last_name: Cheng
    name: Shanbo Cheng
    semantic_scholar_id: https://www.semanticscholar.org/author/Shanbo-Cheng/3456696
    username: ~Shanbo_Cheng1
  - dblp_id: https://dblp.org/pid/57/8451
    emails: '****@nju.edu.cn'
    first_name: Shujian
    google_scholar_id: https://scholar.google.com/citations?user=HF3-E9kAAAAJ&hl=en
    homepage: http://nlp.nju.edu.cn/huangsj/
    institution: Nanjing University
    last_name: Huang
    name: Shujian Huang
    username: ~Shujian_Huang1
  - emails: '****@nju.edu.cn'
    first_name: Jiajun
    google_scholar_id: https://scholar.google.com.tw/citations?user=WIF7VaoAAAAJ
    homepage: https://cs.nju.edu.cn/chenjiajun/index_en.htm
    institution: Nanjing University
    last_name: Chen
    name: Jiajun Chen
    username: ~Jiajun_Chen1
  decision: toMainConference
  end_page: 7081
  file: 756.pdf
  id: 756
  num_pages: 15
  openreview_id: tCZm7aazSY
  pdf_file: 83eae2989ad86e6efb0e935d910683c2b6fa9c98.pdf
  start_page: 7067
  title: 'MT-PATCHER: Selective and Extendable Knowledge Distillation from Large Language
    Models for Machine Translation'
- abstract: 'The proliferation of online toxic speech is a pertinent problem posing
    threats to demographic groups. While explicit toxic speech contains offensive
    lexical signals, implicit one consists of coded or indirect language. Therefore,
    it is crucial for models not only to detect implicit toxic speech but also to
    explain its toxicity. This draws a unique need for unified frameworks that can
    effectively detect and explain implicit toxic speech. Prior works mainly formulated
    the task of toxic speech detection and explanation as a text generation problem.
    Nonetheless, models trained using this strategy can be prone to suffer from the
    consequent error propagation problem. Moreover, our experiments reveal that the
    detection results of such models are much lower than those that focus only on
    the detection task. To bridge these gaps, we introduce ToXCL, a unified framework
    for the detection and explanation of implicit toxic speech. Our model consists
    of three modules: a (i) Target Group Generator to generate the targeted demographic
    group(s) of a given post; an (ii) Encoder-Decoder Model in which the encoder focuses
    on detecting implicit toxic speech and is boosted by a (iii) Teacher Classifier
    via knowledge distillation, and the decoder generates the necessary explanation.
    ToXCL achieves new state-of-the-art effectiveness, and outperforms baselines significantly.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Computational Social Science and Cultural Analytics
  authors:
  - emails: '****@e.ntu.edu.sg'
    first_name: Nhat
    homepage: https://nhathoang2002.github.io/
    last_name: Hoang
    middle_name: M.
    name: Nhat M. Hoang
    username: ~Nhat_M._Hoang4
  - emails: '****@u.nus.edu'
    first_name: Do
    google_scholar_id: https://scholar.google.com/citations?user=uZyF8wwAAAAJ&hl=en&oi=ao
    homepage: https://dxlong2000.github.io/
    institution: National University of Singapore
    last_name: Long
    middle_name: Xuan
    name: Do Xuan Long
    semantic_scholar_id: https://www.semanticscholar.org/author/Xuan-Long-Do/2060491855
    username: ~Do_Xuan_Long1
  - emails: '****@gmail.com'
    first_name: Duc Anh
    homepage: https://www.linkedin.com/in/ducanhdo183/
    last_name: Do
    name: Duc Anh Do
    username: ~Duc_Anh_Do1
  - emails: '****@e.ntu.edu.sg'
    first_name: Duc Anh
    homepage: https://vuducanh0802.github.io/
    last_name: Vu
    name: Duc Anh Vu
    username: ~Duc_Anh_Vu1
  - dblp_id: https://dblp.org/pid/81/8329
    emails: '****@ntu.edu.sg'
    first_name: Anh Tuan
    google_scholar_id: https://scholar.google.com.sg/citations?hl=en&user=d6ixOGYAAAAJ&view_op=list_works
    homepage: https://tuanluu.github.io/
    institution: Nanyang Technological University
    last_name: Luu
    name: Anh Tuan Luu
    semantic_scholar_id: https://www.semanticscholar.org/author/Anh-Tuan-Luu/26336902
    username: ~Anh_Tuan_Luu2
  decision: toMainConference
  end_page: 7094
  file: 757.pdf
  id: 757
  num_pages: 13
  openreview_id: mjhQfhAE7k
  pdf_file: c73c02349a2fd5399c8561b4fb757dd23f7cadcb.pdf
  start_page: 7082
  title: 'ToXCL: A Unified Framework for Toxic Speech Detection and Explanation'
- abstract: Prompt-based learning is a new language model training paradigm that adapts
    the Pre-trained Language Models (PLMs) to downstream tasks, which revitalizes
    the performance benchmarks across various natural language processing (NLP) tasks.
    Instead of using a fixed prompt template to fine-tune the model, some research
    demonstrates the effectiveness of searching for the prompt via optimization. Such
    prompt optimization process of prompt-based learning on PLMs also gives insight
    into generating adversarial prompts to mislead the model, raising concerns about
    the adversarial vulnerability of this paradigm. Recent studies have shown that
    universal adversarial triggers (UATs) can be generated to alter not only the predictions
    of the target PLMs but also the prediction of corresponding Prompt-based Fine-tuning
    Models (PFMs) under the prompt-based learning paradigm. However, UATs found in
    previous works are often unreadable tokens or characters and can be easily distinguished
    from natural texts with adaptive defenses. In this work, we consider the naturalness
    of the UATs and develop $\textit{LinkPrompt}$, an adversarial attack algorithm
    to generate UATs by a gradient-based beam search algorithm that not only effectively
    attacks the target PLMs and PFMs but also maintains the naturalness among the
    trigger tokens. Extensive results demonstrate the effectiveness of $\textit{LinkPrompt}$,
    as well as the transferability of UATs generated by $\textit{LinkPrompt}$ to open-sourced
    Large Language Model (LLM) Llama2 and API-accessed LLM GPT-3.5-turbo. The resource
    is available at https://github.com/SavannahXu79/LinkPrompt.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - emails: '****@gmail.com'
    first_name: Yue
    last_name: Xu
    name: Yue Xu
    orcid: https://orcid.org/0009-0005-0752-0011
    username: ~Yue_Xu8
  - emails: '****@shanghaitech.edu.cn'
    first_name: Wenjie
    homepage: https://sist.shanghaitech.edu.cn/wwj/main.htm
    institution: ShanghaiTech University
    last_name: Wang
    name: Wenjie Wang
    username: ~Wenjie_Wang4
  decision: toMainConference
  end_page: 7108
  file: 758.pdf
  id: 758
  num_pages: 14
  openreview_id: HUdSqycGvS
  pdf_file: 062c040a73dbad33b9d55f461ed505403bc96615.pdf
  start_page: 7095
  title: 'LinkPrompt: Natural and Universal Adversarial Attacks on Prompt-based Language
    Models'
- abstract: Recently, Large Language Models (LLMs) have been demonstrated to possess
    impressive capabilities in a variety of domains and tasks. We investigate the
    issue of prompt design in the multi-turn text-to-SQL task and attempt to enhance
    the LLMs' reasoning capacity when generating SQL queries. In the conversational
    context, the current SQL query can be modified from the preceding SQL query with
    only a few operations due to the context dependency. We introduce our method called
    CoE-SQL which can prompt LLMs to generate the SQL query based on the previously
    generated SQL query with an edition chain. We also conduct extensive ablation
    studies to determine the optimal configuration of our approach. Our approach outperforms
    different in-context learning baselines stably and achieves state-of-the-art performances
    on two benchmarks SParC and CoSQL using LLMs, which is also competitive to the
    SOTA fine-tuned models.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: 'Semantics: Sentence-level Semantics, Textual Inference and Other
      areas'
  authors:
  - dblp_id: https://dblp.org/pid/348/6967
    emails: '****@sjtu.edu.cn'
    first_name: Hanchong
    google_scholar_id: https://scholar.google.com/citations?user=4xNsDNgAAAAJ
    last_name: Zhang
    name: Hanchong Zhang
    orcid: https://orcid.org/0000-0003-1152-4355
    semantic_scholar_id: https://www.semanticscholar.org/author/Hanchong-Zhang/49723847
    username: ~Hanchong_Zhang1
  - dblp_id: https://dblp.org/pid/244/9541.html
    emails: '****@sjtu.edu.cn'
    first_name: Ruisheng
    google_scholar_id: https://scholar.google.com/citations?user=NdK881sAAAAJ&hl=zh-CN
    last_name: Cao
    name: Ruisheng Cao
    orcid: https://orcid.org/0000-0003-4635-4368
    semantic_scholar_id: https://www.semanticscholar.org/author/Ruisheng-Cao/150273321
    username: ~Ruisheng_Cao1
  - emails: '****@sjtu.edu.cn'
    first_name: Hongshen
    homepage: https://speechlab.sjtu.edu.cn/members/hongshen-xu
    institution: Shanghai Jiaotong University
    last_name: Xu
    name: Hongshen Xu
    orcid: https://orcid.org/0000-0002-6770-6564
    username: ~Hongshen_Xu1
  - dblp_id: https://dblp.org/pid/69/157-2
    emails: '****@sjtu.edu.cn'
    first_name: Lu
    google_scholar_id: https://scholar.google.ca/citations?user=Fb3jWaYAAAAJ&hl=en
    homepage: https://coai-sjtu.github.io
    institution: Shanghai Jiaotong University
    last_name: Chen
    name: Lu Chen
    semantic_scholar_id: https://www.semanticscholar.org/author/Lu-Chen/1390833791
    username: ~Lu_Chen3
  - dblp_id: https://dblp.org/pid/197/1322-4
    emails: '****@sjtu.edu.cn'
    first_name: Kai
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=APssqUMAAAAJ
    homepage: https://speechlab.sjtu.edu.cn/members/kai_yu
    institution: Shanghai Jiao Tong University
    last_name: Yu
    name: Kai Yu
    orcid: https://orcid.org/0000-0002-7102-9826
    username: ~Kai_Yu3
  decision: toMainConference
  end_page: 7130
  file: 759.pdf
  id: 759
  num_pages: 22
  openreview_id: R7AmILaIE1
  pdf_file: 27f9afe97db56f053086dbb657e733849d69a9ec.pdf
  start_page: 7109
  title: 'CoE-SQL: In-Context Learning for Multi-Turn Text-to-SQL with Chain-of-Editions'
- abstract: 'In recent times, large language models (LLMs) have shown impressive performance
    on various document-level tasks such as document classification, summarization,
    and question-answering. However, research on understanding their capabilities
    on the task of self-contradictions in long documents has been very limited. In
    this work, we introduce ContraDoc, the first human-annotated dataset to study
    self-contradictions in long documents across multiple domains, varying document
    lengths, self-contradiction types, and appearance scope. We then analyze the current
    capabilities of four state-of-the-art open-source and commercially available LLMs:
    GPT3.5, GPT4, PaLM2, and LLaMAv2 on this dataset.  While GPT4 performs the best
    and can outperform humans on this task, we find that it is still unreliable and
    struggles with self-contradictions that require more nuance and context. We release
    the dataset and all the code associated with the experiments.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - dblp_id: https://dblp.org/pid/245/8687
    emails: '****@utexas.edu'
    first_name: Jierui
    google_scholar_id: https://scholar.google.com/citations?user=bRGcMbIAAAAJ&hl=en
    homepage: https://lijierui.github.io/
    last_name: Li
    name: Jierui Li
    semantic_scholar_id: https://www.semanticscholar.org/author/Jierui-Li/9073069
    username: ~Jierui_Li1
  - dblp_id: https://dblp.org/pid/124/3068
    emails: '****@columbia.edu'
    first_name: Vipul
    google_scholar_id: https://scholar.google.com/citations?user=-Ap77UMAAAAJ&hl=en
    institution: Columbia University, Grammarly and International Institute of Information
      Technology Hyderabad
    last_name: Raheja
    name: Vipul Raheja
    semantic_scholar_id: https://www.semanticscholar.org/author/Vipul-Raheja/2831377
    username: ~Vipul_Raheja1
  - dblp_id: https://dblp.org/pid/159/9419-5
    emails: '****@gmail.com'
    first_name: Dhruv
    google_scholar_id: https://scholar.google.com/citations?user=IiMW328AAAAJ&hl=en&oi=ao
    homepage: https://ddhruvkr.github.io/
    last_name: Kumar
    name: Dhruv Kumar
    orcid: https://orcid.org/0000-0002-8191-0123
    semantic_scholar_id: https://www.semanticscholar.org/author/Dhruv-Kumar/50271213
    username: ~Dhruv_Kumar2
  decision: toMainConference
  end_page: 7145
  file: 761.pdf
  id: 761
  num_pages: 15
  openreview_id: QCIKZF7O1o
  pdf_file: 10d38159695b4a8b473ebe091ef6b11b9603f2e6.pdf
  start_page: 7131
  title: 'ContraDoc: Understanding Self-Contradictions in Documents with Large Language
    Models'
- abstract: "Entity disambiguation (ED), which links the mentions of ambiguous entities\
    \ to their referent entities in a knowledge base, serves as a core component in\
    \ entity linking (EL). Existing generative approaches demonstrate improved accuracy\
    \ compared to classification approaches under the standardized ZELDA benchmark.\
    \ Nevertheless, generative approaches suffer from the need for large-scale pre-training\
    \ and inefficient generation. Most importantly, entity descriptions, which could\
    \ contain crucial information to \ndistinguish similar entities from each other,\
    \ are often overlooked.\nWe propose an encoder-decoder model to disambiguate entities\
    \ with more detailed entity descriptions. Given text and candidate entities, the\
    \ encoder learns interactions between the text and each candidate entity, producing\
    \ representations for each entity candidate. The decoder then fuses the representations\
    \ of entity candidates together and selects the correct entity.\nOur experiments,\
    \ conducted on various entity disambiguation benchmarks, demonstrate the strong\
    \ and robust performance of this model, particularly +1.5\\% in the ZELDA benchmark\
    \ compared with GENRE. Furthermore, we integrate this approach into the retrieval/reader\
    \ framework and observe +1.5\\% improvements in end-to-end entity linking in the\
    \ GERBIL benchmark compared with EntQA."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Retrieval and Text Mining
  authors:
  - emails: '****@cornell.edu'
    first_name: Junxiong
    homepage: http://cs.cornell.edu/~junxiong
    institution: Cornell University
    last_name: Wang
    name: Junxiong Wang
    username: ~Junxiong_Wang1
  - dblp_id: https://dblp.org/pid/24/9
    emails: '****@gmail.com'
    first_name: Ali
    google_scholar_id: https://scholar.google.com/citations?user=Yuxd6uYAAAAJ&hl=en
    homepage: https://alimousavi1988.github.io/
    institution: Apple
    last_name: Mousavi
    name: Ali Mousavi
    username: ~Ali_Mousavi1
  - emails: '****@apple.com'
    first_name: Omar
    institution: Apple
    last_name: Attia
    name: Omar Attia
    username: ~Omar_Attia1
  - dblp_id: https://dblp.org/pid/194/3158
    emails: '****@gmail.com'
    first_name: Saloni
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=D7EmAyEAAAAJ
    homepage: https://salonipotdar.github.io
    institution: Apple
    last_name: Potdar
    name: Saloni Potdar
    username: ~Saloni_Potdar1
  - dblp_id: http://dblp.uni-trier.de/pers/hd/r/Rush:Alexander_M=
    emails: '****@cornell.edu'
    first_name: Alexander
    google_scholar_id: https://scholar.google.com/citations?user=LIjnUGgAAAAJ&hl=en&authuser=1
    homepage: http://rush.seas.harvard.edu/
    institution: Cornell University and School of Engineering and Applied Sciences,
      Harvard University
    last_name: Rush
    middle_name: M
    name: Alexander M Rush
    orcid: https://orcid.org/0000-0002-9900-1606
    semantic_scholar_id: https://www.semanticscholar.org/author/Alexander-M.-Rush/2531268
    username: ~Alexander_M_Rush1
  - emails: '****@gmail.com'
    first_name: Umar Farooq
    google_scholar_id: https://scholar.google.com/citations?user=NKYtfsYAAAAJ&hl=en
    last_name: Minhas
    name: Umar Farooq Minhas
    username: ~Umar_Farooq_Minhas1
  - dblp_id: https://dblp.org/pid/60/2319
    emails: '****@us.ibm.com'
    first_name: Yunyao
    last_name: Li
    name: Yunyao Li
    username: ~Yunyao_Li2
  decision: toMainConference
  end_page: 7157
  file: 762.pdf
  id: 762
  num_pages: 12
  openreview_id: vDLeoIzAbm
  pdf_file: 2637a18e483270c7356da73cd874f2eab609bf09.pdf
  start_page: 7146
  title: Entity Disambiguation via Fusion Entity Decoding
- abstract: In this paper, we conduct a study to utilize LLMs as a solution for decision
    making that requires complex data analysis. We define **Decision QA** as the task
    of answering the best decision, $d_{best}$, for a decision-making question $Q$,
    business rules $R$ and a database $D$. Since there is no benchmark that can examine
    Decision QA, we propose Decision QA benchmark, **DQA**. It has two scenarios,
    Locating and Building, constructed from two video games (Europa Universalis IV
    and Victoria 3) that have almost the same goal as Decision QA. To address Decision
    QA effectively, we also propose a new RAG technique called the *iterative plan-then-retrieval
    augmented generation* (**PlanRAG**). Our PlanRAG-based LM generates the plan for
    decision making as the first step, and the retriever generates the queries for
    data analysis as the second step. The proposed method outperforms the state-of-the-art
    iterative RAG method by 15.8\% in the Locating scenario and by 7.4\% in the Building
    scenario, respectively. We release our code and benchmark at https://github.com/myeon9h/PlanRAG.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Generation
  authors:
  - dblp_id: https://dblp.org/pid/293/7333
    emails: '****@kaist.ac.kr'
    first_name: Myeonghwa
    google_scholar_id: https://scholar.google.com/citations?view_op=list_works&hl=ko&authuser=4&user=SxQdLN0AAAAJ
    homepage: http://infolab.kaist.ac.kr/members/Myeonghwa%20Lee/
    institution: Korea Advanced Institute of Science and Technology
    last_name: Lee
    name: Myeonghwa Lee
    semantic_scholar_id: https://www.semanticscholar.org/author/Myeonghwa-Lee/2108893124
    username: ~Myeonghwa_Lee1
  - emails: '****@kaist.ac.kr'
    first_name: Seonho
    homepage: http://infolab.kaist.ac.kr/members/Seonho%20An/
    institution: School of Computing, KAIST
    last_name: An
    name: Seonho An
    username: ~Seonho_An1
  - dblp_id: http://dblp.uni-trier.de/pers/hd/k/Kim_0002:Min=Soo
    emails: '****@kaist.ac.kr'
    first_name: Min-Soo
    homepage: http://infolab.kaist.ac.kr/
    institution: KAIST
    last_name: Kim
    name: Min-Soo Kim
    orcid: https://orcid.org/0000-0002-5065-0226
    semantic_scholar_id: https://www.semanticscholar.org/author/Min-Soo-Kim/2116504190
    username: ~Min-Soo_Kim1
  decision: toMainConference
  end_page: 7176
  file: 764.pdf
  id: 764
  num_pages: 19
  openreview_id: Rw3YD1jhfc
  pdf_file: 511b550319c746e797b3f8f5155f60d528c6ab59.pdf
  start_page: 7158
  title: 'PlanRAG: A Plan-then-Retrieval Augmented Generation for Generative Large
    Language Models as Decision Makers'
- abstract: "Generative Artificial Intelligence (AI) has enabled the development of\
    \ sophisticated models that are capable of producing high-caliber text, images,\
    \ and other outputs through the utilization of large pre-trained models.\nNevertheless,\
    \ assessing the quality of the generation is an even more arduous task than the\
    \ generation itself, and this issue has not been given adequate consideration\
    \ recently.\nThis paper proposes a novel evaluation framework, GPTScore, which\
    \ utilizes the emergent abilities (e.g., in-context learning, zero-shot instruction)\
    \ of generative pre-trained models to score generated texts. \nThere are 19 pre-trained\
    \ models explored in this paper, ranging in size from 80M (e.g., Flan-T5-small)\
    \  to 175B (e.g., GPT3).\nExperimental results on four text generation tasks,\
    \  22 evaluation aspects, and corresponding 37 datasets demonstrate that this\
    \ approach can effectively allow us to achieve what one desires to evaluate for\
    \ texts simply by natural language instructions.\nThis nature helps us overcome\
    \ several long-standing challenges in text evaluation--how to achieve customized,\
    \ multi-faceted evaluation without model training. We make our code publicly available."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - dblp_id: https://dblp.org/pid/218/7289
    emails: '****@gmail.com'
    first_name: Jinlan
    google_scholar_id: https://scholar.google.com/citations?user=D4vtw8QAAAAJ&hl=zh-CN
    homepage: https://jinlanfu.github.io/
    last_name: Fu
    name: Jinlan Fu
    semantic_scholar_id: https://www.semanticscholar.org/author/Jinlan-Fu/41037252
    username: ~Jinlan_Fu2
  - dblp_id: https://dblp.org/pid/00/5480
    emails: '****@nus.edu.sg'
    first_name: See-Kiong
    google_scholar_id: https://scholar.google.com.tw/citations?user=_wsommYAAAAJ
    homepage: https://www.comp.nus.edu.sg/~ngsk/
    institution: National University of Singapore
    last_name: Ng
    name: See-Kiong Ng
    username: ~See-Kiong_Ng1
  - emails: '****@cs.cmu.edu'
    first_name: Zhengbao
    institution: School of Computer Science, Carnegie Mellon University
    last_name: Jiang
    name: Zhengbao Jiang
    username: ~Zhengbao_Jiang2
  - dblp_id: https://dblp.org/pid/34/3381-3
    emails: '****@gmail.com'
    first_name: Pengfei
    google_scholar_id: https://scholar.google.com/citations?user=oIz_CYEAAAAJ&hl=en
    homepage: http://pfliu.com/
    last_name: Liu
    name: Pengfei Liu
    semantic_scholar_id: https://www.semanticscholar.org/author/Pengfei-Liu/144118452
    username: ~Pengfei_Liu1
  decision: toMainConference
  end_page: 7197
  file: 765.pdf
  id: 765
  num_pages: 21
  openreview_id: UrWeYNAPPl
  pdf_file: 1d5bf4c3e7001cdf304afe3da5e36f2da9a23203.pdf
  start_page: 7177
  title: 'GPTScore: Evaluate as You Desire'
- abstract: Large language models (LLMs) have demonstrated remarkable capabilities
    across a wide range of tasks in various domains. Despite their impressive performance,
    they can be unreliable due to factual errors in their generations. Assessing their
    confidence and calibrating them across different tasks can help mitigate risks
    and enable LLMs to produce better generations. There has been a lot of recent
    research aiming to address this, but there has been no comprehensive overview
    to organize it and to outline the main lessons learned. The present survey aims
    to bridge this gap. In particular, we outline the challenges and we summarize
    recent technical advancements for LLM confidence estimation and calibration. We
    further discuss their applications and suggest promising directions for future
    work.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - emails: '****@gmail.com'
    first_name: Jiahui
    google_scholar_id: https://scholar.google.com/citations?user=eMC-gQUAAAAJ&hl=en&oi=ao
    last_name: Geng
    name: Jiahui Geng
    username: ~Jiahui_Geng3
  - emails: '****@tu-darmstadt.de'
    first_name: Fengyu
    homepage: https://trumancfy.github.io/personal-site/
    institution: "Technische Universit\xE4t Darmstadt"
    last_name: Cai
    name: Fengyu Cai
    username: ~Fengyu_Cai1
  - dblp_id: https://dblp.org/pid/78/7732
    emails: '****@student.unimelb.edu.au'
    first_name: Yuxia
    google_scholar_id: https://scholar.google.com/citations?user=dciz7yMAAAAJ&hl=en
    last_name: Wang
    name: Yuxia Wang
    username: ~Yuxia_Wang1
  - dblp_id: https://dblp.org/pid/41/6084
    emails: '****@bcs.tu-darmstadt.de'
    first_name: Heinz
    google_scholar_id: https://scholar.google.de/citations?user=WaPW80kAAAAJ&hl=en
    last_name: Koeppl
    name: Heinz Koeppl
    username: ~Heinz_Koeppl1
  - dblp_id: https://dblp.uni-trier.de/pid/19/1947
    emails: '****@gmail.com'
    first_name: Preslav
    google_scholar_id: https://scholar.google.com/citations?user=DfXsKZ4AAAAJ
    homepage: https://www.hbku.edu.qa/en/staff/dr-preslav-nakov
    last_name: Nakov
    name: Preslav Nakov
    orcid: https://orcid.org/0000-0002-3600-1510
    semantic_scholar_id: https://www.semanticscholar.org/author/Preslav-Nakov/1683562
    username: ~Preslav_Nakov2
  - dblp_id: https://dblp.org/pid/85/6201
    emails: '****@tu-darmstadt.de'
    first_name: Iryna
    google_scholar_id: https://scholar.google.com.tw/citations?user=t3A39e8AAAAJ
    homepage: https://www.informatik.tu-darmstadt.de/ukp/ukp_home/staff_ukp/prof_dr_iryna_gurevych/index.en.jsp
    institution: Mohamed bin Zayed University of Artificial Intelligence and Technical
      University of Darmstadt
    last_name: Gurevych
    name: Iryna Gurevych
    username: ~Iryna_Gurevych1
  decision: toMainConference
  end_page: 7216
  file: 768.pdf
  id: 768
  num_pages: 19
  openreview_id: vroIqSekFA
  pdf_file: bb9a07c79269b13447ca5736dfc3f4159b14738b.pdf
  start_page: 7198
  title: A Survey of Confidence Estimation and Calibration in Large Language Models
- abstract: Most research about natural language generation (NLG) relies on evaluation
    benchmarks with limited references for a sample, which may result in poor correlations
    with human judgements. The underlying reason is that one semantic meaning can
    actually be expressed in different forms, and the evaluation with a single or
    few references may not accurately reflect the quality of the model's hypotheses.
    To address this issue, this paper presents a simple and effective method, named
    **Div-Ref**, to enhance existing evaluation benchmarks by enriching the number
    of references. We leverage large language models (LLMs) to diversify the expression
    of a single reference into multiple high-quality ones to cover the semantic space
    of the reference sentence as much as possible. We conduct comprehensive experiments
    to empirically demonstrate that diversifying the expression of reference can significantly
    enhance the correlation between automatic evaluation and human evaluation. This
    idea is compatible with recent LLM-based evaluation which can similarly derive
    advantages from incorporating multiple references. *We strongly encourage future
    generation benchmarks to include more references, even if they are generated by
    LLMs, which is once for all.* We release all the code and data at https://github.com/RUCAIBox/Div-Ref
    to facilitate research.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Generation
  authors:
  - dblp_id: https://dblp.org/pid/276/9353
    emails: '****@outlook.com'
    first_name: Tianyi
    google_scholar_id: https://scholar.google.com/citations?user=t1mRUvQAAAAJ
    homepage: https://steventang1998.github.io/
    last_name: Tang
    name: Tianyi Tang
    semantic_scholar_id: https://www.semanticscholar.org/author/Tianyi-Tang/1997234792
    username: ~Tianyi_Tang1
  - emails: '****@link.cuhk.edu.hk'
    first_name: Hongyuan
    last_name: Lu
    name: Hongyuan Lu
    username: ~Hongyuan_Lu2
  - emails: '****@aiwaves.cn'
    first_name: Yuchen
    google_scholar_id: https://scholar.google.com/citations?user=AEiEn6MAAAAJ&hl=en
    homepage: https://www.elejiang.me/
    institution: AIWaves Inc.
    last_name: Jiang
    middle_name: Eleanor
    name: Yuchen Eleanor Jiang
    username: ~Yuchen_Eleanor_Jiang1
  - emails: '****@microsoft.com'
    first_name: Haoyang
    institution: Microsoft Research Asia
    last_name: Huang
    name: Haoyang Huang
    username: ~Haoyang_Huang1
  - dblp_id: https://dblp.org/pid/02/621-1.html
    emails: '****@microsoft.com'
    first_name: Dongdong
    google_scholar_id: https://scholar.google.com/citations?user=w2qu71oAAAAJ&hl=en
    homepage: https://www.microsoft.com/en-us/research/people/dozhang/
    institution: Microsoft Research Asia
    last_name: Zhang
    name: Dongdong Zhang
    semantic_scholar_id: https://www.semanticscholar.org/author/Dongdong-Zhang/40232931
    username: ~Dongdong_Zhang4
  - emails: '****@gmail.com'
    first_name: Xin
    google_scholar_id: https://scholar.google.com/citations?user=JNhNacoAAAAJ&hl=zh-CN
    homepage: https://gsai.ruc.edu.cn/addons/teacher/index/info.html?user_id=5&ruccode=20140041&ln=cn
    institution: Renmin University of China
    last_name: Zhao
    name: Xin Zhao
    username: ~Xin_Zhao10
  - dblp_id: https://dblp.org/pid/185/1333
    emails: '****@gmail.com'
    first_name: Tom
    google_scholar_id: https://scholar.google.cz/citations?user=8dUF8YQAAAAJ
    institution: Microsoft
    last_name: Kocmi
    name: Tom Kocmi
    orcid: https://orcid.org/0000-0002-7993-9859
    semantic_scholar_id: https://www.semanticscholar.org/author/Tom-Kocmi/3452584
    username: ~Tom_Kocmi1
  - dblp_id: https://dblp.org/pid/72/5870
    emails: '****@microsoft.com'
    first_name: Furu
    google_scholar_id: https://scholar.google.com/citations?user=G-V1VpwAAAAJ&hl=en
    homepage: https://www.microsoft.com/en-us/research/people/fuwei/
    institution: Microsoft Research
    last_name: Wei
    name: Furu Wei
    username: ~Furu_Wei1
  decision: toMainConference
  end_page: 7231
  file: 769.pdf
  id: 769
  num_pages: 15
  openreview_id: FYZEhurE0a
  pdf_file: 007c89a59aec70acb7ea06676b15275a9b8680c5.pdf
  start_page: 7217
  title: 'Not All Metrics Are Guilty: Improving NLG Evaluation by Diversifying References'
- abstract: In event argument extraction (EAE), a promising approach involves jointly
    encoding text and argument roles, and performing multiple token linking operations.
    This approach further falls into two categories. One extracts arguments within
    a single event, while the other attempts to extract arguments from multiple events
    simultaneously. However, the former lacks to leverage cross-event information
    and the latter requires tougher predictions with longer encoded role sequences
    and extra linking operations. In this paper, we design a novel separation-and-fusion
    paradigm to separately acquire cross-event information and fuse it into the argument
    extraction of a target event. Following the paradigm, we propose a novel multiple
    token linking model named Sep2F, which can effectively build event correlations
    via roles and preserve the simple linking predictions of single-event extraction.
    In particular, we employ one linking module to extract arguments for the target
    event and another to aggregate the role information of multiple events. More importantly,
    we propose a novel two-fold fusion module to ensure that the aggregated cross-event
    information serves EAE well. We evaluate our proposed model on sentence-level
    and document-level datasets, including ACE05, RAMS, WikiEvents and MLEE. The extensive
    experimental results indicate that our model outperforms the state-of-the-art
    EAE models on all the datasets.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Extraction
  authors:
  - emails: '****@bit.edu.cn'
    first_name: Jing
    google_scholar_id: https://scholar.google.com/citations?user=qaUTl5MAAAAJ&hl=zh-CN
    last_name: Xu
    name: Jing Xu
    username: ~Jing_Xu11
  - dblp_id: https://dblp.org/pid/60/5451
    emails: '****@bit.edu.cn'
    first_name: Dandan
    homepage: https://cs.bit.edu.cn/szdw/jsml/js/sdd_20170821091432439000/index.htm
    institution: Beijing Institute of Technology
    last_name: Song
    name: Dandan Song
    username: ~Dandan_Song1
  - dblp_id: https://dblp.org/pid/65/3225
    emails: '****@ntu.edu.sg'
    first_name: Siu
    google_scholar_id: https://scholar.google.com.tw/citations?user=d4ZYx6gAAAAJ
    homepage: http://research.ntu.edu.sg/expertise/academicprofile/Pages/StaffProfile.aspx?ST_EMAILID=ASSCHUI&CategoryDescription=mathematics/
    institution: Nanyang Technological University
    last_name: Hui
    middle_name: Cheung
    name: Siu Cheung Hui
    username: ~Siu_Cheung_Hui1
  - dblp_id: https://dblp.org/pid/204/0172
    emails: '****@gmail.com'
    first_name: Zhijing
    google_scholar_id: https://scholar.google.com.hk/citations?user=rDTfaqAAAAAJ&hl=en
    homepage: https://eudoraw.github.io/
    institution: Beijing Institute of Technology
    last_name: Wu
    name: Zhijing Wu
    semantic_scholar_id: https://www.semanticscholar.org/author/Zhijing-Wu/47039225
    username: ~Zhijing_Wu1
  - dblp_id: https://dblp.org/pid/150/3494
    emails: '****@bit.edu.cn'
    first_name: Meihuizi
    google_scholar_id: https://scholar.google.com/citations?user=9f9c0J0AAAAJ&hl=zh-CN&oi=sra
    last_name: Jia
    name: Meihuizi Jia
    semantic_scholar_id: https://www.semanticscholar.org/author/Meihuizi-Jia/1762766
    username: ~Meihuizi_Jia1
  - emails: '****@bit.edu.cn'
    first_name: Hao
    last_name: Wang
    name: Hao Wang
    orcid: https://orcid.org/0000-0002-5233-3128
    username: ~Hao_Wang61
  - emails: '****@bit.edu.cn'
    first_name: Yanru
    last_name: Zhou
    name: Yanru Zhou
    orcid: https://orcid.org/0000-0002-2263-5766
    username: ~Yanru_Zhou2
  - emails: '****@bit.edu.cn'
    first_name: Changzhi
    homepage: https://github.com/zhoucz97
    last_name: Zhou
    name: Changzhi Zhou
    username: ~Changzhi_Zhou1
  - emails: '****@bit.edu.cn'
    first_name: Ziyi
    homepage: https://cst.bit.edu.cn/szdw/jsml/sssds/42fd78087e8e459492b4aaf7f0e3eb35.htm
    last_name: Yang
    name: Ziyi Yang
    username: ~Ziyi_Yang5
  decision: toMainConference
  end_page: 7245
  file: 770.pdf
  id: 770
  num_pages: 14
  openreview_id: ejjIgFTOFw
  pdf_file: 7ec579744a584fac22c4ba309f013baa87d77a3b.pdf
  start_page: 7232
  title: 'Separation and Fusion: A Novel Multiple Token Linking Model for Event Argument
    Extraction'
- abstract: The Knowledge Graph Entity Typing (KGET) task aims to predict missing
    type annotations for entities in knowledge graphs. Recent works only utilize the
    \textit{\textbf{structural knowledge}} in the local neighborhood of entities,
    disregarding \textit{\textbf{semantic knowledge}} in the textual representations
    of entities, relations, and types that are also crucial for type inference. Additionally,
    we observe that the interaction between semantic and structural knowledge can
    be utilized to address the false-negative problem. In this paper, we propose a
    novel \textbf{\underline{S}}emantic and \textbf{\underline{S}}tructure-aware KG
    \textbf{\underline{E}}ntity \textbf{\underline{T}}yping~{(SSET)} framework, which
    is composed of three modules. First, the \textit{Semantic Knowledge Encoding}
    module encodes factual knowledge in the KG with a Masked Entity Typing task. Then,
    the \textit{Structural Knowledge Aggregation} module aggregates knowledge from
    the multi-hop neighborhood of entities to infer missing types. Finally, the \textit{Unsupervised
    Type Re-ranking} module utilizes the inference results from the two models above
    to generate type predictions that are robust to false-negative samples. Extensive
    experiments show that SSET significantly outperforms existing state-of-the-art
    methods.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - emails: '****@cse.cuhk.edu.hk'
    first_name: Muzhi
    google_scholar_id: https://scholar.google.com/citations?user=jIInDQMAAAAJ&hl=en
    last_name: Li
    name: Muzhi Li
    username: ~Muzhi_Li1
  - dblp_id: https://dblp.org/pid/260/5462.html
    emails: '****@link.cuhk.edu.hk'
    first_name: Minda
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=uQlkNn8AAAAJ
    last_name: Hu
    name: Minda Hu
    orcid: https://orcid.org/0000-0003-1048-1998
    username: ~Minda_Hu1
  - dblp_id: https://dblp.uni-trier.de/pers/hd/k/King:Irwin
    emails: '****@cse.cuhk.edu.hk'
    first_name: Irwin
    google_scholar_id: https://scholar.google.com/citations?user=MXvC7tkAAAAJ&hl=en
    homepage: https://www.cse.cuhk.edu.hk/irwin.king/
    institution: The Chinese University of Hong Kong
    last_name: King
    name: Irwin King
    orcid: https://orcid.org/0000-0001-8106-6447
    username: ~Irwin_King1
  - dblp_id: https://dblp.org/pid/l/HofungLeung
    emails: '****@outlook.com'
    first_name: Ho-fung
    google_scholar_id: https://scholar.google.com.hk/citations?user=JDErdKcAAAAJ
    homepage: http://www.cse.cuhk.edu.hk/~lhf/
    institution: (Independent Researcher) and The Chinese University of Hong Kong
    last_name: Leung
    name: Ho-fung Leung
    orcid: https://orcid.org/0000-0003-4914-2934
    semantic_scholar_id: https://www.semanticscholar.org/author/Ho-fung-Leung/1701688
    username: ~Ho-fung_Leung1
  decision: toMainConference
  end_page: 7259
  file: 774.pdf
  id: 774
  num_pages: 14
  openreview_id: 7DBFa1m7xU
  pdf_file: 69796591c0958b36b4608ff3f3eee7fd27245cce.pdf
  start_page: 7246
  title: The Integration of Semantic and Structural Knowledge in Knowledge Graph Entity
    Typing
- abstract: 'Contrastive Language-Image Pretraining (CLIP) has demonstrated great
    zero-shot performance for matching images and text. However, it is still challenging
    to adapt vision-language pretrained models like CLIP to compositional image and
    text matching --- a more challenging image and text matching task requiring the
    model''s understanding of compositional word concepts and visual components. Towards
    better compositional generalization in zero-shot image and text matching, in this
    paper, we study the problem from a causal perspective: the erroneous semantics
    of individual entities are essentially confounders that cause the matching failure.
    Therefore, we propose a novel training-free compositional CLIP model (ComCLIP).
    ComCLIP disentangles input images into subjects, objects, and action subimages
    and composes CLIP''s vision encoder and text encoder to perform evolving matching
    over compositional text embedding and subimage embeddings. In this way, ComCLIP
    can mitigate spurious correlations introduced by the pretrained CLIP models and
    dynamically evaluate the importance of each component. Experiments on four compositional
    image-text matching datasets: Winoground, VL-checklist, SVO, and ComVG, and two
    general image-text retrieval datasets: Flick30K, and MSCOCO demonstrate the effectiveness
    of our plug-and-play method, which boosts the zero-shot inference ability of CLIP,
    SLIP, and BLIP2 even without further training or fine-tuning. Our codes can be
    found at https://github.com/eric-ai-lab/ComCLIP.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multimodality and Language Grounding to Vision, Robotics and Beyond
  authors:
  - emails: '****@berkeley.edu'
    first_name: Kenan
    google_scholar_id: https://scholar.google.com/citations?user=HkCq4xwAAAAJ&hl=en
    last_name: Jiang
    name: Kenan Jiang
    username: ~Kenan_Jiang1
  - dblp_id: https://dblp.org/pid/251/0763
    emails: '****@ucsc.edu'
    first_name: Xuehai
    google_scholar_id: https://scholar.google.com/citations?user=kDzxOzUAAAAJ&hl=en&authuser=3
    institution: University of California, San Diego
    last_name: He
    name: Xuehai He
    username: ~Xuehai_He1
  - emails: '****@columbia.edu'
    first_name: Ruize
    homepage: https://Rick-Xu315.github.io
    last_name: Xu
    name: Ruize Xu
    username: ~Ruize_Xu2
  - dblp_id: https://dblp.uni-trier.de/pers/hd/w/Wang_0061:Xin
    emails: '****@ucsc.edu'
    first_name: Xin
    google_scholar_id: https://scholar.google.com/citations?user=YjqluE0AAAAJ
    homepage: https://eric-xw.github.io
    institution: University of California, Santa Cruz
    last_name: Wang
    middle_name: Eric
    name: Xin Eric Wang
    orcid: https://orcid.org/0000-0003-2605-5504
    semantic_scholar_id: https://www.semanticscholar.org/author/Xin-Eric-Wang/48631993
    username: ~Xin_Eric_Wang2
  decision: toMainConference
  end_page: 7280
  file: 775.pdf
  id: 775
  num_pages: 21
  openreview_id: PHONcKZ7YJ
  pdf_file: d7749fbeddbe0b72fa194eba652e67ffe7400c6b.pdf
  start_page: 7260
  title: 'ComCLIP: Training-Free Compositional Image and Text Matching'
- abstract: Extensive efforts in the past have been directed toward the development
    of summarization datasets. However, a predominant number of these resources have
    been (semi)-automatically generated, typically through web data crawling. This
    resulted in subpar resources for training and evaluating summarization systems,
    a quality compromise that is arguably due to the substantial costs associated
    with generating ground-truth summaries, particularly for diverse languages and
    specialized domains. To address this issue, we present ACLSum, a novel summarization
    dataset carefully crafted and evaluated by domain experts. In contrast to previous
    datasets, ACLSum facilitates multi-aspect summarization of scientific papers,
    covering challenges, approaches, and outcomes in depth. Through extensive experiments,
    we evaluate the quality of our resource and the performance of models based on
    pretrained language models (PLMs) and state-of-the-art large language models (LLMs).
    Additionally, we explore the effectiveness of extract-then-abstract versus abstractive
    end-to-end summarization within the scholarly domain on the basis of automatically
    discovered aspects. While the former performs comparably well to the end-to-end
    approach with pretrained language models regardless of the potential error propagation
    issue, the prompting-based approach with LLMs shows a limitation in extracting
    sentences from source documents.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Summarization
  authors:
  - dblp_id: https://dblp.org/pid/321/4075
    emails: '****@uni-mannheim.de'
    first_name: Sotaro
    google_scholar_id: https://scholar.google.com/citations?user=jNhDmicAAAAJ&hl=en
    homepage: https://sotaro.io/about
    institution: "Universit\xE4t Mannheim"
    last_name: Takeshita
    name: Sotaro Takeshita
    orcid: https://orcid.org/0000-0002-6510-7058
    semantic_scholar_id: https://www.semanticscholar.org/author/Sotaro-Takeshita/9390836
    username: ~Sotaro_Takeshita1
  - dblp_id: https://dblp.org/pid/300/0989
    emails: '****@gmail.com'
    first_name: Tommaso
    google_scholar_id: https://scholar.google.com/citations?user=iGSCUq4AAAAJ&hl=en
    homepage: https://green-t.io
    last_name: Green
    name: Tommaso Green
    orcid: https://orcid.org/0000-0002-2231-2385
    semantic_scholar_id: https://www.semanticscholar.org/author/Tommaso-Green/2124689664
    username: ~Tommaso_Green1
  - emails: '****@uni-mannheim.de'
    first_name: Ines
    homepage: https://www.uni-mannheim.de/dws/people/researchers/phd-students/ines-reinig/
    last_name: Reinig
    name: Ines Reinig
    username: ~Ines_Reinig2
  - dblp_id: https://dblp.org/pid/22/5420-1
    emails: '****@hs-mannheim.de'
    first_name: Kai
    google_scholar_id: https://scholar.google.com/citations?user=Nbc2V3sAAAAJ&hl=de&oi=ao
    homepage: http://www.kaiec.de
    institution: Mannheim University of Applied Sciences
    last_name: Eckert
    name: Kai Eckert
    orcid: https://orcid.org/0000-0002-5423-561X
    semantic_scholar_id: https://www.semanticscholar.org/author/Kai-Eckert/2257256476
    username: ~Kai_Eckert1
  - dblp_id: https://dblp.org/pid/04/2532
    emails: '****@informatik.uni-mannheim.de'
    first_name: Simone
    google_scholar_id: https://scholar.google.com/citations?user=VmIFG0EAAAAJ
    homepage: http://dws.informatik.uni-mannheim.de/ponzetto
    institution: University of Mannheim
    last_name: Ponzetto
    middle_name: Paolo
    name: Simone Paolo Ponzetto
    orcid: https://orcid.org/0000-0001-7484-2049
    semantic_scholar_id: https://www.semanticscholar.org/author/Simone-Paolo-Ponzetto/1801255
    username: ~Simone_Paolo_Ponzetto1
  decision: toMainConference
  end_page: 7296
  file: 779.pdf
  id: 779
  num_pages: 16
  openreview_id: vaPhR3moTa
  pdf_file: 92f4e6af8d173ac998ed8ab8263330908c2e317d.pdf
  start_page: 7281
  title: 'ACLSum: A New Dataset for Aspect-based Summarization of Scientific Publications'
- abstract: 'Active learning (AL), which aims to construct an effective training set
    by iteratively curating the most formative unlabeled data for annotation, has
    been widely used in low-resource tasks. Most active learning techniques in classification
    rely on the model''s uncertainty or disagreement to choose unlabeled data, suffering
    from the problem of over-confidence in superficial patterns and a lack of exploration.

    Inspired by the cognitive processes in which humans deduce and predict through
    causal information, we take an initial attempt towards integrating rationales
    into AL and propose a novel Explainable Active Learning framework (XAL) for low-resource
    text classification, which aims to encourage classifiers to justify their inferences
    and delve into unlabeled data for which they cannot provide reasonable explanations.  Specifically,
    besides using a pre-trained bi-directional encoder for classification, we employ
    a pre-trained uni-directional decoder to generate and score the explanation. We
    further facilitate the alignment of the model with human reasoning preference
    through a proposed ranking loss. During the selection of unlabeled data, the predicted
    uncertainty of the encoder and the explanation score of the decoder complement
    each other as the final metric to acquire informative data. Extensive experiments
    on six datasets show that XAL achieves consistent improvement over 9 strong baselines.
    Analysis indicates that the proposed method can generate corresponding explanations
    for its predictions.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Efficient/Low-Resource Methods for NLP
  authors:
  - emails: '****@westlake.edu.cn'
    first_name: Yun
    google_scholar_id: https://scholar.google.com/citations?user=B_bdRlAAAAAJ&hl=zh-CN&authuser=1
    institution: westlake university
    last_name: Luo
    name: Yun Luo
    username: ~Yun_Luo1
  - dblp_id: https://dblp.org/pid/70/2539
    emails: '****@tencent.com'
    first_name: Zhen
    google_scholar_id: https://scholar.google.com/citations?user=cuGFOQsAAAAJ&hl=zh-CN
    last_name: Yang
    name: Zhen Yang
    semantic_scholar_id: https://www.semanticscholar.org/author/Zhen-Yang/121937721
    username: ~Zhen_Yang4
  - dblp_id: https://dblp.org/pid/117/4056
    emails: '****@tencent.com'
    first_name: Fandong
    google_scholar_id: https://scholar.google.com/citations?user=sA8U4S0AAAAJ&hl=en
    homepage: http://fandongmeng.github.io/
    institution: WeChat AI, Tencent Inc.
    last_name: Meng
    name: Fandong Meng
    semantic_scholar_id: https://www.semanticscholar.org/author/Fandong-Meng/33427918
    username: ~Fandong_Meng3
  - emails: '****@westlake.edu.cn'
    first_name: Yingjie
    google_scholar_id: https://scholar.google.com/citations?user=6OWRfPoAAAAJ&hl=en
    institution: Westlake University
    last_name: Li
    name: Yingjie Li
    orcid: https://orcid.org/0000-0003-4015-4576
    semantic_scholar_id: https://www.semanticscholar.org/author/Yingjie-Li/50820271
    username: ~Yingjie_Li2
  - dblp_id: https://dblp.org/pid/48/4689
    emails: '****@westlake.edu.cn'
    first_name: Fang
    google_scholar_id: https://scholar.google.com/citations?user=Qik67zAAAAAJ&hl=zh-TW
    last_name: Guo
    name: Fang Guo
    username: ~Fang_Guo1
  - emails: '****@qq.com'
    first_name: Qinglin
    last_name: Qi
    name: Qinglin Qi
    orcid: https://orcid.org/0009-0005-1856-1484
    username: ~Qinglin_Qi1
  - dblp_id: https://dblp.org/pid/00/5012-16
    emails: '****@tencent.com'
    first_name: Jie
    last_name: Zhou
    name: Jie Zhou
    orcid: https://orcid.org/0000-0002-5899-5165
    semantic_scholar_id: https://www.semanticscholar.org/author/Jie-Zhou/49178343
    username: ~Jie_Zhou8
  - dblp_id: https://dblp.org/pid/47/722-4.html
    emails: '****@westlake.edu.cn'
    first_name: Yue
    homepage: http://frcchang.github.io
    institution: Westlake University
    last_name: Zhang
    name: Yue Zhang
    orcid: https://orcid.org/0000-0002-5214-2268
    username: ~Yue_Zhang7
  decision: toMainConference
  end_page: 7319
  file: 781.pdf
  id: 781
  num_pages: 23
  openreview_id: Fzz1EU7DCY
  pdf_file: 56db16d511eae30bd73a2e58bbc6aa8329bf25de.pdf
  start_page: 7297
  title: 'XAL: EXplainable Active Learning Makes Classifiers Better Low-resource Learners'
- abstract: Diffusion models have exhibited remarkable capabilities in text-to-image
    generation. However, their performance in image-to-text generation, specifically
    image captioning, has lagged behind Auto-Regressive (AR) models, casting doubt
    on their applicability for such tasks. In this work, we revisit diffusion models,
    highlighting their capacity for holistic context modeling and parallel decoding.
    With these benefits, diffusion models can alleviate the inherent limitations of
    AR methods, including their slow inference speed, error propagation, and unidirectional
    constraints. Furthermore, we identify the prior underperformance of diffusion
    models stemming from the absence of an effective latent space for image-text alignment,
    and the discrepancy between continuous diffusion processes and discrete textual
    data. In response, we introduce a novel architecture, LaDiC, which utilizes a
    split BERT to create a dedicated latent space for captions and integrates a regularization
    module to manage varying text lengths. Our framework also includes a diffuser
    for semantic image-to-text conversion and a Back\&Refine technique to enhance
    token interactivity during inference. LaDiC achieves state-of-the-art performance
    for diffusion-based methods on the MS COCO dataset with 38.2 BLEU@4 and 126.2
    CIDEr, demonstrating exceptional performance without pre-training or ancillary
    modules. This indicates strong competitiveness with  AR models, revealing the
    previously untapped potential of diffusion models in image-to-text generation.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multimodality and Language Grounding to Vision, Robotics and Beyond
  authors:
  - emails: '****@163.com'
    first_name: Yuchi
    google_scholar_id: https://scholar.google.com/citations?user=RxuU_0YAAAAJ&hl=zh-CN
    homepage: https://wangyuchi369.github.io/
    last_name: Wang
    name: Yuchi Wang
    username: ~Yuchi_Wang1
  - dblp_id: https://dblp.uni-trier.de/pid/50/9511
    emails: '****@gmail.com'
    first_name: Shuhuai
    google_scholar_id: https://scholar.google.com.hk/citations?user=3X8yS-cAAAAJ&hl=zh-CN
    homepage: https://renshuhuai-andy.github.io/
    last_name: Ren
    name: Shuhuai Ren
    semantic_scholar_id: https://www.semanticscholar.org/author/Shuhuai-Ren/1906099
    username: ~Shuhuai_Ren1
  - emails: '****@gmail.com'
    first_name: Rundong
    google_scholar_id: https://scholar.google.com/citations?user=99RqPq4AAAAJ&hl=zh-CN
    last_name: Gao
    name: Rundong Gao
    username: ~Rundong_Gao1
  - dblp_id: https://dblp.org/pid/262/6579
    emails: '****@stu.pku.edu.cn'
    first_name: Linli
    google_scholar_id: https://scholar.google.com/citations?user=rZJRGlQAAAAJ&hl=en
    homepage: https://github.com/yaolinli
    last_name: Yao
    name: Linli Yao
    orcid: https://orcid.org/0000-0002-9809-8864
    semantic_scholar_id: https://www.semanticscholar.org/author/Linli-Yao/2106357101
    username: ~Linli_Yao2
  - emails: '****@mails.tsinghua.edu.cn'
    first_name: Qingyan
    google_scholar_id: https://scholar.google.com/citations?user=tPYWm_AAAAAJ&hl=zh-CN&oi=ao
    homepage: https://beeevita.github.io/
    last_name: Guo
    name: Qingyan Guo
    username: ~Qingyan_Guo1
  - emails: '****@stu.pku.edu.cn'
    first_name: Kaikai
    google_scholar_id: https://scholar.google.com/citations?user=6TrBRiEAAAAJ&hl=zh-CN
    homepage: https://github.com/kkk-an
    last_name: An
    name: Kaikai An
    username: ~Kaikai_An1
  - emails: '****@zju.edu.cn'
    first_name: Jianhong
    google_scholar_id: https://scholar.google.com/citations?user=U926UgYAAAAJ&hl=en
    homepage: https://jianhongbai.github.io/
    last_name: Bai
    name: Jianhong Bai
    orcid: https://orcid.org/0000-0002-3121-7259
    username: ~Jianhong_Bai2
  - dblp_id: https://dblp.org/pid/37/1971-1
    emails: '****@pku.edu.cn'
    first_name: Xu
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=tpXiQkYAAAAJ&view_op=list_works
    homepage: https://xusun.org/
    last_name: Sun
    name: Xu Sun
    username: ~Xu_Sun1
  decision: toMainConference
  end_page: 7336
  file: 784.pdf
  id: 784
  num_pages: 17
  openreview_id: 3AVt9MjOnT
  pdf_file: e575195e3cb77effc06f3eb1c70e7bd9a6b1589a.pdf
  start_page: 7320
  title: 'LaDiC: Are Diffusion Models Really Inferior to Autoregressive Counterparts
    for Image-to-Text Generation?'
- abstract: Differentiating relationships between entity pairs with limited labeled
    instances poses a significant challenge in few-shot relation classification. Representations
    of textual data extract rich information spanning the domain, entities, and relations.
    In this paper, we introduce a novel approach to enhance information extraction
    combining multiple sentence representations and contrastive learning. While representations
    in relation classification are commonly extracted using entity marker tokens,
    we argue that substantial information within the internal model representations
    remains untapped. To address this, we propose aligning multiple sentence representations,
    such as the \[CLS] token, the [MASK] token used in prompting, and entity marker
    tokens. Our method employs contrastive learning to extract complementary discriminative
    information from these individual representations. This is particularly relevant
    in low-resource settings where information is scarce. Leveraging multiple sentence
    representations is especially effective in distilling discriminative information
    for relation classification when additional information, like relation descriptions,
    are not available. We validate the adaptability of our approach, maintaining robust
    performance in scenarios that include relation descriptions, and showcasing its
    flexibility to adapt to different resource constraints.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Extraction
  authors:
  - dblp_id: https://dblp.org/pid/338/1017
    emails: '****@ieseg.fr'
    first_name: Philipp
    google_scholar_id: https://scholar.google.com/citations?user=efKKfygAAAAJ&hl=en
    homepage: https://icma.ieseg.fr/philipp-borchert/
    institution: "I\xC9SEG School of Management and KU Leuven"
    last_name: Borchert
    name: Philipp Borchert
    orcid: https://orcid.org/0000-0002-5533-4281
    username: ~Philipp_Borchert1
  - dblp_id: https://dblp.org/pid/41/9119.html
    emails: '****@kuleuven.be'
    first_name: Jochen
    google_scholar_id: https://scholar.google.com/citations?user=26i8eZMAAAAJ&hl=en
    homepage: http://www.jochendeweerdt.com/
    institution: KU Leuven
    last_name: De Weerdt
    name: Jochen De Weerdt
    orcid: https://orcid.org/0000-0001-6151-0504
    semantic_scholar_id: https://www.semanticscholar.org/author/Jochen-De-Weerdt/1786504
    username: ~Jochen_De_Weerdt1
  - dblp_id: https://dblp.org/pid/m/MarieFrancineMoens
    emails: '****@cs.kuleuven.be'
    first_name: Marie-Francine
    google_scholar_id: https://scholar.google.com.tw/citations?user=O9hYMUUAAAAJ
    homepage: https://people.cs.kuleuven.be/~sien.moens/
    institution: KU Leuven, KU Leuven
    last_name: Moens
    name: Marie-Francine Moens
    orcid: https://orcid.org/0000-0002-3732-9323
    semantic_scholar_id: https://www.semanticscholar.org/author/Marie-Francine-Moens/100781843
    username: ~Marie-Francine_Moens1
  decision: toMainConference
  end_page: 7345
  file: 786.pdf
  id: 786
  num_pages: 9
  openreview_id: eG9SXqfHYh
  pdf_file: 191e61a651699b737b9866e0aae8a2a8822929b4.pdf
  start_page: 7337
  title: Efficient Information Extraction in Few-Shot Relation Classification through
    Contrastive Representation Learning
- abstract: "Counterspeech, defined as a response to mit\xADigate online hate speech,\
    \ is increasingly used as a non-censorial solution. The effectiveness of addressing\
    \ hate speech involves dispelling the stereotypes, prejudices, and biases often\
    \ subtly implied in brief, single-sentence state\xADments or abuses. These expressions\
    \ challenge language models, especially in seq2seq tasks, as model performance\
    \ typically excels with longer contexts. Our study introduces CoARL, a novel framework\
    \ enhancing counterspeech generation by modeling the pragmatic implica\xADtions\
    \ underlying social biases in hateful state\xADments. The first two phases of\
    \ CoARL involve sequential multi-instruction tuning, teaching the model to understand\
    \ intents, reactions, and harms of offensive statements, and then learn\xADing\
    \ task-specific low-rank adapter weights for generating intent-conditioned counterspeech.\
    \ The final phase uses reinforcement learning to fine-tune outputs for effectiveness\
    \ and non\xADtoxicity. CoARL outperforms existing bench\xADmarks in intent-conditioned\
    \ counterspeech gen\xADeration, showing an average improvement of \u223C3 points\
    \ in intent-conformity and \u223C4 points in argument-quality metrics. Extensive\
    \ human evaluation supports CoARL\u2019s efficacy in gen\xADerating superior and\
    \ more context-appropriate responses compared to existing systems, includ\xAD\
    ing prominent LLMs like ChatGPT."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Computational Social Science and Cultural Analytics
  authors:
  - emails: '****@gmail.com'
    first_name: Amey
    google_scholar_id: https://scholar.google.com/citations?user=hzNuZEoAAAAJ&hl=en
    homepage: https://ameyhengle.github.io/
    last_name: Hengle
    name: Amey Hengle
    username: ~Amey_Hengle1
  - emails: '****@gmail.com'
    first_name: Aswini
    google_scholar_id: https://scholar.google.com/citations?user=SNF6BsQAAAAJ&hl=en
    homepage: https://sites.google.com/view/aswini-for-ai/home?authuser=0
    last_name: Padhi
    middle_name: Kumar
    name: Aswini Kumar Padhi
    username: ~Aswini_Kumar_Padhi1
  - emails: '****@gmail.com'
    first_name: Sahajpreet
    google_scholar_id: https://scholar.google.com/citations?user=VK4nBkkAAAAJ&hl=en
    institution: IIT Delhi
    last_name: Singh
    name: Sahajpreet Singh
    username: ~Sahajpreet_Singh1
  - emails: '****@logically.ai'
    first_name: Anil
    last_name: Bandhakavi
    name: Anil Bandhakavi
    username: ~Anil_Bandhakavi1
  - dblp_id: https://dblp.org/pid/184/8579.html
    emails: '****@gmail.com'
    first_name: Md Shad
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=KUcO6LAAAAAJ
    institution: Indraprastha Institute of Information Technology, Delhi
    last_name: Akhtar
    name: Md Shad Akhtar
    username: ~Md_Shad_Akhtar1
  - dblp_id: https://dblp.org/pid/65/2136-2.html
    emails: '****@gmail.com'
    first_name: Tanmoy
    google_scholar_id: https://scholar.google.co.in/citations?user=C5S9JnIAAAAJ&hl=en
    homepage: http://tanmoychak.com
    institution: Indian Institute of Technology, Delhi
    last_name: Chakraborty
    name: Tanmoy Chakraborty
    orcid: https://orcid.org/0000-0002-0210-0369
    semantic_scholar_id: https://www.semanticscholar.org/author/Tanmoy-Chakraborty/144054829
    username: ~Tanmoy_Chakraborty2
  decision: toMainConference
  end_page: 7363
  file: 788.pdf
  id: 788
  num_pages: 18
  openreview_id: 8W6bZW9zO7
  pdf_file: 5a5e0075126702b307b12d18bf005f60a417a8ea.pdf
  start_page: 7346
  title: Intent-conditioned and Non-toxic Counterspeech Generation using Multi-Task
    Instruction Tuning with RLAIF
- abstract: 'Large Language Models (LLMs) are now commonplace in conversation applications.
    However, their risks of misuse for generating harmful responses have raised serious
    societal concerns and spurred recent research on LLM conversation safety. Therefore,
    in this survey, we provide a comprehensive overview of recent studies, covering
    three critical aspects of LLM conversation safety: attacks, defenses, and evaluations.
    Our goal is to provide a structured summary that enhances understanding of LLM
    conversation safety and encourages further investigation into this important subject.
    For easy reference, we have categorized all the studies mentioned in this survey
    according to our taxonomy, available at: https://github.com/niconi19/LLM-conversation-safety.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Ethics, Bias, and Fairness
  authors:
  - emails: '****@sjtu.edu.cn'
    first_name: Zhichen
    homepage: https://github.com/niconi19
    last_name: Dong
    name: Zhichen Dong
    username: ~Zhichen_Dong1
  - emails: '****@gmail.com'
    first_name: Zhanhui
    homepage: https://github.com/ZHZisZZ
    last_name: Zhou
    name: Zhanhui Zhou
    username: ~Zhanhui_Zhou1
  - dblp_id: https://dblp.org/pid/00/5867-26
    emails: '****@pjlab.org.cn'
    first_name: Chao
    google_scholar_id: https://scholar.google.com/citations?user=5KRbHPMAAAAJ&hl=zh-CN
    last_name: Yang
    name: Chao Yang
    username: ~Chao_Yang3
  - dblp_id: https://dblp.org/pid/91/5981
    emails: '****@pjlab.org.cn'
    first_name: Jing
    google_scholar_id: https://scholar.google.com/citations?user=VU5ObUwAAAAJ&hl=en;
    homepage: https://amandajshao.github.io/
    institution: Shanghai AI Laboratory
    last_name: Shao
    name: Jing Shao
    username: ~Jing_Shao3
  - dblp_id: https://dblp.org/pid/q/YuQiao1
    emails: '****@pjlab.org.cn'
    first_name: Yu
    google_scholar_id: https://scholar.google.com/citations?user=gFtI-8QAAAAJ&hl=zh-CN
    homepage: http://mmlab.siat.ac.cn/yuqiao
    last_name: Qiao
    name: Yu Qiao
    orcid: https://orcid.org/0000-0002-1889-2567
    username: ~Yu_Qiao1
  decision: toMainConference
  end_page: 7377
  file: 789.pdf
  id: 789
  num_pages: 14
  openreview_id: TQ1VuRxysF
  pdf_file: a24d6f880a77f334fc1902d1ba4b4a0b1098a231.pdf
  start_page: 7364
  title: 'Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey'
- abstract: 'Large language models (LLMs) have achieved remarkable advancements in
    natural language processing. However, the massive scale and computational demands
    of these models present formidable challenges when considering their practical
    deployment in resource-constrained environments. While techniques such as chain-of-thought
    (CoT) distillation have displayed promise in distilling LLMs into small language
    models (SLMs), there is a risk that distilled SLMs may still inherit flawed reasoning
    and hallucinations from LLMs. To address these issues, we propose a twofold methodology:
    First, we introduce a novel method for distilling the self-evaluation capability
    from LLMs into SLMs, aiming to mitigate the adverse effects of flawed reasoning
    and hallucinations inherited from LLMs. Second, we advocate for distilling more
    comprehensive thinking by incorporating multiple distinct CoTs and self-evaluation
    outputs, to ensure a more thorough and robust knowledge transfer into SLMs. Experiments
    on three NLP benchmarks demonstrate that our method significantly improves the
    performance of distilled SLMs, offering a new perspective for developing more
    effective and efficient SLMs in resource-constrained environments.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Efficient/Low-Resource Methods for NLP
  authors:
  - emails: '****@163.com'
    first_name: Weize
    last_name: Liu
    name: Weize Liu
    semantic_scholar_id: https://www.semanticscholar.org/author/Weize-Liu/1390516186
    username: ~Weize_Liu1
  - emails: '****@163.com'
    first_name: Guocong
    last_name: Li
    name: Guocong Li
    username: ~Guocong_Li1
  - emails: '****@gmail.com'
    first_name: Kai
    homepage: https://www.linkedin.cn/incareer/in/ACoAADQzT6kBRwrVEDjjuTpJQWyUKBYbPnNOlr4
    institution: Zhejiang University
    last_name: Zhang
    name: Kai Zhang
    username: ~Kai_Zhang27
  - emails: '****@zju.edu.cn'
    first_name: Bang
    last_name: Du
    name: Bang Du
    username: ~Bang_Du2
  - dblp_id: https://dblp.org/pid/319/2575
    emails: '****@gmail.com'
    first_name: Qiyuan
    homepage: https://qiyuan-chen.github.io/
    last_name: Chen
    name: Qiyuan Chen
    username: ~Qiyuan_Chen1
  - dblp_id: https://dblp.org/pid/262/3664
    emails: '****@gmail.com'
    first_name: Xuming
    google_scholar_id: https://scholar.google.com/citations?user=dbBKbXoAAAAJ&hl=zh-CN
    homepage: https://xuminghu.github.io/
    institution: The Hong Kong University of Science and Technology (Guangzhou)
    last_name: Hu
    name: Xuming Hu
    orcid: https://orcid.org/0000-0001-6075-4224
    semantic_scholar_id: https://www.semanticscholar.org/author/2109906988
    username: ~Xuming_Hu1
  - emails: '****@zju.edu.cn'
    first_name: Hongxia
    google_scholar_id: https://scholar.google.com/citations?user=XlpKptAAAAAJ&hl=zh-CN
    last_name: Xu
    name: Hongxia Xu
    username: ~Hongxia_Xu1
  - dblp_id: https://dblp.org/pid/249/3929
    emails: '****@gmail.com'
    first_name: Jintai
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=ZiY3xYEAAAAJ
    homepage: https://whatashot.github.io/
    last_name: Chen
    name: Jintai Chen
    orcid: https://orcid.org/0000-0002-3199-2597
    username: ~Jintai_Chen1
  - dblp_id: https://dblp.org/pid/96/2744
    emails: '****@zju.edu.cn'
    first_name: Jian
    google_scholar_id: https://scholar.google.com/citations?hl=zh-TW&user=VO9XIXYAAAAJ
    homepage: https://scholar.google.com/citations?hl=zh-TW&user=VO9XIXYAAAAJ
    last_name: Wu
    name: Jian Wu
    username: ~Jian_Wu6
  decision: toMainConference
  end_page: 7393
  file: 790.pdf
  id: 790
  num_pages: 16
  openreview_id: ppic9bjGkr
  pdf_file: e90ad15ceb25549fd072d00664692169ba6a37f3.pdf
  start_page: 7378
  title: 'Mind''s Mirror: Distilling Self-Evaluation Capability and Comprehensive
    Thinking from Large Language Models'
- abstract: Large Language Models (LLMs) have reshaped natural language processing
    with their impressive capabilities. However, their ever-increasing size has raised
    concerns about their effective deployment and the need for LLM compression. This
    study introduces the Divergent Token Metrics (DTMs), a novel approach to assessing
    compressed LLMs, addressing the limitations of traditional perplexity or accuracy
    measures that fail to accurately reflect text generation quality. DTMs measure
    token divergences that allow deeper insights into the subtleties of model compression,
    in particular, when evaluating components' impacts individually. Utilizing the
    First Divergent Token Metric (FDTM) in model sparsification reveals that 25\%
    of all attention components can be pruned beyond 90\% on the Llama-2 model family,
    still keeping SOTA performance. For quantization, FDTM suggests that more than
    80\% of parameters can be naively transformed to int8 without special outlier
    management. These evaluations indicate the necessity of choosing appropriate compressions
    for parameters individually---and that FDTM can identify those---while standard
    metrics result in deteriorated outcomes.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Efficient/Low-Resource Methods for NLP
  authors:
  - dblp_id: https://dblp.org/pid/28/10842
    emails: '****@tu-darmstadt.de'
    first_name: "Bj\xF6rn"
    google_scholar_id: https://scholar.google.de/citations?user=OjuwG6YAAAAJ&hl=de&oi=sra
    institution: "Technische Universit\xE4t Darmstadt and Aleph Alpha "
    last_name: Deiseroth
    name: "Bj\xF6rn Deiseroth"
    username: "~Bj\xF6rn_Deiseroth1"
  - emails: '****@aleph-alpha.com'
    first_name: Max
    institution: Aleph Alpha
    last_name: Meuer
    name: Max Meuer
    username: ~Max_Meuer1
  - emails: '****@web.de'
    first_name: Nikolas
    last_name: Gritsch
    name: Nikolas Gritsch
    username: ~Nikolas_Gritsch1
  - dblp_id: https://dblp.org/pid/247/2141
    emails: '****@aleph-alpha.com'
    first_name: Constantin
    institution: Aleph Alpha
    last_name: Eichenberg
    name: Constantin Eichenberg
    orcid: https://orcid.org/0000-0002-9973-2687
    username: ~Constantin_Eichenberg1
  - dblp_id: https://dblp.org/pid/217/1650
    emails: '****@cs.tu-darmstadt.de'
    first_name: Patrick
    google_scholar_id: https://scholar.google.com/citations?user=GD481RkAAAAJ&hl=en
    homepage: https://ml-research.github.io/people/pschramowski/index.html
    institution: German Research Center for AI
    last_name: Schramowski
    name: Patrick Schramowski
    orcid: https://orcid.org/0000-0003-1231-7120
    semantic_scholar_id: https://www.semanticscholar.org/author/P.-Schramowski/40896023
    username: ~Patrick_Schramowski1
  - dblp_id: https://dblp.org/pid/256/0948
    emails: '****@stat.uni-muenchen.de'
    first_name: Matthias
    google_scholar_id: https://scholar.google.com/citations?user=qmQ-l84AAAAJ&hl=de
    homepage: https://www.slds.stat.uni-muenchen.de/people/assenmacher/
    institution: "Ludwig-Maximilians-Universit\xE4t M\xFCnchen"
    last_name: "A\xDFenmacher"
    name: "Matthias A\xDFenmacher"
    orcid: https://orcid.org/0000-0003-2154-5774
    semantic_scholar_id: https://www.semanticscholar.org/author/M.-A%C3%9Fenmacher/7809845
    username: "~Matthias_A\xDFenmacher1"
  - dblp_id: http://dblp.uni-trier.de/pers/hy/k/Kersting:Kristian.html
    emails: '****@cs.tu-darmstadt.de'
    first_name: Kristian
    google_scholar_id: https://scholar.google.com/citations?user=QY-earAAAAAJ&hl=en
    homepage: http://www.ml.informatik.tu-darmstadt.de/
    institution: German Research Center for AI, The Hessian Center for AI and TU Darmstadt
    last_name: Kersting
    name: Kristian Kersting
    orcid: https://orcid.org/0000-0002-2873-9152
    semantic_scholar_id: https://www.semanticscholar.org/author/K.-Kersting/1746871
    username: ~Kristian_Kersting1
  decision: toMainConference
  end_page: 7413
  file: 791.pdf
  id: 791
  num_pages: 20
  openreview_id: lbTuyCgYe2
  pdf_file: bf6266367d7490b34833735f2a8277655148569f.pdf
  start_page: 7394
  title: "Divergent Token Metrics: Measuring degradation to prune away LLM components\
    \ \u2013 and optimize quantization"
- abstract: Large language models (LLMs) have shown remarkable adaptability to diverse
    tasks, by leveraging context prompts containing instructions, or minimal input-output
    examples. However, recent work revealed they also exhibit *label bias*---an undesirable
    preference toward predicting certain answers over others. Still, detecting and
    measuring this bias reliably and at scale has remained relatively unexplored.
    In this study, we evaluate different approaches to quantifying label bias in a
    model's predictions, conducting a comprehensive investigation across 279 classification
    tasks and ten LLMs. Our investigation reveals substantial label bias in models
    both before and after debiasing attempts, as well as highlights the importance
    of outcomes-based evaluation metrics, which were not previously used in this regard.
    We further propose a novel label bias calibration method tailored for few-shot
    prompting, which outperforms recent calibration approaches for both improving
    performance and mitigating label bias. Our results emphasize that label bias in
    the predictions of LLMs remains a barrier to their reliability.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - dblp_id: https://dblp.org/pid/348/6436
    emails: '****@mail.huji.ac.il'
    first_name: Yuval
    institution: Hebrew University of Jerusalem
    last_name: Reif
    name: Yuval Reif
    semantic_scholar_id: https://www.semanticscholar.org/author/Yuval-Reif/2219310473
    username: ~Yuval_Reif1
  - dblp_id: https://dblp.org/pid/19/376-1
    emails: '****@mail.huji.ac.il'
    first_name: Roy
    google_scholar_id: https://scholar.google.com/citations?user=wvfWo9IAAAAJ&hl=en
    homepage: https://schwartz-lab-huji.github.io/
    institution: Hebrew University, Hebrew University of Jerusalem
    last_name: Schwartz
    name: Roy Schwartz
    semantic_scholar_id: https://www.semanticscholar.org/author/Roy-Schwartz/4671928
    username: ~Roy_Schwartz1
  decision: toMainConference
  end_page: 7428
  file: 792.pdf
  id: 792
  num_pages: 15
  openreview_id: xZ87fjXa0M
  pdf_file: 9a67dd0f73aa5b0c632de20e90e2875f492363b3.pdf
  start_page: 7414
  title: 'Beyond Performance: Quantifying and Mitigating Label Bias in LLMs'
- abstract: 'Math word problem (MWP) solving requires generating a reasoning path
    based on a given problem description that often contains irrelevant conditions.

    Existing chain-of-thought (CoT) prompting methods elicited multi-step reasoning
    abilities of large language models (LLMs) to solve MWPs.

    However, they were seriously confused by the irrelevant conditions, resulting
    in low accuracy.

    In this paper, we propose a novel approach named I$^3$C that instructs LLMs to
    identify and ignore irrelevant conditions.

    It identifies a set of irrelevant condition candidates that have a weak semantic
    relevance with the question.

    Then it prompts LLMs to verify the irrelevant conditions.

    Lastly it instructs the LLMs with the verification on relevant and irrelevant
    conditions to avoid confusion and improve reasoning paths.

    Moreover, we propose to select (problem, reasoning paths) pairs as demonstrations
    to enhance I$^3$C with few-shot reasoning. We develop I$^3$C-Select that selects
    the most confusing problems based on the semantic relevance measurement.

    We conduct extensive experiments on eight MWP datasets.

    I$^3$C can be combined with any CoT prompting methods to improve the performance
    of solving MWPs.

    Notably, with GPT-3.5-Turbo and I$^3$C-Select, we achieve an accuracy of $96.0$
    and $94.1$ on GSM-IC2-1K and GSM-ICM-1K, respectively, significantly outperforming
    the state-of-the-art few-shot prompting method Complex-CoT by $+11.7$ and $+11.1$.

    Our implementation is made publicly available at https://wzy6642.github.io/I3C.github.io/.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - emails: '****@163.com'
    first_name: Zhenyu
    homepage: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8823537
    last_name: Wu
    name: Zhenyu Wu
    username: ~Zhenyu_Wu9
  - emails: '****@xjtu.edu.cn'
    first_name: Chao
    homepage: http://gr.xjtu.edu.cn/web/cshen
    institution: "Xi\u2019an Jiaotong University"
    last_name: Shen
    name: Chao Shen
    username: ~Chao_Shen2
  - dblp_id: https://dblp.uni-trier.de/pid/69/339-1.html
    emails: '****@nd.edu'
    first_name: Meng
    google_scholar_id: https://scholar.google.com/citations?user=LZIPfCkAAAAJ
    homepage: http://www.meng-jiang.com/
    institution: University of Notre Dame
    last_name: Jiang
    name: Meng Jiang
    orcid: https://orcid.org/0000-0002-3009-519X
    username: ~Meng_Jiang3
  decision: toMainConference
  end_page: 7449
  file: 793.pdf
  id: 793
  num_pages: 21
  openreview_id: y5FCdCqjLE
  pdf_file: 9f410e151059f9feaa196ae0f18bea3f586ecb81.pdf
  start_page: 7429
  title: Instructing Large Language Models to Identify and Ignore Irrelevant Conditions
- abstract: "The recent successes and spread of large neural language models (LMs)\
    \ call for a thorough understanding of their abilities. Describing their abilities\
    \ through LMs\u2019 representational capacity is a lively area of research. Investigations\
    \ of the representational capacity of neural LMs have predominantly focused on\
    \ their ability to recognize formal languages. For example, recurrent neural networks\
    \ (RNNs) as classifiers are tightly linked to regular languages, i.e., languages\
    \ defined by finite-state automata (FSAs). Such results, however, fall short of\
    \ describing the capabilities of RNN language models (LMs), which are definitionally\
    \ distributions over strings. We take a fresh look at the represen- tational capacity\
    \ of RNN LMs by connecting them to probabilistic FSAs and demonstrate that RNN\
    \ LMs with linearly bounded precision can express arbitrary regular LMs."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - dblp_id: https://dblp.org/pid/259/1164
    emails: '****@protonmail.com'
    first_name: Anej
    google_scholar_id: https://scholar.google.com/citations?hl=en&view_op=list_works&gmla=AHoSzlX0OjFWWC5hGfMWP8xqO8c9JnXG1S8vD-dzV4xF_HUDlMo7iV9MwpDYOUiTeKk3rGs3AmeQ2FCfn-5qNw&user=9ezEOeUAAAAJ
    homepage: https://anejsvete.github.io/
    institution: Department of Computer Science, ETHZ - ETH Zurich
    last_name: Svete
    name: Anej Svete
    semantic_scholar_id: https://www.semanticscholar.org/author/Anej-Svete/1492179067
    username: ~Anej_Svete1
  - emails: '****@ethz.ch'
    first_name: Franz
    google_scholar_id: https://scholar.google.com/citations?user=IgJ4o30AAAAJ&hl=en
    homepage: https://franznowak.github.io
    institution: ETHZ - ETH Zurich
    last_name: Nowak
    name: Franz Nowak
    username: ~Franz_Nowak1
  - emails: '****@student.ethz.ch'
    first_name: Anisha
    homepage: https://www.linkedin.com/in/anisha-mohamed/
    last_name: Sahabdeen
    middle_name: Mohamed
    name: Anisha Mohamed Sahabdeen
    username: ~Anisha_Mohamed_Sahabdeen1
  - dblp_id: https://dblp.org/pid/146/4361.html
    emails: '****@gmail.com'
    first_name: Ryan
    google_scholar_id: https://scholar.google.com/citations?user=DexOqtoAAAAJ&hl=en
    homepage: https://rycolab.io/
    institution: Swiss Federal Institute of Technology
    last_name: Cotterell
    name: Ryan Cotterell
    semantic_scholar_id: https://www.semanticscholar.org/author/Ryan-Cotterell/1750769
    username: ~Ryan_Cotterell1
  decision: toMainConference
  end_page: 7470
  file: 796.pdf
  id: 796
  num_pages: 21
  openreview_id: i7o1y5k4T7
  pdf_file: a4436152a1ff03ab3713b4f98a8c3fcef2dbc303.pdf
  start_page: 7450
  title: Lower Bounds on the Expressivity of Recurrent Neural Language Models
- abstract: "Plenty of existing work has analyzed the abilities of the transformer\
    \ architecture by describing its representational capacity with formal models\
    \ of computation.\n    However, the focus so far has been on analyzing the architecture\
    \ in terms of language \\emph{acceptance}.\n    We contend that this is an ill-suited\
    \ problem in the study of \\emph{language models} (LMs), which are definitionally\
    \ \\emph{probability distributions} over strings.\n    In this paper, we focus\
    \ on the relationship between transformer LMs and $n$-gram LMs, a simple and historically\
    \ relevant class of language models. \n    We show that transformer LMs using\
    \ the hard or sparse attention mechanisms can exactly represent any $n$-gram LM,\
    \ giving us a concrete lower bound on their probabilistic representational capacity.\n\
    \    This provides a first step towards understanding the mechanisms that transformer\
    \ LMs can use to represent probability distributions over strings."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - dblp_id: https://dblp.org/pid/259/1164
    emails: '****@protonmail.com'
    first_name: Anej
    google_scholar_id: https://scholar.google.com/citations?hl=en&view_op=list_works&gmla=AHoSzlX0OjFWWC5hGfMWP8xqO8c9JnXG1S8vD-dzV4xF_HUDlMo7iV9MwpDYOUiTeKk3rGs3AmeQ2FCfn-5qNw&user=9ezEOeUAAAAJ
    homepage: https://anejsvete.github.io/
    institution: Department of Computer Science, ETHZ - ETH Zurich
    last_name: Svete
    name: Anej Svete
    semantic_scholar_id: https://www.semanticscholar.org/author/Anej-Svete/1492179067
    username: ~Anej_Svete1
  - dblp_id: https://dblp.org/pid/146/4361.html
    emails: '****@gmail.com'
    first_name: Ryan
    google_scholar_id: https://scholar.google.com/citations?user=DexOqtoAAAAJ&hl=en
    homepage: https://rycolab.io/
    institution: Swiss Federal Institute of Technology
    last_name: Cotterell
    name: Ryan Cotterell
    semantic_scholar_id: https://www.semanticscholar.org/author/Ryan-Cotterell/1750769
    username: ~Ryan_Cotterell1
  decision: toMainConference
  end_page: 7504
  file: 797.pdf
  id: 797
  num_pages: 34
  openreview_id: oi72vKkKni
  pdf_file: 5b2476b062c4f4a67efe21b786e887c6e149f4ff.pdf
  start_page: 7471
  title: Transformers Can Represent $n$-gram Language Models
- abstract: For nearly three decades, language models derived from the $n$-gram assumption
    held the state of the art on the task. The key to their success lay in the application
    of various smoothing techniques that served to combat overfitting. However, when
    neural language models toppled $n$-gram models as the best performers, $n$-gram
    smoothing techniques became less relevant. Indeed, it would hardly be an understatement
    to suggest that the line of inquiry into $n$-gram smoothing techniques became
    dormant. This paper re-opens the role classical $n$-gram smoothing techniques
    may play in the age of neural language models. First, we draw a formal equivalence
    between label smoothing, a popular regularization technique for neural language
    models, and add-$\lambda$ smoothing. Second, we derive a generalized framework
    for converting any $n$-gram smoothing technique into a regularizer compatible
    with neural language models. Our empirical results find that our novel regularizers
    are comparable to and, indeed, sometimes outperform label smoothing on language
    modeling and machine translation.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - dblp_id: https://dblp.org/pid/287/5054
    emails: '****@inf.ethz.ch'
    first_name: Luca
    institution: Department of Computer Science, ETHZ - ETH Zurich
    last_name: Malagutti
    name: Luca Malagutti
    username: ~Luca_Malagutti1
  - emails: '****@tcd.ie'
    first_name: Andrius
    homepage: http://idonthaveone.gov
    last_name: Buinovskij
    name: Andrius Buinovskij
    username: ~Andrius_Buinovskij1
  - dblp_id: https://dblp.org/pid/259/1164
    emails: '****@protonmail.com'
    first_name: Anej
    google_scholar_id: https://scholar.google.com/citations?hl=en&view_op=list_works&gmla=AHoSzlX0OjFWWC5hGfMWP8xqO8c9JnXG1S8vD-dzV4xF_HUDlMo7iV9MwpDYOUiTeKk3rGs3AmeQ2FCfn-5qNw&user=9ezEOeUAAAAJ
    homepage: https://anejsvete.github.io/
    institution: Department of Computer Science, ETHZ - ETH Zurich
    last_name: Svete
    name: Anej Svete
    semantic_scholar_id: https://www.semanticscholar.org/author/Anej-Svete/1492179067
    username: ~Anej_Svete1
  - dblp_id: https://dblp.uni-trier.de/pid/245/7485
    emails: '****@inf.ethz.ch'
    first_name: Clara
    google_scholar_id: https://scholar.google.com/citations?user=quJhNH8AAAAJ&hl=en
    homepage: https://cimeister.github.io/
    last_name: Meister
    name: Clara Meister
    orcid: https://orcid.org/0000-0002-3775-4426
    semantic_scholar_id: https://www.semanticscholar.org/author/Clara-Meister/150953620
    username: ~Clara_Meister1
  - dblp_id: https://dblp.org/pid/270/4959
    emails: '****@gmail.com'
    first_name: Afra
    institution: ETHZ - ETH Zurich
    last_name: Amini
    name: Afra Amini
    username: ~Afra_Amini1
  - dblp_id: https://dblp.org/pid/146/4361.html
    emails: '****@gmail.com'
    first_name: Ryan
    google_scholar_id: https://scholar.google.com/citations?user=DexOqtoAAAAJ&hl=en
    homepage: https://rycolab.io/
    institution: Swiss Federal Institute of Technology
    last_name: Cotterell
    name: Ryan Cotterell
    semantic_scholar_id: https://www.semanticscholar.org/author/Ryan-Cotterell/1750769
    username: ~Ryan_Cotterell1
  decision: toMainConference
  end_page: 7521
  file: 798.pdf
  id: 798
  num_pages: 17
  openreview_id: tWjx5VarZn
  pdf_file: caf2bc685117ff6c7238cb9cad434379719138f7.pdf
  start_page: 7505
  title: The Role of $n$-gram Smoothing in the Age of Neural Networks
- abstract: 'Evaluating the reliability of news sources is a routine task for journalists
    and organizations committed to acquiring and disseminating accurate information.

    Recent research has shown that predicting sources'' reliability represents an
    important first-prior step in addressing additional challenges such as fake news
    detection and fact-checking.

    In this paper, we introduce a novel approach for source reliability estimation
    that leverages reinforcement learning strategies for estimating the reliability
    degree of news sources. Contrary to previous research, our proposed approach models
    the problem as the estimation of a reliability degree, and not a reliability label,
    based on how all the news media sources interact with each other on the Web.

    We validated the effectiveness of our method on a news media reliability dataset
    that is an order of magnitude larger than comparable existing datasets. Results
    show that the estimated reliability degrees strongly correlates with journalists-provided
    scores (Spearman=0.80) and can effectively predict reliability labels (macro-avg.
    F1 score=81.05).

    We release our implementation and dataset, aiming to provide a valuable resource
    for the NLP community working on information verification.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - dblp_id: https://dblp.org/pid/224/2038
    emails: '****@gmail.com'
    first_name: Sergio
    google_scholar_id: https://scholar.google.com/citations?view_op=list_works&hl=en&user=XOD8lrAAAAAJ
    homepage: https://github.com/sergioburdisso
    institution: Idiap Research Institute
    last_name: Burdisso
    name: Sergio Burdisso
    orcid: https://orcid.org/0000-0002-7694-6834
    semantic_scholar_id: https://www.semanticscholar.org/author/S.-Burdisso/51168007
    username: ~Sergio_Burdisso1
  - emails: '****@idiap.ch'
    first_name: Dairazalia
    google_scholar_id: https://scholar.google.com/citations?user=rL3daVYAAAAJ&hl=en
    last_name: Sanchez-cortes
    name: DAIRAZALIA SANCHEZ-CORTES
    username: ~DAIRAZALIA_SANCHEZ-CORTES1
  - dblp_id: https://dblp.org/pid/21/2544.html
    emails: '****@idiap.ch'
    first_name: "Esa\xFA"
    google_scholar_id: https://scholar.google.com/citations?hl=en&pli=1&user=GzaiunYAAAAJ
    homepage: https://villatoroe.github.io/
    institution: Idiap Research Institute
    last_name: Villatoro-tello
    name: "Esa\xFA VILLATORO-TELLO"
    orcid: https://orcid.org/0000-0002-1322-0358
    semantic_scholar_id: https://www.semanticscholar.org/author/Esa%C3%BA-Villatoro-Tello/1398324221
    username: "~Esa\xFA_VILLATORO-TELLO1"
  - dblp_id: https://dblp.org/pers/hd/m/Motl=iacute=cek:Petr
    emails: '****@idiap.ch'
    first_name: Petr
    homepage: http://www.idiap.ch/~pmotlic
    last_name: Motlicek
    name: Petr Motlicek
    username: ~Petr_Motlicek1
  decision: toMainConference
  end_page: 7540
  file: 799.pdf
  id: 799
  num_pages: 19
  openreview_id: g6hlvwSIMP
  pdf_file: 608f100e6d784dd3afc6c53e955d0b545424cdbc.pdf
  start_page: 7522
  title: 'Reliability Estimation of News Media Sources: Birds of a Feather Flock Together'
- abstract: "Current decoder-based pre-trained language models (PLMs) successfully\
    \ demonstrate multilingual capabilities. \nHowever, it is unclear how these models\
    \ handle multilingualism.\nWe analyze the neuron-level internal behavior of multilingual\
    \ decoder-based PLMs, \nSpecifically examining the existence of neurons that fire\
    \ ``uniquely for each language'' within decoder-only multilingual PLMs.\nWe analyze\
    \ six languages: English, German, French, Spanish, Chinese, and Japanese, and\
    \ show that language-specific neurons are unique, with a slight overlap (< 5%)\
    \ between languages. These neurons are mainly distributed in the models' first\
    \ and last few layers. \nThis trend remains consistent across languages and models.\n\
    Additionally, we tamper with less than 1% of the total neurons in each model during\
    \ inference and demonstrate that tampering with a few language-specific neurons\
    \ drastically changes the probability of target language occurrence in text generation."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - dblp_id: https://dblp.org/pid/41/1448.html
    emails: '****@weblab.t.u-tokyo.ac.jp'
    first_name: Takeshi
    google_scholar_id: https://scholar.google.com/citations?user=KpkgqOsAAAAJ&hl=ja
    institution: The University of Tokyo
    last_name: Kojima
    name: Takeshi Kojima
    semantic_scholar_id: https://www.semanticscholar.org/author/Takeshi-Kojima/2081836120
    username: ~Takeshi_Kojima1
  - emails: '****@weblab.t.u-tokyo.ac.jp'
    first_name: Itsuki
    last_name: Okimura
    name: Itsuki Okimura
    username: ~Itsuki_Okimura1
  - dblp_id: https://dblp.org/pid/117/7377
    emails: '****@weblab.t.u-tokyo.ac.jp'
    first_name: Yusuke
    google_scholar_id: https://scholar.google.co.jp/citations?user=pvvZgj0AAAAJ&hl=ja
    institution: The University of Tokyo
    last_name: Iwasawa
    name: Yusuke Iwasawa
    orcid: https://orcid.org/0000-0002-1321-2622
    username: ~Yusuke_Iwasawa1
  - dblp_id: https://dblp.org/pid/187/5994.html
    emails: '****@is.s.u-tokyo.ac.jp'
    first_name: Hitomi
    google_scholar_id: https://scholar.google.co.jp/citations?user=kwoER0sAAAAJ&hl=ja
    homepage: http://hitomiyanaka.strikingly.com/
    institution: the University of Tokyo
    last_name: Yanaka
    name: Hitomi Yanaka
    orcid: https://orcid.org/0000-0003-0354-6116
    semantic_scholar_id: https://www.semanticscholar.org/author/Hitomi-Yanaka/3486313
    username: ~Hitomi_Yanaka2
  - dblp_id: http://dblp.uni-trier.de/pers/hd/m/Matsuo:Yutaka
    emails: '****@weblab.t.u-tokyo.ac.jp'
    first_name: Yutaka
    google_scholar_id: https://scholar.google.com/citations?user=Dy8iau4AAAAJ&hl=ja
    homepage: http://ymatsuo.com
    institution: The University of Tokyo and The University of Tokyo
    last_name: Matsuo
    name: Yutaka Matsuo
    username: ~Yutaka_Matsuo1
  decision: toMainConference
  end_page: 7593
  file: 800.pdf
  id: 800
  num_pages: 53
  openreview_id: KYaPfCxahm
  pdf_file: c24861174dd0a3653270297c42376565ad6936f5.pdf
  start_page: 7541
  title: 'On the Multilingual Ability of Decoder-based Pre-trained Language Models:
    Finding and Controlling Language-Specific Neurons'
- abstract: The paper focuses on the marginalization of indigenous language communities
    in the face of rapid technological advancements. We highlight the cultural richness
    of these languages and the risk they face of being overlooked in the realm of
    Natural Language Processing (NLP). We aim to bridge the gap between these communities
    and researchers, emphasizing the need for inclusive technological advancements
    that respect indigenous community perspectives. We show the NLP progress of indigenous
    Latin American languages and the survey that covers the status of indigenous languages
    in Latin America, their representation in NLP, and the challenges and innovations
    required for their preservation and development. The paper contributes to the
    current literature in understanding the need and progress of NLP for indigenous
    communities of Latin America, specifically low-resource and indigenous communities
    in general.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: 'Special Theme: Languages of Latin America'
  authors:
  - dblp_id: https://dblp.org/pid/312/3167
    emails: '****@gmail.com'
    first_name: Atnafu
    google_scholar_id: https://scholar.google.com.mx/citations?user=rubyApkAAAAJ&hl=en
    homepage: http://atnafuatx.github.io/
    institution: "Instituto Polit\xE9cnico Nacional"
    last_name: Tonja
    middle_name: Lambebo
    name: Atnafu Lambebo Tonja
    orcid: https://orcid.org/0000-0002-3501-5136
    semantic_scholar_id: https://www.semanticscholar.org/author/Atnafu-Lambebo-Tonja/2148631756
    username: ~Atnafu_Lambebo_Tonja1
  - dblp_id: https://dblp.org/pid/277/0925.html
    emails: '****@cic.ipn.mx'
    first_name: Fazlourrahman
    google_scholar_id: https://scholar.google.com/citations?user=h8oz5RcAAAAJ&hl=en&oi=ao
    homepage: https://sites.google.com/view/fazlfrs/home
    last_name: Balouchzahi
    name: Fazlourrahman Balouchzahi
    username: ~Fazlourrahman_Balouchzahi1
  - dblp_id: https://dblp.org/pid/215/8815.html
    emails: '****@gmail.com'
    first_name: Sabur
    google_scholar_id: https://scholar.google.com/citations?user=re7md-0AAAAJ&hl=en#
    homepage: https://saburbutt.github.io/
    last_name: Butt
    name: SABUR BUTT
    username: ~SABUR_BUTT1
  - emails: '****@gmail.com'
    first_name: Olga
    google_scholar_id: https://scholar.google.com/citations?user=cXgJJqAAAAAJ&hl=en
    institution: "Instituto Polit\xE9cnico Nacional"
    last_name: Kolesnikova
    name: Olga Kolesnikova
    username: ~Olga_Kolesnikova1
  - emails: '****@tec.mx'
    first_name: Hector
    institution: Tecnologico de Monterrey
    last_name: Ceballos
    name: Hector Ceballos
    orcid: https://orcid.org/0000-0002-2460-3442
    username: ~Hector_Ceballos1
  - dblp_id: https://dblp.org/pid/g/AlexanderFGelbukh
    emails: '****@gelbukh.com'
    first_name: Alexander
    google_scholar_id: https://scholar.google.com/citations?user=g-dRTkQAAAAJ
    homepage: http://www.gelbukh.com
    institution: "Instituto Polit\xE9cnico Nacional"
    last_name: Gelbukh
    name: Alexander Gelbukh
    orcid: https://orcid.org/0000-0001-7845-9039
    semantic_scholar_id: https://www.semanticscholar.org/author/Alexander-Gelbukh/1747784
    username: ~Alexander_Gelbukh1
  - dblp_id: https://dblp.org/pid/79/3530
    emails: '****@gmail.com'
    first_name: Thamar
    google_scholar_id: https://scholar.google.com/citations?user=Gmjwy-IAAAAJ&hl=en
    homepage: http://solorio.uh.edu/
    institution: Mohamed bin Zayed University of Artificial Intelligence and University
      of Houston
    last_name: Solorio
    name: Thamar Solorio
    username: ~Thamar_Solorio1
  decision: toMainConference
  end_page: 7609
  file: 801.pdf
  id: 801
  num_pages: 16
  openreview_id: FdFx54hbV9
  pdf_file: ed39339a5c4fc0709f179f11162f442c558015cf.pdf
  start_page: 7594
  title: NLP Progress in Indigenous Latin American Languages
- abstract: Recent work on automated approaches to counterspeech have mostly focused
    on synthetic data but seldom look into how the public deals with abuse. While
    these systems identifying and generating counterspeech have the potential for
    abuse mitigation, it remains unclear how robust a model is against adversarial
    attacks across multiple domains and how models trained on synthetic data can handle
    unseen user-generated abusive content in the real world. To tackle these issues,
    this paper first explores the dynamics of abuse and replies using our novel dataset
    of 6,955 labelled tweets targeted at footballers for studying public figure abuse.
    We then curate DynaCounter, a new English dataset of 1,911 pairs of abuse and
    replies addressing nine minority identity groups, collected in an adversarial
    human-in-the-loop process over four rounds. Our analysis shows that adversarial
    attacks do not necessarily result in better generalisation. We further present
    a study of multi-domain counterspeech generation, comparing Flan-T5 and T5 models.
    We observe that handling certain abuse targets is particularly challenging.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - dblp_id: https://dblp.org/pid/176/1859.html
    emails: '****@turing.ac.uk'
    first_name: Yi-Ling
    google_scholar_id: https://scholar.google.it/citations?hl=en&user=276kTuoAAAAJ
    homepage: https://yilingchung.github.io
    institution: Alan Turing Institute
    last_name: Chung
    name: Yi-Ling Chung
    semantic_scholar_id: https://www.semanticscholar.org/author/Yi-Ling-Chung/3365740
    username: ~Yi-Ling_Chung1
  - emails: '****@turing.ac.uk'
    first_name: Jonathan
    google_scholar_id: https://scholar.google.com/citations?user=QMIKrpYAAAAJ
    homepage: https://www.turing.ac.uk/people/researchers/jonathan-bright
    institution: Alan Turing Institute
    last_name: Bright
    name: Jonathan Bright
    username: ~Jonathan_Bright2
  decision: toMainConference
  end_page: 7624
  file: 804.pdf
  id: 804
  num_pages: 15
  openreview_id: V6SQR7xIBI
  pdf_file: 18a54f57f3089a3775b5efcad360d1a8789c6b3c.pdf
  start_page: 7610
  title: On the Effectiveness of Adversarial Robustness for Abuse Mitigation with
    Counterspeech
- abstract: Most current state-of-the-art approaches for text classification are based
    on fine-tuning the representations computed by large language models (LLMs). This
    strategy has led to significant improvements in classification performance and
    contributed to a reduction of the amount of labeled data required for training
    a model. However, for some challenging classification tasks, providing enough
    annotations to ensure a reliable classification continues to be the main bottleneck.
    This is especially true in settings of highly imbalanced class distributions.
    This paper proposes to tackle this bottleneck by exploiting the structural properties
    of pre-trained embeddings. We develop a label propagation method that uses pre-trained
    embeddings to spread information from the labeled samples to nearby samples in
    the induced space, ensuring the optimal use of annotations. Our approach is simple
    and relatively low-cost since it only requires computing some distances in the
    embedded space. We conduct experiments on different text classification datasets
    showing that the proposed method is efficient and significantly outperforms both
    self-training and random walk label propagation strategies.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Efficient/Low-Resource Methods for NLP
  authors:
  - dblp_id: https://dblp.org/pid/331/2720.html
    emails: '****@upc.edu'
    first_name: Cesar
    homepage: https://ceguel.es
    institution: "Universitat Polit\xE8cnica de Catalunya"
    last_name: Gonzalez-Gutierrez
    name: Cesar Gonzalez-Gutierrez
    orcid: https://orcid.org/0009-0006-7415-950X
    username: ~Cesar_Gonzalez-Gutierrez1
  - emails: '****@gmail.com'
    first_name: Ariadna
    google_scholar_id: https://scholar.google.es/citations?user=D1okUccAAAAJ&hl=ca
    homepage: https://www.cs.upc.edu/~aquattoni/
    institution: "Universidad Polit\xE9cnica de Cataluna"
    last_name: Quattoni
    name: Ariadna Quattoni
    username: ~Ariadna_Quattoni2
  decision: toMainConference
  end_page: 7639
  file: 805.pdf
  id: 805
  num_pages: 15
  openreview_id: 4LL3ffgENb
  pdf_file: 39b7443ea4afbcf3ac26c6610dfe7e343fe94f08.pdf
  start_page: 7625
  title: Leveraging the Structure of Pre-trained Embeddings to Minimize Annotation
    Effort
- abstract: Several recent papers have investigated the potential of language models
    as knowledge bases as well as the existence of severe biases when extracting factual
    knowledge. In this work, we focus on the factual probing performance over unseen
    prompts from tuning, and using a probabilistic view we show the inherent misalignment
    between pre-training and downstream tuning objectives in language models for probing
    knowledge. We hypothesize that simultaneously debiasing these objectives can be
    the key to generalisation over unseen prompts.  We propose an adapter-based framework,
    **UniArk**, for generalised and consistent factual knowledge extraction through
    simple methods without introducing extra parameters. Extensive experiments show
    that UniArk can significantly improve the model's out-of-domain generalisation
    as well as consistency under various prompts. Additionally, we construct **ParaTrex**,
    a large-scale and diverse dataset for measuring the inconsistency and out-of-domain
    generation of models. Further, ParaTrex offers a reference method for constructing
    paraphrased datasets using large language models.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - emails: '****@outlook.com'
    first_name: Yijun
    homepage: https://github.com/Thomasyyj
    institution: Edinburgh University, University of Edinburgh
    last_name: Yang
    name: Yijun Yang
    username: ~Yijun_Yang7
  - dblp_id: https://dblp.org/pid/28/4019-4
    emails: '****@ed.ac.uk'
    first_name: Jie
    google_scholar_id: https://scholar.google.com/citations?user=VMD_HuYAAAAJ&hl=zh-CN
    last_name: He
    name: Jie He
    semantic_scholar_id: https://www.semanticscholar.org/author/Jie-He/2143775574
    username: ~Jie_He3
  - dblp_id: https://dblp.org/pid/268/1225
    emails: '****@ed.ac.uk'
    first_name: Pinzhen
    google_scholar_id: https://scholar.google.com/citations?user=m_HgJe0AAAAJ&hl=en
    homepage: https://pinzhenchen.github.io/
    institution: University of Edinburgh
    last_name: Chen
    name: Pinzhen Chen
    semantic_scholar_id: https://www.semanticscholar.org/author/Pinzhen-Chen/143616669
    username: ~Pinzhen_Chen1
  - emails: '****@cardiff.ac.uk'
    first_name: Victor
    google_scholar_id: https://scholar.google.com/citations?user=L2eFo5IAAAAJ&hl=en
    institution: Cardiff University
    last_name: Gutierrez Basulto
    name: Victor Gutierrez Basulto
    username: ~Victor_Gutierrez_Basulto1
  - dblp_id: https://dblp.org/pid/59/6490
    emails: '****@ed.ac.uk'
    first_name: Jeff
    google_scholar_id: https://scholar.google.co.uk/citations?hl=en&user=zLDAY8QAAAAJ&pagesize=100
    homepage: https://knowledge-representation.org/j.z.pan/
    institution: University of Edinburgh, University of Edinburgh
    last_name: Pan
    middle_name: Z.
    name: Jeff Z. Pan
    orcid: https://orcid.org/0000-0002-9779-2088
    semantic_scholar_id: https://www.semanticscholar.org/author/Jeff-Z.-Pan/9416872
    username: ~Jeff_Z._Pan1
  decision: toMainConference
  end_page: 7657
  file: 807.pdf
  id: 807
  num_pages: 18
  openreview_id: zqhZ6ZGDyX
  pdf_file: 9c195396e05a53d7329f2384244dcf3c911ca565.pdf
  start_page: 7640
  title: 'UniArk: Improving Generalisation and Consistency for Factual Knowledge Extraction
    through Debiasing'
- abstract: 'Retrieval-Augmented Large Language Models (LLMs), which incorporate the
    non-parametric knowledge from external knowledge bases into LLMs, have emerged
    as a promising approach to enhancing response accuracy in several tasks, such
    as Question-Answering (QA). However, even though there are various approaches
    dealing with queries of different complexities, they either handle simple queries
    with unnecessary computational overhead or fail to adequately address complex
    multi-step queries; yet, not all user requests fall into only one of the simple
    or complex categories. In this work, we propose a novel adaptive QA framework
    that can dynamically select the most suitable strategy for (retrieval-augmented)
    LLMs from the simplest to the most sophisticated ones based on the query complexity.
    Also, this selection process is operationalized with a classifier, which is a
    smaller LM trained to predict the complexity level of incoming queries with automatically
    collected labels, obtained from actual predicted outcomes of models and inherent
    inductive biases in datasets. This approach offers a balanced strategy, seamlessly
    adapting between the iterative and single-step retrieval-augmented LLMs, as well
    as the no-retrieval methods, in response to a range of query complexities. We
    validate our model on a set of open-domain QA datasets, covering multiple query
    complexities, and show that ours enhances the overall efficiency and accuracy
    of QA systems, compared to relevant baselines including the adaptive retrieval
    approaches. Code is available at: https://github.com/starsuzi/Adaptive-RAG.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Question Answering
  authors:
  - dblp_id: https://dblp.org/pid/164/0452
    emails: '****@kaist.ac.kr'
    first_name: Soyeong
    google_scholar_id: https://scholar.google.com/citations?user=0wnquCEAAAAJ
    homepage: https://starsuzi.github.io/
    institution: Korea Advanced Institute of Science & Technology
    last_name: Jeong
    name: Soyeong Jeong
    username: ~Soyeong_Jeong1
  - dblp_id: https://dblp.org/pid/262/6003
    emails: '****@kaist.ac.kr'
    first_name: Jinheon
    google_scholar_id: https://scholar.google.com/citations?user=U1FHaSUAAAAJ&hl=ko
    homepage: https://jinheonbaek.github.io
    institution: Korea Advanced Institute of Science & Technology
    last_name: Baek
    name: Jinheon Baek
    orcid: https://orcid.org/0000-0002-9367-560X
    semantic_scholar_id: https://www.semanticscholar.org/author/Jinheon-Baek/90765684
    username: ~Jinheon_Baek1
  - dblp_id: https://dblp.org/pid/316/9906
    emails: '****@kaist.ac.kr'
    first_name: Sukmin
    google_scholar_id: https://scholar.google.co.kr/citations?user=YuV8kEoAAAAJ&hl=ko
    homepage: http://nlpcl.kaist.ac.kr/home/
    last_name: Cho
    name: Sukmin Cho
    username: ~Sukmin_Cho1
  - dblp_id: http://dblp2.uni-trier.de/pers/hd/h/Hwang:Sung_Ju
    emails: '****@gmail.com'
    first_name: Sung Ju
    google_scholar_id: https://scholar.google.com/citations?user=RP4Qx3QAAAAJ&hl=en
    homepage: http://www.sungjuhwang.com/
    institution: Korea Advanced Institute of Science and Technology and AITRICS
    last_name: Hwang
    name: Sung Ju Hwang
    username: ~Sung_Ju_Hwang1
  - dblp_id: https://dblp.org/pid/73/5376
    emails: '****@nlp.kaist.ac.kr'
    first_name: Jong
    google_scholar_id: https://scholar.google.com/citations?user=XP5heVgAAAAJ&hl=ko&oi=ao
    homepage: http://nlpcl.kaist.ac.kr/prof
    institution: Korea Advanced Institute of Science and Technology
    last_name: Park
    middle_name: C.
    name: Jong C. Park
    username: ~Jong_C._Park2
  decision: toMainConference
  end_page: 7672
  file: 810.pdf
  id: 810
  num_pages: 15
  openreview_id: RYyLwb4NcY
  pdf_file: f659dab26e0b154235f0d1ee1cb67598d9e59e21.pdf
  start_page: 7658
  title: 'Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models
    through Question Complexity'
- abstract: "Large Language Models (LLMs) have shown great potential in Natural Language\
    \ Processing (NLP) tasks.\nHowever, recent literature reveals that LLMs hallucinate\
    \ intermittently, which impedes their reliability for further utilization. \n\
    In this paper, we propose a novel self-detection method to detect which questions\
    \ an LLM does not know.\nOur proposal is empirical and applicable for continually\
    \ upgrading LLMs compared with state-of-the-art methods. \nSpecifically, we examine\
    \ the divergence of the LLM's behaviors on different verbalizations for a question\
    \ and examine the atypicality of the verbalized input. \nWe combine the two components\
    \ to identify whether the model generates a non-factual response to the question.\
    \ \nThe above components can be accomplished by utilizing the LLM itself without\
    \ referring to any other external resources. \nWe conduct comprehensive experiments\
    \ and demonstrate the effectiveness of our method for recently released LLMs involving\
    \ Llama 2, Vicuna, ChatGPT, and GPT-4 across factoid question-answering, arithmetic\
    \ reasoning, and commonsense reasoning tasks."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - dblp_id: https://dblp.org/pid/158/3615
    emails: '****@gmail.com'
    first_name: Yukun
    google_scholar_id: https://scholar.google.com/citations?user=7EI-gJAAAAAJ&hl=zh-CN
    last_name: Zhao
    name: Yukun Zhao
    username: ~Yukun_Zhao1
  - dblp_id: https://dblp.org/pid/254/8048
    emails: '****@gmail.com'
    first_name: Lingyong
    google_scholar_id: https://scholar.google.com/citations?user=NksMJFcAAAAJ
    homepage: https://yanlingyong.top
    institution: Baidu Inc.
    last_name: Yan
    name: Lingyong Yan
    orcid: https://orcid.org/0000-0002-6547-1984
    semantic_scholar_id: https://www.semanticscholar.org/author/Lingyong-Yan/1387839383
    username: ~Lingyong_Yan1
  - dblp_id: https://dblp.org/pid/63/6566
    emails: '****@gmail.com'
    first_name: Weiwei
    last_name: Sun
    name: Weiwei Sun
    semantic_scholar_id: https://www.semanticscholar.org/author/Weiwei-Sun/2153198380
    username: ~Weiwei_Sun9
  - emails: '****@gmail.com'
    first_name: Guoliang
    last_name: Xing
    name: Guoliang Xing
    username: ~Guoliang_Xing3
  - emails: '****@baidu.com'
    first_name: Chong
    institution: Baidu
    last_name: Meng
    name: Chong Meng
    username: ~Chong_Meng2
  - dblp_id: https://dblp.org/pid/16/1524
    emails: '****@gmail.com'
    first_name: Shuaiqiang
    google_scholar_id: https://scholar.google.com.hk/citations?user=8SbYYcIAAAAJ&hl=en
    homepage: http://wangshuaiqiang.net/
    institution: Baidu Inc.
    last_name: Wang
    name: Shuaiqiang Wang
    semantic_scholar_id: https://www.semanticscholar.org/author/Shuaiqiang-Wang/2386396
    username: ~Shuaiqiang_Wang2
  - dblp_id: https://dblp.org/pid/88/8024
    emails: '****@baidu.com'
    first_name: Zhicong
    last_name: Cheng
    name: Zhicong Cheng
    username: ~Zhicong_Cheng1
  - dblp_id: https://dblp.org/pid/58/10440
    emails: '****@liacs.leidenuniv.nl'
    first_name: Zhaochun
    google_scholar_id: https://scholar.google.com/citations?user=fPcIPt0AAAAJ&hl=en
    homepage: https://renzhaochun.github.io/
    institution: Leiden University
    last_name: Ren
    name: Zhaochun Ren
    orcid: https://orcid.org/0000-0002-9076-6565
    username: ~Zhaochun_Ren1
  - dblp_id: https://dblp.org/pid/91/4572
    emails: '****@acm.org'
    first_name: Dawei
    google_scholar_id: https://scholar.google.com/citations?user=GuQ9bpAAAAAJ&hl=zh-CN
    institution: Baidu
    last_name: Yin
    name: Dawei Yin
    username: ~Dawei_Yin1
  decision: toMainConference
  end_page: 7685
  file: 811.pdf
  id: 811
  num_pages: 13
  openreview_id: bzD6mFbglz
  pdf_file: 52aadf77c7f5ae9242ff9287b281bd3cd3a6b55e.pdf
  start_page: 7673
  title: 'Knowing What LLMs DO NOT Know: A Simple Yet Effective Self-Detection Method'
- abstract: "Are Large Language Models (LLMs) temporally grounded? Since LLMs cannot\
    \ perceive and interact with the environment, it is impossible to answer this\
    \ question directly. Instead, we provide LLMs with textual narratives and probe\
    \ them with respect to their common-sense knowledge of the structure and duration\
    \ of events, their ability to order events along a timeline, and self-consistency\
    \ within their temporal model (e.g., temporal relations such as after and before\
    \ are mutually exclusive for any pair of events). \nWe evaluate state-of-the-art\
    \ LLMs (such as LLaMA 2 and GPT-4) on three tasks reflecting these abilities.\
    \ Generally, we find that LLMs lag significantly behind both human performance\
    \ as well as small-scale, specialised LMs. In-context learning, instruction tuning,\
    \ and chain-of-thought prompting reduce this gap only to a limited degree. Crucially,\
    \ LLMs struggle the most with self-consistency, displaying incoherent behaviour\
    \ in at least 27.23\\% of their predictions. Contrary to expectations, we also\
    \ find that scaling the model size does not guarantee positive gains in performance.\
    \ To explain these results, we study the sources from which LLMs may gather temporal\
    \ information: we find that sentence ordering in unlabelled texts, available during\
    \ pre-training, is only weakly correlated with event ordering. Moreover, public\
    \ instruction tuning mixtures contain few temporal tasks. Hence, we conclude that\
    \ current LLMs lack a consistent temporal model of textual narratives."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multimodality and Language Grounding to Vision, Robotics and Beyond
  authors:
  - emails: '****@sms.ed.ac.uk'
    first_name: Yifu
    google_scholar_id: https://scholar.google.com/citations?user=OA6GaMwAAAAJ&hl=en
    homepage: https://yfqiu.netlify.app/
    last_name: Qiu
    name: Yifu QIU
    semantic_scholar_id: https://www.semanticscholar.org/author/Yifu-Qiu/2159539050
    username: ~Yifu_QIU1
  - dblp_id: https://dblp.org/pid/75/6680-5
    emails: '****@ed.ac.uk'
    first_name: Zheng
    google_scholar_id: https://scholar.google.com/citations?user=UO0MJeQAAAAJ&hl=en
    homepage: http://www.inf.ed.ac.uk/people/students/Zheng_Zhao.html
    institution: University of Edinburgh, University of Edinburgh
    last_name: Zhao
    name: Zheng Zhao
    semantic_scholar_id: https://www.semanticscholar.org/author/47122617
    username: ~Zheng_Zhao2
  - dblp_id: https://dblp.org/pers/hd/z/Ziser:Yftah
    emails: '****@gmail.com'
    first_name: Yftah
    google_scholar_id: https://scholar.google.co.il/citations?user=37SMCrsAAAAJ&hl=iw
    homepage: https://yftah89.github.io/
    institution: University of Edinburgh
    last_name: Ziser
    name: Yftah Ziser
    username: ~Yftah_Ziser1
  - dblp_id: https://dblp.org/pid/14/6532
    emails: '****@cam.ac.uk'
    first_name: Anna
    google_scholar_id: https://scholar.google.co.uk/citations?user=SCoVoOYAAAAJ&hl=en
    homepage: https://sites.google.com/site/annakorhonen/
    institution: University of Cambridge
    last_name: Korhonen
    name: Anna Korhonen
    username: ~Anna_Korhonen1
  - dblp_id: https://dblp.org/pid/178/8829
    emails: '****@ed.ac.uk'
    first_name: Edoardo
    google_scholar_id: https://scholar.google.ca/citations?user=tklL2q0AAAAJ
    homepage: https://ducdauge.github.io/
    institution: University of Edinburgh
    last_name: Ponti
    name: Edoardo Ponti
    orcid: https://orcid.org/0000-0002-6308-1050
    semantic_scholar_id: https://www.semanticscholar.org/author/E.-Ponti/3381663
    username: ~Edoardo_Ponti1
  - dblp_id: https://dblp.org/pid/04/5629
    emails: '****@inf.ed.ac.uk'
    first_name: Shay
    homepage: http://homepages.inf.ed.ac.uk/scohen
    institution: University of Edinburgh
    last_name: Cohen
    middle_name: B
    name: Shay B Cohen
    semantic_scholar_id: https://www.semanticscholar.org/author/Shay-B.-Cohen/40146204
    username: ~Shay_B_Cohen6
  decision: toMainConference
  end_page: 7705
  file: 812.pdf
  id: 812
  num_pages: 20
  openreview_id: GXFt8vxFwb
  pdf_file: 5ed44aa87ac0ff475e12db764ec866852500af6b.pdf
  start_page: 7686
  title: Are Large Language Model Temporally Grounded?
- abstract: Babel Briefings is a novel dataset featuring 4.7 million news headlines
    from August 2020 to November 2021, across 30 languages and 54 locations worldwide
    with English translations of all articles included. Designed for natural language
    processing and media studies, it serves as a high-quality dataset for training
    or evaluating language models as well as offering a simple, accessible collection
    of articles, for example, to analyze global news coverage and cultural narratives.
    As a simple demonstration of the analyses facilitated by this dataset, we use
    a basic procedure using a TF-IDF weighted similarity metric to group articles
    into clusters about the same event. We then visualize the \emph{event signatures}
    of the event showing articles of which languages appear over time, revealing intuitive
    features based on the proximity of the event and unexpectedness of the event.
    The dataset is available on [Kaggle](https://www.kaggle.com/datasets/felixludos/babel-briefings)
    and [HuggingFace](https://huggingface.co/datasets/felixludos/babel-briefings)
    with accompanying [GitHub](https://github.com/felixludos/babel-briefings) code.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - emails: '****@tuebingen.mpg.de'
    first_name: Felix
    homepage: https://ei.is.mpg.de/person/fleeb
    institution: Max Planck Institute for Intelligent Systems, Max-Planck Institute
    last_name: Leeb
    name: Felix Leeb
    username: ~Felix_Leeb1
  - dblp_id: https://dblp.org/pid/97/119
    emails: '****@tuebingen.mpg.de'
    first_name: Bernhard
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=DZ-fHPgAAAAJ
    homepage: https://ei.is.tuebingen.mpg.de/people/bs/
    institution: ELLIS Institute and Max Planck Institute for Intelligent Systems,
      Max-Planck Institute
    last_name: "Sch\xF6lkopf"
    name: "Bernhard Sch\xF6lkopf"
    orcid: https://orcid.org/0000-0002-8177-0925
    username: "~Bernhard_Sch\xF6lkopf1"
  decision: toMainConference
  end_page: 7711
  file: 814.pdf
  id: 814
  num_pages: 6
  openreview_id: e8eSnXEOEb
  pdf_file: 982b8dabdf241e3609524e200b6363ceba2c44b9.pdf
  start_page: 7706
  title: A diverse Multilingual News Headlines Dataset from around the World
- abstract: "Text image machine translation (TIMT) is a task that translates source\
    \ texts embedded in the image to target translations. \nThe existing TIMT task\
    \ mainly focuses on text-line-level images. \nIn this paper, we extend the current\
    \ TIMT task and propose a novel task, **D**ocument **I**mage **M**achine **T**ranslation\
    \ to **Markdown** (**DIMT2Markdown**), which aims to translate a source document\
    \ image with long context and complex layout structure to markdown-formatted target\
    \ translation.\nWe also introduce a novel framework, **D**ocument **I**mage **M**achine\
    \ **T**ranslation with **D**ynamic multi-pre-trained models **A**ssembling (**DIMTDA**).\n\
    A dynamic model assembler is used to integrate multiple pre-trained models to\
    \ enhance the model\u2019s understanding of layout and translation capabilities.\n\
    Moreover, we build a novel large-scale **Do**cument image machine **T**ranslation\
    \ dataset of **A**rXiv articles in markdown format (**DoTA**), containing 126K\
    \ image-translation pairs.\nExtensive experiments demonstrate the feasibility\
    \ of end-to-end translation of rich-text document images and the effectiveness\
    \ of DIMTDA."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Translation
  authors:
  - emails: '****@ia.ac.cn'
    first_name: Yupu
    google_scholar_id: https://scholar.google.com/citations?user=rC1XVOkAAAAJ
    homepage: https://liangyupu.github.io/
    last_name: Liang
    name: Yupu Liang
    semantic_scholar_id: https://www.semanticscholar.org/author/Yupu-Liang/2183573304
    username: ~Yupu_Liang1
  - dblp_id: https://dblp.org/pid/133/5803
    emails: '****@nlpr.ia.ac.cn'
    first_name: Yaping
    google_scholar_id: https://scholar.google.com.hk/citations?user=bAN6Lj0AAAAJ&hl=zh-CN
    institution: Institute of automation, Chinese academy of science, Chinese Academy
      of Sciences
    last_name: Zhang
    name: Yaping Zhang
    orcid: https://orcid.org/0000-0001-6892-905X
    semantic_scholar_id: https://www.semanticscholar.org/author/Yaping-Zhang/2108115969
    username: ~Yaping_Zhang1
  - dblp_id: https://dblp.org/pid/42/10808
    emails: '****@nlpr.ia.ac.cn'
    first_name: Cong
    google_scholar_id: https://scholar.google.com/citations?user=0yi8K8YAAAAJ&hl=zh-CN
    homepage: https://ericongma.github.io
    institution: Institute of automation, Chinese academy of science
    last_name: MA
    name: Cong MA
    orcid: https://orcid.org/0000-0002-9787-6273
    semantic_scholar_id: https://www.semanticscholar.org/author/Cong-Ma/143731040
    username: ~Cong_MA3
  - emails: '****@ia.ac.cn'
    first_name: Zhiyang
    homepage: https://github.com/zhangzhiyang-2020
    last_name: Zhang
    name: Zhiyang Zhang
    username: ~Zhiyang_Zhang1
  - emails: '****@ia.ac.cn'
    first_name: Yang
    google_scholar_id: https://scholar.google.com.hk/citations?hl=zh-CN&pli=1&user=09YqQNsAAAAJ
    homepage: https://yzhaoiacas.netlify.app/
    institution: Institute of automation, Chinese academy of science, Chinese Academy
      of Sciences
    last_name: Zhao
    name: Yang Zhao
    username: ~Yang_Zhao26
  - dblp_id: https://dblp.org/pid/121/7268.html
    emails: '****@nlpr.ia.ac.cn'
    first_name: Lu
    google_scholar_id: https://scholar.google.com/citations?hl=zh-CN&user=JlmSV-cAAAAJ
    institution: Institute of automation, Chinese academy of science, Chinese Academy
      of Sciences
    last_name: Xiang
    name: Lu Xiang
    username: ~Lu_Xiang1
  - dblp_id: https://dblp.org/pid/38/6093
    emails: '****@nlpr.ia.ac.cn'
    first_name: Chengqing
    google_scholar_id: https://scholar.google.com/citations?user=l8lvKOQAAAAJ&hl=zh-CN
    homepage: http://www.nlpr.ia.ac.cn/cip/english/zong.htm
    institution: Institute of automation, Chinese academy of science, Chinese Academy
      of Sciences
    last_name: Zong
    name: Chengqing Zong
    username: ~Chengqing_Zong1
  - dblp_id: https://dblp.org/pid/36/2728-1.html
    emails: '****@nlpr.ia.ac.cn'
    first_name: Yu
    google_scholar_id: https://scholar.google.com/citations?user=DDpBW7wAAAAJ&hl=zh-CN
    institution: Institute of Automation, Chinese Academy of Sciences
    last_name: Zhou
    name: Yu Zhou
    semantic_scholar_id: https://www.semanticscholar.org/author/2110631853
    username: ~Yu_Zhou8
  decision: toMainConference
  end_page: 7723
  file: 816.pdf
  id: 816
  num_pages: 12
  openreview_id: XH2TgKlXWv
  pdf_file: af41b920eead7612083b6c130a34b125ba98fa50.pdf
  start_page: 7712
  title: Document Image Machine Translation with Dynamic Multi-pre-trained Models
    Assembling
- abstract: 'Continuous-output neural machine translation (CoNMT) replaces the discrete
    next-word prediction problem with an embedding prediction.

    The semantic structure of the target embedding space (*i.e.*, closeness of related
    words) is intuitively believed to be crucial. We challenge this assumption and
    show that completely random output embeddings can outperform laboriously pre-trained
    ones, especially on larger datasets. Further investigation shows this surprising
    effect is strongest for rare words, due to the geometry of their embeddings. We
    shed further light on this finding by designing a mixed strategy that combines
    random and pre-trained embeddings, and that performs best overall.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Translation
  authors:
  - dblp_id: https://dblp.org/pid/290/1940
    emails: '****@uva.nl'
    first_name: Evgeniia
    homepage: https://evgeniia.tokarch.uk
    institution: University of Amsterdam
    last_name: Tokarchuk
    name: Evgeniia Tokarchuk
    semantic_scholar_id: https://www.semanticscholar.org/author/E.-Tokarchuk/2035572365
    username: ~Evgeniia_Tokarchuk1
  - dblp_id: https://dblp.org/pid/40/10489
    emails: '****@uva.nl'
    first_name: Vlad
    google_scholar_id: https://scholar.google.com/citations?user=7_3UAgQAAAAJ
    homepage: https://vene.ro
    institution: University of Amsterdam
    last_name: Niculae
    name: Vlad Niculae
    semantic_scholar_id: https://www.semanticscholar.org/author/Vlad-Niculae/2114966
    username: ~Vlad_Niculae2
  decision: toMainConference
  end_page: 7733
  file: 819.pdf
  id: 819
  num_pages: 10
  openreview_id: eqRqcjHMzV
  pdf_file: 6b1d992d85c31283480b83542b45bdb5257b8491.pdf
  start_page: 7724
  title: The Unreasonable Effectiveness of Random Target Embeddings for Continuous-Output
    Neural Machine Translation
- abstract: Encoder-decoder foundation models have displayed state-of-the-art performance
    on a range of autoregressive sequence tasks. This paper proposes a simple and
    lightweight modification to such systems to control the behaviour according to
    a specific attribute of interest. This paper proposes a novel inference-efficient
    approach to modifying the behaviour of an encoder-decoder system according to
    a specific attribute of interest. Specifically, we show that a small proxy network
    can be used to find a sample-by-sample perturbation of the encoder output of a
    frozen foundation model to trigger the decoder to generate improved decodings.
    This work explores a specific realization of this framework focused on improving
    the COMET performance of Flan-T5 on Machine Translation and the WER of Whisper
    foundation models on Speech Recognition. Results display consistent improvements
    in performance evaluated through COMET and WER respectively. Furthermore, experiments
    also show that the proxies are robust to the exact nature of the data used to
    train them and can extend to other domains.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Efficient/Low-Resource Methods for NLP
  authors:
  - dblp_id: https://dblp.org/pid/254/3044
    emails: '****@gmail.com'
    first_name: Yassir
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=TJQUlhwAAAAJ
    institution: University of Cambridge
    last_name: Fathullah
    name: Yassir Fathullah
    username: ~Yassir_Fathullah1
  - dblp_id: https://dblp.org/pid/74/4419.html
    emails: '****@eng.cam.ac.uk'
    first_name: Mark
    google_scholar_id: https://scholar.google.co.uk/citations?hl=en&user=RSFlmjIAAAAJ&view_op=list_works&sortby=pubdate
    homepage: http://mi.eng.cam.ac.uk/~mjfg/index.html
    institution: University of Cambridge
    last_name: Gales
    name: Mark Gales
    semantic_scholar_id: https://www.semanticscholar.org/author/M.-Gales/1740397
    username: ~Mark_Gales1
  decision: toMainConference
  end_page: 7742
  file: 820.pdf
  id: 820
  num_pages: 9
  openreview_id: FucTPgHoMS
  pdf_file: 7b019ee5011565ff2c186645509681618c62d354.pdf
  start_page: 7734
  title: Efficient Sample-Specific Encoder Perturbations
- abstract: "Generating factual responses is a crucial requirement for dialogue systems.\
    \ To promote\nmore factual responses, a common strategy\nis to ground their responses\
    \ in relevant documents that inform response generation. However, common dialogue\
    \ models still often hallucinate information that was not contained\nin these\
    \ documents and is therefore unfaithful. In this work, we propose to alleviate\
    \ such\nhallucinations by \u2018subtracting\u2019 the parameters\nof a model trained\
    \ to hallucinate from a dialogue response generation model in order to\n\u2018\
    negate\u2019 the contribution of such hallucinated\nexamples from it. Extensive\
    \ automatic and human evaluation shows favourable results when\ncompared to state-of-the-art\
    \ methods that combine the distributions of multiple models, such\nas DExperts\
    \ (Liu et al., 2021), and others that\nchange the training procedure, such as\
    \ Quark\n(Lu et al., 2022a). Finally, we show how we\ncan not only reduce hallucinations\
    \ but also discourage extractive responses, which are often\na consequence of\
    \ reducing hallucinations by\nencouraging copy-pasting of document spans.\nWe\
    \ publicly release our code for reproducibility\nand facilitating further research."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Dialogue and Interactive Systems
  authors:
  - dblp_id: https://dblp.org/pid/285/5587.html
    emails: '****@rwth-aachen.de'
    first_name: Nico
    google_scholar_id: https://scholar.google.com/citations?user=n6wJfqUAAAAJ
    homepage: https://ndaheim.github.io
    institution: "Technische Universit\xE4t Darmstadt"
    last_name: Daheim
    name: Nico Daheim
    username: ~Nico_Daheim1
  - dblp_id: https://dblp.org/pid/220/2036
    emails: '****@gmail.com'
    first_name: Nouha
    google_scholar_id: https://scholar.google.ca/citations?user=IqcCDXkAAAAJ&hl=en&oi=ao
    homepage: http://nouhadziri.com/
    last_name: Dziri
    name: Nouha Dziri
    semantic_scholar_id: https://www.semanticscholar.org/author/Nouha-Dziri/46217681
    username: ~Nouha_Dziri2
  - dblp_id: https://dblp.org/pid/86/10440.html
    emails: '****@inf.ethz.ch'
    first_name: Mrinmaya
    google_scholar_id: https://scholar.google.com/citations?user=Tpp9ZjoAAAAJ&hl=en
    homepage: https://sites.google.com/site/mrinsachan/
    institution: Swiss Federal Institute of Technology
    last_name: Sachan
    name: Mrinmaya Sachan
    username: ~Mrinmaya_Sachan3
  - dblp_id: https://dblp.org/pid/85/6201
    emails: '****@tu-darmstadt.de'
    first_name: Iryna
    google_scholar_id: https://scholar.google.com.tw/citations?user=t3A39e8AAAAJ
    homepage: https://www.informatik.tu-darmstadt.de/ukp/ukp_home/staff_ukp/prof_dr_iryna_gurevych/index.en.jsp
    institution: Mohamed bin Zayed University of Artificial Intelligence and Technical
      University of Darmstadt
    last_name: Gurevych
    name: Iryna Gurevych
    username: ~Iryna_Gurevych1
  - dblp_id: https://dblp.org/pid/178/8829
    emails: '****@ed.ac.uk'
    first_name: Edoardo
    google_scholar_id: https://scholar.google.ca/citations?user=tklL2q0AAAAJ
    homepage: https://ducdauge.github.io/
    institution: University of Edinburgh
    last_name: Ponti
    name: Edoardo Ponti
    orcid: https://orcid.org/0000-0002-6308-1050
    semantic_scholar_id: https://www.semanticscholar.org/author/E.-Ponti/3381663
    username: ~Edoardo_Ponti1
  decision: toMainConference
  end_page: 7759
  file: 823.pdf
  id: 823
  num_pages: 17
  openreview_id: JfGCAEEM7K
  pdf_file: 96727d3bece2d36615076a538092b1c816f44c9f.pdf
  start_page: 7743
  title: Elastic Weight Removal for Faithful and Abstractive Dialogue Generation
- abstract: "Large language models (LLMs) have revolutionized numerous domains with\
    \ their impressive performance but still face their challenges. \nA predominant\
    \ issue is the propensity for these models to generate non-existent facts, a concern\
    \ termed hallucination. Our research is motivated by the observation that previous\
    \ instruction tuning methods force the model to complete a sentence no matter\
    \ whether the model knows the knowledge or not. When the question is out of the\
    \ parametric knowledge, it will try to make up something and fail to indicate\
    \ when it lacks knowledge. In this paper, we present a new approach called Refusal-Aware\
    \ Instruction Tuning (R-Tuning). This approach is formalized by first identifying\
    \ the disparity in knowledge encompassed by pre-trained parameters compared to\
    \ that of instruction tuning data. Then, we construct the refusal-aware data based\
    \ on the knowledge intersection, to tune LLMs to refrain from responding to questions\
    \ beyond its parametric knowledge. Experimental results demonstrate R-Tuning effectively\
    \ improves a model's ability to answer known questions and refrain from answering\
    \ unknown questions. Furthermore, when tested on out-of-domain datasets, the refusal\
    \ ability was found to be a meta-skill that could be generalized to other tasks.\
    \ Further analysis surprisingly finds that learning the uncertainty results in\
    \ better calibration and an improved ability to estimate the uncertainty than\
    \ uncertainty-based testing. Our code is available at https://github.com/shizhediao/R-Tuning"
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - emails: '****@connect.ust.hk'
    first_name: Hanning
    google_scholar_id: https://scholar.google.com/citations?user=T-JDNUoAAAAJ&hl=en&oi=sra
    homepage: https://hanningzhang.github.io
    last_name: Zhang
    name: Hanning Zhang
    username: ~Hanning_Zhang1
  - dblp_id: https://dblp.org/pid/221/3896
    emails: '****@connect.ust.hk'
    first_name: Shizhe
    google_scholar_id: https://scholar.google.com/citations?user=NDFQrLQAAAAJ&hl=en
    homepage: https://shizhediao.github.io/
    institution: Hong Kong University of Science and Technology
    last_name: Diao
    name: Shizhe Diao
    username: ~Shizhe_Diao2
  - dblp_id: https://dblp.org/pid/64/1938
    emails: '****@connect.ust.hk'
    first_name: Yong
    google_scholar_id: https://scholar.google.com/citations?user=M4g0ZvMAAAAJ&hl=en
    homepage: https://linyongver.github.io/yonglin.github.io/
    last_name: Lin
    name: Yong Lin
    username: ~Yong_Lin2
  - dblp_id: https://dblp.org/pid/223/2782
    emails: '****@illinois.edu'
    first_name: Yi
    google_scholar_id: https://scholar.google.com/citations?user=eUae2K0AAAAJ&hl=en&oi=sra
    homepage: https://yrf1.github.io
    last_name: Fung
    name: Yi Fung
    semantic_scholar_id: https://www.semanticscholar.org/author/Y.-Fung/51135899
    username: ~Yi_Fung1
  - dblp_id: https://dblp.org/pid/234/4406
    emails: '****@gmail.com'
    first_name: Qing
    homepage: https://www.lianqing11.github.io
    institution: The Hong Kong University of Science and Technology
    last_name: Lian
    name: Qing LIAN
    username: ~Qing_LIAN3
  - dblp_id: https://dblp.org/pid/264/9892
    emails: '****@illinois.edu'
    first_name: Xingyao
    google_scholar_id: https://scholar.google.com/citations?user=F7qq3YcAAAAJ&hl=en
    homepage: https://xingyaoww.github.io/
    institution: Department of Computer Science, University of Illinois Urbana-Champaign
    last_name: Wang
    name: Xingyao Wang
    orcid: https://orcid.org/0000-0002-3483-8624
    semantic_scholar_id: https://www.semanticscholar.org/author/Xingyao-Wang/2144803999
    username: ~Xingyao_Wang1
  - dblp_id: https://dblp.org/pid/05/10083
    emails: '****@gmail.com'
    first_name: Yangyi
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=5e9tBtQAAAAJ&view_op=list_works&gmla=AJsN-F6ieV5-6P_WzCdbvRYvxWSI33-VELtb0CU6B5dRbXHRE5PhOLn2bmG_5XkhAUdOEgKxiZd864yv2IVcuooJbWq6x7N7lL1nm_vxeK_QPHLncFhdjSA
    homepage: https://yangyi-chen.github.io/
    institution: School of Computer Science, University of Illinois at Urbana-Champaign
    last_name: Chen
    name: Yangyi Chen
    semantic_scholar_id: https://www.semanticscholar.org/author/Yangyi-Chen/123331686
    username: ~Yangyi_Chen1
  - emails: '****@illinois.edu'
    first_name: Heng
    google_scholar_id: https://scholar.google.com/citations?user=z7GCqT4AAAAJ&hl=en
    homepage: http://blender.cs.illinois.edu/hengji.html
    institution: University of Illinois, Urbana-Champaign
    last_name: Ji
    name: Heng Ji
    username: ~Heng_Ji3
  - dblp_id: https://dblp.org/pid/07/4227-1
    emails: '****@tongzhang-ml.org'
    first_name: Tong
    google_scholar_id: https://scholar.google.com/citations?user=LurWtuYAAAAJ&hl=en&oi=ao
    homepage: http://tongzhang-ml.org
    institution: UIUC
    last_name: Zhang
    name: Tong Zhang
    orcid: https://orcid.org/0000-0002-5511-2558
    username: ~Tong_Zhang2
  decision: toMainConference
  end_page: 7786
  file: 825.pdf
  id: 825
  num_pages: 27
  openreview_id: 7n8b5e5ApN
  pdf_file: 4fe77cdae1c2e1419dc68900ea355b51cb7752cb.pdf
  start_page: 7760
  title: 'R-Tuning: Instructing Large Language Models to Say `I Don''t Know'''
- abstract: Ensembling different large language models (LLMs) to unleash their complementary
    potential and harness their individual strengths is highly valuable. Nevertheless,
    vocabulary discrepancies among various LLMs have constrained previous studies
    to either selecting or blending completely generated outputs. This limitation
    hinders the dynamic correction and enhancement of outputs during the generation
    process, resulting in a limited capacity for effective ensemble. To address this
    issue, we propose a novel method to $\textbf{E}$nsemble LLMs via $\textbf{V}$ocabulary
    $\textbf{A}$lignment (EVA). EVA bridges the lexical gap among various LLMs, enabling
    meticulous ensemble at each generation step. Specifically, we first learn mappings
    between the vocabularies of different LLMs with the assistance of overlapping
    tokens. Subsequently, these mappings are employed to project output distributions
    of LLMs into a unified space, facilitating a fine-grained ensemble. Finally, we
    design a filtering strategy to exclude models that generate unfaithful tokens.
    Experimental results on commonsense reasoning, arithmetic reasoning, machine translation,
    and data-to-text generation tasks demonstrate the superiority of our approach
    compared with individual LLMs and previous ensemble methods conducted on complete
    outputs. Further analyses confirm that our approach can leverage knowledge from
    different language models and yield consistent improvement.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - emails: '****@ia.ac.cn'
    first_name: Yangyifan
    homepage: https://www.researchgate.net/profile/Yangyifan_Xu
    institution: University of the Chinese Academy of Sciences
    last_name: Xu
    name: Yangyifan Xu
    username: ~Yangyifan_Xu1
  - dblp_id: https://dblp.org/pid/249/9047
    emails: '****@ia.ac.cn'
    first_name: Jinliang
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=1GwID8EAAAAJ
    homepage: https://jinlianglu96.github.io/about/
    institution: Institute of automation, Chinese Academy of Sciences
    last_name: Lu
    name: Jinliang Lu
    orcid: https://orcid.org/0000-0002-5395-2385
    semantic_scholar_id: https://www.semanticscholar.org/author/Jinliang-Lu/81758928
    username: ~Jinliang_Lu1
  - dblp_id: https://dblp.org/pid/71/6950-1.html
    emails: '****@nlpr.ia.ac.cn'
    first_name: Jiajun
    google_scholar_id: https://scholar.google.com/citations?user=93zngeYAAAAJ&hl=en
    homepage: http://www.nlpr.ia.ac.cn/cip/jjzhang.htm
    institution: Institute of automation, Chinese academy of science, Chinese Academy
      of Sciences
    last_name: Zhang
    name: Jiajun Zhang
    username: ~Jiajun_Zhang1
  decision: toMainConference
  end_page: 7799
  file: 826.pdf
  id: 826
  num_pages: 13
  openreview_id: IOG33p5r6o
  pdf_file: ff5ff9cb31ee79f44c4beae28f1a63a60ce9eb71.pdf
  start_page: 7787
  title: Bridging the Gap between Different Vocabularies for LLM Ensemble
- abstract: Parameter-efficient finetuning (PEFT) is a key technique for adapting
    large language models (LLMs) to downstream tasks. In this paper, we study leveraging
    knowledge graph embeddings to improve the effectiveness of PEFT. We propose a
    knowledgeable adaptation method called KnowLA. It inserts an adaptation layer
    into an LLM to integrate the embeddings of entities appearing in the input text.
    The adaptation layer is trained in combination with LoRA on instruction data.
    Experiments on six benchmarks with two popular LLMs and three knowledge graphs
    demonstrate the effectiveness and robustness of KnowLA. We show that KnowLA can
    help activate the relevant parameterized knowledge in an LLM to answer a question
    without changing its parameters or input prompts.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Efficient/Low-Resource Methods for NLP
  authors:
  - emails: '****@gmail.com'
    first_name: Xindi
    google_scholar_id: https://scholar.google.co.uk/scholar?as_q=&num=10&btnG=Search+Scholar&as_epq=&as_oq=&as_eq=&as_occt=any&as_sauthors=%22Xindi%20Luo%22&as_publication=&as_ylo=&as_yhi=&as_allsubj=all&hl=en
    last_name: Luo
    name: Xindi Luo
    username: ~Xindi_Luo2
  - dblp_id: https://dblp.org/pid/186/9718
    emails: '****@gmail.com'
    first_name: Zequn
    google_scholar_id: https://scholar.google.com/citations?user=ph8SU3EAAAAJ
    homepage: https://sunzequn.github.io
    last_name: Sun
    name: Zequn Sun
    username: ~Zequn_Sun1
  - emails: '****@tencent.com'
    first_name: Jing
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=6MHkR_kAAAAJ
    institution: Tencent AI Lab
    last_name: Zhao
    name: Jing Zhao
    orcid: https://orcid.org/0000-0002-3172-2837
    username: ~Jing_Zhao11
  - emails: '****@ruc.edu.cn'
    first_name: Zhe
    google_scholar_id: https://scholar.google.com.hk/citations?hl=zh-CN&user=Xh7oU4kAAAAJ
    last_name: Zhao
    name: Zhe Zhao
    username: ~Zhe_Zhao1
  - dblp_id: https://dblp.uni-trier.de/pid/52/173-7
    emails: '****@nju.edu.cn'
    first_name: Wei
    google_scholar_id: https://scholar.google.com/citations?user=iWs168sAAAAJ&hl=en
    homepage: http://ws.nju.edu.cn/~whu
    institution: Nanjing University
    last_name: Hu
    name: Wei Hu
    orcid: https://orcid.org/0000-0003-3635-6335
    username: ~Wei_Hu7
  decision: toMainConference
  end_page: 7813
  file: 828.pdf
  id: 828
  num_pages: 14
  openreview_id: Hrwikvl49k
  pdf_file: b0e6003e6af2118b4d30fc25cd9360615eefaecc.pdf
  start_page: 7800
  title: 'KnowLA: Enhancing Parameter-efficient Finetuning with Knowledgeable Adaptation'
- abstract: Extremely weakly-supervised text classification aims to classify texts
    without any labeled data, but only relying on class names as supervision. Existing
    works include prompt-based and seed-based methods. Prompt-based methods prompt
    language model with instructions, while seed-based methods generate pseudo-labels
    with word matching. Both of them have significant flaws, including  zero-shot
    instability and context-dependent ambiguities. This paper introduces SetSync,
    which follows a new paradigm, i.e. wordset-based, which can avoid the above problems.
    In SetSync, a class is represented with wordsets, and pseudo-labels are generated
    with wordsets matching. To facilitate this, we propose to use information bottleneck
    to identify class-relevant wordsets.  Moreover, we regard the classifier training  as
    a hybrid learning of semi-supervised and noisy-labels, and propose a new training
    strategy, termed sync-denoising.  Extensive experiments on 11 datasets show that
    SetSync outperforms all existing prompt and seed methods, exceeding SOTA by an
    impressive average of 8 points.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Learning for NLP
  authors:
  - emails: '****@gmail.com'
    first_name: Lysa
    homepage: https://lysa666.github.io/
    last_name: Xiao
    name: Lysa Xiao
    username: ~Lysa_Xiao1
  decision: toMainConference
  end_page: 7826
  file: 830.pdf
  id: 830
  num_pages: 13
  openreview_id: QM75JUmh3c
  pdf_file: a8282b9c24962c440066ec93191ddd5e8da82f07.pdf
  start_page: 7814
  title: Extremely Weakly-supervised Text Classification with Wordsets Mining and
    Sync-Denoising
- abstract: In the evolving landscape of Neural Machine Translation (NMT), the pretrain-then-finetune
    paradigm has yielded impressive results. However, the persistent challenge of
    Catastrophic Forgetting (CF) remains a hurdle. While previous work has introduced
    Continual Learning (CL) methods to address CF, these approaches grapple with the
    delicate balance between avoiding forgetting and maintaining system extensibility.
    To address this, we propose a CL method, named $\textbf{F-MALLOC}$ ($\textbf{F}$eed-forward
    $\textbf{M}$emory $\textbf{ALLOC}$ation). F-MALLOC is inspired by recent insights
    highlighting that feed-forward layers emulate neural memories and encapsulate
    crucial translation knowledge. It decomposes feed-forward layers into discrete
    memory cells and allocates these memories to different tasks. By learning to allocate
    and safeguard these memories, our method effectively alleviates CF while ensuring
    robust extendability. Besides, we propose a comprehensive assessment protocol
    for multi-stage CL of NMT systems. Experiments conducted following this new protocol
    showcase the superior performance of F-MALLOC, evidenced by higher BLEU scores
    and almost zero forgetting.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Translation
  authors:
  - emails: '****@ia.ac.cn'
    first_name: Junhong
    google_scholar_id: https://scholar.google.com/citations?view_op=list_works&hl=zh-CN&user=Ci4l4yQAAAAJ
    institution: Institute of automation, Chinese academy of science, Chinese Academy
      of Sciences
    last_name: Wu
    name: Junhong Wu
    username: ~Junhong_Wu1
  - dblp_id: https://dblp.org/pid/69/10440-7
    emails: '****@nlpr.ia.ac.cn'
    first_name: Yuchen
    google_scholar_id: https://scholar.google.com/citations?user=jKKnvEcAAAAJ&hl=zh-CN
    last_name: Liu
    name: Yuchen Liu
    username: ~Yuchen_Liu15
  - dblp_id: https://dblp.org/pid/38/6093
    emails: '****@nlpr.ia.ac.cn'
    first_name: Chengqing
    google_scholar_id: https://scholar.google.com/citations?user=l8lvKOQAAAAJ&hl=zh-CN
    homepage: http://www.nlpr.ia.ac.cn/cip/english/zong.htm
    institution: Institute of automation, Chinese academy of science, Chinese Academy
      of Sciences
    last_name: Zong
    name: Chengqing Zong
    username: ~Chengqing_Zong1
  decision: toMainConference
  end_page: 7839
  file: 832.pdf
  id: 832
  num_pages: 13
  openreview_id: V6XsbfnMnZ
  pdf_file: 7a36b5d938728967d551a22b46750022592f7947.pdf
  start_page: 7827
  title: 'F-MALLOC: Feed-forward Memory Allocation for Continual Learning in Neural
    Machine Translation'
- abstract: "Many diagnostic errors occur because clinicians cannot easily access\
    \ relevant information in patient Electronic Health Records (EHRs). In this work\
    \ we propose a method to use LLMs to identify pieces of evidence in patient EHR\
    \ data that indicate increased or decreased risk of specific diagnoses; our ultimate\
    \ aim is to increase access to evidence and reduce diagnostic errors. In particular,\
    \ we propose a Neural Additive Model to make predictions backed by evidence with\
    \ individualized risk estimates at time-points where clinicians are still uncertain,\
    \ aiming to specifically mitigate delays in diagnosis and errors stemming from\
    \ an incomplete differential. To train such a model, it is necessary to infer\
    \ temporally fine-grained retrospective labels of eventual \u201Ctrue\u201D diagnoses.\
    \ We do so with LLMs, to ensure that the input text is from before a confident\
    \ diagnosis can be made. We use an LLM to retrieve an initial pool of evidence,\
    \ but then refine this set of evidence according to correlations learned by the\
    \ model. We conduct an in-depth evaluation of the usefulness of our approach by\
    \ simulating how it might be used by a clinician to decide between a pre-defined\
    \ list of differential diagnoses."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - dblp_id: https://dblp.org/pid/262/6563
    emails: '****@northeastern.edu'
    first_name: Denis
    google_scholar_id: https://scholar.google.com/citations?user=Fc1UmSEAAAAJ&hl=en
    homepage: https://www.khoury.northeastern.edu/people/denis-jered-mcinerney/
    last_name: McInerney
    middle_name: Jered
    name: Denis Jered McInerney
    orcid: https://orcid.org/0000-0001-5828-2379
    semantic_scholar_id: https://www.semanticscholar.org/author/1621628526
    username: ~Denis_Jered_McInerney1
  - emails: '****@bwh.harvard.edu'
    first_name: William
    homepage: https://www.brighamandwomens.org/medicine/general-internal-medicine-and-primary-care/brigham-and-womens-physician-group/overview
    last_name: Dickinson
    name: William Dickinson
    username: ~William_Dickinson2
  - emails: '****@mgb.org'
    first_name: Lucy
    homepage: https://www.brighamandwomens.org/medicine/general-internal-medicine-and-primary-care/brigham-and-womens-physician-group/overview
    institution: "Brigham and Women\u2019s Hospital "
    last_name: Flynn
    middle_name: C.
    name: Lucy C. Flynn
    username: ~Lucy_C._Flynn1
  - emails: '****@bwh.harvard.edu'
    first_name: Andrea
    homepage: https://www.brighamandwomens.org/medicine/general-internal-medicine-and-primary-care/brigham-and-womens-physician-group/overview
    institution: Brigham and Women's Hospital, Harvard University
    last_name: Young
    middle_name: C
    name: Andrea C Young
    username: ~Andrea_C_Young1
  - emails: '****@bwh.harvard.edu'
    first_name: Geoffrey
    institution: Harvard Medical School
    last_name: Young
    name: Geoffrey Young
    orcid: https://orcid.org/0000-0001-8213-865x
    username: ~Geoffrey_Young1
  - dblp_id: https://dblp.org/pid/137/3263
    emails: '****@northeastern.edu'
    first_name: Jan-Willem
    google_scholar_id: https://scholar.google.com/citations?user=CX9Lu38AAAAJ
    homepage: https://jwvdm.github.io/
    institution: Northeastern University, Northeastern University, University of Amsterdam
      and Northeastern University
    last_name: Van De Meent
    name: Jan-Willem van de Meent
    orcid: https://orcid.org/0000-0001-9465-5398
    username: ~Jan-Willem_van_de_Meent1
  - dblp_id: https://dblp.org/pid/00/8247
    emails: '****@northeastern.edu'
    first_name: Byron
    google_scholar_id: https://scholar.google.com/citations?user=KTzRHmwAAAAJ&hl=en
    homepage: http://www.byronwallace.com/
    institution: Northeastern University, Brown University and Northeastern University
    last_name: Wallace
    middle_name: C
    name: Byron C Wallace
    username: ~Byron_C_Wallace1
  decision: toMainConference
  end_page: 7857
  file: 833.pdf
  id: 833
  num_pages: 18
  openreview_id: aY8CWqjoZJ
  pdf_file: f9bb72feca21b8da9d3bfb0e739e8dfa44c309ef.pdf
  start_page: 7840
  title: Towards Reducing Diagnostic Errors with Interpretable Risk Prediction
- abstract: Social media, originally meant for peaceful communication, now faces issues
    with hate speech. Detecting hate speech from social media in Indian languages
    with linguistic diversity and cultural nuances presents a complex and challenging
    task. Furthermore, traditional methods involve sharing of users' sensitive data
    with a server for model training making it undesirable and involving potential
    risk to their privacy remained under-studied. In this paper, we combined various
    low-resource language datasets and propose MultiFED, a federated approach that
    performs effectively to detect hate speech. MultiFED utilizes continuous adaptation
    and fine-tuning to aid generalization using subsets of multilingual data overcoming
    the limitations of data scarcity. Extensive experiments are conducted on 13 Indic
    datasets across five different pre-trained models. The results show that MultiFED
    outperforms the state-of-the-art baselines by 8\% (approx.) in terms of Accuracy
    and by 12\% (approx.) in terms of F-Score.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multilinguality and Language Diversity
  authors:
  - emails: '****@cs.iitr.ac.in'
    first_name: Akshay
    last_name: Singh
    name: Akshay Singh
    orcid: https://orcid.org/0009-0000-4629-3134
    username: ~Akshay_Singh1
  - emails: '****@ieee.org'
    first_name: Rahul
    google_scholar_id: https://scholar.google.com/citations?user=va-wHH4AAAAJ&hl=en
    homepage: https://www.rahulthakur.info
    last_name: Thakur
    name: Rahul Thakur
    username: ~Rahul_Thakur1
  decision: toMainConference
  end_page: 7868
  file: 836.pdf
  id: 836
  num_pages: 11
  openreview_id: Sfs1ribSiv
  pdf_file: a96e04570c48e82fb40a424f692e9bc12d2e7616.pdf
  start_page: 7858
  title: Generalizable Multilingual Hate Speech Detection on Low Resource Indian Languages
    using Fair Selection in Federated Learning
- abstract: Zero-shot cross-lingual transfer, which implies finetuning of the multilingual
    pretrained language model on input-output pairs in one language and using it to
    make task predictions for inputs in other languages, was widely studied for natural
    language understanding but is understudied for generation. Previous works notice
    a frequent problem of generation in a wrong language and propose approaches to
    address it, usually using mT5 as a backbone model. In this work we compare various
    approaches proposed from the literature in unified settings, also including alternative
    backbone models, namely mBART and NLLB-200. We first underline the importance
    of tuning learning rate used for finetuning, which helps to substantially alleviate
    the problem of generation in the wrong language. Then, we show that with careful
    learning rate tuning, the simple full finetuning of the model acts as a very strong
    baseline and alternative approaches bring only marginal improvements. Finally,
    we find that mBART performs similarly to mT5 of the same size, and NLLB-200 can
    be competitive in some cases. Our final zero-shot models reach the performance
    of the approach based on data translation which is usually considered as an upper
    baseline for zero-shot cross-lingual transfer in generation.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multilinguality and Language Diversity
  authors:
  - dblp_id: https://dblp.org/pid/205/2973
    emails: '****@gmail.com'
    first_name: Nadezhda
    google_scholar_id: https://scholar.google.ru/citations?user=8ovzJjEAAAAJ&hl
    homepage: https://nadiinchi.github.io/
    institution: Naver Labs Europe
    last_name: Chirkova
    name: Nadezhda Chirkova
    semantic_scholar_id: https://www.semanticscholar.org/author/Nadezhda-Chirkova/145727842
    username: ~Nadezhda_Chirkova1
  - dblp_id: https://dblp.org/pid/32/11424.html
    emails: '****@naverlabs.com'
    first_name: Vassilina
    google_scholar_id: https://scholar.google.fr/citations?user=IVJ4wN4AAAAJ&hl=fr
    institution: Naver Labs Europe
    last_name: Nikoulina
    name: Vassilina Nikoulina
    semantic_scholar_id: https://www.semanticscholar.org/author/Vassilina-Nikoulina/2841761
    username: ~Vassilina_Nikoulina1
  decision: toMainConference
  end_page: 7885
  file: 837.pdf
  id: 837
  num_pages: 17
  openreview_id: bU8xRikG0d
  pdf_file: c9cd68377ab8045eba977df76532ea58f91d0fb7.pdf
  start_page: 7869
  title: Key ingredients for effective zero-shot cross-lingual knowledge transfer
    in generative tasks
- abstract: 'To process novel sentences, language models (LMs) must generalize compositionally---combine
    familiar elements in new ways. What aspects of a model''s structure promote compositional
    generalization? Focusing on transformers, we test the hypothesis, motivated by
    theoretical and empirical work, that deeper transformers generalize more compositionally.
    Simply adding layers increases the total number of parameters; to address this
    confound between depth and size, we construct three classes of models which trade
    off depth for width such that the total number of parameters is kept constant
    (41M, 134M and 374M parameters). We pretrain all models as LMs and fine-tune them
    on tasks that test for compositional generalization. We report three main conclusions:
    (1) after fine-tuning, deeper models generalize more compositionally than shallower
    models do, but the benefit of additional layers diminishes rapidly; (2) within
    each family, deeper models show better language modeling performance, but returns
    are similarly diminishing; (3) the benefits of depth for compositional generalization
    cannot be attributed solely to better performance on language modeling. Because
    model latency is approximately linear in the number of layers, these results lead
    us to the recommendation that, with a given total parameter budget, transformers
    can be made shallower than is typical without sacrificing performance.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - emails: '****@gmail.com'
    first_name: Jackson
    homepage: https://jacksonpetty.org
    institution: New York University
    last_name: Petty
    name: Jackson Petty
    username: ~Jackson_Petty1
  - dblp_id: https://dblp.org/pid/183/9326
    emails: '****@gmail.com'
    first_name: Sjoerd
    google_scholar_id: https://scholar.google.com/citations?user=i-AStBYAAAAJ&hl=en
    homepage: http://www.sjoerdvansteenkiste.com/
    institution: Google
    last_name: Steenkiste
    middle_name: Van
    name: Sjoerd van Steenkiste
    username: ~Sjoerd_van_Steenkiste1
  - dblp_id: https://dblp.org/pid/169/6218
    emails: '****@gmail.com'
    first_name: Ishita
    institution: DeepMind
    last_name: Dasgupta
    name: Ishita Dasgupta
    username: ~Ishita_Dasgupta1
  - dblp_id: https://dblp.org/pid/13/3601
    emails: '****@google.com'
    first_name: Fei
    google_scholar_id: https://scholar.google.com/citations?user=HDHOS0QAAAAJ&hl=en
    last_name: Sha
    name: Fei Sha
    username: ~Fei_Sha3
  - dblp_id: https://dblp.org/pid/117/4050
    emails: '****@gmail.com'
    first_name: Dan
    google_scholar_id: https://scholar.google.com/citations?user=tT9mhNMAAAAJ
    homepage: http://www.dhgarrette.com/
    institution: Google Research
    last_name: Garrette
    name: Dan Garrette
    semantic_scholar_id: https://www.semanticscholar.org/author/Dan-Garrette/2758616
    username: ~Dan_Garrette1
  - dblp_id: https://dblp.org/pid/169/3438
    emails: '****@nyu.edu'
    first_name: Tal
    google_scholar_id: https://scholar.google.com/citations?user=5mJDXjoAAAAJ&hl=en
    homepage: http://tallinzen.net
    institution: New York University and Google
    last_name: Linzen
    name: Tal Linzen
    semantic_scholar_id: https://www.semanticscholar.org/author/Tal-Linzen/2467508
    username: ~Tal_Linzen1
  decision: toMainConference
  end_page: 7899
  file: 840.pdf
  id: 840
  num_pages: 14
  openreview_id: YDT2O78O7Q
  pdf_file: 15bf044a2e0fd44fd39f1f6bdeafbcd3d83b0975.pdf
  start_page: 7886
  title: The Impact of Depth on Compositional Generalization in Transformer Language
    Models
- abstract: Questions posed by information-seeking users often contain implicit false
    or potentially harmful assumptions. In a high-risk domain such as maternal and
    infant health, a question-answering system must recognize these pragmatic constraints
    and go beyond simply answering user questions, examining them in context to respond
    helpfully. To achieve this, we study assumptions and implications, or pragmatic
    inferences, made when mothers ask questions about pregnancy and infant care by
    collecting a dataset of 2,727 inferences from 500 questions across three diverse
    sources. We study how health experts naturally address these inferences when writing
    answers, and  illustrate that informing  existing QA pipelines with pragmatic
    inferences produces responses that are more complete, mitigating the propagation
    of harmful beliefs.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Discourse and Pragmatics
  authors:
  - dblp_id: https://dblp.org/pid/277/0678
    emails: '****@umd.edu'
    first_name: Neha
    google_scholar_id: https://scholar.google.com/citations?user=ZQwwr8cAAAAJ&hl=en
    homepage: https://nehasrikn.github.io/
    last_name: Srikanth
    name: Neha Srikanth
    username: ~Neha_Srikanth1
  - dblp_id: https://dblp.org/pid/256/0987
    emails: '****@umd.edu'
    first_name: Rupak
    homepage: https://styx97.github.io
    last_name: Sarkar
    name: Rupak Sarkar
    semantic_scholar_id: https://www.semanticscholar.org/author/Rupak-Sarkar/14627064
    username: ~Rupak_Sarkar1
  - emails: '****@umd.edu'
    first_name: Heran
    homepage: https://sph.umd.edu/research-impact/laboratories-projects-and-programs/big-data-health-equity-bd4he
    institution: University of Maryland, College Park
    last_name: Mane
    name: Heran Mane
    username: ~Heran_Mane1
  - emails: '****@umd.edu'
    first_name: Elizabeth
    last_name: Aparicio
    name: Elizabeth Aparicio
    username: ~Elizabeth_Aparicio1
  - emails: '****@umd.edu'
    first_name: Quynh
    last_name: Nguyen
    name: Quynh Nguyen
    username: ~Quynh_Nguyen3
  - dblp_id: https://dblp.org/pid/136/8740
    emails: '****@umd.edu'
    first_name: Rachel
    google_scholar_id: https://scholar.google.com/citations?user=QKCHaHUAAAAJ&hl=en
    homepage: https://rudinger.github.io/
    institution: University of Maryland, College Park
    last_name: Rudinger
    name: Rachel Rudinger
    semantic_scholar_id: https://www.semanticscholar.org/author/Rachel-Rudinger/2034613
    username: ~Rachel_Rudinger1
  - dblp_id: https://dblp.org/pid/57/5950
    emails: '****@umiacs.umd.edu'
    first_name: Jordan
    google_scholar_id: https://scholar.google.com/citations?user=BT4XTP4AAAAJ
    homepage: http://boydgraber.org
    institution: University of Maryland, College Park
    last_name: Boyd-Graber
    middle_name: Lee
    name: Jordan Lee Boyd-Graber
    orcid: https://orcid.org/0000-0002-7770-4431
    semantic_scholar_id: https://www.semanticscholar.org/author/Jordan-L.-Boyd-Graber/1389036863
    username: ~Jordan_Lee_Boyd-Graber1
  decision: toMainConference
  end_page: 7915
  file: 843.pdf
  id: 843
  num_pages: 16
  openreview_id: nB3BZERUkr
  pdf_file: 435aa3b1d208d4b4d3c230088a22e7a67f7bd2d4.pdf
  start_page: 7900
  title: 'Pregnant Questions: The Importance of Pragmatic Awareness in Maternal Health
    Question Answering'
- abstract: Current legal outcome prediction models - a staple of legal NLP - do not
    explain their reasoning. However, to employ these models in the real world, human
    legal actors need to be able to understand the model's decisions. In the case
    of common law, legal practitioners reason towards the outcome of a case by referring
    to past case law, known as precedent. We contend that precedent is, therefore,
    a natural way of facilitating explainability for legal NLP models. In this paper,
    we contribute a novel method for identifying the precedent employed by legal outcome
    prediction models. Furthermore, by developing a taxonomy of legal precedent, we
    are able to compare human judges and neural models with respect to the different
    types of precedent they rely on. We find that while the models learn to predict
    outcomes reasonably well, their use of precedent is unlike that of human judges.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - dblp_id: https://dblp.org/pid/223/0972
    emails: '****@cam.ac.uk'
    first_name: Josef
    google_scholar_id: https://scholar.google.com/citations?user=vLgjEhQAAAAJ&hl=en
    homepage: https://valvoda.github.io/
    last_name: Valvoda
    name: Josef Valvoda
    semantic_scholar_id: https://www.semanticscholar.org/author/Josef-Valvoda/51130686
    username: ~Josef_Valvoda1
  - dblp_id: https://dblp.org/pid/146/4361.html
    emails: '****@gmail.com'
    first_name: Ryan
    google_scholar_id: https://scholar.google.com/citations?user=DexOqtoAAAAJ&hl=en
    homepage: https://rycolab.io/
    institution: Swiss Federal Institute of Technology
    last_name: Cotterell
    name: Ryan Cotterell
    semantic_scholar_id: https://www.semanticscholar.org/author/Ryan-Cotterell/1750769
    username: ~Ryan_Cotterell1
  decision: toMainConference
  end_page: 7936
  file: 846.pdf
  id: 846
  num_pages: 21
  openreview_id: Gkx1Cv7B5h
  pdf_file: 51b2d6304f4a7b8fb12b6c89cf4facca65040745.pdf
  start_page: 7916
  title: Towards Explainability in Legal Outcome Prediction Models
- abstract: Large language models (LLMs) are known to generate biased responses where
    the opinions of certain groups and populations are underrepresented. Here, we
    present a novel approach to achieve controllable generation of specific viewpoints
    using LLMs, that can be leveraged to produce multiple perspectives and to reflect
    the diverse opinions. Moving beyond the traditional reliance on demographics like
    age, gender, or party affiliation, we introduce a data-driven notion of persona
    grounded in collaborative filtering, which is defined as either a single individual
    or a cohort of individuals manifesting similar views across specific inquiries.
    As individuals in the same demographic group may have different personas, our
    data-driven persona definition allows for a more nuanced understanding of different
    (latent) social groups present in the population. In addition to this, we also
    explore an efficient method to steer LLMs toward the personas that we define.
    We show that our data-driven personas significantly enhance model steerability,
    with improvements of between 57%-77% over our best performing baselines.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Generation
  authors:
  - dblp_id: https://dblp.org/pid/28/6612
    emails: '****@gmail.com'
    first_name: Junyi
    google_scholar_id: https://scholar.google.com/citations?user=MzvZSs0AAAAJ&hl=en
    institution: University of Maryland, College Park
    last_name: Li
    name: Junyi Li
    semantic_scholar_id: https://www.semanticscholar.org/author/Junyi-Li/2108933511
    username: ~Junyi_Li1
  - emails: '****@gmail.com'
    first_name: Charith
    google_scholar_id: https://scholar.google.com/citations?user=rFtA7SQAAAAJ&hl=en&oi=ao
    homepage: http://charithperis.com/
    institution: Amazon
    last_name: Peris
    name: Charith Peris
    semantic_scholar_id: https://www.semanticscholar.org/author/Charith-S.-Peris/102648923
    username: ~Charith_Peris1
  - dblp_id: https://dblp.org/pid/230/8151
    emails: '****@amazon.com'
    first_name: Ninareh
    google_scholar_id: https://scholar.google.com/citations?user=1R3XgHQAAAAJ&hl=en
    homepage: https://scf.usc.edu/~ninarehm/
    institution: Amazon
    last_name: Mehrabi
    name: Ninareh Mehrabi
    username: ~Ninareh_Mehrabi2
  - dblp_id: https://dblp.org/pid/183/3699
    emails: '****@amazon.com'
    first_name: Palash
    google_scholar_id: https://scholar.google.com/citations?user=kNeah3kAAAAJ&hl=en&oi=ao
    institution: Amazon
    last_name: Goyal
    name: Palash Goyal
    username: ~Palash_Goyal1
  - dblp_id: https://dblp.org/pid/18/2428
    emails: '****@kwchang.net'
    first_name: Kai-Wei
    google_scholar_id: https://scholar.google.com/citations?user=fqDBtzYAAAAJ&hl=en
    homepage: http://kwchang.net
    institution: University of California, Los Angeles
    last_name: Chang
    name: Kai-Wei Chang
    semantic_scholar_id: https://www.semanticscholar.org/author/Kai-Wei-Chang/2782886
    username: ~Kai-Wei_Chang1
  - dblp_id: https://dblp.org/pid/16/3411
    emails: '****@isi.edu'
    first_name: Aram
    google_scholar_id: https://scholar.google.com/citations?user=rJTwW0MAAAAJ&hl=en
    homepage: http://www.isi.edu/~galstyan
    institution: Information Sciences Institute, University of Southern California
      and Amazon Alexa
    last_name: Galstyan
    name: Aram Galstyan
    username: ~Aram_Galstyan1
  - dblp_id: https://dblp.org/pid/16/6366
    emails: '****@cs.toronto.edu'
    first_name: Richard
    google_scholar_id: https://scholar.google.ca/citations?user=iBeDoRAAAAAJ&hl=en
    homepage: http://www.cs.columbia.edu/~zemel
    institution: Department of Computer Science, Columbia University and Department
      of Computer Science, University of Toronto
    last_name: Zemel
    name: Richard Zemel
    username: ~Richard_Zemel1
  - emails: '****@amazon.com'
    first_name: Rahul
    google_scholar_id: https://scholar.google.com/citations?user=1CFrm2YAAAAJ&hl=en
    last_name: Gupta
    name: Rahul Gupta
    username: ~Rahul_Gupta3
  decision: toMainConference
  end_page: 7952
  file: 852.pdf
  id: 852
  num_pages: 16
  openreview_id: 6GM5X0ZxJb
  pdf_file: 37e097e26bc13800af85768c468ade80b31bf196.pdf
  start_page: 7937
  title: The steerability of large language models toward data-driven personas
- abstract: 'Training a supervised news summarization model requires large amounts
    of high-quality training data consisting of news articles paired with reference
    summaries. However, obtaining such data is costly, and existing datasets contain
    considerable amount of noise. We present a new large-scale and high-quality dataset
    for supervised abstractive news summarization containing 1.3 million training
    samples, which we call CCSum. In creating this dataset, we take advantage of the
    journalistic inverted-pyramid style in news writing: In some articles, the first
    sentence can be considered a summary of the reported story. Accordingly, among
    35 million CommonCrawl News articles, we identify pairs of articles about the
    same news story and use one article''s first sentence as the summary for the other
    article. To ensure high quality, we apply strict filters whose parameters we optimize
    using Bayesian optimization. We show that the resulting dataset is more factual
    and informative than established summarization datasets; less than 1% of the summaries
    have major factual inconsistencies with the corresponding news articles, compared
    to 5.5% to 15.4% in existing datasets, according to our human evaluation. Summarization
    models trained on our dataset are more favored compared to those trained on CNN/Daily
    Mail. The proposed dataset can open new opportunities for future research in abstractive
    summarization.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Summarization
  authors:
  - dblp_id: https://dblp.org/pid/123/4421
    emails: '****@dal.ca'
    first_name: Xiang
    google_scholar_id: https://scholar.google.ca/citations?user=9K7gwtoAAAAJ&hl=en&authuser=2
    institution: Amazon
    last_name: Jiang
    name: Xiang Jiang
    username: ~Xiang_Jiang1
  - dblp_id: https://dblp.org/pid/37/4227
    emails: '****@gmail.com'
    first_name: Markus
    google_scholar_id: https://scholar.google.com/citations?user=0a1AxxQAAAAJ
    homepage: https://markusdreyer.org/
    institution: Amazon
    last_name: Dreyer
    name: Markus Dreyer
    semantic_scholar_id: https://www.semanticscholar.org/author/Markus-Dreyer/40262269
    username: ~Markus_Dreyer1
  decision: toMainConference
  end_page: 7983
  file: 856.pdf
  id: 856
  num_pages: 31
  openreview_id: HOMVMUOeyw
  pdf_file: 7cfbf4f490095dfc77f53117e262d62aa2b04b07.pdf
  start_page: 7953
  title: 'CCSum: A Large-Scale and High-Quality Dataset for Abstractive News Summarization'
- abstract: "Supervised classification heavily depends on datasets annotated by humans.\
    \ However, in subjective tasks such as toxicity classification, these annotations\
    \ often exhibit low agreement among raters. Annotations have commonly been aggregated\
    \ by employing methods like majority voting to determine a single ground truth\
    \ label. In subjective tasks, aggregating labels will result in biased labeling\
    \ and, consequently, biased models that can overlook minority opinions. Previous\
    \ studies have shed light on the pitfalls of label aggregation and have introduced\
    \ a handful of practical approaches to tackle this issue. Recently proposed multi-annotator\
    \ models, which predict labels individually per annotator, are vulnerable to under-determination\
    \ for annotators with few samples. This problem is exacerbated in crowdsourced\
    \ datasets. In this work, we propose Annotator Aware Representations for Texts\
    \ (AART) for subjective classification tasks. Our approach involves learning representations\
    \ of annotators, allowing for exploration of annotation behaviors. \nWe show the\
    \ improvement of our method on metrics that assess the performance on capturing\
    \ individual annotators' perspectives. Additionally, we demonstrate fairness metrics\
    \ to evaluate our model's equability of performance for marginalized annotators\
    \ compared to others."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Ethics, Bias, and Fairness
  authors:
  - emails: '****@usc.edu'
    first_name: Negar
    google_scholar_id: https://scholar.google.com/citations?user=7o1I4IkAAAAJ&hl=en
    last_name: Mokhberian
    name: Negar Mokhberian
    username: ~Negar_Mokhberian1
  - dblp_id: https://dblp.org/pid/264/5804
    emails: '****@gmail.com'
    first_name: Myrl
    google_scholar_id: https://scholar.google.com/citations?user=MNyK6-QAAAAJ
    homepage: https://myrl.marmarel.is/
    institution: University of Southern California and USC/ISI
    last_name: Marmarelis
    middle_name: G
    name: Myrl G Marmarelis
    username: ~Myrl_G_Marmarelis1
  - emails: '****@uva.nl'
    first_name: Frederic
    google_scholar_id: https://scholar.google.com/citations?user=JHcPqTAAAAAJ&hl=en
    homepage: https://fhopp.github.io/
    institution: University of Amsterdam
    last_name: Hopp
    middle_name: Rene
    name: Frederic Rene Hopp
    username: ~Frederic_Rene_Hopp1
  - dblp_id: https://dblp.uni-trier.de/pid/86/11425.html
    emails: '****@unito.it'
    first_name: Valerio
    google_scholar_id: https://scholar.google.it/citations?user=5VCe4aAAAAAJ&hl=en
    homepage: http://valeriobasile.github.io/
    institution: University of Turin
    last_name: Basile
    name: Valerio Basile
    orcid: https://orcid.org/0000-0001-8110-6832
    semantic_scholar_id: https://www.semanticscholar.org/author/Valerio-Basile/3101511
    username: ~Valerio_Basile2
  - dblp_id: https://dblp.org/pid/51/9687
    emails: '****@isi.edu'
    first_name: Fred
    homepage: http://fred.science
    institution: University of Southern California and USC/ISI
    last_name: Morstatter
    name: Fred Morstatter
    username: ~Fred_Morstatter1
  - dblp_id: https://dblp.org/pid/99/433
    emails: '****@isi.edu'
    first_name: Kristina
    homepage: http://www.isi.edu/~lerman/
    institution: University of Southern California and USC Information Sciences Institute
    last_name: Lerman
    name: Kristina Lerman
    username: ~Kristina_Lerman1
  decision: toMainConference
  end_page: 7996
  file: 857.pdf
  id: 857
  num_pages: 13
  openreview_id: tGJl7NnCv2
  pdf_file: 2a387ffc8cb391f99303521003d439bdf92ea7de.pdf
  start_page: 7984
  title: Capturing Perspectives of Crowdsourced Annotators in Subjective Learning
    Tasks
- abstract: Neural Table-to-Text models tend to hallucinate, producing texts that
    contain factual errors.  We investigate whether such errors in the output can
    be traced back to problems with the input.  We manually annotated 1,837 texts
    generated by multiple models in the politics domain of the ToTTo dataset.  We
    identify the input problems that are responsible for many output errors and show
    that fixing these inputs reduces factual errors by between 52\% and 76\% (depending
    on the model).  In addition, we observe that models struggle in processing tabular
    inputs that are structured in a non-standard way, particularly when the input
    lacks distinct row and column values or when the column headers are not correctly
    mapped to corresponding values.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Generation
  authors:
  - emails: '****@abdn.ac.uk'
    first_name: Barkavi
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=8t6ZxaMAAAAJ
    institution: University of Aberdeen
    last_name: Sundararajan
    name: Barkavi Sundararajan
    username: ~Barkavi_Sundararajan1
  - dblp_id: https://dblp.org/pid/64/1429
    emails: '****@abdn.ac.uk'
    first_name: Yaji
    google_scholar_id: https://scholar.google.com/citations?user=tap6WEEAAAAJ&hl=en
    homepage: https://www.abdn.ac.uk/ncs/profiles/yaji.sripada
    institution: Arria NLG and University of Aberdeen
    last_name: Sripada
    name: Yaji Sripada
    username: ~Yaji_Sripada1
  - dblp_id: https://dblp.org/pid/21/3710
    emails: '****@abdn.ac.uk'
    first_name: Ehud
    google_scholar_id: https://scholar.google.co.uk/citations?user=Ns0YuP0AAAAJ
    homepage: http://ehudreiter.com
    institution: University of Aberdeen
    last_name: Reiter
    name: Ehud Reiter
    username: ~Ehud_Reiter1
  decision: toMainConference
  end_page: 8023
  file: 859.pdf
  id: 859
  num_pages: 27
  openreview_id: dKxK3KCohw
  pdf_file: bc3bddca4d2a0712726a27567c7f63728696329b.pdf
  start_page: 7997
  title: Improving Factual Accuracy of Neural Table-to-Text Output by Addressing Input
    Problems in ToTTo
- abstract: Large Language Models (LLMs) are powerful models for generation tasks,
    but they may not generate good quality outputs in their first attempt. Apart from
    model fine-tuning, existing approaches to improve prediction accuracy and quality
    typically involve LLM self-improvement / self-reflection that incorporate feedback
    from models themselves. Despite their effectiveness, these methods are hindered
    by their high computational cost and lack of scalability. In this work, we propose
    CERET, a method for refining text generations by considering semantic stability,
    entailment and inter-sample uncertainty measures. Experimental results show that
    CERET outperforms Self-consistency and Self-rerank baselines consistently under
    various task setups, by 1.6% in Rouge-1 for abstractive summarization and 3.5%
    in hit rate for question answering. Compared to LLM Self-rerank method, our approach
    only requires 9.4% of its latency and is more cost-effective.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Efficient/Low-Resource Methods for NLP
  authors:
  - emails: '****@gmail.com'
    first_name: Jason
    google_scholar_id: https://scholar.google.com/citations?user=NYjfyB4AAAAJ&hl=en
    homepage: https://www.linkedin.com/in/jinglun-cai
    institution: Amazon
    last_name: Cai
    name: Jason Cai
    semantic_scholar_id: https://www.semanticscholar.org/author/Jinglun-Cai/2115670967
    username: ~Jason_Cai1
  - emails: '****@gmail.com'
    first_name: Hang
    google_scholar_id: https://scholar.google.com/citations?user=UxOvKVUAAAAJ&hl=en
    institution: Amazon
    last_name: Su
    name: Hang Su
    username: ~Hang_Su7
  - emails: '****@gmail.com'
    first_name: Monica
    homepage: http://monicasunkara.com/
    last_name: Sunkara
    name: MONICA SUNKARA
    username: ~MONICA_SUNKARA1
  - dblp_id: https://dblp.org/pid/205/8962
    emails: '****@gmail.com'
    first_name: Igor
    google_scholar_id: https://scholar.google.com/citations?user=TVs0lP8AAAAJ
    homepage: https://shalyminov.com
    institution: Amazon
    last_name: Shalyminov
    name: Igor Shalyminov
    orcid: https://orcid.org/0000-0001-9664-1774
    semantic_scholar_id: https://www.semanticscholar.org/author/Igor-Shalyminov/24879056
    username: ~Igor_Shalyminov1
  - dblp_id: https://dblp.org/pid/03/8053
    emails: '****@gmail.com'
    first_name: Saab
    google_scholar_id: https://scholar.google.de/citations?user=1tCbwIQAAAAJ&hl=en
    institution: Amazon
    last_name: Mansour
    name: Saab Mansour
    semantic_scholar_id: https://www.semanticscholar.org/author/Saab-Mansour/39674628
    username: ~Saab_Mansour1
  decision: toMainConference
  end_page: 8037
  file: 861.pdf
  id: 861
  num_pages: 14
  openreview_id: pp2dRxx6jg
  pdf_file: dd01fe1a0a783dfcd4c39d61809fd76a01004c05.pdf
  start_page: 8024
  title: 'CERET: Cost-Effective Extrinsic Refinement for Text Generation'
- abstract: 'We study the problem of automatically annotating relevant numerals (GAAP
    metrics) occurring in the financial documents with their corresponding XBRL tags.
    Different from prior works, we investigate the feasibility of solving this extreme
    classification problem using a generative paradigm through instruction tuning
    of Large Language Models (LLMs). To this end, we leverage metric metadata information

    to frame our target outputs while proposing a parameter efficient solution for
    the task using LoRA. We perform experiments on two recently released financial
    numeric labeling datasets. Our proposed model, **FLAN-FinXC**, achieves new state-of-the-art
    performances on both the datasets, outperforming several strong baselines. We
    explain the better scores of our proposed model by demonstrating its capability
    for zero-shot as well as the least frequently occurring tags. Also, even when
    we fail to predict the XBRL tags correctly, our generated output has substantial
    overlap with the ground-truth in majority of the cases.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - emails: '****@gmail.com'
    first_name: Subhendu
    google_scholar_id: https://scholar.google.com/citations?user=m4iT4HEAAAAJ&hl=en&authuser=3
    homepage: https://subhendukhatuya.github.io/
    last_name: Khatuya
    name: Subhendu Khatuya
    username: ~Subhendu_Khatuya2
  - dblp_id: https://dblp.org/pid/124/3803
    emails: '****@gmail.com'
    first_name: Rajdeep
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=Ac-0jgMAAAAJ
    homepage: https://rajdeep345.github.io/
    institution: Indian Institute of Technology Kharagpur
    last_name: Mukherjee
    name: Rajdeep Mukherjee
    orcid: https://orcid.org/0000-0002-2267-1695
    semantic_scholar_id: https://www.semanticscholar.org/author/Rajdeep-Mukherjee/2182282
    username: ~Rajdeep_Mukherjee1
  - emails: '****@gmail.com'
    first_name: Akash
    google_scholar_id: https://scholar.google.com/citations?user=y2ZL3XwAAAAJ
    last_name: Ghosh
    name: Akash Ghosh
    orcid: https://orcid.org/0000-0002-3907-6294
    username: ~Akash_Ghosh1
  - dblp_id: https://dblp.org/pid/169/3429
    emails: '****@gmail.com'
    first_name: Manjunath
    last_name: Hegde
    name: Manjunath Hegde
    username: ~Manjunath_Hegde1
  - dblp_id: https://dblp.org/pid/06/111
    emails: '****@gmail.com'
    first_name: Koustuv
    last_name: Dasgupta
    name: Koustuv Dasgupta
    username: ~Koustuv_Dasgupta1
  - dblp_id: https://dblp.org/pers/hd/g/Ganguly:Niloy
    emails: '****@cse.iitkgp.ac.in'
    first_name: Niloy
    google_scholar_id: https://scholar.google.com/citations?user=hCbFmUUAAAAJ&hl=en
    homepage: http://www.facweb.iitkgp.ac.in/~niloy/
    institution: Indian Institute of Technology Kharagpur,
    last_name: Ganguly
    name: Niloy Ganguly
    username: ~Niloy_Ganguly1
  - dblp_id: https://dblp.org/pid/06/900-1
    emails: '****@gmail.com'
    first_name: Saptarshi
    google_scholar_id: https://scholar.google.co.in/citations?user=7TmKZv0AAAAJ
    homepage: http://cse.iitkgp.ac.in/~saptarshi
    institution: Indian Institute of Technology Kharagpur
    last_name: Ghosh
    name: Saptarshi Ghosh
    semantic_scholar_id: https://www.semanticscholar.org/author/Saptarshi-Ghosh/143841814
    username: ~Saptarshi_Ghosh1
  - dblp_id: https://dblp.org/pid/77/2307-2
    emails: '****@cse.iitkgp.ac.in'
    first_name: Pawan
    google_scholar_id: https://scholar.google.com.tw/citations?user=F14FHsIAAAAJ
    homepage: http://cse.iitkgp.ac.in/~pawang/
    institution: IIT Kharagpur
    last_name: Goyal
    name: Pawan Goyal
    semantic_scholar_id: https://www.semanticscholar.org/author/P.-Goyal/2111980452
    username: ~Pawan_Goyal1
  decision: toMainConference
  end_page: 8050
  file: 863.pdf
  id: 863
  num_pages: 13
  openreview_id: Hjloh1YUxW
  pdf_file: 68acfb9df83151e8d04e879668429c52982c24dc.pdf
  start_page: 8038
  title: Parameter-Efficient Instruction Tuning of Large Language Models For Extreme
    Financial Numeral Labelling
- abstract: 'Social media data has been used for detecting users with mental disorders,
    such as depression. Despite the global significance of cross-cultural representation
    and its potential impact on model performance, publicly available datasets often
    lack crucial metadata related

    to this aspect. In this work, we evaluate the generalization of benchmark datasets
    to build AI models on cross-cultural Twitter data. We gather a custom geo-located
    Twitter dataset of depressed users from seven countries as a test dataset. Our
    results show that depression

    detection models do not generalize globally. The models perform worse on Global
    South users compared to Global North. Pre-trained

    language models achieve the best generalization compared to Logistic Regression,
    though still show significant gaps in performance on depressed and non-Western
    users. We quantify our findings and provide several actionable suggestions to
    mitigate this issue'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Computational Social Science and Cultural Analytics
  authors:
  - emails: '****@umn.edu'
    first_name: Nuredin Ali
    google_scholar_id: https://scholar.google.com/citations?user=s4f7EPsAAAAJ&hl=en
    homepage: https://nuredinali.github.io/
    last_name: Abdelkadir
    name: Nuredin Ali Abdelkadir
    username: ~Nuredin_Ali_Abdelkadir1
  - emails: '****@umn.edu'
    first_name: Charles
    google_scholar_id: https://scholar.google.com/citations?user=s1gLqg8AAAAJ&hl=en
    homepage: http://chuankaizhang.com
    last_name: Zhang
    middle_name: Chuankai
    name: Charles Chuankai Zhang
    orcid: https://orcid.org/0000-0002-1027-9733
    username: ~Charles_Chuankai_Zhang1
  - emails: '****@gmail.com'
    first_name: Ned
    last_name: Mayo
    name: Ned Mayo
    username: ~Ned_Mayo1
  - emails: '****@umn.edu'
    first_name: Stevie
    homepage: http://www.steviechancellor.com
    institution: University of Minnesota - Twin Cities
    last_name: Chancellor
    name: Stevie Chancellor
    username: ~Stevie_Chancellor1
  decision: toMainConference
  end_page: 8059
  file: 865.pdf
  id: 865
  num_pages: 9
  openreview_id: NgJiFmLHS4
  pdf_file: 12d76206851caa83676cc10ade644c302e4f2b12.pdf
  start_page: 8051
  title: 'Diverse Perspectives, Divergent Models: Cross-Cultural Evaluation of Depression
    Detection on Twitter'
- abstract: "State bills have a significant impact on various aspects of society,\
    \ including health, education, and the economy. Consequently, it is crucial to\
    \ conduct systematic research on state bills before and after they are enacted\
    \ to evaluate their benefits and drawbacks, thereby guiding future decision-making.\
    \ In this work, we developed the first state-level deep learning framework that\
    \ (1) handles the complex and inconsistent language of policies across US states\
    \ using generative large language models and (2) decodes legislators\u2019 behavior\
    \ and implications of state policies by establishing a shared nationwide network,\
    \ enriched with diverse contexts, such as information on interest groups influencing\
    \ public policy and legislators' courage test results, which reflect their political\
    \ positions."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - dblp_id: https://dblp.org/pid/220/3858
    emails: '****@purdue.edu'
    first_name: Maryam
    institution: Purdue University
    last_name: Davoodi
    name: Maryam Davoodi
    username: ~Maryam_Davoodi1
  - dblp_id: https://dblp.org/pid/38/3382
    emails: '****@purdue.edu'
    first_name: Dan
    google_scholar_id: https://scholar.google.com.tw/citations?user=u8358QgAAAAJ
    homepage: https://www.cs.purdue.edu/homes/dgoldwas/
    institution: Purdue University, Purdue University and Purdue University
    last_name: Goldwasser
    name: Dan Goldwasser
    username: ~Dan_Goldwasser1
  decision: toMainConference
  end_page: 8078
  file: 869.pdf
  id: 869
  num_pages: 19
  openreview_id: 3LQcGEVJ0a
  pdf_file: 1dcc8e90e959c1453936338e64f243969acbb6af.pdf
  start_page: 8060
  title: Analysis of State-Level Legislative Process in Enhanced Linguistic and Nationwide
    Network Contexts
- abstract: 'Pre-trained multilingual models have enabled deployment of NLP technologies
    for multiple languages. However, optimally fine-tuning these models under an annotation
    budget, such that performance on desired target languages is jointly maximized,
    still remains an open question. In this paper, we introduce DeMuX, a framework
    that prescribes the exact data-points to label from vast amounts of unlabelled
    multilingual data, having unknown degrees of overlap with the target set. Unlike
    most prior works, our end-to-end framework is language-agnostic, accounts for
    model representations, and supports multilingual target configurations. Our active
    learning strategies rely upon distance and uncertainty measures to select task-specific
    neighbors that are most informative to label, given a model. DeMuX outperforms
    strong baselines in 84\% of the test cases, in the zero-shot setting of disjoint
    source and target language sets (including multilingual target pools), across
    three models and four tasks. Notably, in low-budget settings (5-100 examples),
    we observe gains of up to 8-11 F1 points. Our code is released here: https://github.com/simran-khanuja/demux.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multilinguality and Language Diversity
  authors:
  - dblp_id: https://dblp.org/pid/255/5469
    emails: '****@andrew.cmu.edu'
    first_name: Simran
    google_scholar_id: https://scholar.google.com/citations?user=yInhszwAAAAJ&hl=en
    homepage: https://simran-khanuja.github.io/
    institution: CMU, Carnegie Mellon University and Google
    last_name: Khanuja
    name: Simran Khanuja
    semantic_scholar_id: https://www.semanticscholar.org/author/Simran-Khanuja/1452678825?sort=influence&page=2
    username: ~Simran_Khanuja1
  - emails: '****@andrew.cmu.edu'
    first_name: Srinivas
    homepage: https://srinivas-gowriraj.github.io
    last_name: Gowriraj
    name: Srinivas Gowriraj
    username: ~Srinivas_Gowriraj1
  - dblp_id: https://dblp.org/pid/211/7773
    emails: '****@andrew.cmu.edu'
    first_name: Lucio
    google_scholar_id: https://scholar.google.com/citations?user=ggFzw0MAAAAJ&hl=en
    homepage: https://ldery.github.io/
    institution: Carnegie Mellon University
    last_name: Dery
    middle_name: M.
    name: Lucio M. Dery
    username: ~Lucio_M._Dery1
  - dblp_id: https://dblp.org/pid/03/8155
    emails: '****@cs.cmu.edu'
    first_name: Graham
    google_scholar_id: https://scholar.google.com/citations?user=wlosgkoAAAAJ
    homepage: http://phontron.com
    institution: Carnegie Mellon University
    last_name: Neubig
    name: Graham Neubig
    semantic_scholar_id: https://www.semanticscholar.org/author/Graham-Neubig/1700325
    username: ~Graham_Neubig1
  decision: toMainConference
  end_page: 8092
  file: 872.pdf
  id: 872
  num_pages: 14
  openreview_id: lsFZtMRXCK
  pdf_file: 87a4ffe7d8102a2f13b4dabbb7120b9b7bc6a1f2.pdf
  start_page: 8079
  title: 'DeMuX: Data-efficient Multilingual Learning'
- abstract: "As large language models (LLMs) have increased in their capabilities,\
    \ so does\ntheir potential for dual use. To reduce harmful outputs, produces and\
    \ vendors of\nLLMs have used reinforcement learning with human feedback (RLHF).\
    \ In tandem,\nLLM vendors have been increasingly enabling fine-tuning of their\
    \ most powerful\nmodels. However, concurrent work has shown that fine-tuning can\
    \ remove RLHF\nprotections. We may expect that the most powerful models currently\
    \ available\n(GPT-4) are less susceptible to fine-tuning attacks. \n\nIn this\
    \ work, we show the contrary: fine-tuning allows attackers to remove RLHF\nprotections\
    \ with as few as 340 examples and a 95\\% success rate. These training\nexamples\
    \ can be automatically generated with weaker models. We further show that\nremoving\
    \ RLHF protections does not decrease usefulness on non-censored outputs,\nproviding\
    \ evidence that our fine-tuning strategy does not decrease usefulness\ndespite\
    \ using weaker models to generate training data. Our results show the need\nfor\
    \ further research on protections on LLMs."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Ethics, Bias, and Fairness
  authors:
  - dblp_id: https://dblp.org/pid/321/4704.html
    emails: '****@illinois.edu'
    first_name: Qiusi
    google_scholar_id: https://scholar.google.com/citations?user=XaYJrgoAAAAJ&hl=en&oi=ao
    homepage: https://zqs1943.github.io/
    institution: University of Illinois Urbana-Champaign
    last_name: Zhan
    name: Qiusi Zhan
    semantic_scholar_id: https://www.semanticscholar.org/author/Qiusi-Zhan/2167027235
    username: ~Qiusi_Zhan1
  - emails: '****@illinois.edu'
    first_name: Richard
    last_name: Fang
    name: Richard Fang
    orcid: https://orcid.org/0009-0009-8104-9917
    username: ~Richard_Fang1
  - emails: '****@illinois.edu'
    first_name: Rohan
    last_name: Bindu
    name: Rohan Bindu
    username: ~Rohan_Bindu1
  - emails: '****@illinois.edu'
    first_name: Akul
    google_scholar_id: https://scholar.google.com/citations?user=65-B1ZwAAAAJ&hl=en
    last_name: Gupta
    name: Akul Gupta
    username: ~Akul_Gupta1
  - emails: '****@stanford.edu'
    first_name: Tatsunori
    google_scholar_id: https://scholar.google.com/citations?user=5ygiTwsAAAAJ&hl=ja
    homepage: https://thashim.github.io
    institution: Stanford University
    last_name: Hashimoto
    name: Tatsunori Hashimoto
    username: ~Tatsunori_Hashimoto1
  - emails: '****@illinois.edu'
    first_name: Daniel
    google_scholar_id: https://scholar.google.com/citations?user=CpMjT0YAAAAJ&hl=en
    homepage: https://ddkang.github.io/
    institution: Department of Computer Science
    last_name: Kang
    name: Daniel Kang
    username: ~Daniel_Kang1
  decision: toMainConference
  end_page: 8099
  file: 873.pdf
  id: 873
  num_pages: 7
  openreview_id: h94G68NA9V
  pdf_file: 5e18991592659d3166b7ffc7a166bab5d3c9f6e5.pdf
  start_page: 8093
  title: Removing RLHF Protections in GPT-4 via Fine-Tuning
- abstract: State-of-the-art neural rankers pre-trained on large task-specific training
    data such as MS-MARCO, have been shown to exhibit strong performance on various
    ranking tasks without domain adaptation, also called zero-shot. However, zero-shot
    neural ranking may be sub-optimal, as it does not take advantage of the target
    domain information. Unfortunately, acquiring sufficiently large and high quality
    target training data to improve a modern neural ranker can be costly and time-consuming.
    To address this problem, we propose a new approach to unsupervised domain adaptation
    for ranking, DUQGen, which addresses a critical gap in prior literature, namely
    how to automatically generate both effective and diverse synthetic training data
    to fine tune a modern neural ranker for a new domain. Specifically, DUQGen produces
    a more effective representation of the target domain by identifying clusters of
    similar documents; and generates a more diverse training dataset by probabilistic
    sampling over the resulting document clusters. Our extensive experiments, over
    the standard BEIR collection, demonstrate that DUQGen consistently outperforms
    all zero-shot baselines and substantially outperforms the SOTA baselines on 16
    out of 18 datasets, for an average of 4% relative improvement across all datasets.
    We complement our results with a thorough analysis for more in-depth understanding
    of the proposed method's performance and to identify promising areas for further
    improvements.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Retrieval and Text Mining
  authors:
  - emails: '****@emory.edu'
    first_name: Ramraj
    google_scholar_id: https://scholar.google.com/citations?user=TBl8dYgAAAAJ&hl=en
    last_name: Chandradevan
    name: Ramraj Chandradevan
    username: ~Ramraj_Chandradevan2
  - emails: '****@hotmail.com'
    first_name: Kaustubh
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=xSGbQ3oAAAAJ&view_op=list_works&sortby=pubdate
    institution: BITS Pilani
    last_name: Dhole
    name: Kaustubh Dhole
    username: ~Kaustubh_Dhole1
  - dblp_id: https://dblp.org/pid/26/1185
    emails: '****@emory.edu'
    first_name: Eugene
    google_scholar_id: https://scholar.google.com/citations?user=3BX3vWcAAAAJ&hl=en
    homepage: https://www.cs.emory.edu/~eugene
    institution: Amazon and Emory University
    last_name: Agichtein
    name: Eugene Agichtein
    orcid: https://orcid.org/0000-0002-3148-5448
    username: ~Eugene_Agichtein1
  decision: toMainConference
  end_page: 8114
  file: 885.pdf
  id: 885
  num_pages: 15
  openreview_id: oQhciTQoii
  pdf_file: 4edd6a18b2033568bf9208096381a8175644c9d2.pdf
  start_page: 8100
  title: 'DUQGen: Effective Unsupervised Domain Adaptation of Neural Rankers by Diversifying
    Synthetic Query Generation'
- abstract: 'Throughout a conversation, the way participants interact with each other
    is in constant flux: their tones may change, they may resort to different strategies
    to convey their points, or they might alter their interaction patterns. An understanding
    of these dynamics can complement that of the actual facts and opinions discussed,
    offering a more holistic view of the trajectory of the conversation: how it arrived
    at its current state and where it is likely heading.



    In this work, we introduce the task of summarizing the dynamics of conversations,
    by constructing a dataset of human-written summaries, and exploring several automated
    baselines.  We evaluate whether such summaries can capture the trajectory of conversations
    via an established downstream task: forecasting whether an ongoing conversation
    will eventually derail into toxic behavior.  We show that they help both humans
    and automated systems with this forecasting task.  Humans make predictions three
    times faster, and with greater confidence, when reading the summaries than when
    reading the transcripts.  Furthermore, automated forecasting systems are more
    accurate when constructing, and then predicting based on, summaries of conversation
    dynamics, compared to directly predicting on the transcripts.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Computational Social Science and Cultural Analytics
  authors:
  - dblp_id: https://dblp.org/pid/339/6615
    emails: '****@cornell.edu'
    first_name: Yilun
    institution: Department of Computer Science, Cornell University
    last_name: Hua
    name: Yilun Hua
    semantic_scholar_id: https://www.semanticscholar.org/author/Yilun-Hua/1453408294
    username: ~Yilun_Hua1
  - emails: '****@cornell.edu'
    first_name: Nicholas
    homepage: https://nickchernogor.github.io
    institution: Cornell University
    last_name: Chernogor
    name: Nicholas Chernogor
    username: ~Nicholas_Chernogor1
  - emails: '****@seas.upenn.edu'
    first_name: Yuzhe
    last_name: Gu
    name: Yuzhe Gu
    username: ~Yuzhe_Gu2
  - emails: '****@gmail.com'
    first_name: Seoyeon
    last_name: Jeong
    middle_name: Julie
    name: Seoyeon Julie Jeong
    username: ~Seoyeon_Julie_Jeong1
  - emails: '****@cornell.edu'
    first_name: Miranda
    institution: Cornell University
    last_name: Luo
    name: Miranda Luo
    username: ~Miranda_Luo1
  - dblp_id: https://dblp.org/pid/49/3976
    emails: '****@cs.cornell.edu'
    first_name: Cristian
    google_scholar_id: https://scholar.google.com/citations?user=njczUPMAAAAJ&hl=en
    homepage: https://www.cs.cornell.edu/~cristian/
    institution: Cornell University and Cornell University
    last_name: Danescu-Niculescu-Mizil
    name: Cristian Danescu-Niculescu-Mizil
    semantic_scholar_id: https://www.semanticscholar.org/author/Cristian-Danescu-Niculescu-Mizil/1388368997
    username: ~Cristian_Danescu-Niculescu-Mizil1
  decision: toMainConference
  end_page: 8140
  file: 887.pdf
  id: 887
  num_pages: 26
  openreview_id: aJ1VqJ3Maa
  pdf_file: 4d6c16d82233c1c72ec1318d3228c88046913da5.pdf
  start_page: 8115
  title: How did we get here? Summarizing conversation dynamics
- abstract: As large language models become increasingly integrated into daily life,
    detecting implicit toxicity across diverse contexts is crucial. To this end, we
    introduce $\texttt{LifeTox}$, a dataset designed for identifying implicit toxicity
    within a broad range of advice-seeking scenarios. Unlike existing safety datasets,
    $\texttt{LifeTox}$ comprises diverse contexts derived from personal experiences
    through open-ended questions. Our experiments demonstrate that RoBERTa fine-tuned
    on $\texttt{LifeTox}$ matches or surpasses the zero-shot performance of large
    language models in toxicity classification tasks. These results underscore the
    efficacy of $\texttt{LifeTox}$ in addressing the complex challenges inherent in
    implicit toxicity. We open-sourced the dataset\footnote{\url{https://huggingface.co/datasets/mbkim/LifeTox}}
    and the $\texttt{LifeTox}$ moderator family; 350M, 7B, and 13B.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - dblp_id: https://dblp.org/pid/119/4240
    emails: '****@snu.ac.kr'
    first_name: Minbeom
    google_scholar_id: https://scholar.google.co.kr/citations?user=-3wahsUAAAAJ&hl=ko
    homepage: https://minbeomkim.github.io/
    last_name: Kim
    name: Minbeom Kim
    semantic_scholar_id: https://www.semanticscholar.org/author/Minbeom-Kim/2250540
    username: ~Minbeom_Kim2
  - emails: '****@snu.ac.kr'
    first_name: Jahyun
    homepage: https://www.linkedin.com/in/jahyun-koo-23a019242/
    institution: Seoul National University
    last_name: Koo
    name: Jahyun Koo
    username: ~Jahyun_Koo2
  - dblp_id: https://dblp.org/pid/218/5402
    emails: '****@cau.ac.kr'
    first_name: Hwanhee
    google_scholar_id: https://scholar.google.com/citations?user=eRM8zHkAAAAJ&hl=en
    homepage: https://hwanheelee1993.github.io/
    institution: Chung-Ang University
    last_name: Lee
    name: Hwanhee Lee
    semantic_scholar_id: https://www.semanticscholar.org/author/Hwanhee-Lee/2109339794
    username: ~Hwanhee_Lee1
  - dblp_id: https://dblp.org/pid/50/9717
    emails: '****@gmail.com'
    first_name: Joonsuk
    google_scholar_id: https://scholar.google.com/citations?user=3SPMM3oAAAAJ&hl=en&oi=ao
    homepage: http://www.joonsuk.org
    institution: University of Richmond
    last_name: Park
    name: Joonsuk Park
    orcid: https://orcid.org/0000-0002-1182-4836
    semantic_scholar_id: https://www.semanticscholar.org/author/Joonsuk-Park/48490725
    username: ~Joonsuk_Park1
  - dblp_id: https://dblp.org/pid/127/9475
    emails: '****@gmail.com'
    first_name: Hwaran
    google_scholar_id: https://scholar.google.co.kr/citations?user=Jf6padoAAAAJ&hl=en
    homepage: https://hwaranlee.github.io
    institution: NAVER AI Lab
    last_name: Lee
    name: Hwaran Lee
    orcid: https://orcid.org/0000-0002-3773-4871
    semantic_scholar_id: https://www.semanticscholar.org/author/Hwaran-Lee/2294014
    username: ~Hwaran_Lee1
  - dblp_id: https://dblp.org/pid/48/3867
    emails: '****@snu.ac.kr'
    first_name: Kyomin
    google_scholar_id: https://scholar.google.co.kr/citations?user=u3uMl4MAAAAJ&hl=en
    homepage: http://milab.snu.ac.kr/kjung/index.html
    last_name: Jung
    name: Kyomin Jung
    username: ~Kyomin_Jung1
  decision: toMainConference
  end_page: 8151
  file: 888.pdf
  id: 888
  num_pages: 11
  openreview_id: v01uv1izOx
  pdf_file: 9cd709d220bd7eedacdd5c8fd885e405100bfd29.pdf
  start_page: 8141
  title: 'LifeTox: Unveiling Implicit Toxicity in Life Advice'
- abstract: Conversational moderation of online communities is crucial to maintaining
    civility for a constructive environment, but it is challenging to scale and harmful
    to moderators. The inclusion of sophisticated natural language generation modules
    as a force multiplier to aid human moderators is a tantalizing prospect, but adequate
    evaluation approaches have so far been elusive. In this paper, we establish a
    systematic definition of conversational moderation effectiveness grounded on moderation
    literature and establish design criteria for conducting realistic yet safe evaluation.
    We then propose a comprehensive evaluation framework to assess models' moderation
    capabilities independently of human intervention. With our framework, we conduct
    the first known study of language models as conversational moderators, finding
    that appropriately prompted models that incorporate insights from social science
    can provide specific and fair feedback on toxic behavior but struggle to influence
    users to increase their levels of respect and cooperation.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - dblp_id: https://dblp.org/pid/263/6759
    emails: '****@isi.edu'
    first_name: Hyundong
    google_scholar_id: https://scholar.google.com/citations?hl=en&authuser=2&user=2sUUj2oAAAAJ
    homepage: https://justin-cho.com
    institution: USC/ISI
    last_name: Cho
    middle_name: Justin
    name: Hyundong Justin Cho
    semantic_scholar_id: https://www.semanticscholar.org/author/91009922
    username: ~Hyundong_Justin_Cho1
  - emails: '****@isi.edu'
    first_name: Shuai
    institution: University of Southern California, Information Sciences Institute
    last_name: Liu
    name: Shuai Liu
    semantic_scholar_id: https://www.semanticscholar.org/author/Shuai-Liu/2156268596
    username: ~Shuai_Liu15
  - emails: '****@usc.edu'
    first_name: Taiwei
    google_scholar_id: https://scholar.google.com/citations?user=yv6nCnMAAAAJ&hl=en
    homepage: https://maksimstw.github.io/
    last_name: Shi
    name: Taiwei Shi
    username: ~Taiwei_Shi1
  - emails: '****@gmail.com'
    first_name: Darpan
    google_scholar_id: https://scholar.google.com/citations?user=U5jJRi8AAAAJ&hl=en
    homepage: https://darpanjain.com/
    last_name: Jain
    name: Darpan Jain
    semantic_scholar_id: https://www.semanticscholar.org/author/Darpan-Jain/2267332702
    username: ~Darpan_Jain1
  - emails: '****@usc.edu'
    first_name: Basem
    google_scholar_id: https://scholar.google.com/citations?user=zwJYNbQAAAAJ&hl=en
    homepage: https://linktr.ee/brizk
    institution: USC Institute for Creative Technologies, University of Southern California
    last_name: Rizk
    name: Basem Rizk
    username: ~Basem_Rizk1
  - emails: '****@gmail.com'
    first_name: Yuyang
    last_name: Huang
    name: Yuyang Huang
    username: ~Yuyang_Huang2
  - emails: '****@gmail.com'
    first_name: Zixun
    homepage: https://www.linkedin.com/in/zixunlu/
    institution: University of Southern California
    last_name: Lu
    name: Zixun Lu
    username: ~Zixun_Lu1
  - emails: '****@usc.edu'
    first_name: Nuan
    google_scholar_id: https://scholar.google.com/citations?user=bmCQoYsAAAAJ&hl=en&oi=ao
    institution: University of Southern California
    last_name: Wen
    name: Nuan Wen
    username: ~Nuan_Wen1
  - dblp_id: https://dblp.org/pid/71/3911.html
    emails: '****@ict.usc.edu'
    first_name: Jonathan
    google_scholar_id: https://scholar.google.com/citations?user=HF448PMAAAAJ&hl=en&oi=ao
    homepage: https://people.ict.usc.edu/~gratch/
    institution: University of Southern California
    last_name: Gratch
    name: Jonathan Gratch
    orcid: https://orcid.org/0000-0002-5959-809X
    semantic_scholar_id: https://www.semanticscholar.org/author/J.-Gratch/145438097
    username: ~Jonathan_Gratch1
  - dblp_id: https://dblp.org/pid/38/8773
    emails: '****@usc.edu'
    first_name: Emilio
    institution: University of Southern California
    last_name: Ferrara
    name: Emilio Ferrara
    username: ~Emilio_Ferrara1
  - dblp_id: https://dblp.org/pid/00/4758
    emails: '****@isi.edu'
    first_name: Jonathan
    google_scholar_id: https://scholar.google.com/citations?user=tmK5EPEAAAAJ&hl=en
    homepage: http://jonmay.net
    institution: University of Southern California and USC/ISI
    last_name: May
    name: Jonathan May
    orcid: https://orcid.org/0000-0002-5284-477X
    semantic_scholar_id: https://www.semanticscholar.org/author/Jonathan-May/143823227
    username: ~Jonathan_May1
  decision: toMainConference
  end_page: 8170
  file: 892.pdf
  id: 892
  num_pages: 19
  openreview_id: lO3yh7ltbL
  pdf_file: d0739d6e54fe032373e48213109c0d17dca07559.pdf
  start_page: 8152
  title: Can Language Model Moderators Improve the Health of Online Discourse?
- abstract: "Large language models (LLMs) often struggle with complex logical reasoning\
    \ due to logical inconsistencies and the inherent difficulty of\nsuch reasoning.\
    \ We use Lean, a theorem proving framework, to address these challenges. By formalizing\
    \ logical reasoning problems into\ntheorems within Lean, we can solve them by\
    \ proving or disproving the corresponding theorems. This method reduces the risk\
    \ of logical inconsistencies with the help of Lean\u2019s symbolic solver. It\
    \ also enhances our ability to treat complex reasoning tasks using Lean\u2019\
    s extensive library of theorem proofs. Our method achieves state-of-the-art performance\
    \ on the FOLIO dataset and achieves performance near this level on ProofWriter.\
    \ Notably, these results were accomplished by fine-tuning on fewer than 100 in-domain\
    \ samples for each dataset"
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Question Answering
  authors:
  - dblp_id: https://dblp.org/pid/220/3204.html
    emails: '****@gmail.com'
    first_name: Dongwei
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=sLHuGtUAAAAJ
    last_name: Jiang
    name: Dongwei Jiang
    semantic_scholar_id: https://www.semanticscholar.org/author/Dongwei-Jiang/2113405469
    username: ~Dongwei_Jiang2
  - dblp_id: https://dblp.org/pid/307/9330
    emails: '****@gmail.com'
    first_name: Marcio
    google_scholar_id: https://scholar.google.com/citations?user=8KRI-ooAAAAJ&hl=en
    homepage: http://marciofonseca.me
    institution: University of Edinburgh, University of Edinburgh
    last_name: Fonseca
    name: Marcio Fonseca
    semantic_scholar_id: https://www.semanticscholar.org/author/2142910610
    username: ~Marcio_Fonseca1
  - dblp_id: https://dblp.org/pid/04/5629
    emails: '****@inf.ed.ac.uk'
    first_name: Shay
    homepage: http://homepages.inf.ed.ac.uk/scohen
    institution: University of Edinburgh
    last_name: Cohen
    middle_name: B
    name: Shay B Cohen
    semantic_scholar_id: https://www.semanticscholar.org/author/Shay-B.-Cohen/40146204
    username: ~Shay_B_Cohen6
  decision: toMainConference
  end_page: 8184
  file: 898.pdf
  id: 898
  num_pages: 14
  openreview_id: 3Ovsg8aZd8
  pdf_file: 7d51554284091d018d07adbb9e6846bd3e334fbf.pdf
  start_page: 8171
  title: 'LeanReasoner: Boosting Complex Logical Reasoning with Lean'
- abstract: 'Many large language models (LLMs) struggle to consistently generate UI
    code that compiles and produces visually relevant designs. Existing approaches
    to improve generation rely either on expensive human feedback or distilling a
    proprietary model. In this paper, we explore the use of automated feedback (compilers
    and multi-modal models) to guide LLMs to generate high-quality UI code. Our method
    starts with an existing LLM and iteratively produces improved models by self-generating
    a large synthetic dataset using an original model, applying automated tools to
    aggressively filter, score, and de-duplicate the data into a refined higher quality
    dataset, and producing a new LLM by finetuning the original on the refined dataset.

    We applied our approach to several open-source LLMs and compared the resulting
    performance to baseline models with both automated metrics and human preferences.

    Our results show the resulting models outperform all other downloadable baselines
    and approach the performance of larger proprietary models.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - dblp_id: https://dblp.org/pid/78/5374
    emails: '****@cmu.edu'
    first_name: Jason
    google_scholar_id: https://scholar.google.com/citations?user=aKqh7zIAAAAJ
    homepage: http://jasonwunix.com
    last_name: Wu
    name: Jason Wu
    orcid: https://orcid.org/0000-0001-5101-0557
    semantic_scholar_id: https://www.semanticscholar.org/author/Jason-Wu/2109186166
    username: ~Jason_Wu1
  decision: toMainConference
  end_page: 8199
  file: 906.pdf
  id: 906
  num_pages: 15
  openreview_id: P4JFROtUov
  pdf_file: 513bd3594f3a1de90ca2855ed04ce2846abc3922.pdf
  start_page: 8185
  title: 'UICoder: Finetuning Large Language Models to Generate User Interface Code
    through Automated Feedback'
- abstract: 'Multilingual pretraining has been a successful solution to the challenges
    posed by the lack of resources for languages. These models can transfer knowledge
    to target languages with minimal or no examples. Recent research suggests that
    monolingual models also have a similar capability, but the mechanisms behind this
    transfer remain unclear. Some studies have explored factors like language contamination
    and syntactic similarity. An emerging line of research suggests that the representations
    learned by language models contain two components: a language-specific and a language-agnostic
    component. The latter is responsible for transferring a more universal knowledge.
    However, there is a lack of comprehensive exploration of these properties across
    diverse target languages. To investigate this hypothesis, we conducted an experiment
    inspired by the work on the Scaling Laws for Transfer. We measured the amount
    of data transferred from a source language to a target language and found that
    models initialized from diverse languages perform similarly to a target language
    in a cross-lingual setting. This was surprising because the amount of data transferred
    to 10 diverse target languages, such as Spanish, Korean, and Finnish, was quite
    similar. We also found evidence that this transfer is not related to language
    contamination or language proximity, which strengthens the hypothesis that the
    model also relies on language-agnostic knowledge. Our experiments have opened
    up new possibilities for measuring how much data represents the language-agnostic
    representations learned during pretraining.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multilinguality and Language Diversity
  authors:
  - emails: '****@g.unicamp.br'
    first_name: Leandro
    google_scholar_id: https://scholar.google.com/citations?hl=en&authuser=2&user=DI9radEAAAAJ
    institution: Universidade Estadual de Campinas
    last_name: De Souza
    middle_name: Rodrigues
    name: Leandro Rodrigues De Souza
    username: ~Leandro_Rodrigues_De_Souza1
  - emails: '****@dac.unicamp.br'
    first_name: Thales
    google_scholar_id: https://scholar.google.com/citations?user=GFZ5iz8AAAAJ&hl=en
    last_name: Almeida
    middle_name: Sales
    name: Thales Sales Almeida
    orcid: https://orcid.org/0009-0006-9568-9331
    username: ~Thales_Sales_Almeida1
  - emails: '****@dca.fee.unicamp.br'
    first_name: Roberto
    google_scholar_id: https://scholar.google.com.br/citations?user=IQt4hvoAAAAJ
    institution: University of Campinas, Universidade Estadual de Campinas
    last_name: Lotufo
    name: Roberto Lotufo
    orcid: https://orcid.org/0000-0002-5652-0852
    username: ~Roberto_Lotufo1
  - dblp_id: ''
    emails: '****@gmail.com'
    first_name: Rodrigo
    google_scholar_id: ''
    homepage: ''
    last_name: Frassetto Nogueira
    name: Rodrigo Frassetto Nogueira
    username: ~Rodrigo_Frassetto_Nogueira1
  decision: toMainConference
  end_page: 8211
  file: 907.pdf
  id: 907
  num_pages: 12
  openreview_id: nEkdU2U5I4
  pdf_file: 358e6230a4112c3b040179c32c5bd9e0da8dab79.pdf
  start_page: 8200
  title: Measuring Cross-lingual Transfer in Bytes
- abstract: Instructing large language models (LLMs) to solve elementary school math
    problems has shown great success using Chain of Thought (CoT). However, the CoT
    approach relies on an LLM to generate a sequence of arithmetic calculations which
    can be prone to cascaded calculation errors. We hypothesize that an LLM should
    focus on extracting predicates and generating symbolic formulas from the math
    problem description so that the underlying calculation can be done via an external
    code interpreter. We investigate using LLM to generate Prolog programs to solve
    mathematical questions. Experimental results show that our Prolog-based arithmetic
    problem-solving outperforms CoT generation in the GSM8K benchmark across three
    distinct LLMs. In addition, given the insensitive ordering of predicates and symbolic
    formulas in Prolog, we propose to permute the ground truth predicates for more
    robust LLM training via data augmentation.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Question Answering
  authors:
  - emails: '****@nyu.edu'
    first_name: Xiaocheng
    homepage: https://github.com/yxc-cyber
    last_name: Yang
    name: Xiaocheng Yang
    username: ~Xiaocheng_Yang2
  - emails: '****@nyu.edu'
    first_name: Bingsen
    homepage: https://balechen.github.io
    last_name: Chen
    name: Bingsen Chen
    username: ~Bingsen_Chen1
  - emails: '****@nyu.edu'
    first_name: Yik-Cheung
    homepage: https://shanghai.nyu.edu/academics/faculty/directory/yik-cheung-wilson-tam
    institution: New York University
    last_name: Tam
    name: Yik-Cheung Tam
    username: ~Yik-Cheung_Tam2
  decision: toMainConference
  end_page: 8223
  file: 910.pdf
  id: 910
  num_pages: 12
  openreview_id: XwEuZ01g4M
  pdf_file: 19f4aaac2859f3886f4cc3a2b6ee32ff28d3049e.pdf
  start_page: 8212
  title: 'Arithmetic Reasoning with LLM: Prolog Generation \& Permutation'
- abstract: 'Content Warning: This paper contains examples of misgendering and erasure
    that could be offensive and potentially triggering.


    Misgendering, the act of incorrectly addressing someone''s gender, inflicts serious
    harm and is pervasive in everyday technologies, yet there is a notable lack of
    research to combat it. We are the first to address this lack of research into
    interventions for misgendering by conducting a survey of gender-diverse individuals
    in the US to understand perspectives about automated interventions for text-based
    misgendering. Based on survey insights on the prevalence of misgendering, desired
    solutions, and associated concerns, we introduce a misgendering interventions
    task and evaluation dataset, MisgenderMender. We define the task with two sub-tasks:
    (i) detecting misgendering, followed by (ii) correcting misgendering where misgendering
    is present, in domains where editing is appropriate. MisgenderMender comprises
    3790 instances of social media content and LLM-generations about non-cisgender
    public figures, annotated for the presence of misgendering, with additional annotations
    for correcting misgendering in LLM-generated text. Using this dataset, we set
    initial benchmarks by evaluating existing NLP systems and highlighting challenges
    for future models to address. We release the full dataset, code, and demo at https://tamannahossainkay.github.io/misgendermender/'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Ethics, Bias, and Fairness
  authors:
  - emails: '****@uci.edu'
    first_name: Tamanna
    homepage: https://www.tamanna-hossain-kay.com/
    institution: University of California, Irvine
    last_name: Hossain
    name: Tamanna Hossain
    username: ~Tamanna_Hossain1
  - dblp_id: https://dblp.org/pid/222/1630
    emails: '****@gmail.com'
    first_name: Sunipa
    google_scholar_id: https://scholar.google.com/citations?user=EV1DgP0AAAAJ&hl=en
    homepage: https://sunipa.github.io/
    institution: Google
    last_name: Dev
    name: Sunipa Dev
    username: ~Sunipa_Dev1
  - dblp_id: https://dblp.org/pid/13/3568-1
    emails: '****@uci.edu'
    first_name: Sameer
    google_scholar_id: https://scholar.google.com/citations?user=-hGZC54AAAAJ
    homepage: http://sameersingh.org
    institution: University of California, Irvine and Allen Institute for Artificial
      Intelligence
    last_name: Singh
    name: Sameer Singh
    orcid: https://orcid.org/0000-0003-0621-6323
    semantic_scholar_id: https://www.semanticscholar.org/author/Sameer-Singh/34650964
    username: ~Sameer_Singh1
  decision: toMainConference
  end_page: 8244
  file: 911.pdf
  id: 911
  num_pages: 21
  openreview_id: Cn0OwEhhRq
  pdf_file: d97bd027f2cb115186127315a65523182c2bc299.pdf
  start_page: 8224
  title: 'MisgenderMender: A Community-Informed Approach to Interventions for Misgendering'
- abstract: 'We investigate two research questions: (1) how do machine translation
    (MT) and diacritization influence the performance of each other in a multi-task
    learning setting (2) the effect of keeping (vs. removing) diacritics on MT performance.
    We examine these two questions in both high-resource (HR) and low-resource (LR)
    settings across 55 different languages (36 African languages and 19 European languages).
    For (1), results show that diacritization significantly benefits MT in the LR
    scenario, doubling or even tripling performance for some languages, but harms
    MT in the HR scenario. We find that MT harms diacritization in LR but benefits
    significantly in HR for some languages. For (2), MT performance is similar regardless
    of diacritics being kept or removed. In addition, we propose two classes of metrics
    to measure the complexity of a diacritical system, finding these metrics to correlate
    positively with the performance of our diacritization models. Overall, our work
    provides insights for developing MT and diacritization systems under different
    data size conditions and may have implications that generalize beyond the 55 languages
    we investigate.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Translation
  authors:
  - emails: '****@student.ubc.ca'
    first_name: Wei-Rui
    homepage: https://weiruichen01.github.io/
    institution: University of British Columbia
    last_name: Chen
    name: Wei-Rui Chen
    username: ~Wei-Rui_Chen1
  - dblp_id: https://dblp.org/pid/167/9928
    emails: '****@ubc.ca'
    first_name: Ife
    google_scholar_id: https://scholar.google.co.id/citations?user=TXMQQngAAAAJ&hl=en&oi=ao
    institution: University of British Columbia
    last_name: Adebara
    name: Ife Adebara
    semantic_scholar_id: https://www.semanticscholar.org/author/Ife-Adebara/1983323
    username: ~Ife_Adebara1
  - dblp_id: https://dblp.org/pid/49/9389.html
    emails: '****@ubc.ca'
    first_name: Muhammad
    google_scholar_id: https://scholar.google.com/citations?user=SOjQhl8AAAAJ&hl=en&oi=ao
    homepage: https://mageed.arts.ubc.ca
    institution: University of British Columbia
    last_name: Abdul-Mageed
    name: Muhammad Abdul-Mageed
    orcid: https://orcid.org/0000-0002-8590-2040
    semantic_scholar_id: https://www.semanticscholar.org/author/Muhammad-Abdul-Mageed/1388437494
    username: ~Muhammad_Abdul-Mageed2
  decision: toMainConference
  end_page: 8287
  file: 912.pdf
  id: 912
  num_pages: 43
  openreview_id: wZxHzxEatv
  pdf_file: c293698a0f4e23b68009c5f36782e25306423a77.pdf
  start_page: 8245
  title: Interplay of Machine Translation, Diacritics, and Diacritization
- abstract: In the realm of Large Language Models (LLMs), the balance between instruction
    data quality and quantity is a focal point. Recognizing this, we introduce a self-guided
    methodology for LLMs to autonomously discern and select cherry samples from open-source
    datasets, effectively minimizing manual curation and potential cost for instruction
    tuning an LLM. Our key innovation, the Instruction-Following Difficulty (IFD)
    metric, emerges as a pivotal metric to identify discrepancies between a model's
    expected responses and its intrinsic generation capability. Through the application
    of IFD, cherry samples can be pinpointed, leading to a marked uptick in model
    training efficiency. Empirical validations on datasets like Alpaca and WizardLM
    underpin our findings; with a mere 10\% of original data input, our strategy showcases
    improved results. This synthesis of self-guided cherry-picking and the IFD metric
    signifies a transformative leap in the instruction tuning of LLMs, promising both
    efficiency and resource-conscious advancements. Codes, data, and models are available.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Learning for NLP
  authors:
  - emails: '****@gmail.com'
    first_name: Ming
    google_scholar_id: https://scholar.google.com/citations?user=MpEoJegAAAAJ&hl=en
    homepage: https://mingliiii.github.io/
    institution: University of Maryland, College Park
    last_name: Li
    name: Ming Li
    orcid: https://orcid.org/0009-0001-6491-4827
    semantic_scholar_id: https://www.semanticscholar.org/author/Ming-Li/2150655891
    username: ~Ming_Li18
  - emails: '****@my.cityu.edu.hk'
    first_name: Yong
    institution: Pingan Technology
    last_name: Zhang
    name: Yong Zhang
    semantic_scholar_id: https://www.semanticscholar.org/author/Yong-Zhang/2144289768
    username: ~Yong_Zhang13
  - emails: '****@live.com'
    first_name: Zhitao
    homepage: http://pingan.com
    institution: Pingan Technology
    last_name: Li
    name: Zhitao Li
    username: ~Zhitao_Li1
  - emails: '****@umd.edu'
    first_name: Jiuhai
    homepage: https://www.linkedin.com/in/jiuhai-chen-6a486715a/
    last_name: Chen
    name: Jiuhai Chen
    username: ~Jiuhai_Chen1
  - dblp_id: https://dblp.org/pid/151/6212
    emails: '****@umd.edu'
    first_name: Lichang
    google_scholar_id: https://scholar.google.com/citations?hl=zh-CN&user=XesgJyUAAAAJ
    last_name: Chen
    name: Lichang Chen
    semantic_scholar_id: https://www.semanticscholar.org/author/Lichang-Chen/2108451006
    username: ~Lichang_Chen2
  - dblp_id: https://dblp.org/pid/86/797-1
    emails: '****@pingan.com.cn'
    first_name: Ning
    homepage: https://largeaudiomodel.com/author/ning-cheng/
    institution: Pingan Technology
    last_name: Cheng
    name: Ning Cheng
    semantic_scholar_id: https://www.semanticscholar.org/author/Ning-Cheng/145292435
    username: ~Ning_Cheng2
  - dblp_id: https://dblp.org/pid/70/8380
    emails: '****@188.com'
    first_name: Jianzong
    google_scholar_id: https://scholar.google.co.uk/citations?user=noi4qcUAAAAJ
    homepage: https://largeaudiomodel.com/author/jianzong-wang/
    institution: Pingan Technology
    last_name: Wang
    name: Jianzong Wang
    orcid: https://orcid.org/0000-0002-9237-4231
    semantic_scholar_id: https://www.semanticscholar.org/author/Jianzong-Wang/66063851
    username: ~Jianzong_Wang2
  - dblp_id: https://dblp.org/pid/88/8205-1
    emails: '****@umd.edu'
    first_name: Tianyi
    google_scholar_id: https://scholar.google.com/citations?user=OKvgizMAAAAJ&hl=en
    homepage: https://tianyizhou.github.io/
    institution: University of Maryland, College Park
    last_name: Zhou
    name: Tianyi Zhou
    orcid: https://orcid.org/0000-0001-5348-0632
    semantic_scholar_id: https://www.semanticscholar.org/author/Tianyi-Zhou/1805655
    username: ~Tianyi_Zhou2
  - dblp_id: https://dblp.org/pid/67/4008
    emails: '****@pingan.com.cn'
    first_name: Jing
    google_scholar_id: https://scholar.google.com/citations?user=mcBd8KUAAAAJ&hl=zh-CN&oi=ao
    homepage: http://www.cs.cmu.edu/~jxiao/
    institution: Pingan Group
    last_name: Xiao
    name: Jing Xiao
    username: ~Jing_Xiao3
  decision: toMainConference
  end_page: 8321
  file: 913.pdf
  id: 913
  num_pages: 34
  openreview_id: PU0z7eGwQs
  pdf_file: 2701f7f364b9961cf7ce23c7fe51cd79290a8a57.pdf
  start_page: 8288
  title: 'From Quantity to Quality: Boosting LLM Performance with Self-Guided Data
    Selection for Instruction Tuning'
- abstract: Reinforcement learning from human feedback (RLHF) is a vital strategy
    for enhancing model capability in language models. However, annotating preference
    data for RLHF is a resource-intensive and creativity-demanding process, while
    existing automatic generation methods face limitations in data diversity and quality.
    In response, we present Safer-Instruct, a novel pipeline for automatically constructing
    large-scale preference data. Our approach leverages reversed instruction tuning,
    instruction induction, and expert model evaluation to efficiently generate high-quality
    preference data without human annotators. To verify the effectiveness of Safer-Instruct,
    we apply the pipeline to construct a safety preference dataset as a case study.
    Finetuning an Alpaca model on this synthetic dataset not only demonstrates improved
    harmlessness but also outperforms models fine-tuned on human-annotated safety
    preference data, all the while maintaining a competitive edge in downstream tasks.
    Importantly, our Safer-Instruct framework is versatile and can be applied to generate
    preference data across various domains, extending its utility beyond safety preferences.
    It addresses the challenges in preference data acquisition and advances the development
    of more capable and responsible AI systems. For dataset and code implementation,
    see https://github.com/uscnlp-lime/safer-instruct/.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - emails: '****@usc.edu'
    first_name: Taiwei
    google_scholar_id: https://scholar.google.com/citations?user=yv6nCnMAAAAJ&hl=en
    homepage: https://maksimstw.github.io/
    last_name: Shi
    name: Taiwei Shi
    username: ~Taiwei_Shi1
  - emails: '****@usc.edu'
    first_name: Kai
    google_scholar_id: https://scholar.google.com/citations?user=ErRPAGgAAAAJ
    last_name: Chen
    name: Kai Chen
    username: ~Kai_Chen29
  - dblp_id: https://dblp.org/pid/59/2379
    emails: '****@usc.edu'
    first_name: Jieyu
    google_scholar_id: https://scholar.google.com/citations?user=9VaGBCQAAAAJ&hl=en
    homepage: http://jyzhao.net/
    institution: University of Southern California
    last_name: Zhao
    name: Jieyu Zhao
    semantic_scholar_id: https://www.semanticscholar.org/author/Jieyu-Zhao/33524946
    username: ~Jieyu_Zhao1
  decision: toMainConference
  end_page: 8337
  file: 917.pdf
  id: 917
  num_pages: 16
  openreview_id: S20wD4KZO9
  pdf_file: b44f827eb89af7426aa278bc00d2618c30f97e68.pdf
  start_page: 8322
  title: 'Safer-Instruct: Aligning Language Models with Automated Preference Data'
- abstract: 'We investigate pre-training techniques for abstractive multi-document
    summarization (MDS), which is much less studied than summarizing single documents.
    Though recent work has demonstrated the effectiveness of highlighting information
    salience for pre-training strategy design, they struggle to generate abstractive
    and reflective summaries, which are critical properties for MDS. To this end,
    we present **PELMS**, a pre-trained model that uses pre-training objectives based
    on semantic coherence heuristics and faithfulness constraints together with unlabeled
    multi-document inputs, to promote the generation of concise, fluent, and faithful
    summaries. To support the training of PELMS, we compile **MultiPT**, a multi-document
    pre-training corpus containing over 93 million documents to form more than 3

    million unlabeled topic-centric document clusters, covering diverse genres such
    as product reviews, news, and general knowledge. We perform extensive evaluation
    of PELMS in low-shot settings on a wide range of MDS datasets. Our approach consistently
    outperforms competitive comparisons with respect to overall informativeness, abstractiveness,
    coherence, and faithfulness, and with minimal fine-tuning can match performance
    of language models at a much larger scale (e.g., GPT-4).'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Summarization
  authors:
  - emails: '****@umich.edu'
    first_name: Joseph
    google_scholar_id: https://scholar.google.com/citations?user=3ZxqnhwAAAAJ&hl=en
    institution: University of Michigan - Ann Arbor
    last_name: Peper
    middle_name: J
    name: Joseph J Peper
    username: ~Joseph_J_Peper1
  - emails: '****@umich.edu'
    first_name: Wenzhao
    last_name: Qiu
    name: Wenzhao Qiu
    username: ~Wenzhao_Qiu1
  - dblp_id: https://dblp.org/pid/49/3800-8
    emails: '****@umich.edu'
    first_name: Lu
    google_scholar_id: https://scholar.google.com/citations?user=uczqEdUAAAAJ&hl=en
    homepage: https://web.eecs.umich.edu/~wangluxy/
    institution: University of Michigan
    last_name: Wang
    name: Lu Wang
    username: ~Lu_Wang9
  decision: toMainConference
  end_page: 8360
  file: 918.pdf
  id: 918
  num_pages: 23
  openreview_id: Z5cnnT3hmP
  pdf_file: e886efb361c87fa4f3b510f1077ea168480a7516.pdf
  start_page: 8338
  title: 'PELMS: Pre-training for Effective Low-Shot Multi-Document Summarization'
- abstract: "Despite the high performances of large language models (LLMs) across\
    \ numerous benchmarks, recent research has unveiled their suffering from hallucinations\
    \ and unfaithful reasoning. This work studies a type of hallucination induced\
    \ by semantic associations. We investigate to what extent LLMs take shortcuts\
    \ from certain keyword/entity biases in the prompt instead of following correct\
    \ reasoning paths. To quantify this phenomenon, we propose a novel probing method\
    \ and benchmark called EUREQA. EUREQA is an entity-searching task where a model\
    \ finds a missing entity based on described multi-hop relations with other entities.\
    \ These deliberately designed multi-hop relations create deceptive semantic associations,\
    \ and models must stick to the correct reasoning path instead of incorrect shortcuts\
    \ to find the correct answer.\nExperiments show that existing LLMs cannot follow\
    \ correct reasoning paths and resist the attempt of greedy shortcuts, with GPT-4\
    \ only achieving 62\\% accuracy. Analyses provide further evidence that LLMs rely\
    \ on semantic biases to solve the task \ninstead of proper reasoning, questioning\
    \ the validity and generalizability of current LLMs' high performances."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - emails: '****@gmail.com'
    first_name: Bangzheng
    google_scholar_id: https://scholar.google.com/citations?user=UcegV-cAAAAJ&hl=en
    institution: University of Southern California
    last_name: Li
    name: Bangzheng Li
    username: ~Bangzheng_Li1
  - dblp_id: https://dblp.org/pid/219/5276
    emails: '****@seas.upenn.edu'
    first_name: Ben
    google_scholar_id: https://scholar.google.com/citations?user=0Cb4mtIAAAAJ
    homepage: http://xuanyu.me
    institution: University of Pennsylvania
    last_name: Zhou
    name: Ben Zhou
    semantic_scholar_id: https://www.semanticscholar.org/author/Ben-Zhou/145360756
    username: ~Ben_Zhou1
  - dblp_id: https://dblp.org/pid/52/3194-60
    emails: '****@gmail.com'
    first_name: Fei
    google_scholar_id: https://scholar.google.com/citations?user=N1O2KT8AAAAJ
    homepage: https://feiwang96.github.io/
    institution: University of Southern California
    last_name: Wang
    name: Fei Wang
    semantic_scholar_id: https://www.semanticscholar.org/author/F.-Wang/47939052
    username: ~Fei_Wang12
  - dblp_id: https://dblp.org/pid/118/4769
    emails: '****@seas.upenn.edu'
    first_name: Xingyu
    google_scholar_id: https://scholar.google.com/citations?user=5p_uBNQAAAAJ&hl=en
    homepage: https://zeyofu.github.io/
    institution: University of Pennsylvania, University of Pennsylvania
    last_name: Fu
    name: Xingyu Fu
    semantic_scholar_id: https://www.semanticscholar.org/author/36021248
    username: ~Xingyu_Fu1
  - dblp_id: https://dblp.org/pid/r/DanRoth
    emails: '****@seas.upenn.edu'
    first_name: Dan
    google_scholar_id: https://scholar.google.com/citations?user=E-bpPWgAAAAJ&hl=en
    homepage: https://www.cis.upenn.edu/~danroth/
    institution: Amazon and University of Pennsylvania
    last_name: Roth
    name: Dan Roth
    semantic_scholar_id: https://www.semanticscholar.org/author/D.-Roth/144590225
    username: ~Dan_Roth3
  - dblp_id: https://dblp.org/pid/173/2608
    emails: '****@ucdavis.edu'
    first_name: Muhao
    google_scholar_id: https://scholar.google.com/citations?user=k79yEZkAAAAJ&hl=en
    homepage: https://muhaochen.github.io/
    institution: University of California, Davis and University of Southern California
    last_name: Chen
    name: Muhao Chen
    orcid: https://orcid.org/0000-0003-0118-3147
    semantic_scholar_id: https://www.semanticscholar.org/author/Muhao-Chen/1998918
    username: ~Muhao_Chen1
  decision: toMainConference
  end_page: 8374
  file: 920.pdf
  id: 920
  num_pages: 14
  openreview_id: UZ5U8JwXMX
  pdf_file: 1a38be79c75d6dd350642228b1b8d4860fcf0aae.pdf
  start_page: 8361
  title: 'Deceptive Semantic Shortcuts on Reasoning Chains: How Far Can Models Go
    without Hallucination?'
- abstract: Sentiment analysis, a fundamental aspect of Natural Language Processing
    (NLP), involves the classification of emotions, opinions, and attitudes in text
    data. In the context of India, with its vast linguistic diversity and low-resource
    languages, the challenge is to support sentiment analysis in numerous Indian languages.
    This study explores the use of machine translation to bridge this gap. The investigation
    examines the feasibility of machine translation for creating sentiment analysis
    datasets in 22 Indian languages. Google Translate, with its extensive language
    support, is employed for this purpose in translating the Sentiment140 dataset.
    The study aims to provide insights into the practicality of using machine translation
    in the context of India's linguistic diversity for sentiment analysis datasets.
    Our findings indicate that a dataset generated using Google Translate has the
    potential to serve as a foundational framework for tackling the low-resource challenges
    commonly encountered in sentiment analysis for Indian languages.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - emails: '****@iitg.ac.in'
    first_name: Saurabh
    institution: Indian Institute of Technology, Guwahati
    last_name: Kumar
    name: Saurabh Kumar
    orcid: https://orcid.org/0000-0002-0717-9512
    username: ~Saurabh_Kumar3
  - emails: '****@iitg.ac.in'
    first_name: Ranbir
    homepage: https://www.iitg.ac.in/ranbir/research/
    institution: Indian Institute of Technology, Guwahati, Dhirubhai Ambani Institute
      Of Information and Communication Technology
    last_name: Sanasam
    middle_name: Singh
    name: Ranbir Singh Sanasam
    username: ~Ranbir_Singh_Sanasam1
  - dblp_id: https://dblp.org/pid/20/4077.html
    emails: '****@iitg.ac.in'
    first_name: Sukumar
    google_scholar_id: https://scholar.google.co.in/citations?hl=en&user=ChtruacAAAAJ
    homepage: https://www.iitg.ac.in/sukumar
    institution: Indian Institute of Technology, Guwahati
    last_name: Nandi
    name: Sukumar Nandi
    orcid: https://orcid.org/my-orcid?orcid=0000-0002-5869-10
    username: ~Sukumar_Nandi1
  decision: toMainConference
  end_page: 8384
  file: 921.pdf
  id: 921
  num_pages: 10
  openreview_id: SZfgWBYrIg
  pdf_file: 64d9e672b2e85ce069fd2e75f4a9a0fd0e0d68b2.pdf
  start_page: 8375
  title: 'IndiSentiment140: Sentiment Analysis Dataset for Indian Languages with Emphasis
    on Low-Resource Languages using Machine Translation'
- abstract: 'There has been limited success for dense retrieval models in multilingual
    retrieval, due to uneven and scarce training data available across multiple languages.
    Synthetic training data generation is promising (e.g., InPars or Promptagator),
    but has been investigated only for English. Therefore, to study model capabilities
    across both cross-lingual and monolingual retrieval tasks, we develop **SWIM-IR**,
    a synthetic retrieval training dataset containing 33 (high to very-low resource)
    languages for fine-tuning multilingual dense retrievers without requiring any
    human supervision. To construct SWIM-IR, we propose SAP (summarize-then-ask prompting),
    where the large language model (LLM) generates a textual summary prior to the
    query generation step. SAP assists the LLM in generating informative queries in
    the target language. Using SWIM-IR, we explore synthetic fine-tuning of multilingual
    dense retrieval models and evaluate them robustly on three retrieval benchmarks:
    XOR-Retrieve (cross-lingual), MIRACL (monolingual) and XTREME-UP (cross-lingual).  Our
    models, called SWIM-X, are competitive with human-supervised dense retrieval models,
    e.g., mContriever-X, finding that SWIM-IR can cheaply substitute for expensive
    human-labeled retrieval training data. SWIM-IR dataset and SWIM-X models are available
    at: https://github.com/google-research-datasets/SWIM-IR.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Retrieval and Text Mining
  authors:
  - dblp_id: https://dblp.org/pid/276/6898
    emails: '****@gmail.com'
    first_name: Nandan
    google_scholar_id: https://scholar.google.com/citations?user=CE9GJoMAAAAJ&hl=en
    institution: University of Waterloo
    last_name: Thakur
    name: Nandan Thakur
    orcid: https://orcid.org/0000-0001-6107-2460
    semantic_scholar_id: https://www.semanticscholar.org/author/Nandan-Thakur/47583894
    username: ~Nandan_Thakur1
  - dblp_id: https://dblp.org/pid/161/2449
    emails: '****@google.com'
    first_name: Jianmo
    google_scholar_id: https://scholar.google.com/citations?user=VECFLiAAAAAJ&hl=en&oi=ao
    institution: Google and Google
    last_name: Ni
    name: Jianmo Ni
    username: ~Jianmo_Ni2
  - emails: '****@google.com'
    first_name: Gustavo
    google_scholar_id: https://scholar.google.com/citations?user=VXxG1RIAAAAJ&hl=en
    institution: Google
    last_name: Hernandez Abrego
    name: Gustavo Hernandez Abrego
    username: ~Gustavo_Hernandez_Abrego1
  - dblp_id: https://dblp.org/pid/156/0158
    emails: '****@cs.cmu.edu'
    first_name: John
    institution: Google DeepMind
    last_name: Wieting
    middle_name: Frederick
    name: John Frederick Wieting
    username: ~John_Frederick_Wieting1
  - dblp_id: https://dblp.org/pid/00/7739
    emails: '****@uwaterloo.ca'
    first_name: Jimmy
    homepage: https://cs.uwaterloo.ca/~jimmylin/
    institution: University of Waterloo
    last_name: Lin
    name: Jimmy Lin
    username: ~Jimmy_Lin2
  - dblp_id: https://dblp.org/pid/16/6461
    emails: '****@acm.org'
    first_name: Daniel
    google_scholar_id: https://scholar.google.com/citations?user=BrT1NW8AAAAJ&hl=en&oi=ao
    homepage: https://www.ischool.berkeley.edu/people/daniel-cer
    institution: Google
    last_name: Cer
    name: Daniel Cer
    orcid: https://orcid.org/0000-0002-4681-4371
    semantic_scholar_id: https://www.semanticscholar.org/author/Daniel-Matthew-Cer/46724030
    username: ~Daniel_Cer1
  decision: toMainConference
  end_page: 8410
  file: 922.pdf
  id: 922
  num_pages: 26
  openreview_id: oBbVJiN45Y
  pdf_file: b0ea5f39b547aae038e60efc810d2261c3ddf612.pdf
  start_page: 8385
  title: Leveraging LLMs for Synthesizing Training Data Across Many Languages in Multilingual
    Dense Retrieval
- abstract: 'There are several linguistic claims about situations where words are
    more likely to be used as metaphors.

    However, few studies have sought to verify such claims with large corpora.

    This study entails a large-scale, corpus-based analysis of certain existing claims
    about verb metaphors, by applying metaphor detection to sentences extracted from
    Common Crawl and using the statistics obtained from the results.

    The verification results indicate that the direct objects of verbs used as metaphors
    tend to have lower degrees of concreteness, imageability, and familiarity, and
    that metaphors are more likely to be used in emotional and subjective sentences.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Linguistic theories, Cognitive Modeling and Psycholinguistics
  authors:
  - emails: '****@s.mail.nagoya-u.ac.jp'
    first_name: Kotaro
    homepage: https://github.com/kotao2700
    last_name: Aono
    name: Kotaro Aono
    username: ~Kotaro_Aono1
  - dblp_id: https://dblp.org/pid/17/757
    emails: '****@i.nagoya-u.ac.jp'
    first_name: Ryohei
    google_scholar_id: https://scholar.google.com/citations?user=g9mNQ9MAAAAJ&hl=en
    homepage: http://cr.fvcrc.i.nagoya-u.ac.jp/~sasano/index-e.html
    institution: Nagoya University
    last_name: Sasano
    name: Ryohei Sasano
    semantic_scholar_id: https://www.semanticscholar.org/author/Ryohei-Sasano/2293543
    username: ~Ryohei_Sasano2
  - dblp_id: https://dblp.org/pid/24/4299-3.html
    emails: '****@i.nagoya-u.ac.jp'
    first_name: Koichi
    google_scholar_id: https://scholar.google.com/citations?user=IaZThNIAAAAJ&hl=ja
    homepage: http://cr.fvcrc.i.nagoya-u.ac.jp/~takedasu/index-e.html
    institution: Nagoya University
    last_name: Takeda
    name: Koichi Takeda
    username: ~Koichi_Takeda1
  decision: toMainConference
  end_page: 8419
  file: 924.pdf
  id: 924
  num_pages: 9
  openreview_id: HNv0DgsLsC
  pdf_file: f1182702956335e7544d8ba412f5628b338e943a.pdf
  start_page: 8411
  title: Verifying Claims About Metaphors with Large-Scale Automatic Metaphor Identification
- abstract: "Recent advances in named entity recognition (NER) have pushed the boundary\
    \ of the task to incorporate visual signals, leading to many variants, including\
    \ multi-modal NER (MNER) or grounded MNER (GMNER). \nA key challenge to these\
    \ tasks is that the model should be able to generalize to the entities unseen\
    \ during the training, and should be able to handle the training samples with\
    \ noisy annotations.\n\nTo address this obstacle, we propose SCANNER (Span CANdidate\
    \ detection and recognition for NER), a model capable of effectively handling\
    \ all three NER variants.\nSCANNER is a two-stage structure; we extract entity\
    \ candidates in the first stage and use it as a query to get knowledge, effectively\
    \ pulling knowledge from various sources.\nWe can boost our performance by utilizing\
    \ this entity-centric extracted knowledge to address unseen entities.\nFurthermore,\
    \ to tackle the challenges arising from noisy annotations in NER datasets, we\
    \ introduce a novel self-distillation method, enhancing the robustness and accuracy\
    \ of our model in processing training data with inherent uncertainties.\n\nOur\
    \ approach demonstrates competitive performance on the NER benchmark and surpasses\
    \ existing methods on both MNER and GMNER benchmarks.\nFurther analysis shows\
    \ that the proposed distillation and knowledge utilization methods improve the\
    \ performance of our model on various benchmarks."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Extraction
  authors:
  - emails: '****@gmail.com'
    first_name: Hyunjong
    homepage: https://github.com/OKHYUNJONG
    last_name: Ok
    name: Hyunjong Ok
    username: ~Hyunjong_Ok1
  - dblp_id: https://dblp.org/pid/315/9685
    emails: '****@navercorp.com'
    first_name: Taeho
    google_scholar_id: https://scholar.google.co.kr/citations?user=cV4h5MsAAAAJ&hl=ko
    institution: NAVER Cloud
    last_name: Kil
    name: Taeho Kil
    orcid: https://orcid.org/0000-0003-1607-2079
    username: ~Taeho_Kil1
  - emails: '****@navercorp.com'
    first_name: Sukmin
    homepage: https://github.com/min1321
    institution: NAVER
    last_name: Seo
    name: Sukmin Seo
    username: ~Sukmin_Seo1
  - dblp_id: https://dblp.org/pid/78/6080-1
    emails: '****@postech.ac.kr'
    first_name: Jaeho
    google_scholar_id: https://scholar.google.com/citations?user=t91zoQMAAAAJ&hl=en
    homepage: https://jaeho-lee.github.io
    institution: Google and Pohang University of Science and Technology
    last_name: Lee
    name: Jaeho Lee
    semantic_scholar_id: https://www.semanticscholar.org/author/Jaeho-Lee/48173961
    username: ~Jaeho_Lee3
  decision: toMainConference
  end_page: 8432
  file: 926.pdf
  id: 926
  num_pages: 13
  openreview_id: 1WUvfx8iWs
  pdf_file: 8a68647aaac8aaf2c09c2e8e112f1701584be4ba.pdf
  start_page: 8420
  title: 'SCANNER: Knowledge-Enhanced Approach for Robust Multi-modal Named Entity
    Recognition of Unseen Entities'
- abstract: Metaphor detection is a challenging task in figurative language processing,
    which aims to distinguish between metaphorical and literal expressions in text.
    Existing methods tackle metaphor detection via training or fine-tuning discriminative
    models on labeled data. However, these approaches struggle to explain the underlying
    reasoning process behind the metaphorical/literal judgment. Recently, large language
    models (LLMs) have shown promise in language reasoning tasks. Although promising,
    LLM-based methods for metaphor detection and reasoning are still faced with the
    challenging issue of bringing the explainable concepts for metaphor reasoning
    and their linguistic manifestation. To fill this gap, we propose a novel Theory
    guided Scaffolding Instruction (TSI) framework that instructs an LLM to infer
    the underlying reasoning process of metaphor detection guided by metaphor theories
    for the first time. Our work is inspired by a pedagogical strategy called scaffolding
    instruction, which encourages educators to provide questioning and support as
    scaffolding so as to assist learners in constructing the understanding of pedagogical
    goals step by step. We first construct a metaphor knowledge graph grounded in
    metaphor theory which serves as the instructional structure to obtain a series
    of scaffolding questions, directing the LLM to incrementally generate the reasoning
    process for metaphor understanding through dialogue interactions. During this
    theory guided instruction process, we explore the LLM's mastery boundary and provide
    the relevant knowledge as scaffolding support when the question is beyond the
    LLM's capability. Experimental results verify that our method significantly outperforms
    both the LLM-based reasoning methods and the SOTA methods in metaphor detection,
    indicating the facilitation of metaphor and instruction theories in guiding LLM-based
    reasoning process.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Retrieval and Text Mining
  authors:
  - emails: '****@ia.ac.cn'
    first_name: Yuan
    google_scholar_id: https://scholar.google.com.hk/citations?user=64aS6SgAAAAJ&hl=zh-CN
    last_name: Tian
    name: Yuan Tian
    orcid: https://orcid.org/0000-0002-8965-3942
    semantic_scholar_id: https://www.semanticscholar.org/author/Yuan-Tian/144966427
    username: ~Yuan_Tian10
  - emails: '****@ia.ac.cn'
    first_name: Nan
    google_scholar_id: https://scholar.google.com.hk/citations?user=oJcp__wAAAAJ&hl=zh-CN
    last_name: Xu
    name: Nan Xu
    username: ~Nan_Xu1
  - dblp_id: https://dblp.org/pid/16/2159
    emails: '****@ia.ac.cn'
    first_name: Wenji
    google_scholar_id: https://scholar.google.com/citations?user=h6m4X_AAAAAJ
    institution: Institute of automation, Chinese academy of science, Chinese Academy
      of Sciences
    last_name: Mao
    name: Wenji Mao
    username: ~Wenji_Mao1
  decision: toMainConference
  end_page: 8450
  file: 928.pdf
  id: 928
  num_pages: 18
  openreview_id: DejnuFcafm
  pdf_file: 43bed999c41d60963e1dbed0ebc8184dd4858a55.pdf
  start_page: 8433
  title: A Theory Guided Scaffolding Instruction Framework for LLM-Enabled Metaphor
    Reasoning
- abstract: 'Large language models (LLMs) are great at processing multiple natural
    language processing tasks, but their abilities are constrained by inferior performance
    with long context, slow inference speed, and the high cost of computing the results.
    Deploying LLMs with precise and informative context helps users process large-scale
    datasets more effectively and cost-efficiently. Existing works rely on compressing
    long prompt contexts into soft prompts. However, soft prompt compression encounters
    limitations in transferability across different LLMs, especially API-based LLMs.
    To this end, this work aims to compress lengthy prompts in the form of natural
    language with LLM transferability. This poses two challenges: (i) Natural Language
    (NL) prompts are incompatible with back-propagation, and (ii) NL prompts lack
    flexibility in imposing length constraints. In this work, we propose a Natural
    Language Prompt Encapsulation (Nano-Capsulator) framework compressing original
    prompts into NL formatted Capsule Prompt while maintaining prompt utility and
    transferability. Specifically, to tackle the first challenge, the Nano-Capsulator
    is optimized by a reward function that interacts with the proposed semantics preserving
    loss. To address the second question, the Nano-Capsulator is optimized by a reward
    function featuring length constraints. Experimental results demonstrate that the
    Capsule Prompt can reduce 81.4\% of the original length, decrease inference latency
    up to 4.5x, and save 80.1\% of budget overheads while providing transferability
    across diverse LLMs and different datasets.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Efficient/Low-Resource Methods for NLP
  authors:
  - dblp_id: https://dblp.org/pid/207/7875
    emails: '****@rice.edu'
    first_name: Yu-Neng
    institution: Rice University
    last_name: Chuang
    name: Yu-Neng Chuang
    username: ~Yu-Neng_Chuang1
  - dblp_id: https://dblp.org/pid/122/8740
    emails: '****@gmail.com'
    first_name: Tianwei
    institution: Samsung Research America
    last_name: Xing
    name: Tianwei Xing
    username: ~Tianwei_Xing1
  - emails: '****@tamu.edu'
    first_name: Chia-Yuan
    last_name: Chang
    name: Chia-Yuan Chang
    username: ~Chia-Yuan_Chang3
  - dblp_id: https://dblp.org/pid/196/8629.html
    emails: '****@rice.edu'
    first_name: Zirui
    homepage: https://warai-0toko.github.io/
    institution: Rice University
    last_name: Liu
    name: Zirui Liu
    username: ~Zirui_Liu1
  - dblp_id: https://dblp.org/pid/34/6795
    emails: '****@samsung.com'
    first_name: Xun
    google_scholar_id: https://scholar.google.com/citations?user=70mGgQoAAAAJ&hl=en
    institution: Samsung Research America
    last_name: Chen
    name: Xun Chen
    username: ~Xun_Chen1
  - dblp_id: https://dblp.org/pid/24/7536
    emails: '****@rice.edu'
    first_name: Xia
    google_scholar_id: https://scholar.google.com.tw/citations?user=pcCS60IAAAAJ
    homepage: http://faculty.cs.tamu.edu/xiahu/
    institution: Rice University
    last_name: Hu
    name: Xia Hu
    username: ~Xia_Hu4
  decision: toMainConference
  end_page: 8462
  file: 932.pdf
  id: 932
  num_pages: 12
  openreview_id: WR2i6Fi3Qn
  pdf_file: f96db78fc5a5316ef3e0e6c3ff2f82959a027176.pdf
  start_page: 8451
  title: Learning to Compress Prompt in Natural Language Formats
- abstract: "We introduce InstructABSA, an instruction learning paradigm for Aspect-Based\
    \ Sentiment Analysis (ABSA) subtasks.\nOur method introduces positive, negative,\
    \ and neutral examples to each training sample, and instruction tune the model\
    \ (T$k$-Instruct) for ABSA subtasks, yielding significant performance improvements.\
    \ Experimental results on the Sem Eval 2014, 15, and 16 datasets demonstrate that\
    \ InstructABSA outperforms the previous state-of-the-art (SOTA) approaches on\
    \ Term Extraction (ATE), Sentiment Classification(ATSC) and Sentiment Pair Extraction\
    \ (ASPE) subtasks.\nIn particular, InstructABSA outperforms the previous state-of-the-art\
    \ (SOTA) on the Rest14 ATE subtask by 5.69% points, the Rest15 ATSC subtask by\
    \ 9.59% points, and the Lapt14 AOPE subtask by 3.37% points, surpassing 7x larger\
    \ models.\nWe get competitive results on AOOE, AOPE, AOSTE, and ACOSQE subtasks\
    \ indicating strong generalization ability to all subtasks. \nExploring sample\
    \ efficiency reveals that just 50% train data is required to get competitive results\
    \ with other instruction tuning approaches. \nLastly, we assess the quality of\
    \ instructions and observe that InstructABSA's performance experiences a decline\
    \ of $\\sim10\\%$ when adding misleading examples"
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Sentiment Analysis, Stylistic Analysis, and Argument Mining
  authors:
  - emails: '****@asu.edu'
    first_name: Kevin
    google_scholar_id: https://scholar.google.com/citations?user=nsYohr8AAAAJ&hl=en
    last_name: Scaria
    name: Kevin Scaria
    semantic_scholar_id: https://www.semanticscholar.org/author/Kevin-Scaria/2187874548
    username: ~Kevin_Scaria1
  - emails: '****@asu.edu'
    first_name: Himanshu
    google_scholar_id: https://scholar.google.com/citations?user=ydjuhxsAAAAJ&hl=en
    homepage: https://him1411.github.io
    institution: Amazon
    last_name: Gupta
    name: Himanshu Gupta
    username: ~Himanshu_Gupta5
  - emails: '****@asu.edu'
    first_name: Siddharth
    google_scholar_id: https://scholar.google.com/citations?user=31QlQecAAAAJ&hl=en&authuser=1
    last_name: Goyal
    name: Siddharth Goyal
    username: ~Siddharth_Goyal3
  - dblp_id: https://dblp.org/pid/331/2777
    emails: '****@asu.edu'
    first_name: Saurabh
    google_scholar_id: https://scholar.google.com/citations?hl=en&authuser=1&user=2OImZegAAAAJ
    last_name: Sawant
    middle_name: Arjun
    name: Saurabh Arjun Sawant
    semantic_scholar_id: https://www.semanticscholar.org/author/S.-Sawant/2106116649
    username: ~Saurabh_Arjun_Sawant1
  - emails: '****@gmail.com'
    first_name: Swaroop
    google_scholar_id: https://scholar.google.com/citations?user=-7LK2SwAAAAJ&hl=en
    homepage: https://swarooprm.github.io/
    institution: Google
    last_name: Mishra
    name: Swaroop Mishra
    username: ~Swaroop_Mishra1
  - dblp_id: https://dblp.uni-trier.de/pers/hd/b/Baral:Chitta
    emails: '****@asu.edu'
    first_name: Chitta
    google_scholar_id: https://scholar.google.com/citations?user=9Yd716IAAAAJ&hl=en&oi=ao
    homepage: http://www.public.asu.edu/~cbaral
    institution: Arizona State University, Arizona State University and Arizona State
      University
    last_name: Baral
    name: Chitta Baral
    orcid: https://orcid.org/0000-0002-7549-723X
    semantic_scholar_id: https://www.semanticscholar.org/author/Chitta-Baral/1760291
    username: ~Chitta_Baral1
  decision: toMainConference
  end_page: 8479
  file: 935.pdf
  id: 935
  num_pages: 17
  openreview_id: 3xG7YnfDsS
  pdf_file: be9840dd2d87514771acd9fe610f1a7954af06bf.pdf
  start_page: 8463
  title: 'InstructABSA: Instruction Learning for Aspect Based Sentiment Analysis'
- abstract: 'Multimodal summarization with multimodal output (MSMO) has attracted
    increasing research interests recently as multimodal summary could provide more
    comprehensive information compared to text-only summary, effectively improving
    the user experience and satisfaction. As one of the most fundamental components
    for the development of MSMO, evaluation is an emerging yet underexplored research
    topic. In this paper, we fill this gap and propose a research framework that studies
    three research questions of MSMO evaluation: (1) Automatic Evaluation: We propose
    a novel metric mLLM-EVAL, which utilizes multimodal Large Language Model for MSMO
    EVALuation. (2) Meta-Evaluation: We create a meta-evaluation benchmark dataset
    by collecting human-annotated scores for multimodal summaries. With our benchmark,
    we conduct meta-evaluation analysis to assess the quality of different evaluation
    metrics and show the effectiveness of our proposed mLLM-EVAL. (3) Human Evaluation:
    To provide more objective and unbiased human annotations for meta-evaluation,
    we hypothesize and verify three types of cognitive biases in human evaluation.
    We also incorporate our findings into the human annotation process in the meta-evaluation
    benchmark. Overall, our research framework provides an evaluation metric, a meta-evaluation
    benchmark dataset annotated by humans and an analysis of cognitive biases in human
    evaluation, which we believe would serve as a valuable and comprehensive resource
    for the MSMO research community.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - emails: '****@adelaide.edu.au'
    first_name: Haojie
    homepage: https://github.com/hjzhuang
    institution: University of Adelaide
    last_name: Zhuang
    name: Haojie Zhuang
    username: ~Haojie_Zhuang1
  - dblp_id: https://dblp.uni-trier.de/pers/hd/z/Zhang_0098:Wei
    emails: '****@adelaide.edu.au'
    first_name: Wei Emma
    google_scholar_id: https://scholar.google.com.au/citations?user=NFzUTiEAAAAJ&hl=en
    homepage: https://weiezhang.github.io/
    institution: The University of Adelaide
    last_name: Zhang
    name: Wei Emma Zhang
    orcid: https://orcid.org/0000-0002-0406-5974
    username: ~Wei_Emma_Zhang1
  - emails: '****@gmail.com'
    first_name: Leon
    homepage: https://www.adelaide.edu.au/
    last_name: Xie
    name: Leon Xie
    username: ~Leon_Xie1
  - dblp_id: https://dblp.org/pid/173/4662.html
    emails: '****@adelaide.edu.au'
    first_name: Weitong
    homepage: https://researchers.adelaide.edu.au/profile/weitong.chen
    institution: University of Adelaide
    last_name: Chen
    name: Weitong Chen
    orcid: https://orcid.org/0000-0003-1001-7925
    username: ~Weitong_Chen2
  - dblp_id: https://dblp.org/pid/y/JianYang1
    emails: '****@mq.edu.au'
    first_name: Jian
    google_scholar_id: https://scholar.google.com.au/citations?user=r5jS8eYAAAAJ&hl=en
    homepage: https://researchers.mq.edu.au/en/persons/jian-yang
    institution: Macquarie University
    last_name: Yang
    name: Jian Yang
    orcid: https://orcid.org/0000-0002-4408-1952
    username: ~Jian_Yang13
  - dblp_id: https://dblp.org/pid/s/QuanZSheng
    emails: '****@mq.edu.au'
    first_name: Quan
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=lwy2C5YAAAAJ
    homepage: http://web.science.mq.edu.au/~qsheng/
    institution: Macquarie University
    last_name: Sheng
    middle_name: Z.
    name: Quan Z. Sheng
    orcid: https://orcid.org/0000-0002-3326-4147
    semantic_scholar_id: https://www.semanticscholar.org/author/Quan-Z.-Sheng/1713128
    username: ~Quan_Z._Sheng1
  decision: toMainConference
  end_page: 8502
  file: 937.pdf
  id: 937
  num_pages: 23
  openreview_id: 3YkFvyODjz
  pdf_file: 64d5bebd54287724ddf355e857e60f727dc32ab3.pdf
  start_page: 8480
  title: Automatic, Meta and Human Evaluation for Multimodal Summarization with Multimodal
    Output
- abstract: 'Large Language Models (LLMs) have shown promising in-context learning
    abilities. However, conventional  In-Context Learning (ICL) approaches are often
    impeded by length limitations of transformer architecture, which pose challenges
    when attempting to effectively integrate supervision from a substantial number
    of demonstration examples. In this paper, we introduce a novel framework, called
    Naive Bayes-based Context Extension (NBCE), to enable existing LLMs to perform
    ICL with an increased number of demonstrations by significantly expanding their
    context size. Importantly, this expansion does not require fine-tuning or dependence
    on particular model architectures, all the while preserving linear efficiency.
    NBCE initially splits the context into equal-sized windows fitting the target
    LLM''s maximum length. Then, it introduces a voting mechanism to select the most
    relevant window, regarded as the posterior context. Finally, it employs Bayes''
    theorem to generate the test task. Our  experimental results demonstrate that
    NBCE substantially enhances performance, particularly as the number of demonstration
    examples increases, consistently outperforming alternative methods. The NBCE code
    will be made publicly accessible. The code NBCE is available at: https://github.com/amurtadha/NBCE-master'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Generation
  authors:
  - dblp_id: https://dblp.org/pid/223/4243
    emails: '****@spaces.ac.cn'
    first_name: Jianlin
    google_scholar_id: https://scholar.google.com/citations?user=cdbdaksAAAAJ
    homepage: http://jianlin.su
    last_name: Su
    name: Jianlin Su
    username: ~Jianlin_Su1
  - dblp_id: https://dblp.org/pid/208/0019.html
    emails: '****@wezhuiyi.com'
    first_name: Murtadha
    google_scholar_id: https://scholar.google.com/citations?user=gqjKJukAAAAJ&hl=en
    institution: Zhuiyi AI Lab
    last_name: Ahmed
    name: Murtadha Ahmed
    orcid: https://orcid.org/0000-0002-0741-0710
    semantic_scholar_id: https://www.semanticscholar.org/author/Ahmed-Murtadha/28011568
    username: ~Murtadha_Ahmed2
  - emails: '****@wezhuiyi.com'
    first_name: Bo
    last_name: Wen
    name: Bo Wen
    username: ~Bo_Wen2
  - emails: '****@wezhuiyi.com'
    first_name: Luo
    homepage: https://github.com/desperadoola
    institution: Zhuiyi Technology Co., Ltd.
    last_name: Ao
    name: Luo Ao
    username: ~Luo_Ao1
  - emails: '****@wezhuiyi.com'
    first_name: Mingren
    homepage: https://scholar.google.com/citations?view_op=list_works&hl=zh-CN&user=FnRygiUAAAAJ&gmla=AJ1KiT0wXG9sVJ3yffwVvzUEKYyrUGmZKDDOamRLsDDR77JLziiulQT2aW32hzHgo-dYBvikCsHWwFDd7sxX_eLM2--d0tB4b-caRWMtLnRqBNyF7mVA91T_
    institution: Shenzhen Zhuiyi Technology Co., Ltd
    last_name: Zhu
    name: Mingren Zhu
    username: ~Mingren_Zhu1
  - emails: '****@wezhuiyi.com'
    first_name: Yunfeng
    last_name: Liu
    name: Yunfeng Liu
    username: ~Yunfeng_Liu1
  decision: toMainConference
  end_page: 8519
  file: 940.pdf
  id: 940
  num_pages: 17
  openreview_id: 5UFNc34qIj
  pdf_file: c9e39dfa2f6577b04b5981aab59ea6ea509c28d4.pdf
  start_page: 8503
  title: Naive Bayes-based Context Extension for Large Language Models
- abstract: 'Cross-lingual continual learning aims to continuously fine-tune a downstream
    model on emerging data from new languages. One major challenge in cross-lingual
    continual learning is catastrophic forgetting: a stability-plasticity dilemma,
    where performance on previously seen languages decreases as the model learns to
    transfer to new languages. Experience replay, which revisits data from a fixed-size
    memory of old languages while training on new ones, is among the most successful
    approaches for solving this dilemma. Faced with the challenge of dynamically storing
    the memory with high-quality examples while complying with its fixed size limitations,
    we consider Leitner queuing, a human-inspired spaced-repetition technique, to
    determine what should be replayed at each phase of learning. Via a controlled
    set of quantitative and qualitative analyses across different memory strategies,
    we show that, just like humans, carefully picking informative examples to be prioritized
    in cross-lingual memory replay helps tame the stability-plasticity dilemma. Compared
    to vanilla and strong memory replay baselines, our Leitner-guided approach significantly
    and consistently decreases forgetting while maintaining accuracy across natural
    language understanding tasks, language orders, and languages.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multilinguality and Language Diversity
  authors:
  - dblp_id: https://dblp.org/pid/225/5366.html
    emails: '****@usc.edu'
    first_name: Meryem
    google_scholar_id: https://scholar.google.ch/citations?user=7xflIKgAAAAJ&hl=en
    homepage: https://meryemmhamdi1.github.io/
    institution: University of Southern California
    last_name: M'hamdi
    name: Meryem M'hamdi
    semantic_scholar_id: https://www.semanticscholar.org/author/Meryem-M'hamdi/1411352299
    username: ~Meryem_M'hamdi1
  - dblp_id: https://dblp.org/pid/00/4758
    emails: '****@isi.edu'
    first_name: Jonathan
    google_scholar_id: https://scholar.google.com/citations?user=tmK5EPEAAAAJ&hl=en
    homepage: http://jonmay.net
    institution: University of Southern California and USC/ISI
    last_name: May
    name: Jonathan May
    orcid: https://orcid.org/0000-0002-5284-477X
    semantic_scholar_id: https://www.semanticscholar.org/author/Jonathan-May/143823227
    username: ~Jonathan_May1
  decision: toMainConference
  end_page: 8533
  file: 942.pdf
  id: 942
  num_pages: 14
  openreview_id: B9zd6ker4J
  pdf_file: 6f9a685afab80e18976d060acd1b07473b7175d6.pdf
  start_page: 8520
  title: Leitner-Guided Memory Replay for Cross-lingual Continual Learning
- abstract: "We introduce SPUD (Semantically Perturbed Universal Dependencies), a\
    \ framework for creating nonce treebanks for the multilingual Universal Dependencies\
    \ (UD) corpora. SPUD data satisfies syntactic argument structure, provides syntactic\
    \ annotations, and ensures grammaticality via language-specific rules. We create\
    \ nonce data in Arabic, English, French, German, and Russian, and demonstrate\
    \ two use cases of SPUD treebanks. \nFirst, we investigate the effect of nonce\
    \ data on word co-occurrence statistics, as measured by perplexity scores of autoregressive\
    \ (ALM) and masked language models (MLM). We find that ALM scores are significantly\
    \ more affected by nonce data than MLM scores. Second, we show how nonce data\
    \ affects the performance of syntactic dependency probes. We replicate the findings\
    \ of M\xFCller-Eberstein et al. (2022) on nonce test data and show that the performance\
    \ declines on both MLMs and ALMs wrt. original test data. However, a majority\
    \ of the performance is kept, suggesting that the probe indeed learns syntax independently\
    \ from semantics."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - dblp_id: https://dblp.org/pid/219/5595
    emails: '****@hhu.de'
    first_name: David
    google_scholar_id: https://scholar.google.com/citations?user=T0kG3rIAAAAJ&hl=de
    institution: "HHU D\xFCsseldorf"
    last_name: Arps
    name: David Arps
    semantic_scholar_id: https://www.semanticscholar.org/author/David-Arps/2081584597
    username: ~David_Arps1
  - dblp_id: https://dblp.org/pid/25/5562
    emails: '****@phil.hhu.de'
    first_name: Laura
    google_scholar_id: https://scholar.google.de/citations?user=gmFgdBwAAAAJ&hl=de&oi=ao
    homepage: https://user.phil.hhu.de/kallmeyer/
    institution: "Heinrich Heine University D\xFCsseldorf, Germany"
    last_name: Kallmeyer
    name: Laura Kallmeyer
    orcid: https://orcid.org/0000-0001-9691-5990
    username: ~Laura_Kallmeyer1
  - dblp_id: https://dblp.org/pid/01/10953
    emails: '****@ibm.com'
    first_name: Younes
    google_scholar_id: https://scholar.google.com/citations?user=LAIWEuIAAAAJ&hl=de
    homepage: https://user.phil-fak.uni-duesseldorf.de/~samih/
    last_name: Samih
    name: Younes Samih
    semantic_scholar_id: https://www.semanticscholar.org/author/Younes-Samih/3103210
    username: ~Younes_Samih2
  - dblp_id: https://dblp.org/pid/73/5938
    emails: '****@gmail.com'
    first_name: Hassan
    google_scholar_id: https://scholar.google.de/citations?user=t3BH6NkAAAAJ&hl=en
    homepage: https://hsajjad.github.io/
    institution: Dalhousie University
    last_name: Sajjad
    name: Hassan Sajjad
    username: ~Hassan_Sajjad1
  decision: toMainConference
  end_page: 8556
  file: 944.pdf
  id: 944
  num_pages: 23
  openreview_id: XE42zjFq73
  pdf_file: 5767102de72e8bfc04e1d3ae9715301288802189.pdf
  start_page: 8534
  title: 'Multilingual Nonce Dependency Treebanks: Understanding how Language Models
    Represent and Process Syntactic Structure'
- abstract: 'Generalized category discovery faces a key issue: the lack of supervision
    for new and unseen data categories. Traditional methods typically combine supervised
    pretraining with self-supervised learning to create models, and then employ clustering
    for category identification. However, these approaches tend to become overly tailored
    to known categories, failing to fully resolve the core issue. Hence, we propose
    to integrate the feedback from LLMs into an active learning paradigm. Specifically,
    our method innovatively employs uncertainty propagation to select data samples
    from high-uncertainty regions, which are then labeled using LLMs through a comparison-based
    prompting scheme. This not only eases the labeling task but also enhances accuracy
    in identifying new categories. Additionally, a soft feedback propagation mechanism
    is introduced to minimize the spread of inaccurate feedback. Experiments on various
    datasets demonstrate our framework''s efficacy and generalizability, significantly
    improving baseline models at a nominal average cost.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Dialogue and Interactive Systems
  authors:
  - dblp_id: https://dblp.org/pid/275/8702
    emails: '****@gmail.com'
    first_name: Jinggui
    institution: Singapore Management University
    last_name: Liang
    name: Jinggui Liang
    semantic_scholar_id: https://www.semanticscholar.org/author/Jinggui-Liang/2205539393
    username: ~Jinggui_Liang1
  - dblp_id: https://dblp.org/pid/149/1249
    emails: '****@gmail.com'
    first_name: Lizi
    google_scholar_id: https://scholar.google.com.sg/citations?user=W2b08EUAAAAJ&hl=en
    homepage: https://liziliao.github.io/
    institution: Singapore Management University
    last_name: Liao
    name: Lizi Liao
    username: ~Lizi_Liao1
  - dblp_id: https://dblp.uni-trier.de/pid/81/3569-1
    emails: '****@nus.edu.sg'
    first_name: Hao
    google_scholar_id: https://scholar.google.com/citations?user=YGDX46AAAAAJ
    homepage: https://haofei.vip/
    institution: National University of Singapore
    last_name: Fei
    name: Hao Fei
    semantic_scholar_id: https://www.semanticscholar.org/author/Hao-Fei/46959445
    username: ~Hao_Fei1
  - emails: '****@gmail.com'
    first_name: Bobo
    institution: Wuhan University
    last_name: Li
    name: Bobo Li
    orcid: https://orcid.org/0000-0002-0513-5540
    semantic_scholar_id: https://www.semanticscholar.org/author/Bobo-Li/2132446579
    username: ~Bobo_Li1
  - dblp_id: https://dblp.org/pid/68/1974-1
    emails: '****@smu.edu.sg'
    first_name: Jing
    google_scholar_id: https://scholar.google.com.sg/citations?user=hVTK2YwAAAAJ&hl=en
    homepage: http://www.mysmu.edu/faculty/jingjiang/
    institution: Singapore Management University
    last_name: Jiang
    name: Jing Jiang
    semantic_scholar_id: https://www.semanticscholar.org/author/Jing-Jiang/144924150
    username: ~Jing_Jiang1
  decision: toMainConference
  end_page: 8570
  file: 945.pdf
  id: 945
  num_pages: 14
  openreview_id: U1pndIsPDJ
  pdf_file: 13508347891fd435e383c902e7b1603a94f3bed1.pdf
  start_page: 8557
  title: Actively Learn from LLMs with Uncertainty Propagation for Generalized Category
    Discovery
- abstract: As Transformers have become state-of-the-art models for natural language
    processing (NLP) tasks, the need to understand and explain their predictions is
    increasingly apparent. Especially in unsupervised applications, such as information
    retrieval tasks, similarity models built on top of foundation model representations
    have been widely applied. However, their inner prediction mechanisms have mostly
    remained opaque. Recent advances in explainable AI have made it possible to mitigate
    these limitations by leveraging improved explanations for Transformers through
    layer-wise relevance propagation (LRP). Using BiLRP, an extension developed for
    computing second-order explanations in bilinear similarity models, we investigate
    which feature interactions drive similarity in NLP models. We validate the resulting
    explanations and demonstrate their utility in three corpus-level use cases, analyzing
    grammatical interactions, multilingual semantics, and biomedical text retrieval.
    Our findings contribute to a deeper understanding of different semantic similarity
    tasks and models, highlighting how novel explainable AI methods enable in-depth
    analyses and corpus-level insights.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - dblp_id: https://dblp.org/pid/260/6891
    emails: '****@tu-berlin.de'
    first_name: Oliver
    google_scholar_id: https://scholar.google.com/citations?user=vZB4qw0AAAAJ&hl=de
    homepage: https://www.tu.berlin/
    institution: "Technische Universit\xE4t Berlin"
    last_name: Eberle
    name: Oliver Eberle
    username: ~Oliver_Eberle1
  - dblp_id: https://dblp.org/pid/231/2887-2.html
    emails: '****@gmx.de'
    first_name: Alexandros
    last_name: Vasileiou
    name: Alexandros Vasileiou
    username: ~Alexandros_Vasileiou1
  decision: toMainConference
  end_page: 8585
  file: 949.pdf
  id: 949
  num_pages: 15
  openreview_id: pfkJDwn7qX
  pdf_file: 9824510a7cbcc0c53c8be2743222082953a98c1c.pdf
  start_page: 8571
  title: Explaining Text Similarity in Transformer Models
- abstract: Recently, large language models (LLMs) have emerged as a groundbreaking
    technology and their unparalleled text generation capabilities have sparked interest
    in their application to the fundamental sentence representation learning task.
    Existing methods have explored utilizing LLMs as data annotators to generate synthesized
    data for training contrastive learning based sentence embedding models such as
    SimCSE. However, since contrastive learning models are sensitive to the quality
    of sentence pairs, the effectiveness of these methods is largely influenced by
    the content generated from LLMs, highlighting the need for more refined generation
    in the context of sentence representation learning. Building upon this premise,
    we propose MultiCSR, a multi-level contrastive sentence representation learning
    framework that decomposes the process of prompting LLMs to generate a corpus for
    training base sentence embedding models into three stages (i.e., sentence generation,
    sentence pair construction, in-batch training) and refines the generated content
    at these three distinct stages, ensuring only high-quality sentence pairs are
    utilized to train a base contrastive learning model. Our extensive experiments
    reveal that MultiCSR enables a less advanced LLM to surpass the performance of
    ChatGPT, while applying it to ChatGPT achieves better state-of-the-art results.
    Comprehensive analyses further underscore the potential of our framework in various
    application scenarios and achieving better sentence representation learning with
    LLMs.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: 'Semantics: Sentence-level Semantics, Textual Inference and Other
      areas'
  authors:
  - emails: '****@mymail.sutd.edu.sg'
    first_name: Huiming
    homepage: https://circle-ming.github.io/
    institution: Singapore University of Technology and Design
    last_name: Wang
    name: Huiming Wang
    username: ~Huiming_Wang1
  - emails: '****@e.ntu.edu.sg'
    first_name: Zhaodonghui
    institution: Nanyang Technological University
    last_name: Li
    name: Zhaodonghui Li
    username: ~Zhaodonghui_Li1
  - dblp_id: https://dblp.org/pid/221/0115
    emails: '****@gmail.com'
    first_name: Liying
    google_scholar_id: https://scholar.google.com.sg/citations?user=xkZCRy0kBHEC&hl=en
    homepage: https://liyingcheng95.github.io/
    last_name: Cheng
    name: Liying Cheng
    semantic_scholar_id: https://www.semanticscholar.org/author/123962152
    username: ~Liying_Cheng1
  - emails: '****@sutd.edu.sg'
    first_name: De Wen
    homepage: https://istd.sutd.edu.sg/people/faculty/soh-de-wen
    institution: Singapore University of Technology and Design
    last_name: Soh
    name: De Wen Soh
    username: ~De_Wen_Soh3
  - dblp_id: https://dblp.org/pid/53/6625
    emails: '****@gmail.com'
    first_name: Lidong
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=_oYzrzAAAAAJ
    homepage: https://lidongbing.github.io/
    institution: Alibaba Group
    last_name: Bing
    name: Lidong Bing
    semantic_scholar_id: https://www.semanticscholar.org/author/Lidong-Bing/1996394
    username: ~Lidong_Bing2
  decision: toMainConference
  end_page: 8603
  file: 956.pdf
  id: 956
  num_pages: 18
  openreview_id: ztm5F9KRmG
  pdf_file: f7f1a62b183365b177845b27112450d883516584.pdf
  start_page: 8586
  title: Large Language Models can Contrastively Refine their Generation for Better
    Sentence Representation Learning
- abstract: Retrieval augmentation is a powerful but expensive method to make language
    models more knowledgeable about the world. Memory-based methods like LUMEN (de
    Jong et al., 2023a) pre-compute token representations for retrieved passages to
    drastically speed up inference. However, memory also leads to much greater storage
    requirements from storing pre-computed representations. We propose MEMORY-VQ,
    a new method to reduce storage requirements of memory-augmented models without
    sacrificing performance. Our method uses a vector quantization variational autoencoder
    (VQ-VAE) to compress token representations. We apply MEMORY-VQ to the LUMEN model
    to obtain LUMEN-VQ, a memory model that achieves a 16x compression rate with comparable
    performance on the KILT benchmark. LUMEN-VQ enables practical retrieval augmentation
    even for extremely large retrieval corpora.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Learning for NLP
  authors:
  - dblp_id: https://dblp.org/pid/225/5302
    emails: '****@gmail.com'
    first_name: Yury
    google_scholar_id: https://scholar.google.com/citations?user=fkkxyJUAAAAJ&hl=en
    homepage: https://urikz.github.io/
    last_name: Zemlyanskiy
    name: Yury Zemlyanskiy
    semantic_scholar_id: https://www.semanticscholar.org/author/Yury-Zemlyanskiy/51199981
    username: ~Yury_Zemlyanskiy1
  - dblp_id: https://dblp.org/pid/223/0153
    emails: '****@gmail.com'
    first_name: Michiel
    google_scholar_id: https://scholar.google.com/citations?user=R7wXId8AAAAJ&hl=en
    institution: Augment Computing
    last_name: De Jong
    name: Michiel de Jong
    semantic_scholar_id: https://www.semanticscholar.org/author/Michiel-de-Jong/21379393
    username: ~Michiel_de_Jong1
  - dblp_id: https://dblp.org/pid/153/2155
    emails: '****@gmail.com'
    first_name: Luke
    google_scholar_id: https://scholar.google.com/citations?user=xWrOthYAAAAJ&hl=en
    homepage: http://people.cs.umass.edu/~luke/
    institution: Google
    last_name: Vilnis
    name: Luke Vilnis
    username: ~Luke_Vilnis1
  - dblp_id: https://dblp.org/pers/o/Onta=ntilde==oacute=n:Santiago.html
    emails: '****@gmail.com'
    first_name: Santiago
    google_scholar_id: https://scholar.google.com/citations?user=aS-DrOwAAAAJ&hl=en
    homepage: https://sites.google.com/site/santiagoontanonvillar/
    institution: Google and Drexel University
    last_name: Ontanon
    name: Santiago Ontanon
    username: ~Santiago_Ontanon1
  - dblp_id: https://dblp.uni-trier.de/pers/hd/c/Cohen:William_W=
    emails: '****@google.com'
    first_name: William
    google_scholar_id: https://scholar.google.com/citations?user=8ys-38kAAAAJ&hl=en&oi=ao
    homepage: https://wwcohen.github.io/
    institution: Google DeepMind
    last_name: Cohen
    middle_name: W.
    name: William W. Cohen
    username: ~William_W._Cohen2
  - emails: '****@google.com'
    first_name: Sumit
    institution: Research, Google
    last_name: Sanghai
    name: Sumit Sanghai
    username: ~Sumit_Sanghai1
  - dblp_id: https://dblp.org/pid/263/3363
    emails: '****@google.com'
    first_name: Joshua
    institution: Google
    last_name: Ainslie
    name: Joshua Ainslie
    semantic_scholar_id: https://www.semanticscholar.org/author/Joshua-Ainslie/1643737606
    username: ~Joshua_Ainslie1
  decision: toMainConference
  end_page: 8611
  file: 957.pdf
  id: 957
  num_pages: 8
  openreview_id: 031zwaYL10
  pdf_file: 023fa5e1e39695cf1d8079afb017de3962478987.pdf
  start_page: 8604
  title: 'MEMORY-VQ: Compression for Tractable Internet-Scale Memory'
- abstract: 'Advancements in dense retrieval models have brought ColBERT to prominence
    in Information Retrieval (IR) with its advanced interaction techniques.

    However, ColBERT is reported to frequently underperform in zero-shot scenarios,
    where traditional techniques such as BM25 still exceed it.

    Addressing this, we propose to balance representation isotropy and anisotropy
    for zero-shot model performance, based on our observations that isotropy can enhance
    cosine similarity computations and anisotropy may aid in generalizing to unseen
    data.

    Striking a balance between these isotropic and anisotropic qualities stands as
    a critical objective to refine model efficacy.

    Based on this, we present ours, a Hybrid Isotropy Learning (HIL) architecture
    that integrates isotropic and anisotropic representations.

    Our experiments with the BEIR benchmark show that our model significantly outperforms
    the baseline ColBERT model, highlighting the importance of harmonized isotropy
    in improving zero-shot retrieval performance.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Retrieval and Text Mining
  authors:
  - emails: '****@snu.ac.kr'
    first_name: JaeYoung
    institution: Seoul National University
    last_name: Kim
    name: JaeYoung Kim
    username: ~JaeYoung_Kim5
  - dblp_id: https://dblp.org/pid/297/3811
    emails: '****@snu.ac.kr'
    first_name: Dohyeon
    google_scholar_id: https://scholar.google.com/citations?user=Gjk6Zk0AAAAJ
    homepage: https://github.com/waylight3
    institution: Seoul National University
    last_name: Lee
    name: Dohyeon Lee
    username: ~Dohyeon_Lee1
  - dblp_id: https://dblp.org/pid/h/SeungwonHwang
    emails: '****@snu.ac.kr'
    first_name: Seung-won
    google_scholar_id: https://scholar.google.com/citations?user=63bBmc3mYrAC&hl=ko
    homepage: http://seungwonh.github.io
    institution: Seoul National University
    last_name: Hwang
    name: seung-won hwang
    semantic_scholar_id: https://www.semanticscholar.org/author/Seung-won-Hwang/1716415
    username: ~seung-won_hwang2
  decision: toMainConference
  end_page: 8623
  file: 959.pdf
  id: 959
  num_pages: 12
  openreview_id: 5WwIvd66Pz
  pdf_file: b17c9e32c43589eaffe17ade687ef304ae20ab2e.pdf
  start_page: 8612
  title: 'HIL: Hybrid Isotropy Learning for Zero-shot Performance in Dense retrieval'
- abstract: We assemble a broad Natural Language Understanding benchmark suite for
    the German language and consequently evaluate a wide array of existing German-capable
    models in order to create a better understanding of the current state of German
    LLMs. Our benchmark consists of 29 different tasks ranging over different types
    such as document classification, sequence tagging, sentence similarity, and question
    answering, on which we evaluate 10 different German-pretrained models, thereby
    charting the landscape of German LLMs. In our comprehensive evaluation we find
    that encoder models are a good choice for most tasks, but also that the largest
    encoder model does not necessarily perform best for all tasks. We make our benchmark
    suite and a leaderboard publically available at https://supergleber.professor-x.de
    and encourage the community to contribute new tasks and evaluate more models on
    it (https://github.com/LSX-UniWue/SuperGLEBer).
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - dblp_id: https://dblp.org/pid/267/0295
    emails: '****@informatik.uni-wuerzburg.de'
    first_name: Jan
    google_scholar_id: https://scholar.google.com/citations?user=jMEqiKUAAAAJ
    homepage: https://www.informatik.uni-wuerzburg.de/datascience/staff/pfister/
    institution: "Bayerische Julius-Maximilians-Universit\xE4t W\xFCrzburg"
    last_name: Pfister
    name: Jan Pfister
    username: ~Jan_Pfister1
  - dblp_id: https://dblp.org/pid/h/AndreasHotho
    emails: '****@informatik.uni-wuerzburg.de'
    first_name: Andreas
    google_scholar_id: https://scholar.google.de/citations?user=eWTzXFAAAAAJ
    homepage: https://www.informatik.uni-wuerzburg.de/datascience/staff/hotho/
    institution: "Bayerische Julius-Maximilians-Universit\xE4t W\xFCrzburg"
    last_name: Hotho
    name: Andreas Hotho
    semantic_scholar_id: https://www.semanticscholar.org/author/A.-Hotho/1792623
    username: ~Andreas_Hotho1
  decision: toMainConference
  end_page: 8643
  file: 962.pdf
  id: 962
  num_pages: 20
  openreview_id: qlIPnDVAX4
  pdf_file: 20aaf78edd7213ad454bcc804f08df09042ca78b.pdf
  start_page: 8624
  title: 'SuperGLEBer: German Language Understanding Evaluation Benchmark'
- abstract: "Labeling corpora constitutes a bottleneck to create models for new tasks\
    \ or domains. Large language models mitigate the issue with automatic corpus labeling\
    \ methods, particularly for categorical annotations. Some NLP tasks such as emotion\
    \ intensity prediction, however, require text regression, but there is no work\
    \ on automating annotations for continuous label assignments. Regression is considered\
    \ more challenging than classification: The fact that humans perform worse when\
    \ tasked to choose values from a rating scale lead to comparative annotation methods,\
    \ including best\u2013worst scaling. This raises the question if large language\
    \ model-based annotation methods show similar patterns, namely that they perform\
    \ worse on rating scale annotation tasks than on comparative annotation tasks.\
    \ To study this, we automate emotion intensity predictions and compare direct\
    \ rating scale predictions, pairwise comparisons and best\u2013worst scaling.\
    \ We find that the latter shows the highest reliability. A transformer regressor\
    \ fine-tuned on these data performs nearly on par with a model trained on the\
    \ original manual annotations."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - emails: '****@ims.uni-stuttgart.de'
    first_name: Christopher
    homepage: https://de.linkedin.com/in/christopher-bagdon-7152351b6
    last_name: Bagdon
    name: Christopher Bagdon
    username: ~Christopher_Bagdon1
  - emails: '****@merckgroup.com'
    first_name: Prathamesh
    homepage: https://www.linkedin.com/in/prathamesh-karmalkar/
    last_name: Karmalkar
    name: Prathamesh Karmalkar
    username: ~Prathamesh_Karmalkar1
  - emails: '****@merckgroup.com'
    first_name: Harsha
    google_scholar_id: https://scholar.google.com/citations?user=CgcIWTYAAAAJ&hl=en
    last_name: Gurulingappa
    name: Harsha Gurulingappa
    username: ~Harsha_Gurulingappa1
  - dblp_id: https://dblp.org/pid/21/4183
    emails: '****@uni-bamberg.de'
    first_name: Roman
    google_scholar_id: https://scholar.google.de/citations?user=1flvefwAAAAJ&hl=de
    homepage: https://www.romanklinger.de
    institution: "Otto-Friedrich Universit\xE4t Bamberg"
    last_name: Klinger
    name: Roman Klinger
    orcid: https://orcid.org/0000-0002-2014-6619
    semantic_scholar_id: https://www.semanticscholar.org/author/Roman-Klinger/66339110
    username: ~Roman_Klinger1
  decision: toMainConference
  end_page: 8656
  file: 963.pdf
  id: 963
  num_pages: 13
  openreview_id: Cqd8bfEVuj
  pdf_file: 029aab154c8bb6ccbd410959266ef7c895efbcea.pdf
  start_page: 8644
  title: "\"You are an expert annotator\": Automatic Best\u2013Worst-Scaling Annotations\
    \ for Emotion Intensity Modeling"
- abstract: Recent advancements in GPT-4V have displayed remarkable multi-modal capabilities
    in processing image inputs and following open-ended instructions. Despite these
    advancements, there is considerable scope for enhancing open-source multi-modal
    LLMs, especially in terms of multi-modal understanding accuracy and instruction-following
    proficiency. In this paper, we conduct a comprehensive study on training GPT4-style
    models. We introduce Lynx a multi-modal LLM developed through a series of controlled
    experiments comparing various model variants. This process allowed us to identify
    and implement an optimal training strategy tailored for multi-modal LLMs. In addition
    to our model development, we propose a plug-and-play technique designed to augment
    the instruction-following capabilities of multi-modal LLMs. We have validated
    the performance of Lynx on multiple benchmarks. Results demonstrate that Lynx
    not only achieves strong image understanding accuracy but also excels in instruction-following
    tasks, paving the path for ongoing enhancements in multi-modal LLMs.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multimodality and Language Grounding to Vision, Robotics and Beyond
  authors:
  - dblp_id: https://dblp.org/pid/83/4665-3.html
    emails: '****@bytedance.com'
    first_name: Yan
    google_scholar_id: https://scholar.google.com/citations?hl=zh-CN&user=-zT0sBsAAAAJ&view_op=list_works&gmla=AJsN-F7UhA_4RFGsvEDy43HN0pPyF81wg2h4JGXZwmoHLKETnkUZIhpRt11SgOI2E4CR3u74yWBxbACxlVkBcIAkyVrxTmqF0TIR4xMKro_wB42tHDtuimU
    institution: ByteDance
    last_name: Zeng
    name: Yan Zeng
    username: ~Yan_Zeng1
  - dblp_id: https://dblp.org/pid/119/1807
    emails: '****@bytedance.com'
    first_name: Hanbo
    google_scholar_id: https://scholar.google.com/citations?user=1qfEEwsAAAAJ&hl=zh-CN
    institution: ByteDance Ltd
    last_name: Zhang
    name: Hanbo Zhang
    username: ~Hanbo_Zhang1
  - emails: '****@bytedance.com'
    first_name: Jiani
    google_scholar_id: https://scholar.google.com/citations?view_op=list_works&hl=en&user=SiEGHqUAAAAJ
    last_name: Zheng
    name: Jiani zheng
    username: ~Jiani_zheng1
  - emails: '****@alibaba-inc.com'
    first_name: Jiangnan
    last_name: Xia
    name: Jiangnan Xia
    username: ~Jiangnan_Xia1
  - dblp_id: https://dblp.org/pid/234/8900
    emails: '****@mail.ustc.edu.cn'
    first_name: Guoqiang
    google_scholar_id: https://scholar.google.com/citations?hl=en&view_op=list_works&gmla=AJsN-F6gjEpVF1ZmTWj5mmLF_EdRWWIomdQxzqkzfhfCdISQrBBPwkq3bkgCYFzrETsUA5_-vdjdXrTnhEas9Zk7VuehvzpEknGsI2fEjqL9loKXevJ9sZw&user=TxeZUTgAAAAJ
    homepage: https://guoqiangwei.xyz/
    last_name: Wei
    name: Guoqiang Wei
    orcid: https://orcid.org/0000-0003-1846-5693
    username: ~Guoqiang_Wei1
  - emails: '****@bytedance.com'
    first_name: Yang
    homepage: https://godweiyang.com
    institution: East China Normal University
    last_name: Wei
    name: Yang Wei
    username: ~Yang_Wei6
  - dblp_id: https://dblp.org/pid/09/5661
    emails: '****@cs.stanford.edu'
    first_name: Yuchen
    google_scholar_id: https://scholar.google.com/citations?user=Om4Lag0AAAAJ&hl=zh-CN
    institution: ' ByteDance Research'
    last_name: Zhang
    name: Yuchen Zhang
    username: ~Yuchen_Zhang1
  - dblp_id: https://dblp.org/pid/01/2492
    emails: '****@gmail.com'
    first_name: Tao
    google_scholar_id: https://scholar.google.com/citations?user=kSUXLPkAAAAJ&hl=en
    homepage: http://www.taokong.org
    institution: Bytedance
    last_name: Kong
    name: Tao Kong
    username: ~Tao_Kong3
  - dblp_id: https://dblp.org/pid/s/RuihuaSong
    emails: '****@outlook.com'
    first_name: Ruihua
    institution: Renmin University of China
    last_name: Song
    name: Ruihua Song
    username: ~Ruihua_Song1
  decision: toMainConference
  end_page: 8684
  file: 966.pdf
  id: 966
  num_pages: 28
  openreview_id: OheoiZpqYu
  pdf_file: 0048bc4320faa4703abd8f0bf6592446d635a8f8.pdf
  start_page: 8657
  title: What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?
- abstract: Human evaluation serves as the gold standard for assessing the quality
    of Natural Language Generation (NLG) systems. Nevertheless, the evaluation guideline,
    as a pivotal element ensuring reliable and reproducible human assessment, has
    received limited attention. Our investigation revealed that only 29.84\% of recent
    papers involving human evaluation at top conferences release their evaluation
    guidelines, with vulnerabilities identified in 77.09\% of these guidelines. Unreliable
    evaluation guidelines can yield inaccurate assessment outcomes, potentially impeding
    the advancement of NLG in the right direction. To address these challenges, we
    take an initial step towards reliable evaluation guidelines and propose the first
    human evaluation guideline dataset by collecting annotations of guidelines extracted
    from existing papers as well as generated via Large Language Models (LLMs). We
    then introduce a taxonomy of eight vulnerabilities and formulate a principle for
    composing  evaluation guidelines. Furthermore, a method for detecting guideline
    vulnerabilities has been explored using LLMs, and we offer a set of recommendations
    to enhance reliability in human evaluation. The annotated human evaluation guideline
    dataset and code for the vulnerability detection method are publicly available
    online.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Generation
  authors:
  - dblp_id: https://dblp.org/pid/03/523
    emails: '****@stu.pku.edu.cn'
    first_name: Jie
    google_scholar_id: https://scholar.google.com/citations?user=XpBR1Z8AAAAJ&hl=zh-CN&oi=ao
    last_name: Ruan
    name: Jie Ruan
    semantic_scholar_id: https://www.semanticscholar.org/author/Jie-Ruan/51300695
    username: ~Jie_Ruan1
  - emails: '****@stu.pku.edu.cn'
    first_name: WangWenqing
    last_name: WangWenqing
    name: WangWenqing
    orcid: https://orcid.org/0000-0001-8238-077X
    semantic_scholar_id: https://www.semanticscholar.org/author/Wenqing-Wang/2108476934
    username: ~WangWenqing1
  - dblp_id: https://dblp.org/pid/07/1521
    emails: '****@pku.edu.cn'
    first_name: Xiaojun
    google_scholar_id: https://scholar.google.com/citations?user=lTTeBdkAAAAJ
    homepage: https://wanxiaojun.github.io
    institution: Peking University
    last_name: Wan
    name: Xiaojun Wan
    semantic_scholar_id: https://www.semanticscholar.org/author/Xiaojun-Wan/145078589
    username: ~Xiaojun_Wan1
  decision: toMainConference
  end_page: 8709
  file: 968.pdf
  id: 968
  num_pages: 25
  openreview_id: VYKb4DHlEb
  pdf_file: a4fd2aafb539390b978dfd21a69a0978d3801f27.pdf
  start_page: 8685
  title: 'Defining and Detecting Vulnerability in Human Evaluation Guidelines: A Preliminary
    Study Towards Reliable NLG Evaluation'
- abstract: Several Natural Language Understanding (NLU) tasks focus on linking text
    to explicit knowledge, including Word Sense Disambiguation, Semantic Role Labeling,
    Semantic Parsing, and Relation Extraction. In addition to the importance of connecting
    raw text with explicit knowledge bases, the integration of such carefully curated
    knowledge into deep learning models has been shown to be beneficial across a diverse
    range of applications, including Language Modeling and Machine Translation. Nevertheless,
    the scarcity of semantically-annotated corpora across various tasks and languages
    limits the potential advantages significantly. To address this issue, we put forward
    MOSAICo, the first endeavor aimed at equipping the research community with the
    key ingredients to model explicit semantic knowledge at a large scale, providing
    hundreds of millions of silver yet high-quality annotations for four NLU tasks
    across five languages. We describe the creation process of MOSAICo, demonstrate
    its quality and variety, and analyze the interplay between different types of
    semantic information. MOSAICo, available at https://github.com/SapienzaNLP/mosaico,
    aims to drop the requirement of closed, licensed datasets and represents a step
    towards a level playing field across languages and tasks in NLU.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: 'Semantics: Sentence-level Semantics, Textual Inference and Other
      areas'
  authors:
  - dblp_id: https://dblp.org/pid/254/8205
    emails: '****@uniroma1.it'
    first_name: Simone
    google_scholar_id: https://scholar.google.com/citations?user=S1tqbTcAAAAJ
    homepage: https://c-simone.github.io
    institution: Sapienza University of Rome
    last_name: Conia
    name: Simone Conia
    semantic_scholar_id: https://www.semanticscholar.org/author/Simone-Conia/1396456007
    username: ~Simone_Conia1
  - dblp_id: https://dblp.org/pid/269/4565
    emails: '****@gmail.com'
    first_name: Edoardo
    google_scholar_id: https://scholar.google.com/citations?user=hVjbi_QAAAAJ&hl=en&authuser=1
    homepage: https://edobobo.github.io/
    last_name: Barba
    name: Edoardo Barba
    semantic_scholar_id: https://www.semanticscholar.org/author/1810690342
    username: ~Edoardo_Barba1
  - dblp_id: https://dblp.org/pid/313/2215
    emails: '****@di.uniroma1.it'
    first_name: Abelardo Carlos
    google_scholar_id: https://scholar.google.it/citations?user=hRQS9fIAAAAJ&hl=it
    homepage: https://carlosml26.github.io/
    institution: University of Roma "La Sapienza"
    last_name: Martinez Lorenzo
    name: Abelardo Carlos Martinez Lorenzo
    orcid: https://orcid.org/0000-0003-0587-632X
    semantic_scholar_id: https://www.semanticscholar.org/author/Abelardo-Carlos-Mart%C3%ADnez-Lorenzo/2165227698
    username: ~Abelardo_Carlos_Martinez_Lorenzo2
  - dblp_id: https://dblp.org/pid/278/1916
    emails: '****@gmail.com'
    first_name: "Pere-Llu\xEDs"
    google_scholar_id: https://scholar.google.com/citations?user=HeqN6q8AAAAJ
    homepage: https://littlepea13.github.io
    last_name: Huguet Cabot
    name: "Pere-Llu\xEDs Huguet Cabot"
    orcid: https://orcid.org/0000-0002-8960-3873
    username: "~Pere-Llu\xEDs_Huguet_Cabot1"
  - emails: '****@diag.uniroma1.it'
    first_name: Riccardo
    google_scholar_id: https://scholar.google.com/citations?user=pYfNxg4AAAAJ&hl=en
    homepage: https://riccardorlando.xyz/
    last_name: Orlando
    name: Riccardo Orlando
    semantic_scholar_id: https://www.semanticscholar.org/author/Riccardo-Orlando/2140489779
    username: ~Riccardo_Orlando1
  - emails: '****@gmail.com'
    first_name: Luigi
    google_scholar_id: https://scholar.google.com/citations?user=zQBf4YIAAAAJ
    homepage: https://poccio.github.io/
    last_name: Procopio
    name: Luigi Procopio
    semantic_scholar_id: https://www.semanticscholar.org/author/1810690563
    username: ~Luigi_Procopio1
  - dblp_id: https://dblp.org/pers/n/Navigli:Roberto.html
    emails: '****@diag.uniroma1.it'
    first_name: Roberto
    google_scholar_id: https://scholar.google.it/citations?user=BsgVJ-EAAAAJ
    homepage: http://wwwusers.di.uniroma1.it/~navigli/
    institution: Sapienza University of Rome
    last_name: Navigli
    name: Roberto Navigli
    orcid: https://orcid.org/0000-0003-3831-9706
    semantic_scholar_id: https://www.semanticscholar.org/author/R.-Navigli/1733928
    username: ~Roberto_Navigli2
  decision: toMainConference
  end_page: 8724
  file: 971.pdf
  id: 971
  num_pages: 15
  openreview_id: DZ208FzkVd
  pdf_file: 2a5c9d3d4830940c873f790abbf8cc906293a8b6.pdf
  start_page: 8710
  title: 'MOSAICo: a Multilingual Open-text Semantically Annotated Interlinked Corpus'
- abstract: Language models (LMs) are indispensable tools for natural language processing
    tasks, but their vulnerability to adversarial attacks remains a concern. While
    current research has explored adversarial training techniques, their improvements
    to defend against word-level attacks have been limited. In this work, we propose
    a novel approach called Semantic Robust Defence (SemRoDe), a Macro Adversarial
    Training strategy to enhance the robustness of LMs. Drawing inspiration from recent
    studies in the image domain, we investigate and later confirm that in a discrete
    data setting such as language, adversarial samples generated via word substitutions
    do indeed belong to an adversarial domain exhibiting a high Wasserstein distance
    from the base domain. Our method learns a robust representation that bridges these
    two domains. We hypothesize that if samples were not projected into an adversarial
    domain, but instead to a domain with minimal shift, it would improve attack robustness.
    We align the domains by incorporating a new distance-based objective. With this,
    our model is able to learn more generalized representations by aligning the model's
    high-level output features and therefore better handling unseen adversarial samples.
    This method can be generalized across word embeddings, even when they share minimal
    overlap at both vocabulary and word-substitution levels. To evaluate the effectiveness
    of our approach, we conduct experiments on BERT and RoBERTa models on three datasets.
    The results demonstrate promising state-of-the-art robustness.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Learning for NLP
  authors:
  - emails: '****@u.nus.edu'
    first_name: Brian
    homepage: https://brianformento.com
    institution: national university of singaore, National University of Singapore
    last_name: Formento
    name: Brian Formento
    username: ~Brian_Formento1
  - dblp_id: https://dblp.org/pid/126/2373-1
    emails: '****@gmail.com'
    first_name: Wenjie
    google_scholar_id: https://scholar.google.com/citations?user=EV1kntYAAAAJ&hl=en
    homepage: https://wenchieh.github.io
    institution: National University of Singapore
    last_name: Feng
    name: Wenjie Feng
    username: ~Wenjie_Feng1
  - dblp_id: https://dblp.org/pid/73/1823
    emails: '****@gmail.com'
    first_name: Chuan-Sheng
    google_scholar_id: https://scholar.google.com/citations?user=AgbeqGkAAAAJ&hl=en
    homepage: http://ai.stanford.edu/~csfoo
    institution: Centre for Frontier AI Research, A*STAR and Institute for Infocomm
      Research, A*STAR
    last_name: Foo
    name: Chuan-Sheng Foo
    orcid: https://orcid.org/0000-0002-4748-5792
    username: ~Chuan-Sheng_Foo1
  - dblp_id: https://dblp.org/pid/81/8329
    emails: '****@ntu.edu.sg'
    first_name: Anh Tuan
    google_scholar_id: https://scholar.google.com.sg/citations?hl=en&user=d6ixOGYAAAAJ&view_op=list_works
    homepage: https://tuanluu.github.io/
    institution: Nanyang Technological University
    last_name: Luu
    name: Anh Tuan Luu
    semantic_scholar_id: https://www.semanticscholar.org/author/Anh-Tuan-Luu/26336902
    username: ~Anh_Tuan_Luu2
  - dblp_id: https://dblp.org/pid/00/5480
    emails: '****@nus.edu.sg'
    first_name: See-Kiong
    google_scholar_id: https://scholar.google.com.tw/citations?user=_wsommYAAAAJ
    homepage: https://www.comp.nus.edu.sg/~ngsk/
    institution: National University of Singapore
    last_name: Ng
    name: See-Kiong Ng
    username: ~See-Kiong_Ng1
  decision: toMainConference
  end_page: 8748
  file: 973.pdf
  id: 973
  num_pages: 24
  openreview_id: sfs3mEiUDv
  pdf_file: 1d7bc44111c896d8bbeac960e2c1377170714501.pdf
  start_page: 8725
  title: 'SemRoDe: Macro Adversarial Training to Learn Representations that are Robust
    to Word-Level Attacks'
- abstract: "We introduce BUST, a comprehensive benchmark designed to evaluate detectors\
    \ of texts generated by instruction-tuned large language models (LLMs). Unlike\
    \ previous benchmarks, our focus lies on evaluating the performance of detector\
    \ systems, acknowledging the inevitable influence of the underlying tasks and\
    \ different LLM generators. Our benchmark dataset consists of 25K texts from humans\
    \ and 7 LLMs responding to instructions across 10 tasks from 3 diverse sources.\
    \ Using the benchmark, we evaluated 5 detectors and found substantial performance\
    \ variance across tasks. A meta-analysis of the dataset characteristics was conducted\
    \ to guide the examination of detector performance. The dataset was analyzed using\
    \ diverse metrics assessing linguistic features like fluency and coherence, readability\
    \ scores, and writer attitudes, such as emotions, convincingness, and persuasiveness.\
    \ Features impacting detector performance were investigated with surrogate models,\
    \ revealing emotional content in texts enhanced some detectors, yet the most effective\
    \ detector demonstrated consistent performance, irrespective of writer\u2019s\
    \ attitudes and text styles. Our approach focused on investigating relationships\
    \ between the detectors' performance and two key factors: text characteristics\
    \ and LLM generators. We believe BUST will provide valuable insights into selecting\
    \ detectors tailored to specific text styles and tasks and facilitate a more practical\
    \ and in-depth investigation of detection systems for LLM-generated text."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - emails: '****@idsia.ch'
    first_name: Joseph
    institution: SUPSI - University of Applied Sciences Southern Switzerland
    last_name: Cornelius
    name: Joseph Cornelius
    orcid: https://orcid.org/0000-0002-5427-5005
    username: ~Joseph_Cornelius1
  - emails: '****@gmail.com'
    first_name: Oscar
    institution: The Swiss AI Lab (IDSIA)
    last_name: Lithgow-Serrano
    middle_name: William
    name: Oscar William Lithgow-Serrano
    orcid: https://orcid.org/0000-0003-1995-1669
    username: ~Oscar_William_Lithgow-Serrano1
  - dblp_id: https://dblp.org/pid/167/3607.html
    emails: '****@idsia.ch'
    first_name: Sandra
    google_scholar_id: https://scholar.google.com/citations?user=SBQZHb0AAAAJ&hl=en&oi=ao
    institution: IDSIA-USI/SUPSI
    last_name: Mitrovic
    name: Sandra Mitrovic
    orcid: https://orcid.org/0000-0002-5697-5865
    username: ~Sandra_Mitrovic1
  - dblp_id: https://dblp.org/pid/82/5397.html
    emails: '****@ar.admin.ch'
    first_name: Ljiljana
    institution: armasuisse
    last_name: Dolamic
    name: Ljiljana Dolamic
    username: ~Ljiljana_Dolamic1
  - dblp_id: https://dblp.org/pid/48/6798
    emails: '****@gmail.com'
    first_name: Fabio
    google_scholar_id: https://scholar.google.com/citations?user=aCD4rSEAAAAJ&hl=en
    homepage: https://nlp.idsia.ch/fabiorinaldi.html
    institution: IDSIA
    last_name: Rinaldi
    name: Fabio Rinaldi
    orcid: https://orcid.org/0000-0001-5718-5462
    semantic_scholar_id: https://www.semanticscholar.org/author/Fabio-Rinaldi/144386741
    username: ~Fabio_Rinaldi2
  decision: toMainConference
  end_page: 8777
  file: 976.pdf
  id: 976
  num_pages: 29
  openreview_id: RxQ5bNXlMN
  pdf_file: dda5b36b4509734111e949e77104b395447d35ae.pdf
  start_page: 8749
  title: 'BUST: Benchmark for the evaluation of detectors of LLM-Generated Text'
- abstract: Multilingual generative models obtain remarkable cross-lingual in-context
    learning capabilities through pre-training on large-scale corpora. However, they
    still exhibit a performance bias toward high-resource languages and learn isolated
    distributions of multilingual sentence representations, which may hinder knowledge
    transfer across languages. To bridge this gap, we propose a simple yet effective
    cross-lingual alignment framework exploiting pairs of translation sentences. It
    aligns the internal sentence representations across different languages via multilingual
    contrastive learning and aligns outputs by following cross-lingual instructions
    in the target language. Experimental results show that even with less than 0.1${\textperthousand}$
    of pre-training tokens, our alignment framework significantly boosts the cross-lingual
    abilities of generative language models and mitigates the performance gap. Further
    analyses reveal that it results in a better internal multilingual representation
    distribution of multilingual models.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - dblp_id: https://dblp.org/pid/50/3011
    emails: '****@ia.ac.cn'
    first_name: Chong
    google_scholar_id: https://scholar.google.com.hk/citations?user=aftZkxsAAAAJ&hl=zh-CN&oi=sra
    institution: Institute of automation, Chinese Academy of Sciences
    last_name: Li
    name: Chong Li
    semantic_scholar_id: https://www.semanticscholar.org/author/Chong-Li/2109665081
    username: ~Chong_Li6
  - dblp_id: https://dblp.org/pid/29/8236
    emails: '****@nlpr.ia.ac.cn'
    first_name: Shaonan
    google_scholar_id: https://scholar.google.com/citations?user=ydFT-G8AAAAJ&hl=zh-CN
    homepage: https://wangshaonan.github.io/
    last_name: Wang
    name: Shaonan Wang
    username: ~Shaonan_Wang1
  - dblp_id: https://dblp.org/pid/71/6950-1.html
    emails: '****@nlpr.ia.ac.cn'
    first_name: Jiajun
    google_scholar_id: https://scholar.google.com/citations?user=93zngeYAAAAJ&hl=en
    homepage: http://www.nlpr.ia.ac.cn/cip/jjzhang.htm
    institution: Institute of automation, Chinese academy of science, Chinese Academy
      of Sciences
    last_name: Zhang
    name: Jiajun Zhang
    username: ~Jiajun_Zhang1
  - dblp_id: https://dblp.org/pid/38/6093
    emails: '****@nlpr.ia.ac.cn'
    first_name: Chengqing
    google_scholar_id: https://scholar.google.com/citations?user=l8lvKOQAAAAJ&hl=zh-CN
    homepage: http://www.nlpr.ia.ac.cn/cip/english/zong.htm
    institution: Institute of automation, Chinese academy of science, Chinese Academy
      of Sciences
    last_name: Zong
    name: Chengqing Zong
    username: ~Chengqing_Zong1
  decision: toMainConference
  end_page: 8796
  file: 977.pdf
  id: 977
  num_pages: 19
  openreview_id: D2BBHlBjV1
  pdf_file: 8f7e5f85f77f40f373bdaf97636781d246d56127.pdf
  start_page: 8778
  title: Improving In-context Learning of Multilingual Generative Language Models
    with Cross-lingual Alignment
- abstract: "Pre-trained language models (PLMs) that rely solely on textual data may\
    \ exhibit limitations in multimodal semantics comprehension. Existing solutions\
    \ attempt to alleviate this issue by incorporating explicit image retrieval or\
    \ generation techniques.\nHowever, these methods: (1) focus exclusively on the\
    \ static image modality; (2) inevitably encounter modality gaps and noise; \n\
    (3) indiscriminately treat all modalities.\nIn this paper, we propose a novel\
    \ multimodal-augmented framework termed MaCSC, which can infuse multimodal semantics\
    \ into PLMs and \nfacilitate a self-balancing calibration of information allocation.\n\
    Specifically, MaCSC obtains modal-specific conceptual prototypes from contrastive\
    \ pre-training models (e.g., CLIP),\nand aggregates the intra- and inter-modal\
    \ semantics of the conceptual prototype to enhance PLMs.\nIn addition, we utilize\
    \ a novel self-balancing contrastive loss to achieve multi-scale self-balancing\
    \ calibration of multimodal information during fine-tuning PLMs.\nExperimental\
    \ results show that MaCSC consistently improves the performance of PLMs across\
    \ various architectures and scales, \nand outperforms competitive baselines on\
    \ multiple NLP tasks."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Efficient/Low-Resource Methods for NLP
  authors:
  - dblp_id: https://dblp.org/pid/339/2318
    emails: '****@stu.pku.edu.cn'
    first_name: Xianwei
    google_scholar_id: https://scholar.google.com/citations?user=A1TGx8kAAAAJ&hl=zh-CN
    last_name: Zhuang
    name: Xianwei Zhuang
    username: ~Xianwei_Zhuang2
  - emails: '****@stu.pku.edu.cn'
    first_name: Zhichang
    google_scholar_id: https://scholar.google.com/citations?hl=zh-CN&user=PBLUJ5kAAAAJ
    last_name: Wang
    name: Zhichang Wang
    username: ~Zhichang_Wang1
  - dblp_id: https://dblp.org/pid/239/5637
    emails: '****@stu.pku.edu.cn'
    first_name: Xuxin
    homepage: https://www.linkedin.com/in/chengxx/
    last_name: Cheng
    name: Xuxin Cheng
    username: ~Xuxin_Cheng3
  - emails: '****@stu.pku.edu.cn'
    first_name: Yuxin
    homepage: https://www.google.com.hk/
    last_name: Xie
    name: Yuxin Xie
    username: ~Yuxin_Xie2
  - emails: '****@stu.pku.edu.cn'
    first_name: Liming
    homepage: https://github.com/RachoLiang
    last_name: Liang
    name: Liming Liang
    username: ~Liming_Liang1
  - emails: '****@gmail.com'
    first_name: Yuexian
    google_scholar_id: https://scholar.google.com/citations?user=sfyr7zMAAAAJ&hl=zh-CN
    institution: Peking University
    last_name: Zou
    name: Yuexian Zou
    username: ~Yuexian_Zou4
  decision: toMainConference
  end_page: 8810
  file: 978.pdf
  id: 978
  num_pages: 14
  openreview_id: 6w8BaE1msx
  pdf_file: a5de5f0b4634fd218f11a9671f94f5ff6d9f534d.pdf
  start_page: 8797
  title: 'MaCSC: Towards Multimodal-augmented Pre-trained Language Models via Conceptual
    Prototypes and Self-balancing Calibration'
- abstract: Knowledge graphs (KGs) consist of links that describe relationships between
    entities. Due to the difficulty of manually enumerating all relationships between
    entities, automatically completing them is essential for KGs. Knowledge Graph
    Completion (KGC) is a task that infers unseen relationships between entities in
    a KG. Traditional embedding-based KGC methods (e.g. RESCAL, TransE, DistMult,
    ComplEx, RotatE, HAKE, HousE, etc.) infer missing links using only the knowledge
    from training data. In contrast, the recent Pre-trained Language Model (PLM)-based
    KGC utilizes knowledge obtained during pre-training, which means it can estimate
    missing links between entities by reusing memorized knowledge from pre-training
    without inference. This part is problematic because building KGC models aims to
    infer unseen links between entities. However, conventional evaluations in KGC
    do not consider inference and memorization abilities separately. Thus, a PLM-based
    KGC method, which achieves high performance in current KGC evaluations, may be
    ineffective in practical applications. To address this issue, we analyze whether
    PLM-based KGC methods make inferences or merely access memorized knowledge. For
    this purpose, we propose a method for constructing synthetic datasets specified
    in this analysis and conclude that PLMs acquire the inference abilities required
    for KGC through pre-training, even though the performance improvements mostly
    come from textual information of entities and relations.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Extraction
  authors:
  - dblp_id: https://dblp.org/pid/332/6403
    emails: '****@is.naist.jp'
    first_name: Yusuke
    google_scholar_id: https://scholar.google.co.jp/citations?user=a61O-DwAAAAJ
    homepage: https://www.yusuke1997.jp/
    institution: Nara Institute of Science and Technology, Japan
    last_name: Sakai
    name: Yusuke Sakai
    semantic_scholar_id: https://www.semanticscholar.org/author/Yusuke-Sakai/2204051777
    username: ~Yusuke_Sakai1
  - dblp_id: https://dblp.org/pers/k/Kamigaito:Hidetaka
    emails: '****@is.naist.jp'
    first_name: Hidetaka
    google_scholar_id: https://scholar.google.co.jp/citations?user=cyZpch8AAAAJ&hl=en
    homepage: https://sites.google.com/site/hidetakakamigaito/
    institution: Division of Information Science, Nara Institute of Science and Technology
    last_name: Kamigaito
    name: Hidetaka Kamigaito
    orcid: https://orcid.org/0000-0002-5249-5813
    semantic_scholar_id: https://www.semanticscholar.org/author/Hidetaka-Kamigaito/2300756
    username: ~Hidetaka_Kamigaito2
  - dblp_id: https://dblp.org/pid/23/9282
    emails: '****@gmail.com'
    first_name: Katsuhiko
    institution: The University of Tokyo
    last_name: Hayashi
    name: Katsuhiko Hayashi
    semantic_scholar_id: https://www.semanticscholar.org/author/K.-Hayashi/145245267
    username: ~Katsuhiko_Hayashi2
  - dblp_id: https://dblp.org/pid/50/4741
    emails: '****@is.naist.jp'
    first_name: Taro
    google_scholar_id: https://scholar.google.com/citations?user=zsEEy7kAAAAJ&hl=en
    homepage: https://sites.google.com/site/tarowtnb/
    institution: Nara Institute of Science and Technology, Japan
    last_name: Watanabe
    name: Taro Watanabe
    orcid: https://orcid.org/0000-0001-8349-3522
    semantic_scholar_id: https://www.semanticscholar.org/author/Taro-Watanabe/2110694221
    username: ~Taro_Watanabe1
  decision: toMainConference
  end_page: 8826
  file: 979.pdf
  id: 979
  num_pages: 16
  openreview_id: HFQDIcjV5f
  pdf_file: 400b3b6b35605922fb24250a9f32f5785dc7eb5b.pdf
  start_page: 8811
  title: Does Pre-trained Language Model Actually Infer Unseen Links in Knowledge
    Graph Completion?
- abstract: We discover alignments of views between interest groups (lobbies) and
    members of the European Parliament (MEPs) by automatically analyzing their texts.
    Specifically, we do so by collecting novel datasets of lobbies' position papers
    and MEPs' speeches, and comparing these texts on the basis of semantic similarity
    and entailment. In the absence of ground-truth, we perform an indirect validation
    by comparing the discovered alignments with a dataset, which we curate, of retweet
    links between MEPs and lobbies, and with the publicly disclosed meetings of MEPs.
    Our best method performs significantly better than several baselines. Moreover,
    an aggregate analysis of the discovered alignments, between groups of related
    lobbies and political groups of MEPs, correspond to the expectations from the
    ideology of the groups (e.g., groups on the political left are more aligned with
    humanitarian and environmental organisations). We believe that this work is a
    step towards enhancing the transparency of the intricate decision-making processes
    within democratic institutions.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Computational Social Science and Cultural Analytics
  authors:
  - dblp_id: https://dblp.org/pid/221/0356
    emails: '****@gmail.com'
    first_name: Aswin
    google_scholar_id: https://scholar.google.ch/citations?user=-tpZsh8AAAAJ&hl=en
    homepage: https://people.epfl.ch/aswin.suresh?lang=en
    institution: Independent Consultant
    last_name: Suresh
    name: Aswin Suresh
    username: ~Aswin_Suresh1
  - emails: '****@epfl.ch'
    first_name: Lazar
    institution: EPFL - EPF Lausanne
    last_name: "Radojevi\u0107"
    name: "Lazar Radojevi\u0107"
    username: "~Lazar_Radojevi\u01071"
  - emails: '****@epfl.ch'
    first_name: Francesco
    google_scholar_id: https://scholar.google.com/citations?user=70M6sE8AAAAJ&hl=en&oi=ao
    homepage: http://frasalvi.github.io/
    institution: EPFL - EPF Lausanne
    last_name: Salvi
    name: Francesco Salvi
    orcid: https://orcid.org/0009-0001-6884-6825
    semantic_scholar_id: https://www.semanticscholar.org/author/2268398487
    username: ~Francesco_Salvi1
  - emails: '****@epfl.ch'
    first_name: Antoine
    homepage: https://github.com/magantoine
    last_name: Magron
    name: Antoine Magron
    username: ~Antoine_Magron1
  - dblp_id: https://dblp.org/pid/186/7793
    emails: '****@me.com'
    first_name: Victor
    google_scholar_id: https://scholar.google.com/citations?user=OCD50S4AAAAJ
    homepage: https://victorkristof.me
    last_name: Kristof
    name: Victor Kristof
    username: ~Victor_Kristof1
  - dblp_id: https://dblp.org/pid/g/MGrossglauser
    emails: '****@epfl.ch'
    first_name: Matthias
    google_scholar_id: https://scholar.google.com/citations?user=0PNdFHcAAAAJ&hl=en
    homepage: https://indy.epfl.ch/grossglauser/
    institution: EPFL
    last_name: Grossglauser
    name: Matthias Grossglauser
    username: ~Matthias_Grossglauser1
  decision: toMainConference
  end_page: 8840
  file: 987.pdf
  id: 987
  num_pages: 14
  openreview_id: qdRpyJXg0k
  pdf_file: 38f5e51f5113b46a4aaf1d2d01e1d672092aebae.pdf
  start_page: 8827
  title: Discovering Lobby-Parliamentarian Alignments through NLP
- abstract: "Conversational search aims to retrieve passages containing essential\
    \ information to answer queries in a multi-turn conversation. \nIn conversational\
    \ search, reformulating context-dependent conversational queries into stand-alone\
    \ forms is imperative to effectively utilize off-the-shelf retrievers. \nPrevious\
    \ methodologies for conversational query reformulation frequently depend on human-annotated\
    \ rewrites.\nHowever, these manually crafted queries often result in sub-optimal\
    \ retrieval performance and require high collection costs.\nTo address these challenges,\
    \ we propose **Iter**ative **C**onversational **Q**uery **R**eformulation (**IterCQR**),\
    \ a methodology that conducts query reformulation without relying on human rewrites.\
    \ \nIterCQR iteratively trains the conversational query reformulation (CQR) model\
    \ by directly leveraging information retrieval (IR) signals as a reward.\nOur\
    \ IterCQR training guides the CQR model such that generated queries contain necessary\
    \ information from the previous dialogue context.\nOur proposed method shows state-of-the-art\
    \ performance on two widely-used datasets, demonstrating its effectiveness on\
    \ both sparse and dense retrievers. \nMoreover, IterCQR exhibits superior performance\
    \ in challenging settings such as generalization on unseen datasets and low-resource\
    \ scenarios."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Dialogue and Interactive Systems
  authors:
  - emails: '****@snu.ac.kr'
    first_name: Yunah
    last_name: Jang
    name: Yunah Jang
    orcid: https://orcid.org/0000-0002-2805-7530
    username: ~Yunah_Jang1
  - emails: '****@snu.ac.kr'
    first_name: Kang-il
    google_scholar_id: https://scholar.google.co.kr/citations?user=-YroyxsAAAAJ&hl=en
    institution: Seoul National University
    last_name: Lee
    name: Kang-il Lee
    semantic_scholar_id: https://www.semanticscholar.org/author/K.-Lee/2115495428
    username: ~Kang-il_Lee1
  - emails: '****@lgresearch.ai'
    first_name: Hyunkyung
    homepage: https://github.com/jennybae1024
    institution: LG AI Research
    last_name: Bae
    name: Hyunkyung Bae
    username: ~Hyunkyung_Bae1
  - dblp_id: https://dblp.org/pid/218/5402
    emails: '****@cau.ac.kr'
    first_name: Hwanhee
    google_scholar_id: https://scholar.google.com/citations?user=eRM8zHkAAAAJ&hl=en
    homepage: https://hwanheelee1993.github.io/
    institution: Chung-Ang University
    last_name: Lee
    name: Hwanhee Lee
    semantic_scholar_id: https://www.semanticscholar.org/author/Hwanhee-Lee/2109339794
    username: ~Hwanhee_Lee1
  - dblp_id: https://dblp.org/pid/48/3867
    emails: '****@snu.ac.kr'
    first_name: Kyomin
    google_scholar_id: https://scholar.google.co.kr/citations?user=u3uMl4MAAAAJ&hl=en
    homepage: http://milab.snu.ac.kr/kjung/index.html
    last_name: Jung
    name: Kyomin Jung
    username: ~Kyomin_Jung1
  decision: toMainConference
  end_page: 8858
  file: 988.pdf
  id: 988
  num_pages: 18
  openreview_id: SCOqjSWRf6
  pdf_file: 000943c13c77759a69ce16b3ae6a24596a5f17d8.pdf
  start_page: 8841
  title: 'IterCQR: Iterative Conversational Query Reformulation with Retrieval Guidance'
- abstract: Retrieval-augmented generation framework addresses the limitations of
    large language models by enabling real-time knowledge updates for more accurate
    answers. An efficient way in the training phase of retrieval-augmented models
    is attention distillation, which uses attention scores as supervision signals
    instead of manually annotated query-document pairs. Despite its growing popularity,
    the detailed mechanisms behind the success of attention distillation remain unexplored,
    particularly the specific patterns it leverages to benefit training. In this paper,
    we address this gap by conducting a comprehensive investigation of attention distillation
    workflow and identifying key factors influencing the learning performance of retrieval-augmented
    language models. We further propose several insightful indicators for optimizing
    models' training methods and avoiding ineffective training.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - emails: '****@ucdavis.edu'
    first_name: Zizhong
    institution: University of California, Davis
    last_name: Li
    name: Zizhong Li
    orcid: https://orcid.org/0000-0001-5350-8076
    username: ~Zizhong_Li1
  - dblp_id: https://dblp.org/pid/324/2199
    emails: '****@hawaii.edu'
    first_name: Haopeng
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=DT_HlbYAAAAJ
    homepage: https://hpzhang94.github.io/
    last_name: Zhang
    name: Haopeng Zhang
    semantic_scholar_id: https://www.semanticscholar.org/author/2135688409
    username: ~Haopeng_Zhang3
  - dblp_id: https://dblp.org/pid/10/239-1
    emails: '****@ifmlab.org'
    first_name: Jiawei
    google_scholar_id: https://scholar.google.com/citations?user=7AkZSJsAAAAJ&hl=en
    homepage: http://jiaweizhang.net/
    institution: University of California, Davis
    last_name: Zhang
    name: Jiawei Zhang
    username: ~Jiawei_Zhang3
  decision: toMainConference
  end_page: 8868
  file: 989.pdf
  id: 989
  num_pages: 10
  openreview_id: PAgcLyNqYO
  pdf_file: 80e04a1eda54dd0606698eacc93d6a1308749aef.pdf
  start_page: 8859
  title: 'Unveiling the Magic: Investigating Attention Distillation in Retrieval-Augmented
    Generation'
- abstract: Factual accuracy is an important property of neural abstractive summarization
    models, especially in fact-critical domains such as the clinical literature. In
    this work, we introduce a guided continued pre-training stage for encoder-decoder
    models that improves their understanding of the factual attributes of documents,
    which is followed by supervised fine-tuning on summarization. Our approach extends
    the pre-training recipe of BART to incorporate 3 additional objectives based on
    PICO spans, which capture the population, intervention, comparison, and outcomes
    related to a clinical study. Experiments on multi-document summarization in the
    clinical domain demonstrate that our approach is competitive with prior work,
    improving the quality and factuality of the summaries and achieving the best-published
    results in factual accuracy on the MSLR task.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Summarization
  authors:
  - emails: '****@gmail.com'
    first_name: Ahmed
    homepage: https://ahmedselhady.github.io/
    last_name: Elhady
    middle_name: Salem
    name: Ahmed Salem Elhady
    username: ~Ahmed_Salem_Elhady1
  - emails: '****@zewailcity.edu.eg'
    first_name: Khaled
    google_scholar_id: https://scholar.google.com.eg/citations?user=cnfPK10AAAAJ&hl=en
    institution: Cairo University
    last_name: Elsayed
    middle_name: Mostafa
    name: Khaled Mostafa Elsayed
    username: ~Khaled_Mostafa_Elsayed1
  - dblp_id: https://dblp.org/pid/a/EnekoAgirre
    emails: '****@ehu.eus'
    first_name: Eneko
    google_scholar_id: https://scholar.google.es/citations?user=kSuqts0AAAAJ&hl=en
    homepage: http://ixa.si.ehu.eus/eneko
    institution: University of the Basque Country (UPV/EHU)
    last_name: Agirre
    name: Eneko Agirre
    username: ~Eneko_Agirre1
  - dblp_id: https://dblp.org/pid/168/0354
    emails: '****@reka.ai'
    first_name: Mikel
    google_scholar_id: https://scholar.google.com/citations?user=N5InzP8AAAAJ
    homepage: http://www.mikelartetxe.com
    institution: Reka AI
    last_name: Artetxe
    name: Mikel Artetxe
    semantic_scholar_id: https://www.semanticscholar.org/author/Mikel-Artetxe/2347956
    username: ~Mikel_Artetxe1
  decision: toMainConference
  end_page: 8875
  file: 991.pdf
  id: 991
  num_pages: 7
  openreview_id: KoElJ1eOn0
  pdf_file: f0c29d84dddfb50cbff8e93b9f2b5b5d8cb566e1.pdf
  start_page: 8869
  title: Improving Factuality in Clinical Abstractive Multi-Document Summarization
    by Guided Continued Pre-training
- abstract: "This paper is devoted to the development of \n a localized Large Language\
    \ Model (LLM) specifically for Arabic, a language imbued with unique cultural\
    \ characteristics inadequately addressed by current mainstream models. Significant\
    \ concerns emerge when addressing cultural sensitivity and local values. To address\
    \ this, the paper proposes a comprehensive solution that includes further pre-training\
    \ with Arabic texts, Supervised Fine-Tuning (SFT) utilizing native Arabic instructions,\
    \ and GPT-4 responses in Arabic, alongside Reinforcement Learning with AI Feedback\
    \ (RLAIF) employing a reward model attuned to local culture and values. The goal\
    \ is to cultivate culturally cognizant and value-aligned Arabic LLMs capable of\
    \ accommodating the diverse, application-specific needs of Arabic-speaking communities.\n\
    \ Comprehensive evaluations reveal that the resulting model, dubbed `AceGPT',\
    \ sets the state-of-the-art standard for open Arabic LLMs across various benchmarks.\
    \ Codes, data, and models are in https://github.com/FreedomIntelligence/AceGPT."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multilinguality and Language Diversity
  authors:
  - emails: '****@sribd.cn'
    first_name: Huang
    google_scholar_id: https://scholar.google.com/citations?user=0JhMor8AAAAJ&hl=zh-CN
    institution: Shenzhen Research Institute of Big Data
    last_name: Huang
    name: Huang Huang
    username: ~Huang_Huang2
  - emails: '****@link.cuhk.edu.cn'
    first_name: Fei
    google_scholar_id: https://scholar.google.com/citations?user=EsCgPkQAAAAJ&hl=en
    last_name: Yu
    name: Fei Yu
    username: ~Fei_Yu3
  - emails: '****@emails.bjut.edu.cn'
    first_name: Jianqing
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=u6zwQf8AAAAJ&view_op=list_works&gmla=AJsN-F4Q2T4tKa5K_Oxs5FGDQwZ1Q5Ch2ie6K-eYlAix0DXPxvuYjMBpbu2s8QfFmJgDhd2KQb9spSpTT-hBLwLZFdYYjawvjKeOBtVqmsWftgGsIgw5II5mJyxqBjsIa2loztjt-uqTxlodCxlT1hqg9iFbjNy5wQ
    last_name: Zhu
    name: Jianqing Zhu
    username: ~Jianqing_Zhu2
  - emails: '****@163.com'
    first_name: Xuening
    homepage: https://github.com/614479467
    last_name: Sun
    name: Xuening Sun
    username: ~Xuening_Sun2
  - emails: '****@link.cuhk.edu.cn'
    first_name: Hao
    homepage: https://markch00.github.io/
    last_name: Cheng
    name: Hao Cheng
    username: ~Hao_Cheng18
  - emails: '****@outlook.com'
    first_name: Song
    homepage: https://github.com/bbsngg
    last_name: Dingjie
    name: Song Dingjie
    username: ~Song_Dingjie1
  - dblp_id: https://dblp.org/pid/78/3726
    emails: '****@stanford.edu'
    first_name: Zhihong
    google_scholar_id: https://scholar.google.com/citations?user=y55sF8cAAAAJ&hl=en
    institution: Stanford University and THE CHINESE UNIVERSITY OF HONG KONG, SHENZHEN
    last_name: Chen
    name: Zhihong Chen
    semantic_scholar_id: https://www.semanticscholar.org/author/Zhihong-Chen/46843171
    username: ~Zhihong_Chen2
  - emails: '****@kaust.edu.sa'
    first_name: Mosen
    homepage: https://scholar.google.com/citations?hl=en&user=eMfvcJ4AAAAJ
    institution: King Abdullah University of Science and Technology
    last_name: Alharthi
    name: Mosen Alharthi
    username: ~Mosen_Alharthi1
  - emails: '****@kaust.edu.sa'
    first_name: Bang
    homepage: https://cemse.kaust.edu.sa/amcs/people/person/bang
    last_name: An
    name: Bang An
    username: ~Bang_An3
  - dblp_id: https://dblp.org/pid/223/4286
    emails: '****@pku.edu.cn'
    first_name: Juncai
    google_scholar_id: https://scholar.google.com/citations?user=CG5GBW0AAAAJ&hl=zh-CN&oi=ao
    homepage: https://juncaihe.github.io
    institution: King Abdullah University of Science and Technology
    last_name: He
    name: Juncai He
    username: ~Juncai_He1
  - emails: '****@link.cuhk.edu.cn'
    first_name: Ziche
    homepage: https://github.com/tREeFrOGcoder
    last_name: Liu
    name: Ziche Liu
    username: ~Ziche_Liu1
  - dblp_id: https://dblp.org/pid/06/3331
    emails: '****@gmail.com'
    first_name: Junying
    google_scholar_id: https://scholar.google.com.hk/citations?user=I0raPTYAAAAJ
    last_name: Chen
    name: Junying Chen
    semantic_scholar_id: https://www.semanticscholar.org/author/Junying-Chen/2108170007
    username: ~Junying_Chen2
  - emails: '****@163.com'
    first_name: Jianquan
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=PKqbGxoAAAAJ
    last_name: Li
    name: Jianquan Li
    username: ~Jianquan_Li1
  - dblp_id: https://dblp.org/pid/169/1793
    emails: '****@gmail.com'
    first_name: Benyou
    google_scholar_id: https://scholar.google.com/citations?user=Jk4vJU8AAAAJ&hl=en
    homepage: https://wabyking.github.io/old.html
    institution: The Chinese University of Hong Kong, Shenzhen
    last_name: Wang
    name: Benyou Wang
    orcid: https://orcid.org/0000-0002-1501-9914
    username: ~Benyou_Wang2
  - emails: '****@multigrid.org'
    first_name: Lian
    institution: Shenzhen Research Institute of Big Data
    last_name: Zhang
    name: Lian Zhang
    username: ~Lian_Zhang2
  - dblp_id: https://dblp.org/pid/30/9879-1
    emails: '****@illinois.edu'
    first_name: Ruoyu
    google_scholar_id: https://scholar.google.com/citations?user=PsfzbCMAAAAJ&hl=en&oi=ao
    homepage: https://ruoyus.github.io/
    institution: University of Illinois, Urbana-Champaign
    last_name: Sun
    name: Ruoyu Sun
    username: ~Ruoyu_Sun1
  - emails: '****@sribd.cn'
    first_name: Xiang
    homepage: http://www.sribd.cn/teacher/28
    institution: Shenzhen Research Institute of Big Data
    last_name: Wan
    name: Xiang Wan
    username: ~Xiang_Wan1
  - dblp_id: https://dblp.org/pid/36/4118
    emails: '****@cuhk.edu.cn'
    first_name: Haizhou
    google_scholar_id: https://scholar.google.com.sg/citations?user=z8_x7C8AAAAJ&hl=en
    homepage: https://colips.org/~eleliha/
    institution: The Chinese University of Hong Kong (Shenzhen); National University
      of Singapore and National University of Singapore
    last_name: Li
    name: Haizhou Li
    orcid: https://orcid.org/0000-0001-9158-9401
    semantic_scholar_id: https://www.semanticscholar.org/author/Haizhou-Li/1711271
    username: ~Haizhou_Li3
  - emails: '****@multigrid.org'
    first_name: Jinchao
    google_scholar_id: https://scholar.google.com/citations?user=pBHiYxcAAAAJ&hl=fr&oi=ao
    homepage: https://www.personal.psu.edu/jxx1/
    institution: King Abdullah University of Science and Technology and Pennsylvania
      State University
    last_name: Xu
    name: Jinchao Xu
    username: ~Jinchao_Xu1
  decision: toMainConference
  end_page: 8900
  file: 994.pdf
  id: 994
  num_pages: 25
  openreview_id: kPbt4MnKet
  pdf_file: 2e861a18fdf87da3099ce7226e9595990938e275.pdf
  start_page: 8876
  title: AceGPT, Localizing Large Language Models in Arabic
- abstract: 'Insufficient modeling of human preferences within the reward model is
    a major obstacle for leveraging human feedback to improve translation quality.
    Fortunately, quality estimation (QE), which predicts the quality of a given translation
    without reference, has achieved impressive alignment with human evaluations in
    the last two years. In this work, we investigate the potential of employing the
    QE model as the reward model to predict human preferences for feedback training.
    We first identify the overoptimization problem during QE-based feedback training,
    manifested as an increase in reward while translation quality declines. We examine
    the problem and argue that the vulnerability of the QE model might lead to high
    rewards for incorrect translations, resulting in overoptimization and error propagation.
    To address the problem, we adopt a simple yet effective method that uses heuristic
    rules to detect the incorrect translations and assigns a penalty term to the reward
    scores of them. Experimental results show that the proposed QE-based feedback
    training achieves consistent and significant improvements across various settings,
    further verified through human preference studies. Our subsequent analysis demonstrates
    the high data efficiency of the proposed QE-based feedback training: it outperforms
    systems using larger parallel corpora by a small amount of monolingual data. Our
    code is available at: https://github.com/zwhe99/FeedbackMT'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Translation
  authors:
  - dblp_id: https://dblp.org/pid/52/6077-2
    emails: '****@gmail.com'
    first_name: Zhiwei
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=SL-AdukAAAAJ
    homepage: https://zwhe99.github.io/
    institution: Shanghai Jiao Tong University
    last_name: He
    name: Zhiwei He
    orcid: https://orcid.org/0000-0002-4807-0062
    semantic_scholar_id: https://www.semanticscholar.org/author/Zhiwei-He/2610876
    username: ~Zhiwei_He1
  - dblp_id: https://dblp.org/pid/02/3674-7
    emails: '****@gmail.com'
    first_name: Xing
    google_scholar_id: https://scholar.google.com/citations?user=6AqRKa0AAAAJ&hl=en
    homepage: http://xingwang4nlp.com/
    institution: Tencent AI Lab
    last_name: Wang
    name: Xing Wang
    orcid: https://orcid.org/0000-0002-0737-9653
    semantic_scholar_id: https://www.semanticscholar.org/author/Xing-Wang/48631170
    username: ~Xing_Wang1
  - dblp_id: https://dblp.org/pid/239/4883
    emails: '****@tencent.com'
    first_name: Wenxiang
    google_scholar_id: https://scholar.google.com/citations?user=CvtODukAAAAJ&hl=en
    homepage: https://wxjiao.github.io/
    institution: Tencent AI Lab
    last_name: Jiao
    name: Wenxiang Jiao
    semantic_scholar_id: https://www.semanticscholar.org/author/Wenxiang-Jiao/12386833
    username: ~Wenxiang_Jiao1
  - dblp_id: https://dblp.org/pid/06/9708
    emails: '****@sjtu.edu.cn'
    first_name: Zhuosheng
    google_scholar_id: https://scholar.google.co.jp/citations?user=63LTQhgAAAAJ
    homepage: https://bcmi.sjtu.edu.cn/~zhangzs/
    institution: Shanghai Jiao Tong University
    last_name: Zhang
    name: Zhuosheng Zhang
    orcid: https://orcid.org/0000-0002-4183-3645
    semantic_scholar_id: https://www.semanticscholar.org/author/Zhuosheng-Zhang/3322871
    username: ~Zhuosheng_Zhang1
  - dblp_id: https://dblp.org/pid/w/RuiWang15
    emails: '****@sjtu.edu.cn'
    first_name: Rui
    google_scholar_id: https://scholar.google.com/citations?user=oTU0v5IAAAAJ&h
    homepage: https://wangruinlp.github.io/
    institution: Shanghai Jiao Tong University
    last_name: Wang
    name: Rui Wang
    orcid: https://orcid.org/0000-0001-8007-2503
    username: ~Rui_Wang10
  - dblp_id: https://dblp.org/pid/s/ShumingShi
    emails: '****@hotmail.com'
    first_name: Shuming
    google_scholar_id: https://scholar.google.com/citations?user=Lg31AKMAAAAJ&hl=en&oi=ao
    institution: Tencent AI Lab
    last_name: Shi
    name: Shuming Shi
    semantic_scholar_id: https://www.semanticscholar.org/author/Shuming-Shi/34720053
    username: ~Shuming_Shi1
  - dblp_id: https://dblp.org/pid/71/9281
    emails: '****@gmail.com'
    first_name: Zhaopeng
    google_scholar_id: https://scholar.google.com/citations?user=IvE2zRgAAAAJ&hl=en
    homepage: http://www.zptu.net
    institution: Tencent AI Lab
    last_name: Tu
    name: Zhaopeng Tu
    semantic_scholar_id: https://www.semanticscholar.org/author/Zhaopeng-Tu/2909321
    username: ~Zhaopeng_Tu1
  decision: toMainConference
  end_page: 8917
  file: 996.pdf
  id: 996
  num_pages: 17
  openreview_id: B8UKRKeHJ4
  pdf_file: 67d6305dda8a1bb0f76e0a7520511d957eb90daa.pdf
  start_page: 8901
  title: 'Improving Machine Translation with Human Feedback: An Exploration of Quality
    Estimation as a Reward Model'
- abstract: Depression is a widespread mental health disorder affecting millions globally.
    Clinical interviews are the gold standard for assessing depression, but they heavily
    rely on scarce professional clinicians, highlighting the need for automated detection
    systems. However, existing methods only capture part of the relevant elements
    in clinical interviews, unable to incorporate all depressive cues. Moreover, the
    scarcity of participant data, due to privacy concerns and collection challenges,
    intrinsically constrains interview modeling. To address these limitations, in
    this paper, we propose a structural element graph (SEGA), which transforms the
    clinical interview into an expertise-inspired directed acyclic graph for comprehensive
    modeling. Additionally, we further empower SEGA by devising novel principle-guided
    data augmentation with large language models (LLMs) to supplement high-quality
    synthetic data and enable graph contrastive learning.  Extensive evaluations on
    two real-world clinical datasets, in both English and Chinese, show that SEGA
    significantly outperforms baseline methods and powerful LLMs like GPT-3.5 and
    GPT-4.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Linguistic theories, Cognitive Modeling and Psycholinguistics
  authors:
  - dblp_id: https://dblp.uni-trier.de/pid/54/8492-2
    emails: '****@mail.tsinghua.edu.cn'
    first_name: Zhuang
    google_scholar_id: https://scholar.google.com/citations?user=KzPXllsAAAAJ&hl
    homepage: http://zhuangchen.tech
    last_name: Chen
    name: Zhuang Chen
    orcid: https://orcid.org/0000-0002-7048-7833
    semantic_scholar_id: https://www.semanticscholar.org/author/Zhuang-Chen/46842388
    username: ~Zhuang_Chen1
  - emails: '****@gmail.com'
    first_name: Jiawen
    google_scholar_id: https://scholar.google.com/citations?user=fseN_08AAAAJ&hl=zh-CN&oi=ao
    institution: University of Electronic Science and Technology of China
    last_name: Deng
    name: Jiawen Deng
    orcid: https://orcid.org/0000-0003-0602-8250
    semantic_scholar_id: https://www.semanticscholar.org/author/Deng-Jiawen/150076569
    username: ~Jiawen_Deng1
  - dblp_id: https://dblp.org/pid/305/6557
    emails: '****@tju.edu.cn'
    first_name: Jinfeng
    google_scholar_id: https://scholar.google.com/citations?user=y58dUQgAAAAJ&hl=zh-CN
    last_name: Zhou
    name: Jinfeng Zhou
    semantic_scholar_id: https://www.semanticscholar.org/author/Jinfeng-Zhou/2145787511
    username: ~Jinfeng_Zhou1
  - emails: '****@gmail.com'
    first_name: Jincenzi
    homepage: https://kkkkkkkkkkim.github.io/
    last_name: Wu
    name: Jincenzi Wu
    username: ~Jincenzi_Wu1
  - dblp_id: https://dblp.org/pid/17/5583
    emails: '****@whu.edu.cn'
    first_name: Tieyun
    google_scholar_id: https://scholar.google.com/citations?user=MYTt4EwAAAAJ
    institution: Wuhan University
    last_name: Qian
    name: Tieyun Qian
    orcid: https://orcid.org/0000-0003-4667-5794
    semantic_scholar_id: https://www.semanticscholar.org/author/34559283
    username: ~Tieyun_Qian1
  - dblp_id: https://dblp.org/pid/47/6668.html
    emails: '****@tsinghua.edu.cn'
    first_name: Minlie
    google_scholar_id: https://scholar.google.com/citations?hl=zh-CN&user=P1jPSzMAAAAJ&view_op=list_works
    homepage: http://coai.cs.tsinghua.edu.cn/hml
    institution: Tsinghua University, Tsinghua University
    last_name: Huang
    name: Minlie Huang
    semantic_scholar_id: https://www.semanticscholar.org/author/Minlie-Huang/1730108
    username: ~Minlie_Huang1
  decision: toMainConference
  end_page: 8931
  file: 1000.pdf
  id: 1000
  num_pages: 14
  openreview_id: 1Fwr2JmLVU
  pdf_file: e2efccace5c0df90dede3c36b0ee655ab3d52e3c.pdf
  start_page: 8918
  title: Depression Detection in Clinical Interviews with LLM-Empowered Structural
    Element Graph
- abstract: "Task-oriented dialogue (TOD) systems help users execute well-defined\
    \ tasks across a variety of domains (e.g., \\textit{flight booking} or \\textit{food\
    \ ordering}), with their Natural Language Understanding (NLU) components being\
    \ dedicated to the analysis of user utterances, predicting users\u2019 intents\
    \ (\\textit{Intent Detection}, ID) and extracting values for informational slots\
    \ (\\textit{Value Extraction}, VE). In most domains, labelled NLU data is scarce,\
    \ making sample-efficient learning \u2013 enabled with effective transfer paradigms\
    \ \u2013 paramount. In this work, we introduce SQATIN, a new framework for dialog\
    \ NLU based on (i) instruction tuning and (ii) question-answering-based formulation\
    \ of ID and VE tasks. According to the evaluation on established NLU benchmarks,\
    \ SQATIN sets the new state of the art in dialogue NLU, substantially surpassing\
    \ the performance of current models based on standard fine-tuning objectives in\
    \ both in-domain training and cross-domain transfer, and it also surpasses off-the-shelf\
    \ large language models for the same task, both in terms of performance and inference\
    \ efficiency.  Furthermore, SQATIN yields particularly large performance gains\
    \ in cross-domain transfer, owing to the fact that our QA-based instruction tuning\
    \ leverages similarities between natural language descriptions of classes (i.e.,\
    \ slots and intents) across domains."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Dialogue and Interactive Systems
  authors:
  - dblp_id: https://dblp.org/pid/234/7680
    emails: '****@cam.ac.uk'
    first_name: Evgeniia
    google_scholar_id: https://scholar.google.com/citations?user=grFuVx0AAAAJ&hl=en&oi=ao
    homepage: https://evgeniiaraz.github.io/
    institution: University of Cambridge
    last_name: Razumovskaia
    name: Evgeniia Razumovskaia
    semantic_scholar_id: https://www.semanticscholar.org/author/E.-Razumovskaia/66879943
    username: ~Evgeniia_Razumovskaia1
  - dblp_id: https://dblp.org/pid/50/11059
    emails: '****@gmail.com'
    first_name: Goran
    google_scholar_id: https://scholar.google.com/citations?user=Ym0myOwAAAAJ&hl=hr
    homepage: https://sites.google.com/view/goranglavas
    institution: "Julius-Maximilians-Universit\xE4t W\xFCrzburg"
    last_name: "Glava\u0161"
    name: "Goran Glava\u0161"
    semantic_scholar_id: https://www.semanticscholar.org/author/Goran-Glavas/2472657
    username: "~Goran_Glava\u01611"
  - dblp_id: https://dblp.org/pid/14/6532
    emails: '****@cam.ac.uk'
    first_name: Anna
    google_scholar_id: https://scholar.google.co.uk/citations?user=SCoVoOYAAAAJ&hl=en
    homepage: https://sites.google.com/site/annakorhonen/
    institution: University of Cambridge
    last_name: Korhonen
    name: Anna Korhonen
    username: ~Anna_Korhonen1
  - dblp_id: https://dblp.org/pid/77/9768
    emails: '****@cam.ac.uk'
    first_name: Ivan
    google_scholar_id: https://scholar.google.com/citations?user=ZX8js60AAAAJ&hl=en
    homepage: https://sites.google.com/site/ivanvulic/
    institution: University of Cambridge and PolyAI Limited
    last_name: "Vuli\u0107"
    name: "Ivan Vuli\u0107"
    semantic_scholar_id: https://www.semanticscholar.org/author/Ivan-Vulic/1747849
    username: "~Ivan_Vuli\u01071"
  decision: toMainConference
  end_page: 8948
  file: 1009.pdf
  id: 1009
  num_pages: 17
  openreview_id: fDJ9zyKNEQ
  pdf_file: 70ccc6adfa2bb78b8e59e94da55fb5fb96bf0942.pdf
  start_page: 8932
  title: 'SQATIN: Supervised Instruction Tuning Meets Question Answering for Improved
    Dialogue NLU'
- abstract: The proliferation of social media platforms has given rise to the amount
    of online debates and arguments. Consequently, the need for automatic summarization
    methods for such debates is imperative, however this area of summarization is
    rather understudied. The  Key Point Analysis (KPA) task formulates argument summarization
    as representing the summary of a large collection of  arguments in the form of
    concise sentences in bullet-style format, called key points. A sub-task of KPA,
    called Key Point Generation (KPG), focuses on generating these key points given
    the arguments. This paper introduces a novel extractive approach for key point
    generation, that outperforms previous state-of-the-art methods for the task. Our
    method utilizes an extractive clustering based approach that offers concise, high
    quality generated key points with higher coverage of reference summaries, and
    less redundant outputs. In addition, we show that the existing evaluation metrics
    for summarization such as ROUGE are incapable of differentiating between generated
    key points of different qualities. To this end, we propose a new evaluation metric
    for assessing the generated key points by their coverage.  Our code can be accessed
    online.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Sentiment Analysis, Stylistic Analysis, and Argument Mining
  authors:
  - emails: '****@gmail.com'
    first_name: Mohammad
    last_name: Khosravani
    name: Mohammad Khosravani
    username: ~Mohammad_Khosravani1
  - dblp_id: https://dblp.org/pid/168/0934-1
    emails: '****@ualberta.ca'
    first_name: Chenyang
    google_scholar_id: https://scholar.google.ca/citations?user=uu2UN8AAAAAJ&hl=en
    homepage: http://webdocs.cs.ualberta.ca/~chuang8/
    last_name: Huang
    name: Chenyang Huang
    orcid: https://orcid.org/0000-0003-2811-6008
    semantic_scholar_id: https://www.semanticscholar.org/author/Chenyang-Huang/50418713
    username: ~Chenyang_Huang1
  - dblp_id: https://dblp.org/pid/67/3066
    emails: '****@usherbrooke.ca'
    first_name: Amine
    google_scholar_id: https://scholar.google.com/citations?user=YSrvU5AAAAAJ&hl=en&oi=ao
    institution: "Universit\xE9 de Sherbrooke"
    last_name: Trabelsi
    name: Amine Trabelsi
    semantic_scholar_id: https://www.semanticscholar.org/author/Amine-Trabelsi/2880251
    username: ~Amine_Trabelsi1
  decision: toMainConference
  end_page: 8961
  file: 1010.pdf
  id: 1010
  num_pages: 13
  openreview_id: 80tEsVqk5W
  pdf_file: 6d0829d77c17670f7430551c251fbab70dd05bd9.pdf
  start_page: 8949
  title: 'Enhancing Argument Summarization: Prioritizing Exhaustiveness in Key Point
    Generation and Introducing an Automatic Coverage Evaluation Metric'
- abstract: "While large language models (LLMs) trained with large-scale unsupervised\
    \ learning acquire a wide variety of world knowledge and skills, its behavior\
    \ does not necessarily align with human preferences. RLHF methods achieve successes\
    \ in aligning LLM responses with human preferences and improving the controllability\
    \ of LLM behavior with human instruction. However, RLHF methods are considerably\
    \ complicated to implement, computationally expensive to train, and notoriously\
    \ tricky to tune. In this work, we propose Alignment with Residual Energy-Based\
    \ Model (ARM), as a simple and flexible alternative to RLHF methods. Our method\
    \ is driven by an observation that we can learn an aligned policy by minimizing\
    \ a forward Kullback\u2013Leibler (KL) divergence from a target policy (in the\
    \ form of a residual energy-based model) to a parameteric policy (LLM), instead\
    \ of a reverse KL as in RLHF methods. With samples from the energy-based target\
    \ policy, we can leverage the power of DPO (or other offline methods) to learn\
    \ an aligned policy efficiently. ARM is simple to implement and applicable in\
    \ various data settings. Our extensive experiments demonstrate its strong performance\
    \ across multiple datasets, compared to strong baselines like PPO, DPO."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Learning for NLP
  authors:
  - dblp_id: https://dblp.org/pid/16/6344
    emails: '****@g.ucla.edu'
    first_name: Bo
    google_scholar_id: https://scholar.google.com/citations?user=s9fNEVEAAAAJ&hl=en
    institution: SalesForce.com and University of California, Los Angeles
    last_name: Pang
    name: Bo Pang
    username: ~Bo_Pang4
  - dblp_id: https://dblp.org/pid/80/7282
    emails: '****@gmail.com'
    first_name: Caiming
    google_scholar_id: https://scholar.google.com/citations?user=vaSdahkAAAAJ&hl=en
    homepage: http://cmxiong.com/
    institution: Salesforce Research
    last_name: Xiong
    name: Caiming Xiong
    username: ~Caiming_Xiong1
  - dblp_id: http://dblp.uni-trier.de/pers/hd/z/Zhou:Yingbo
    emails: '****@salesforce.com'
    first_name: Yingbo
    google_scholar_id: https://scholar.google.com/citations?user=H_6RQ7oAAAAJ&hl=en
    institution: Salesforce Research
    last_name: Zhou
    name: Yingbo Zhou
    username: ~Yingbo_Zhou1
  decision: toMainConference
  end_page: 8973
  file: 1012.pdf
  id: 1012
  num_pages: 12
  openreview_id: qvtwf8hW4Y
  pdf_file: cdf31a037d86b5e0a664f6259e966064228647d5.pdf
  start_page: 8962
  title: 'ARM: Alignment with Residual Energy-Based Model'
- abstract: 'Language models (LMs) as conversational assistants recently became popular
    tools that help people accomplish a variety of tasks. These typically result from
    adapting LMs pretrained on general domain text sequences through further instruction-tuning
    and possibly preference optimisation methods. The evaluation of such LMs would
    ideally be performed using human judgement, however, this is not scalable. On
    the other hand, automatic evaluation featuring auxiliary LMs as judges and/or
    knowledge-based tasks is scalable but struggles with assessing conversational
    ability and adherence to instructions. To help accelerate the development of LMs
    as conversational assistants, we propose a novel automatic evaluation task: HumanRankEval
    (HRE). It consists of a large-scale, diverse and high-quality set of questions,
    each with several answers authored and scored by humans. To perform evaluation,
    HRE ranks these answers based on their log-likelihood under the LM''s distribution,
    and subsequently calculates their correlation with the corresponding human rankings.
    We support HRE''s efficacy by investigating how efficiently it separates pretrained
    and instruction-tuned LMs of various sizes. We show that HRE correlates well with
    human judgements and is particularly responsive to model changes following instruction-tuning.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - dblp_id: https://dblp.org/pid/203/9368
    emails: '****@gmail.com'
    first_name: Milan
    google_scholar_id: https://scholar.google.com/citations?user=LSyAqp4AAAAJ&hl=en
    homepage: https://github.com/milangritta
    last_name: Gritta
    name: Milan Gritta
    semantic_scholar_id: https://www.semanticscholar.org/author/Milan-Gritta/22168669
    username: ~Milan_Gritta1
  - dblp_id: https://dblp.org/pid/99/1688
    emails: '****@gmail.com'
    first_name: Gerasimos
    google_scholar_id: https://scholar.google.com/citations?user=z2UqpsoAAAAJ
    homepage: http://glampouras.github.io
    institution: Huawei Technologies Ltd.
    last_name: Lampouras
    name: Gerasimos Lampouras
    semantic_scholar_id: https://www.semanticscholar.org/author/Gerasimos-Lampouras/2346538
    username: ~Gerasimos_Lampouras2
  - dblp_id: https://dblp.org/pid/166/1757
    emails: '****@gmail.com'
    first_name: Ignacio
    google_scholar_id: https://scholar.google.com/citations?user=zOmsu9EAAAAJ&hl=en&oi=ao
    homepage: http://iiacobac.wordpress.com
    institution: Huawei Noah's Ark Lab
    last_name: Iacobacci
    name: Ignacio Iacobacci
    orcid: https://orcid.org/0000-0002-8913-8561
    semantic_scholar_id: https://www.semanticscholar.org/author/Ignacio-Iacobacci/2676143
    username: ~Ignacio_Iacobacci1
  decision: toMainConference
  end_page: 8986
  file: 1014.pdf
  id: 1014
  num_pages: 13
  openreview_id: 149tU5cKlf
  pdf_file: 53f4d85bf553f3f1094c86131fc5e6c963dfa8fb.pdf
  start_page: 8974
  title: 'HumanRankEval: Automatic Evaluation of LMs as Conversational Assistants'
- abstract: 'Understanding event descriptions is a central aspect of language processing,
    but current approaches focus overwhelmingly on single sentences or documents.
    Aggregating information about an event across documents can offer a much richer
    understanding. To this end, we present FAMuS, a new corpus of Wikipedia passages
    that report on some event, paired with underlying, genre-diverse (non-Wikipedia)
    source articles for the same event. Events and (cross-sentence) arguments in both
    report and source are annotated against FrameNet, providing broad coverage of
    different event types. We present results on two key event understanding tasks
    enabled by FAMuS: source validation---determining whether a document is a valid
    source for a target report event---and cross-document argument extraction---full-document
    argument extraction for a target event from both its report and the correct source
    article.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Extraction
  authors:
  - dblp_id: https://dblp.org/pid/236/4588
    emails: '****@cs.rochester.edu'
    first_name: Siddharth
    google_scholar_id: https://scholar.google.com/citations?user=4Q4zhC0AAAAJ&hl=en
    homepage: https://sidsvash26.github.io/
    institution: University of Rochester
    last_name: Vashishtha
    name: Siddharth Vashishtha
    semantic_scholar_id: https://www.semanticscholar.org/author/Siddharth-Vashishtha/68972934
    username: ~Siddharth_Vashishtha1
  - emails: '****@u.rochester.edu'
    first_name: Alexander
    google_scholar_id: https://scholar.google.com/citations?user=6_4ethMAAAAJ&hl=en
    homepage: https://alexmartin1722.github.io/
    last_name: Martin
    name: Alexander Martin
    orcid: https://orcid.org/0000-0002-8863-4002
    username: ~Alexander_Martin1
  - dblp_id: https://dblp.org/pid/277/0935
    emails: '****@gmail.com'
    first_name: William
    google_scholar_id: https://scholar.google.com/citations?user=SpOIH2MAAAAJ&hl=en&inst=10118598557142643180
    homepage: https://wgantt.github.io/
    institution: Department of Computer Science, University of Rochester
    last_name: Gantt
    name: William Gantt
    orcid: https://orcid.org/0000-0001-9931-2861
    semantic_scholar_id: https://www.semanticscholar.org/author/William-Gantt/1447311948
    username: ~William_Gantt1
  - dblp_id: https://dblp.org/pid/06/4775
    emails: '****@cs.jhu.edu'
    first_name: Benjamin
    google_scholar_id: https://scholar.google.com.tw/citations?user=wIujAJoAAAAJ
    homepage: http://www.cs.jhu.edu/~vandurme/
    institution: Johns Hopkins University, Johns Hopkins University, Johns Hopkins
      University and Microsoft
    last_name: Van Durme
    name: Benjamin Van Durme
    username: ~Benjamin_Van_Durme2
  - dblp_id: https://dblp.org/pid/188/5734
    emails: '****@rochester.edu'
    first_name: Aaron
    google_scholar_id: https://scholar.google.com/citations?user=R-ZVWNEAAAAJ&hl=en
    homepage: http://aaronstevenwhite.io
    institution: University of Rochester
    last_name: White
    middle_name: Steven
    name: Aaron Steven White
    semantic_scholar_id: https://www.semanticscholar.org/author/Aaron-Steven-White/2352617
    username: ~Aaron_Steven_White1
  decision: toMainConference
  end_page: 9010
  file: 1015.pdf
  id: 1015
  num_pages: 24
  openreview_id: UaG8jQXG7j
  pdf_file: a1ab73505df7f2b8c41e8393c6abebc1d3f92218.pdf
  start_page: 8987
  title: 'FAMuS: Frames Across Multiple Sources'
- abstract: 'Opinion summarization aims to generate concise summaries that present
    popular opinions of a large group of reviews. However, these summaries can be
    too generic and lack supporting details. To address these issues, we propose a
    new paradigm for summarizing reviews, rationale-based opinion summarization. Rationale-based
    opinion summaries output the representative opinions as well as one or more corresponding
    rationales. To extract good rationales, we define four desirable properties: relatedness,
    specificity, popularity, and diversity and present a Gibbs-sampling-based method
    to extract rationales. Overall, we propose RATION, an unsupervised extractive
    system that has two components: an Opinion Extractor (to extract representative
    opinions) and Rationales Extractor (to extract corresponding rationales). We conduct
    automatic and human evaluations to show that rationales extracted by RATION have
    the proposed properties and its summaries are more useful than conventional summaries.
    The implementation of our work is available at https://github.com/leehaoyuan/RATION.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Summarization
  authors:
  - emails: '****@cs.unc.edu'
    first_name: Haoyuan
    google_scholar_id: https://scholar.google.com/citations?user=BdfQ44MAAAAJ
    last_name: Li
    name: Haoyuan Li
    semantic_scholar_id: https://www.semanticscholar.org/author/Haoyuan-Li/2145538062
    username: ~Haoyuan_Li3
  - dblp_id: https://dblp.org/pid/77/8700
    emails: '****@cs.unc.edu'
    first_name: Snigdha
    google_scholar_id: https://scholar.google.com/citations?user=gZD3EesAAAAJ&hl=en
    homepage: https://sites.google.com/site/snigdhac/
    institution: Department of Computer Science, University of North Carolina, Chapel
      Hill
    last_name: Chaturvedi
    name: Snigdha Chaturvedi
    semantic_scholar_id: https://www.semanticscholar.org/author/Snigdha-Chaturvedi/37202877
    username: ~Snigdha_Chaturvedi2
  decision: toMainConference
  end_page: 9029
  file: 1016.pdf
  id: 1016
  num_pages: 19
  openreview_id: kMIOZ6KaXd
  pdf_file: db53f3bb6eee7740503443395b1a8b11b49e693f.pdf
  start_page: 9011
  title: Rationale-based Opinion Summarization
- abstract: 'The quality of the text-to-music models has reached new heights due to
    recent advancements in diffusion models. The controllability of various musical
    aspects, however, has barely been explored. In this paper, we propose Mustango:
    a music-domain-knowledge-inspired text-to-music system based on diffusion. Mustango
    aims to control the generated music, not only with general text captions, but
    with more rich captions that can include specific instructions related to chords,
    beats, tempo, and key. At the core of Mustango is MuNet, a Music-Domain-Knowledge-Informed
    UNet guidance module that steers the generated music to include the music-specific
    conditions, which we predict from the text prompt, as well as the general text
    embedding, during the reverse diffusion process. To overcome the limited availability
    of open datasets of music with text captions, we propose a novel data augmentation
    method that includes altering the harmonic, rhythmic, and dynamic aspects of music
    audio and using state-of-the-art Music Information Retrieval methods to extract
    the music features which will then be appended to the existing descriptions in
    text format. We release the resulting MusicBench dataset which contains over 52K
    instances and includes music-theory-based descriptions in the caption text. Through
    extensive experiments, we show that the quality of the music generated by Mustango
    is state-of-the-art, and the controllability through music-specific text prompts
    greatly outperforms other models such as MusicGen and AudioLDM2.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multimodality and Language Grounding to Vision, Robotics and Beyond
  authors:
  - emails: '****@mymail.sutd.edu.sg'
    first_name: Jan
    google_scholar_id: https://scholar.google.com/citations?user=_oCOqNkAAAAJ
    institution: Singapore University of Technology and Design
    last_name: Melechovsky
    name: Jan Melechovsky
    username: ~Jan_Melechovsky1
  - emails: '****@gmail.com'
    first_name: Zixun
    google_scholar_id: https://scholar.google.com/citations?user=Zn25KFQAAAAJ&hl=en
    homepage: https://guozixunnicolas.github.io/
    last_name: Guo
    name: Zixun Guo
    username: ~Zixun_Guo1
  - dblp_id: https://dblp.org/pid/203/9407
    emails: '****@gmail.com'
    first_name: Deepanway
    google_scholar_id: https://scholar.google.co.in/citations?user=95YiIWUAAAAJ&hl=en
    institution: Singapore University of Technology and Design
    last_name: Ghosal
    name: Deepanway Ghosal
    semantic_scholar_id: https://www.semanticscholar.org/author/Deepanway-Ghosal/32528506
    username: ~Deepanway_Ghosal2
  - dblp_id: https://dblp.org/pid/198/3608
    emails: '****@gmail.com'
    first_name: Navonil
    google_scholar_id: https://scholar.google.com/citations?user=jPfEvuQAAAAJ&hl=en&oi=ao
    institution: Singapore University of Technology and Design
    last_name: Majumder
    name: Navonil Majumder
    semantic_scholar_id: https://www.semanticscholar.org/author/Navonil-Majumder/35122767
    username: ~Navonil_Majumder1
  - emails: '****@gmail.com'
    first_name: Dorien
    google_scholar_id: https://scholar.google.com.tw/citations?user=Hp5W5f0AAAAJ
    homepage: http://dorienherremans.com/
    institution: Singapore University of Technology and Design
    last_name: Herremans
    name: Dorien Herremans
    orcid: https://orcid.org/0000-0001-8607-1640
    username: ~Dorien_Herremans1
  - dblp_id: https://dblp.org/pid/116/4904
    emails: '****@sutd.edu.sg'
    first_name: Soujanya
    google_scholar_id: https://scholar.google.co.in/citations?user=oS6gRc4AAAAJ&hl=en
    homepage: https://sporia.info
    institution: Singapore University of Technology and Design
    last_name: Poria
    name: Soujanya Poria
    username: ~Soujanya_Poria1
  decision: toMainConference
  end_page: 9053
  file: 1018.pdf
  id: 1018
  num_pages: 24
  openreview_id: 0AX6nepd81
  pdf_file: 312c9b9af7d34327dbc1b6557310f819b077c736.pdf
  start_page: 9030
  title: 'Mustango: Toward Controllable Text-to-Music Generation'
- abstract: Zero-Shot Cross-lingual Transfer (ZS-XLT) utilizes a model trained in
    a source language to make predictions in another language, often with a performance
    loss. To alleviate this, additional improvements can be achieved through subsequent
    adaptation using examples in the target language. In this paper, we exploit In-Context
    Tuning (ICT) for One-Shot Cross-lingual transfer in the classification task by
    introducing In-Context Cross-lingual Transfer (IC-XLT). The novel concept involves
    training a model to learn from context examples and subsequently adapting it during
    inference to a target language by prepending a One-Shot context demonstration
    in that language. Our results show that IC-XLT successfully leverages target-language
    examples to improve the cross-lingual capabilities of the evaluated mT5 model,
    outperforming prompt-based models in the Zero and Few-shot scenarios adapted through
    fine-tuning. Moreover, we show that when source-language data is limited, the
    fine-tuning framework employed for IC-XLT performs comparably to prompt-based
    fine-tuning with significantly more training data in the source language.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multilinguality and Language Diversity
  authors:
  - emails: '****@gmail.com'
    first_name: Emilio
    google_scholar_id: https://scholar.google.com/citations?user=uYz6zaIAAAAJ
    homepage: https://villacu.github.io/
    last_name: Cueva
    middle_name: Villa
    name: Emilio Villa Cueva
    username: ~Emilio_Villa_Cueva1
  - dblp_id: https://dblp.org/pers/hd/l/L=oacute=pez=Monroy:Adri=aacute=n_Pastor
    emails: '****@gmail.com'
    first_name: Adrian
    google_scholar_id: https://scholar.google.com.mx/citations?user=S1K3A6wAAAAJ&hl=es
    homepage: https://www.cimat.mx/es/adri%C3%A1n-pastor-l%C3%B3pez-monroy
    last_name: Lopez Monroy
    middle_name: Pastor
    name: ADRIAN PASTOR LOPEZ MONROY
    orcid: https://orcid.org/0000-0003-1018-4221
    username: ~ADRIAN_PASTOR_LOPEZ_MONROY1
  - emails: '****@cimat.mx'
    first_name: Fernando
    homepage: https://www.cimat.mx/author/sanchez-vega-fernando/
    institution: Center for Research in Mathematics (CIMAT)
    last_name: "S\xE1nchez-Vega"
    name: "Fernando S\xE1nchez-Vega"
    username: "~Fernando_S\xE1nchez-Vega1"
  - dblp_id: https://dblp.org/pid/79/3530
    emails: '****@gmail.com'
    first_name: Thamar
    google_scholar_id: https://scholar.google.com/citations?user=Gmjwy-IAAAAJ&hl=en
    homepage: http://solorio.uh.edu/
    institution: Mohamed bin Zayed University of Artificial Intelligence and University
      of Houston
    last_name: Solorio
    name: Thamar Solorio
    username: ~Thamar_Solorio1
  decision: toMainConference
  end_page: 9072
  file: 1020.pdf
  id: 1020
  num_pages: 19
  openreview_id: 74kFTg9RKc
  pdf_file: 21470d81c78063a5e59bb8ffb7c060da19fffa02.pdf
  start_page: 9054
  title: Adaptive Cross-lingual Text Classification through In-Context One-Shot Demonstrations
- abstract: Named entities -- typically expressed via proper nouns -- play a key role
    in Natural Language Processing, as their identification and comprehension are
    crucial in tasks such as Relation Extraction, Coreference Resolution and Question
    Answering, among others. Tasks like these also often entail dealing with concepts
    -- typically represented by common nouns -- which, however, have not received
    as much attention. Indeed, the potential of their identification and understanding
    remains underexplored, as does the benefit of a synergistic formulation with named
    entities. To fill this gap, we introduce Concept and Named Entity Recognition
    (CNER), a new unified task that handles concepts and entities mentioned in unstructured
    texts seamlessly. We put forward a comprehensive set of categories that can be
    used to model concepts and named entities jointly, and propose new approaches
    for the creation of CNER datasets. We evaluate the benefits of performing CNER
    as a unified task extensively, showing that a CNER model gains up to +5.4 and
    +8 macro F1 points when compared to specialized named entity and concept recognition
    systems, respectively. Finally, to encourage the development of CNER systems,
    we release our datasets and models at https://github.com/Babelscape/cner.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Extraction
  authors:
  - emails: '****@diag.uniroma1.it'
    first_name: Giuliano
    homepage: https://g185.github.io/
    institution: University of Roma "La Sapienza"
    last_name: Martinelli
    name: Giuliano Martinelli
    username: ~Giuliano_Martinelli1
  - emails: '****@diag.uniroma1.it'
    first_name: Francesco
    institution: University of Roma "La Sapienza"
    last_name: Molfese
    middle_name: Maria
    name: Francesco Maria Molfese
    orcid: https://orcid.org/0009-0007-7598-2343
    username: ~Francesco_Maria_Molfese1
  - dblp_id: https://dblp.org/pid/305/9751
    emails: '****@babelscape.com'
    first_name: Simone
    google_scholar_id: https://scholar.google.com/citations?user=wXvkdigAAAAJ&hl=it
    last_name: Tedeschi
    name: Simone Tedeschi
    semantic_scholar_id: https://www.semanticscholar.org/author/Simone-Tedeschi/2140370472
    username: ~Simone_Tedeschi1
  - emails: '****@gmail.com'
    first_name: Alberte
    last_name: "Fern\xE1ndez-Castro"
    name: "Alberte Fern\xE1ndez-Castro"
    username: "~Alberte_Fern\xE1ndez-Castro1"
  - dblp_id: https://dblp.org/pers/n/Navigli:Roberto.html
    emails: '****@diag.uniroma1.it'
    first_name: Roberto
    google_scholar_id: https://scholar.google.it/citations?user=BsgVJ-EAAAAJ
    homepage: http://wwwusers.di.uniroma1.it/~navigli/
    institution: Sapienza University of Rome
    last_name: Navigli
    name: Roberto Navigli
    orcid: https://orcid.org/0000-0003-3831-9706
    semantic_scholar_id: https://www.semanticscholar.org/author/R.-Navigli/1733928
    username: ~Roberto_Navigli2
  decision: toMainConference
  end_page: 9088
  file: 1021.pdf
  id: 1021
  num_pages: 16
  openreview_id: 9uj4FCOUKE
  pdf_file: b5e15a1fafd5ef7f1a6f38a4ed44f38444720df4.pdf
  start_page: 9073
  title: 'CNER: Concept and Named Entity Recognition'
- abstract: Facts are subject to contingencies and can be true or false in different
    circumstances. One such contingency is time, wherein some facts mutate over a
    given period, e.g., the president of a country or the winner of a championship.
    Trustworthy language models ideally identify mutable facts as such and process
    them accordingly. We create MuLan, a benchmark for evaluating the ability of English
    language models to anticipate time-contingency, covering both 1:1 and 1:N relations.
    We hypothesize that mutable facts are encoded differently than immutable ones,
    hence being easier to update. In a detailed evaluation of six popular large language
    models, we consistently find differences in the LLMs' confidence, representations,
    and update behavior, depending on the mutability of a fact. Our findings should
    inform future work on the injection of and induction of time-contingent knowledge
    to/from LLMs.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Interpretability and Analysis of Models for NLP
  authors:
  - dblp_id: https://dblp.org/pid/205/9159
    emails: '****@gmail.com'
    first_name: Constanza
    google_scholar_id: https://scholar.google.com/citations?user=uYCIJSEAAAAJ&hl=en
    institution: Copenhagen University
    last_name: Fierro
    name: Constanza Fierro
    semantic_scholar_id: https://www.semanticscholar.org/author/Constanza-Fierro/50110151
    username: ~Constanza_Fierro1
  - dblp_id: https://dblp.org/pid/234/3369
    emails: '****@di.ku.dk'
    first_name: Nicolas
    last_name: Garneau
    name: Nicolas Garneau
    semantic_scholar_id: https://www.semanticscholar.org/author/Nicolas-Garneau/116105856
    username: ~Nicolas_Garneau1
  - dblp_id: https://dblp.org/pid/241/9497
    emails: '****@google.com'
    first_name: Emanuele
    google_scholar_id: https://scholar.google.com/citations?user=9yc1aXYAAAAJ&hl=en
    homepage: http://e-bug.github.io/
    institution: Google
    last_name: Bugliarello
    name: Emanuele Bugliarello
    orcid: https://orcid.org/0000-0002-2999-7081
    semantic_scholar_id: https://www.semanticscholar.org/author/Emanuele-Bugliarello/83574123
    username: ~Emanuele_Bugliarello1
  - dblp_id: https://dblp.org/pid/225/7708
    emails: '****@mbzuai.ac.ae'
    first_name: Yova
    institution: Mohamed bin Zayed University of Artificial Intelligence
    last_name: Kementchedjhieva
    name: Yova Kementchedjhieva
    username: ~Yova_Kementchedjhieva1
  - dblp_id: https://dblp.org/pid/30/2756
    emails: '****@di.ku.dk'
    first_name: Anders
    google_scholar_id: https://scholar.google.com.tw/citations?user=x3I4CrYAAAAJ
    homepage: https://anderssoegaard.github.io/
    institution: Copenhagen University
    last_name: "S\xF8gaard"
    name: "Anders S\xF8gaard"
    username: "~Anders_S\xF8gaard1"
  decision: toMainConference
  end_page: 9098
  file: 1022.pdf
  id: 1022
  num_pages: 10
  openreview_id: VI3OaJvMj7
  pdf_file: 480fc0f4bb6f016c5bc4a3b02bf20ba72f3759bd.pdf
  start_page: 9089
  title: 'MuLan: A Study of Fact Mutability in Language Models'
- abstract: Finetuning pretrained models on downstream generation tasks often leads
    to catastrophic forgetting in zero-shot conditions. In this work, we focus on
    summarization and tackle the problem through the lens of language-independent
    representations. After training on monolingual summarization, we perform zero-shot
    transfer to new languages or language pairs. We first show naively finetuned models
    are highly language-specific in both output behavior and internal representations,
    resulting in poor zero-shot performance. Next, we propose query-key (QK) finetuning
    to decouple task-specific knowledge from the pretrained language generation abilities.
    Then, after showing downsides of the standard adversarial language classifier,
    we propose a balanced variant that more directly enforces language-agnostic representations.
    Moreover, our qualitative analyses show removing source language identity correlates
    to zero-shot summarization performance. Our code is openly available.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Summarization
  authors:
  - emails: '****@student.kit.edu'
    first_name: Vladimir
    last_name: Solovyev
    name: Vladimir Solovyev
    username: ~Vladimir_Solovyev1
  - emails: '****@kit.edu'
    first_name: Danni
    google_scholar_id: https://scholar.google.com/citations?user=cCO0x_AAAAAJ&hl=en&oi=sra
    institution: "Karlsruher Institut f\xFCr Technologie"
    last_name: Liu
    name: Danni Liu
    semantic_scholar_id: https://www.semanticscholar.org/author/Danni-Liu/1609468282
    username: ~Danni_Liu1
  - dblp_id: https://dblp.org/pid/120/0365
    emails: '****@niehues.info'
    first_name: Jan
    google_scholar_id: https://scholar.google.com/citations?user=fO9cszYAAAAJ&hl=en
    homepage: https://dke.maastrichtuniversity.nl/jan.niehues/
    last_name: Niehues
    name: Jan Niehues
    username: ~Jan_Niehues1
  decision: toMainConference
  end_page: 9109
  file: 1031.pdf
  id: 1031
  num_pages: 11
  openreview_id: EcHZWagE1p
  pdf_file: 5d8914303ec5f9fdd93504c7345adccd7d36a7bf.pdf
  start_page: 9099
  title: Language-Independent Representations Improve Zero-Shot Summarization
- abstract: Large Language Models (LLMs) are frequently used for multi-faceted language
    generation and evaluation tasks that involve satisfying intricate user constraints
    or taking into account multiple aspects and criteria. However, their performance
    can fall short, due to the model's lack of coherence and inability to plan and
    decompose the problem. We propose Branch-Solve-Merge (BSM), a Large Language Model
    program (Schlag et al., 2023) for tackling such challenging natural language tasks.
    It consists of branch, solve, and merge modules that are parameterized with specific
    prompts to the base LLM. These three modules plan a decomposition of the task
    into multiple parallel sub-tasks, independently solve them, and fuse the solutions
    to the sub-tasks. We apply our method to the tasks of LLM response evaluation
    and constrained text generation and evaluate its effectiveness with multiple LLMs,
    including Vicuna, LLaMA-2-chat, and GPT-4. BSM improves the evaluation correctness
    and consistency for each LLM by enhancing human-LLM agreement by up to 26%, reducing
    length and pairwise position biases by up to 50%, and allowing LLaMA-2-chat to
    match or outperform GPT-4 on most domains. On a constraint story generation task,
    BSM improves the coherence of stories while also improving constraint satisfaction
    by 12%.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - dblp_id: https://dblp.org/pid/203/9296
    emails: '****@cs.unc.edu'
    first_name: Swarnadeep
    google_scholar_id: https://scholar.google.com/citations?user=sY5SyBgAAAAJ&hl=en
    homepage: https://swarnahub.github.io/
    institution: Department of Computer Science, University of North Carolina, Chapel
      Hill
    last_name: Saha
    name: Swarnadeep Saha
    semantic_scholar_id: https://www.semanticscholar.org/author/Swarnadeep-Saha/35106509
    username: ~Swarnadeep_Saha2
  - dblp_id: https://dblp.org/pid/117/4866
    emails: '****@gmail.com'
    first_name: Omer
    google_scholar_id: https://scholar.google.com/citations?user=PZVd2h8AAAAJ
    institution: Facebook
    last_name: Levy
    name: Omer Levy
    orcid: https://orcid.org/0000-0001-7300-8191
    semantic_scholar_id: https://www.semanticscholar.org/author/Omer-Levy/39455775
    username: ~Omer_Levy1
  - dblp_id: https://dblp.org/pid/15/3724
    emails: '****@live.com'
    first_name: Asli
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=aLHWnHsAAAAJ&view_op=list_works&alert_preview_top_rm=2&sortby=pubdate
    homepage: https://asli.us
    institution: 'FAIR '
    last_name: Celikyilmaz
    name: Asli Celikyilmaz
    semantic_scholar_id: https://www.semanticscholar.org/author/1709797
    username: ~Asli_Celikyilmaz1
  - dblp_id: https://dblp.org/pid/32/5243.html
    emails: '****@cs.unc.edu'
    first_name: Mohit
    google_scholar_id: https://scholar.google.com/citations?user=DN8QtscAAAAJ&hl=en
    homepage: https://www.cs.unc.edu/~mbansal/
    institution: University of North Carolina at Chapel Hill
    last_name: Bansal
    name: Mohit Bansal
    username: ~Mohit_Bansal2
  - dblp_id: https://dblp.org/pid/29/6977.html
    emails: '****@gmail.com'
    first_name: Jason
    google_scholar_id: https://scholar.google.com/citations?user=lMkTx0EAAAAJ&hl=en
    homepage: http://www.jaseweston.com
    institution: New York University and Facebook
    last_name: Weston
    middle_name: E
    name: Jason E Weston
    username: ~Jason_E_Weston1
  - dblp_id: https://dblp.org/pid/82/1763
    emails: '****@fb.com'
    first_name: Xian
    google_scholar_id: https://scholar.google.com/citations?user=v_sIgawAAAAJ&hl=en
    institution: Facebook AI
    last_name: Li
    name: Xian Li
    username: ~Xian_Li1
  decision: toMainConference
  end_page: 9128
  file: 1033.pdf
  id: 1033
  num_pages: 19
  openreview_id: 1thdYtNtha
  pdf_file: e05320fd063c782c1f0d103aced3f81d188ceefb.pdf
  start_page: 9110
  title: Branch-Solve-Merge Improves Large Language Model Evaluation and Generation
- abstract: "Language models (LMs) often struggle to pay enough attention to the input\
    \ context, and generate texts that are unfaithful or contain hallucinations. To\
    \ mitigate this issue, we present context-aware decoding (CAD), which follows\
    \ a contrastive output distribution that amplifies the difference between the\
    \ output probabilities when a model is used with and without context. Our experiments\
    \ show that CAD, without additional training, significantly improves the faithfulness\
    \ of different LM families, including OPT, GPT, LLaMA, and FLAN-T5 for summarization\
    \ tasks (e.g., 14.3% gain for LLaMA in factuality metrics). Furthermore, CAD is\
    \ particularly effective in overriding a model\u2019s prior knowledge when it\
    \ contradicts the provided context, leading to substantial improvements in tasks\
    \ where resolving the knowledge conflict is essential. Our code is publicly released\
    \ at https://github.com/xhan77/context-aware-decoding."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Generation
  authors:
  - dblp_id: https://dblp.org/pid/132/8060
    emails: '****@uw.edu'
    first_name: Weijia
    last_name: Shi
    name: Weijia Shi
    username: ~Weijia_Shi1
  - dblp_id: https://dblp.org/pid/216/6755
    emails: '****@gmail.com'
    first_name: Xiaochuang
    google_scholar_id: https://scholar.google.com/citations?user=GamSVF0AAAAJ&hl=en
    homepage: https://xhan77.github.io/
    institution: Department of Computer Science, University of Washington
    last_name: Han
    name: Xiaochuang Han
    semantic_scholar_id: https://www.semanticscholar.org/author/Xiaochuang-Han/40500540
    username: ~Xiaochuang_Han1
  - dblp_id: https://dblp.org/pid/19/6214
    emails: '****@fb.com'
    first_name: Mike
    google_scholar_id: https://scholar.google.com/citations?user=SnQnQicAAAAJ&hl=en
    institution: Facebook AI Research
    last_name: Lewis
    name: Mike Lewis
    username: ~Mike_Lewis1
  - dblp_id: https://dblp.org/pid/75/8157
    emails: '****@cs.washington.edu'
    first_name: Yulia
    google_scholar_id: https://scholar.google.com/citations?user=SEDPkrsAAAAJ&hl=en
    homepage: https://homes.cs.washington.edu/~yuliats/
    institution: Department of Computer Science, University of Washington
    last_name: Tsvetkov
    name: Yulia Tsvetkov
    username: ~Yulia_Tsvetkov1
  - dblp_id: https://dblp.org/pid/21/6793
    emails: '****@cs.washington.edu'
    first_name: Luke
    google_scholar_id: https://scholar.google.com.tw/citations?user=UjpbO6IAAAAJ
    homepage: https://www.cs.washington.edu/people/faculty/lsz/
    institution: University of Washington, Facebook and Meta
    last_name: Zettlemoyer
    name: Luke Zettlemoyer
    semantic_scholar_id: https://www.semanticscholar.org/author/Luke-Zettlemoyer/1982950
    username: ~Luke_Zettlemoyer1
  - dblp_id: https://dblp.org/pid/07/7129
    emails: '****@gmail.com'
    first_name: Wen-tau
    google_scholar_id: https://scholar.google.com/citations?user=8rDNIMsAAAAJ
    homepage: http://scottyih.org
    institution: Meta Platforms, Inc.
    last_name: Yih
    name: Wen-tau Yih
    semantic_scholar_id: https://www.semanticscholar.org/author/Wen-tau-Yih/144105277
    username: ~Wen-tau_Yih1
  decision: toMainConference
  end_page: 9137
  file: 1036.pdf
  id: 1036
  num_pages: 9
  openreview_id: PfZYHRMwRY
  pdf_file: ff836c39e604e9df52339327ad45d7e7ad9434e1.pdf
  start_page: 9129
  title: 'Trusting Your Evidence: Hallucinate Less with Context-aware Decoding'
- abstract: We introduce REPLUG, a retrieval-augmented language modeling framework
    that treats the language model (LM) as a black box and augments it with a tuneable
    retrieval model. Unlike prior retrieval-augmented LMs that train language models
    with special cross-attention mechanisms to encode the retrieved text, REPLUG simply
    prepends retrieved documents to the input for the frozen black-box LM. This simple
    design can be easily applied to any existing language models. Furthermore, we
    show that the LM can be used to supervise the retrieval model, which can then
    find documents that help the LM make better predictions. Our experiments demonstrate
    that REPLUG with the tuned retriever significantly improves the performance of
    GPT-3 (175B) on language modeling by 6.3%, as well as the performance of Codex
    on five-shot MMLU by 5.1%. Code is publicly released at github.com/swj0419/REPLUG.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Retrieval and Text Mining
  authors:
  - dblp_id: https://dblp.org/pid/132/8060
    emails: '****@uw.edu'
    first_name: Weijia
    last_name: Shi
    name: Weijia Shi
    username: ~Weijia_Shi1
  - dblp_id: https://dblp.org/pid/203/9401
    emails: '****@gmail.com'
    first_name: Sewon
    google_scholar_id: https://scholar.google.ca/citations?user=jU4IZs4AAAAJ&hl=en
    homepage: https://shmsw25.github.io
    institution: Facebook and Department of Computer Science, University of Washington
    last_name: Min
    name: Sewon Min
    semantic_scholar_id: https://www.semanticscholar.org/author/Sewon-Min/48872685
    username: ~Sewon_Min1
  - dblp_id: https://dblp.org/pid/202/1809
    emails: '****@cs.stanford.edu'
    first_name: Michihiro
    google_scholar_id: https://scholar.google.com/citations?user=SieJYoEAAAAJ&hl=en
    institution: Stanford University
    last_name: Yasunaga
    name: Michihiro Yasunaga
    username: ~Michihiro_Yasunaga1
  - dblp_id: https://dblp.org/pid/149/1367
    emails: '****@gmail.com'
    first_name: Minjoon
    google_scholar_id: https://scholar.google.com/citations?user=zYze5fIAAAAJ&hl=en
    homepage: https://seominjoon.github.io
    institution: Korea Advanced Institute of Science and Technology
    last_name: Seo
    name: Minjoon Seo
    semantic_scholar_id: https://www.semanticscholar.org/author/Minjoon-Seo/4418074
    username: ~Minjoon_Seo1
  - emails: '****@meta.com'
    first_name: Richard
    homepage: http://www.richjames.ai
    institution: Research, Facebook
    last_name: James
    name: Richard James
    username: ~Richard_James2
  - dblp_id: https://dblp.org/pid/19/6214
    emails: '****@fb.com'
    first_name: Mike
    google_scholar_id: https://scholar.google.com/citations?user=SnQnQicAAAAJ&hl=en
    institution: Facebook AI Research
    last_name: Lewis
    name: Mike Lewis
    username: ~Mike_Lewis1
  - dblp_id: https://dblp.org/pid/21/6793
    emails: '****@cs.washington.edu'
    first_name: Luke
    google_scholar_id: https://scholar.google.com.tw/citations?user=UjpbO6IAAAAJ
    homepage: https://www.cs.washington.edu/people/faculty/lsz/
    institution: University of Washington, Facebook and Meta
    last_name: Zettlemoyer
    name: Luke Zettlemoyer
    semantic_scholar_id: https://www.semanticscholar.org/author/Luke-Zettlemoyer/1982950
    username: ~Luke_Zettlemoyer1
  - dblp_id: https://dblp.org/pid/07/7129
    emails: '****@gmail.com'
    first_name: Wen-tau
    google_scholar_id: https://scholar.google.com/citations?user=8rDNIMsAAAAJ
    homepage: http://scottyih.org
    institution: Meta Platforms, Inc.
    last_name: Yih
    name: Wen-tau Yih
    semantic_scholar_id: https://www.semanticscholar.org/author/Wen-tau-Yih/144105277
    username: ~Wen-tau_Yih1
  decision: toMainConference
  end_page: 9151
  file: 1037.pdf
  id: 1037
  num_pages: 14
  openreview_id: in9M0R653A
  pdf_file: e633061b5743e6e6918d0746c2f714b507f91dee.pdf
  start_page: 9138
  title: 'REPLUG: Retrieval-Augmented Black-Box Language Models'
- abstract: 'Diffusion-based language models are emerging as a promising alternative
    to autoregressive LMs: they approach the competence of autoregressive LMs while
    offering nuanced controllability at inference time. While autoregressive LMs have
    benefited immensely from scaling and instruction-based learning, existing studies
    of diffusion LMs have been conducted on a smaller scale. Starting with a recently
    proposed diffusion model SSD-LM, in this work we first explore methods to scale
    it from 0.4B to 13B parameters, proposing techniques to improve its training and
    inference efficiency, and to finetune the model to follow instructions. Armed
    with a more powerful, general purpose diffusion LM, we introduce the primary contribution
    of this work -- SSD-2 -- an approach to easily ensemble at inference time a large
    general-purpose diffusion LM with smaller, but specialized and contextualized
    diffusion LMs. We show that SSD-2 facilitates novel ensembles with 100x smaller
    models that can be customized and deployed by individual users. We find that compared
    to autoregressive models, the collaboration between diffusion LMs is more effective,
    leading to higher-quality model responses due to their ability to dynamically
    incorporate bi-directional contexts.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Generation
  authors:
  - dblp_id: https://dblp.org/pid/216/6755
    emails: '****@gmail.com'
    first_name: Xiaochuang
    google_scholar_id: https://scholar.google.com/citations?user=GamSVF0AAAAJ&hl=en
    homepage: https://xhan77.github.io/
    institution: Department of Computer Science, University of Washington
    last_name: Han
    name: Xiaochuang Han
    semantic_scholar_id: https://www.semanticscholar.org/author/Xiaochuang-Han/40500540
    username: ~Xiaochuang_Han1
  - dblp_id: https://dblp.org/pid/31/4484
    emails: '****@gmail.com'
    first_name: Sachin
    google_scholar_id: https://scholar.google.com/citations?user=qO38fRIAAAAJ&hl=en
    homepage: https://shocheen.com
    institution: Ohio State University, Columbus
    last_name: Kumar
    name: Sachin Kumar
    semantic_scholar_id: https://www.semanticscholar.org/author/Sachin-Kumar/51467955
    username: ~Sachin_Kumar1
  - dblp_id: https://dblp.org/pid/75/8157
    emails: '****@cs.washington.edu'
    first_name: Yulia
    google_scholar_id: https://scholar.google.com/citations?user=SEDPkrsAAAAJ&hl=en
    homepage: https://homes.cs.washington.edu/~yuliats/
    institution: Department of Computer Science, University of Washington
    last_name: Tsvetkov
    name: Yulia Tsvetkov
    username: ~Yulia_Tsvetkov1
  - dblp_id: https://dblp.org/pid/50/10813
    emails: '****@gmail.com'
    first_name: Marjan
    institution: Facebook AI Research
    last_name: Ghazvininejad
    name: Marjan Ghazvininejad
    username: ~Marjan_Ghazvininejad1
  decision: toMainConference
  end_page: 9167
  file: 1040.pdf
  id: 1040
  num_pages: 16
  openreview_id: 2W8AyqPumX
  pdf_file: 23054287ecbf10d8c8d555480f391a7fec79b256.pdf
  start_page: 9152
  title: 'David helps Goliath: Inference-Time Collaboration Between Small Specialized
    and Large General Diffusion LMs'
- abstract: "Understanding visually situated language requires interpreting complex\
    \ layouts of textual and visual elements. \nPre-processing tools, such as optical\
    \ character recognition (OCR), can map document image inputs to textual tokens,\
    \ then large language models (LLMs) can reason over text.\nHowever, such methods\
    \ have high computational and engineering complexity. \nCan small pretrained image-to-text\
    \ models accurately understand visual documents through similar recognition and\
    \ reasoning steps instead?\nWe propose Rationale Distillation (RD), which incorporates\
    \ the outputs of OCR tools, LLMs, and larger multimodal models as intermediate\
    \ \"rationales\", and trains a small student model to predict both rationales\
    \ and answers. \nOn three visual document understanding benchmarks representing\
    \ infographics, scanned documents, and figures, our Pix2Struct (282M parameters)\
    \ student model finetuned with RD outperforms the base model by 4-5\\% absolute\
    \ accuracy with only 1\\% higher computational cost."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multimodality and Language Grounding to Vision, Robotics and Beyond
  authors:
  - dblp_id: https://dblp.uni-trier.de/pid/223/4711-1
    emails: '****@usc.edu'
    first_name: Wang
    google_scholar_id: https://scholar.google.com/citations?user=dMkqNF8AAAAJ&hl=en
    homepage: https://billzhu.me
    institution: University of Southern California
    last_name: Zhu
    name: Wang Zhu
    orcid: https://orcid.org/0000-0002-6821-4115
    semantic_scholar_id: https://www.semanticscholar.org/author/Wang-Zhu/143750633
    username: ~Wang_Zhu1
  - emails: '****@gmail.com'
    first_name: Alekh
    google_scholar_id: https://scholar.google.com/citations?user=9nnDvooAAAAJ&hl=en
    homepage: https://alekhagarwal.net
    institution: Google
    last_name: Agarwal
    name: Alekh Agarwal
    username: ~Alekh_Agarwal2
  - dblp_id: https://dblp.org/pid/85/1261
    emails: '****@cs.washington.edu'
    first_name: Mandar
    homepage: https://homes.cs.washington.edu/~mandar90
    institution: Google DeepMind
    last_name: Joshi
    name: Mandar Joshi
    username: ~Mandar_Joshi1
  - dblp_id: https://dblp.org/pid/182/2556
    emails: '****@usc.edu'
    first_name: Robin
    google_scholar_id: https://scholar.google.com/citations?user=ajZ-_O0AAAAJ&hl=en
    homepage: https://robinjia.github.io/
    institution: University of Southern California
    last_name: Jia
    name: Robin Jia
    semantic_scholar_id: https://www.semanticscholar.org/author/Robin-Jia/3422908
    username: ~Robin_Jia1
  - dblp_id: https://dblp.org/pid/130/2863
    emails: '****@gmail.com'
    first_name: Jesse
    google_scholar_id: https://scholar.google.com/citations?user=8BeTDr0AAAAJ&hl=en
    homepage: https://jessethomason.com/
    institution: University of Southern California and Amazon
    last_name: Thomason
    name: Jesse Thomason
    orcid: https://orcid.org/0000-0001-9199-0633
    semantic_scholar_id: https://www.semanticscholar.org/author/Jesse-Thomason/2665873
    username: ~Jesse_Thomason1
  - dblp_id: https://dblp.org/pid/25/1520
    emails: '****@google.com'
    first_name: Kristina
    google_scholar_id: https://scholar.google.com/citations?user=9qY7NPEAAAAJ&hl=en
    homepage: http://kristinatoutanova.com/
    institution: Google
    last_name: Toutanova
    name: Kristina Toutanova
    semantic_scholar_id: https://www.semanticscholar.org/author/Kristina-Toutanova/3259253
    username: ~Kristina_Toutanova1
  decision: toMainConference
  end_page: 9191
  file: 1042.pdf
  id: 1042
  num_pages: 24
  openreview_id: evfwQsk1fS
  pdf_file: 82e6b4f3307bb8e995bc9388689d5ccdba4d4a28.pdf
  start_page: 9168
  title: Efficient End-to-End Visual Document Understanding with Rationale Distillation
- abstract: 'A central component of rational behavior is logical inference: the process
    of determining which conclusions follow from a set of premises. Psychologists
    have documented several ways in which humans'' inferences deviate from the rules
    of logic. Do language models, which are trained on text generated by humans, replicate
    such human biases, or are they able to overcome them? Focusing on the case of
    syllogisms---inferences from two simple premises---we show that, within the PaLM~2
    family of transformer language models, larger models are more logical than smaller
    ones, and also more logical than humans. At the same time, even the largest models
    make systematic errors, some of which mirror human reasoning biases: they show
    sensitivity to the (irrelevant) ordering of the variables in the syllogism, and
    draw confident but incorrect inferences from particular syllogisms (syllogistic
    fallacies). Overall, we find that language models often mimic the human biases
    included in their training data, but are able to overcome them in some cases.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Linguistic theories, Cognitive Modeling and Psycholinguistics
  authors:
  - emails: '****@mit.edu'
    first_name: Tiwalayo
    homepage: https://eisape.github.io/
    institution: Massachusetts Institute of Technology
    last_name: Eisape
    name: Tiwalayo Eisape
    semantic_scholar_id: https://www.semanticscholar.org/author/Tiwalayo-Eisape/102487474
    username: ~Tiwalayo_Eisape1
  - emails: '****@gmail.com'
    first_name: Michael
    google_scholar_id: https://scholar.google.com/citations?user=DQjm2rAAAAAJ
    homepage: https://www.mit.edu/~tessler/
    institution: DeepMind
    last_name: Tessler
    middle_name: Henry
    name: Michael Henry Tessler
    username: ~Michael_Henry_Tessler1
  - dblp_id: https://dblp.org/pid/169/6218
    emails: '****@gmail.com'
    first_name: Ishita
    institution: DeepMind
    last_name: Dasgupta
    name: Ishita Dasgupta
    username: ~Ishita_Dasgupta1
  - dblp_id: https://dblp.org/pid/13/3601
    emails: '****@google.com'
    first_name: Fei
    google_scholar_id: https://scholar.google.com/citations?user=HDHOS0QAAAAJ&hl=en
    last_name: Sha
    name: Fei Sha
    username: ~Fei_Sha3
  - dblp_id: https://dblp.org/pid/183/9326
    emails: '****@gmail.com'
    first_name: Sjoerd
    google_scholar_id: https://scholar.google.com/citations?user=i-AStBYAAAAJ&hl=en
    homepage: http://www.sjoerdvansteenkiste.com/
    institution: Google
    last_name: Steenkiste
    middle_name: Van
    name: Sjoerd van Steenkiste
    username: ~Sjoerd_van_Steenkiste1
  - dblp_id: https://dblp.org/pid/169/3438
    emails: '****@nyu.edu'
    first_name: Tal
    google_scholar_id: https://scholar.google.com/citations?user=5mJDXjoAAAAJ&hl=en
    homepage: http://tallinzen.net
    institution: New York University and Google
    last_name: Linzen
    name: Tal Linzen
    semantic_scholar_id: https://www.semanticscholar.org/author/Tal-Linzen/2467508
    username: ~Tal_Linzen1
  decision: toMainConference
  end_page: 9211
  file: 1044.pdf
  id: 1044
  num_pages: 20
  openreview_id: kKFOo6p8yn
  pdf_file: 72e430637bca1125cd38d3d5c40c0c203e5ff955.pdf
  start_page: 9192
  title: A Systematic Comparison of Syllogistic Reasoning in Humans and Language Models
- abstract: "Active learning for imbalanced classification tasks is challenging as\
    \ the minority classes naturally occur rarely. Gathering a large pool of unlabelled\
    \ data is thus essential to capture minority instances. Standard pool-based active\
    \ learning is computationally expensive on large pools and often reaches low accuracy\
    \ by overfitting the initial decision boundary, thus failing to explore the input\
    \ space and find minority instances.\n    To address these issues we propose AnchorAL.\
    \ At each iteration, AnchorAL chooses class-specific instances from the labelled\
    \ set, or *anchors*, and retrieves the most similar unlabelled instances from\
    \ the pool. This resulting *subpool* is then used for active learning.\n    Using\
    \ a small, fixed-sized subpool AnchorAL allows scaling any active learning strategy\
    \ to large pools. By dynamically selecting different anchors at each iteration\
    \ it promotes class balance and prevents overfitting the initial decision boundary,\
    \ thus promoting the discovery of new clusters of minority instances.\n    Experiments\
    \ across different classification tasks, active learning strategies, and model\
    \ architectures AnchorAL is *(i)* faster, often reducing runtime from hours to\
    \ minutes, *(ii)* trains more performant models, *(iii)* and returns more balanced\
    \ datasets than competing methods."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Learning for NLP
  authors:
  - dblp_id: https://dblp.org/pid/348/6871
    emails: '****@outlook.com'
    first_name: Pietro
    google_scholar_id: https://scholar.google.com/citations?user=uRIcVlAAAAAJ
    homepage: https://pietrolesci.github.io/
    institution: University of Cambridge
    last_name: Lesci
    name: Pietro Lesci
    semantic_scholar_id: https://www.semanticscholar.org/author/Pietro-Lesci/2219036385
    username: ~Pietro_Lesci1
  - dblp_id: https://dblp.org/pid/18/1071-1
    emails: '****@cam.ac.uk'
    first_name: Andreas
    google_scholar_id: https://scholar.google.es/citations?user=XjWnyM4AAAAJ&hl=en
    homepage: http://andreasvlachos.github.io/
    institution: University of Cambridge
    last_name: Vlachos
    name: Andreas Vlachos
    orcid: https://orcid.org/0000-0003-2123-5071
    semantic_scholar_id: https://www.semanticscholar.org/author/Andreas-Vlachos/2064056928
    username: ~Andreas_Vlachos1
  decision: toMainConference
  end_page: 9231
  file: 1046.pdf
  id: 1046
  num_pages: 20
  openreview_id: bcNRhkVoAK
  pdf_file: 041384d7e060bbc4fb28e91ccc986291181c7d58.pdf
  start_page: 9212
  title: 'AnchorAL: Computationally Efficient Active Learning for Large and Imbalanced
    Datasets'
- abstract: The majority of the recently developed models for automated essay scoring
    (AES) are evaluated solely on the ASAP corpus. However, ASAP is not without its
    limitations. For instance, it is not clear whether models trained on ASAP can
    generalize well when evaluated on other corpora. In light of these limitations,
    we introduce ICLE++, a corpus of persuasive student essays annotated with both
    holistic scores and trait-specific scores. Not only can ICLE++ be used to test
    the generalizability of AES models trained on ASAP, but it can also facilitate
    the evaluation of models developed for newer AES problems such as multi-trait
    scoring and cross-prompt scoring. We believe that ICLE++, which represents a culmination
    of our long-term effort in annotating the essays in the ICLE corpus, contributes
    to the set of much-needed annotated corpora for AES research.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - dblp_id: https://dblp.org/pid/123/9368-2
    emails: '****@hlt.utdallas.edu'
    first_name: Shengjie
    institution: University of Texas at Dallas
    last_name: Li
    name: Shengjie Li
    orcid: https://orcid.org/0000-0002-5442-5464
    semantic_scholar_id: https://www.semanticscholar.org/author/51019115
    username: ~Shengjie_Li2
  - dblp_id: https://dblp.org/pid/67/3142
    emails: '****@hlt.utdallas.edu'
    first_name: Vincent
    google_scholar_id: https://scholar.google.com/citations?user=4UyniXYAAAAJ&hl=en&oi=ao
    homepage: https://www.hlt.utdallas.edu/~vince/
    institution: University of Texas at Dallas, Central China Normal University, State
      University of New York at Stony Brook, Tohoku University, Tokyo Institute of
      Technology and Mohamed bin Zayed University of Artificial Intelligence
    last_name: Ng
    name: Vincent Ng
    semantic_scholar_id: https://www.semanticscholar.org/author/Vincent-Ng/145106110
    username: ~Vincent_Ng1
  decision: toMainConference
  end_page: 9252
  file: 1051.pdf
  id: 1051
  num_pages: 21
  openreview_id: Trex1KSiWz
  pdf_file: 51d17948f782bd030663efc5db7b95ea6c53778c.pdf
  start_page: 9232
  title: 'ICLE++: Modeling Fine-Grained Traits for Holistic Essay Scoring'
- abstract: 'While major languages often enjoy substantial attention and resources,
    the linguistic diversity across the globe encompasses a multitude of smaller,
    indigenous, and regional languages that lack the same level of computational support.
    One such region is the Caribbean. While commonly labeled as "English speaking",
    the ex-British Caribbean region consists of a myriad of Creole languages thriving
    alongside English. In this paper, we present Guylingo: a comprehensive corpus
    designed for advancing NLP research in the domain of Creolese (Guyanese English-lexicon
    Creole), the most widely spoken language in the culturally rich nation of Guyana.
    We first outline our framework for gathering and digitizing this diverse corpus,
    inclusive of colloquial expressions, idioms, and regional variations in a low-resource
    language. We then demonstrate the challenges of training and evaluating NLP models
    for machine translation for Creolese. Lastly, we discuss the unique opportunities
    presented by recent NLP advancements for accelerating the formal adoption of Creole
    languages as official languages in the Caribbean.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: 'Special Theme: Languages of Latin America'
  authors:
  - dblp_id: https://dblp.org/pid/84/6475
    emails: '****@gmail.com'
    first_name: Christopher
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=IaFEAbsAAAAJ
    institution: University of Michigan - Ann Arbor and University of Guyana
    last_name: Clarke
    name: Christopher Clarke
    orcid: https://orcid.org/0000-0001-8741-3155
    semantic_scholar_id: https://www.semanticscholar.org/author/143640106
    username: ~Christopher_Clarke1
  - emails: '****@umich.edu'
    first_name: Roland
    institution: University of Michigan - Ann Arbor
    last_name: Daynauth
    name: Roland Daynauth
    orcid: https://orcid.org/0009-0008-1149-0179
    username: ~Roland_Daynauth1
  - dblp_id: https://dblp.org/pid/48/6796
    emails: '****@umich.edu'
    first_name: Jason
    homepage: http://jasonmars.org
    last_name: Mars
    name: Jason Mars
    username: ~Jason_Mars1
  - emails: '****@uog.edu.gy'
    first_name: Charlene
    last_name: Wilkinson
    name: Charlene Wilkinson
    username: ~Charlene_Wilkinson1
  - emails: '****@gmail.com'
    first_name: Hubert
    last_name: Devonish
    name: Hubert Devonish
    username: ~Hubert_Devonish1
  decision: toMainConference
  end_page: 9259
  file: 1060.pdf
  id: 1060
  num_pages: 7
  openreview_id: SnCxqO8WSy
  pdf_file: c1a78fe2cad1513dbdb3021d229e6951a822b31b.pdf
  start_page: 9253
  title: 'GuyLingo: The Republic of Guyana Creole Corpora'
- abstract: Language technologies that accurately model the dynamics of events must
    perform commonsense reasoning. Existing work evaluating commonsense reasoning
    focuses on making inferences about common, everyday situations. To instead investigate
    the ability to model unusual, unexpected, and unlikely situations, we explore
    the task of uncommonsense abductive reasoning. Given a piece of context with an
    unexpected outcome, this task requires reasoning abductively to generate an explanation
    that makes the unexpected outcome more likely in the context. To this end, we
    curate and release a new English language corpus called UNcommonsense. We characterize
    the performance differences between human explainers and the best-performing large
    language models, finding that model-enhanced human-written explanations achieve
    the highest quality by trading off between specificity and diversity. Finally,
    we experiment with several imitation learning algorithms to train open and accessible
    language models on this task. When compared with the vanilla supervised fine-tuning
    approach, these methods consistently reduce lose rates on both common and uncommonsense
    abductive reasoning judged by human evaluators.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Question Answering
  authors:
  - dblp_id: https://dblp.org/pid/41/10049-2.html
    emails: '****@cs.cornell.edu'
    first_name: Wenting
    google_scholar_id: https://scholar.google.com/citations?user=sycHskQAAAAJ
    institution: Cornell University
    last_name: Zhao
    name: Wenting Zhao
    semantic_scholar_id: https://www.semanticscholar.org/author/Wenting-Zhao/2954369
    username: ~Wenting_Zhao1
  - dblp_id: https://dblp.org/pid/278/2437.html
    emails: '****@gmail.com'
    first_name: Justin
    google_scholar_id: https://scholar.google.com/citations?user=043r6toAAAAJ&hl=en
    institution: Cornell University
    last_name: Chiu
    middle_name: T
    name: Justin T Chiu
    username: ~Justin_T_Chiu1
  - dblp_id: https://dblp.org/pid/83/10905
    emails: '****@allenai.org'
    first_name: Jena
    google_scholar_id: https://scholar.google.com/citations?user=9QuMhLgAAAAJ&hl=en
    homepage: https://jenahwang.github.io/
    institution: Allen Institute for Artificial Intelligence
    last_name: Hwang
    middle_name: D.
    name: Jena D. Hwang
    semantic_scholar_id: https://www.semanticscholar.org/author/Jena-D.-Hwang/2012510
    username: ~Jena_D._Hwang1
  - dblp_id: https://dblp.org/pid/276/6005
    emails: '****@allenai.org'
    first_name: Faeze
    google_scholar_id: https://scholar.google.com/citations?user=4Da7Li4AAAAJ&hl=en
    homepage: https://users.soe.ucsc.edu/~hannahbrahman/
    institution: Allen Institute for AI
    last_name: Brahman
    name: Faeze Brahman
    semantic_scholar_id: https://www.semanticscholar.org/author/Faeze-Brahman/9252833
    username: ~Faeze_Brahman1
  - dblp_id: https://dblp.uni-trier.de/pid/132/5250.html
    emails: '****@gmail.com'
    first_name: Jack
    google_scholar_id: https://scholar.google.com/citations?user=SxQQ1msAAAAJ
    homepage: https://www.jmhessel.com
    institution: Samaya AI
    last_name: Hessel
    name: Jack Hessel
    orcid: https://orcid.org/0000-0002-4012-8979
    semantic_scholar_id: https://www.semanticscholar.org/author/Jack-Hessel/2689239
    username: ~Jack_Hessel1
  - dblp_id: https://dblp.org/pid/135/8207
    emails: '****@cornell.edu'
    first_name: Sanjiban
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=5lB_d78AAAAJ
    homepage: https://www.sanjibanchoudhury.com/
    institution: Cornell University
    last_name: Choudhury
    name: Sanjiban Choudhury
    username: ~Sanjiban_Choudhury3
  - dblp_id: https://dblp.org/pid/89/579
    emails: '****@cs.washington.edu'
    first_name: Yejin
    google_scholar_id: https://scholar.google.com/citations?user=vhP-tlcAAAAJ&hl=en
    homepage: https://homes.cs.washington.edu/~yejin/
    institution: Department of Computer Science, University of Washington
    last_name: Choi
    name: Yejin Choi
    semantic_scholar_id: https://www.semanticscholar.org/author/Yejin-Choi/1699545?sort=year
    username: ~Yejin_Choi1
  - dblp_id: https://dblp.org/pid/40/1491-69
    emails: '****@gmail.com'
    first_name: Xiang
    google_scholar_id: https://scholar.google.com/citations?user=SRgRwSoAAAAJ&hl=zh-CN
    homepage: https://people.cs.pitt.edu/~xianglli/
    last_name: Li
    middle_name: Lorraine
    name: Xiang Lorraine Li
    username: ~Xiang_Lorraine_Li1
  - dblp_id: https://dblp.org/pid/203/9306
    emails: '****@berkeley.edu'
    first_name: Alane
    google_scholar_id: https://scholar.google.com/citations?user=daslsUkAAAAJ&hl=en&authuser=1
    homepage: http://www.alanesuhr.com
    institution: University of California, Berkeley
    last_name: Suhr
    name: Alane Suhr
    username: ~Alane_Suhr1
  decision: toMainConference
  end_page: 9278
  file: 1064.pdf
  id: 1064
  num_pages: 19
  openreview_id: Sw2vQ56PoN
  pdf_file: 48551c55a070baae53972f764789e4f1b780441b.pdf
  start_page: 9260
  title: 'UNcommonsense Reasoning: Abductive Reasoning about Uncommon Situations'
- abstract: Text-based false information permeates online discourses, yet evidence
    of people's ability to discern truth from such deceptive textual content is scarce.
    We analyze a novel TV game show data where conversations in a high-stake environment
    between individuals with conflicting objectives result in lies. We investigate
    the manifestation of potentially verifiable language cues of deception in the
    presence of objective truth, a distinguishing feature absent in previous text-based
    deception datasets. We show that there exists a class of detectors (algorithms)
    that have similar truth detection performance compared to human subjects, even
    when the former accesses only the language cues while the latter engages in conversations
    with complete access to all potential sources of cues (language and audio-visual).
    Our model, built on a large language model, employs a bottleneck framework to
    learn discernible cues to determine truth, an act of reasoning in which human
    subjects often perform poorly, even with incentives. Our model detects novel but
    accurate language cues in many cases where humans failed to detect deception,
    opening up the possibility of humans collaborating with algorithms and ameliorating
    their ability to detect the truth.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - emails: '****@utah.edu'
    first_name: Sanchaita
    homepage: https://sanchaitahazra.com/
    last_name: Hazra
    name: Sanchaita Hazra
    username: ~Sanchaita_Hazra1
  - dblp_id: https://dblp.org/pid/138/6177
    emails: '****@allenai.org'
    first_name: Bodhisattwa Prasad
    google_scholar_id: https://scholar.google.com/citations?user=cEM1a5gAAAAJ&hl=en
    homepage: http://www.majumderb.com/
    institution: Allen Institute for Artificial Intelligence
    last_name: Majumder
    name: Bodhisattwa Prasad Majumder
    semantic_scholar_id: https://www.semanticscholar.org/author/Bodhisattwa-Prasad-Majumder/3165738
    username: ~Bodhisattwa_Prasad_Majumder1
  decision: toMainConference
  end_page: 9293
  file: 1065.pdf
  id: 1065
  num_pages: 15
  openreview_id: kmm14MVIxe
  pdf_file: f2fa7e85bedc2b7240703be3e0d94c8e56cebd17.pdf
  start_page: 9279
  title: 'To Tell The Truth: Language of Deception and Language Models'
- abstract: "We present experiments on Automatic Speech Recognition (ASR) for Bribri\
    \ and Cab\xE9car, two languages from the Chibchan family. We fine-tune four ASR\
    \ algorithms (Wav2Vec2, Whisper, MMS & WavLM) to create monolingual models, with\
    \ the Wav2Vec2 model demonstrating the best performance. We then proceed to use\
    \ Wav2Vec2 for (1) experiments on training joint and transfer learning models\
    \ for both languages, and (2) an analysis of the errors, with a focus on the transcription\
    \ of tone. Results show effective transfer learning for both Bribri and Cab\xE9\
    car, but especially for Bribri. A post-processing spell checking step further\
    \ reduced character and word error rates. As for the errors, tone is where the\
    \ Bribri models make the most errors, whereas the simpler tonal system of Cab\xE9\
    car is better transcribed by the model. Our work contributes to developing better\
    \ ASR technology, an important tool that could facilitate transcription, one of\
    \ the major bottlenecks in language documentation efforts. Our work also assesses\
    \ how existing pre-trained models and algorithms perform for genuine extremely\
    \ low resource-languages."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: 'Special Theme: Languages of Latin America'
  authors:
  - dblp_id: https://dblp.org/pid/199/8538.html
    emails: '****@dartmouth.edu'
    first_name: Rolando
    google_scholar_id: https://scholar.google.com/citations?user=dOq_eUYAAAAJ&hl=en&oi=ao
    institution: Dartmouth College
    last_name: Coto-Solano
    name: Rolando Coto-Solano
    orcid: https://orcid.org/0000-0003-2100-1857
    semantic_scholar_id: https://www.semanticscholar.org/author/Rolando-Coto-Solano/1405433180
    username: ~Rolando_Coto-Solano1
  - emails: '****@dartmouth.edu'
    first_name: Tai Wan
    homepage: https://github.com/taytwkim
    last_name: Kim
    name: Tai Wan Kim
    username: ~Tai_Wan_Kim1
  - emails: '****@gmail.com'
    first_name: Alexander
    google_scholar_id: https://scholar.google.com/citations?user=rIO9rpQAAAAJ&hl=en
    last_name: Jones
    name: Alexander Jones
    username: ~Alexander_Jones1
  - dblp_id: https://dblp.org/pid/03/11426
    emails: '****@gu.se'
    first_name: Sharid
    google_scholar_id: https://scholar.google.com/citations?user=FgrwX34AAAAJ&hl=en
    homepage: https://sites.google.com/site/loaicigasharid/
    institution: University of Gothenburg, Sweden
    last_name: "Lo\xE1iciga"
    name: "Sharid Lo\xE1iciga"
    orcid: https://orcid.org/0000-0002-1204-0400
    semantic_scholar_id: https://www.semanticscholar.org/author/S.-Lo%C3%A1iciga/2947553
    username: "~Sharid_Lo\xE1iciga1"
  decision: toMainConference
  end_page: 9308
  file: 1066.pdf
  id: 1066
  num_pages: 15
  openreview_id: PESUHX34ZL
  pdf_file: 5a14a1909894ebb773cf6ece4d64468e0f016561.pdf
  start_page: 9294
  title: Multilingual Models for ASR in Chibchan Languages
- abstract: 'While legal AI has made strides in recent years, it still struggles with
    basic legal concepts: _when_ does a law apply? _Who_ does it applies to? _What_
    does it do? We take a _discourse_ approach to addressing these problems and introduce
    a novel taxonomy for span-and-relation parsing of legal texts. We create a dataset,
    _LegalDiscourse_ of 602 state-level law paragraphs consisting of 3,715 discourse
    spans and 1,671 relations. Our trained annotators have an agreement-rate $\kappa>.8$,
    yet few-shot GPT3.5 performs poorly at span identification and relation classification.
    Although fine-tuning improves performance, GPT3.5 still lags far below human level.
    We demonstrate the usefulness of our schema by creating a web application with
    journalists. We collect over $100,000$ laws for $52$ U.S. states and territories
    using $20$ scrapers we built, and apply our trained models to $6,000$ laws using
    U.S. Census population numbers. We describe two journalistic outputs stemming
    from this application: (1) an investigation into the increase in liquor licenses
    following population growth and (2) a decrease in applicable laws under different
    under-count projections.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Discourse and Pragmatics
  authors:
  - dblp_id: https://dblp.org/pid/227/2512
    emails: '****@usc.edu'
    first_name: Alexander
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=pZEaPR8AAAAJ
    homepage: http://alexander-spangher.com/
    institution: University of Southern California
    last_name: Spangher
    name: Alexander Spangher
    semantic_scholar_id: https://www.semanticscholar.org/author/Alexander-Spangher/51444076
    username: ~Alexander_Spangher2
  - emails: '****@gmail.com'
    first_name: Zihan
    institution: University of California, Los Angeles
    last_name: Xue
    name: Zihan Xue
    username: ~Zihan_Xue1
  - dblp_id: https://dblp.org/pid/166/3298
    emails: '****@g.ucla.edu'
    first_name: Te-Lin
    institution: University of California, Los Angeles
    last_name: Wu
    name: Te-Lin Wu
    semantic_scholar_id: https://www.semanticscholar.org/author/Te-Lin-Wu/2015467
    username: ~Te-Lin_Wu1
  - emails: '****@columbia.edu'
    first_name: Mark
    homepage: http://brown.columbia.edu
    institution: Columbia University
    last_name: Hansen
    name: Mark Hansen
    username: ~Mark_Hansen2
  - dblp_id: https://dblp.org/pid/00/4758
    emails: '****@isi.edu'
    first_name: Jonathan
    google_scholar_id: https://scholar.google.com/citations?user=tmK5EPEAAAAJ&hl=en
    homepage: http://jonmay.net
    institution: University of Southern California and USC/ISI
    last_name: May
    name: Jonathan May
    orcid: https://orcid.org/0000-0002-5284-477X
    semantic_scholar_id: https://www.semanticscholar.org/author/Jonathan-May/143823227
    username: ~Jonathan_May1
  decision: toMainConference
  end_page: 9332
  file: 1067.pdf
  id: 1067
  num_pages: 24
  openreview_id: EUsHx4g2LQ
  pdf_file: 4c50100d4e3b6bc2bfddc6e5f03b8bb88351108d.pdf
  start_page: 9309
  title: 'LegalDiscourse: Interpreting When Laws Apply and To Whom'
- abstract: 'Natural Language Generation (NLG) typically involves evaluating the generated
    text in various aspects (e.g., consistency and naturalness) to obtain a comprehensive
    assessment. However, multi-aspect evaluation remains challenging as it may require
    the evaluator to generalize to any given evaluation aspect even if it''s absent
    during training. In this paper, we introduce X-Eval, a two-stage instruction tuning
    framework to evaluate text in both seen and unseen aspects customized by end users.
    X-Eval consists of two learning stages: the vanilla instruction tuning stage that
    improves the model''s ability to follow evaluation instructions, and an enhanced
    instruction tuning stage that exploits the connections between fine-grained evaluation
    aspects to better assess text quality. To support the training of X-Eval, we collect
    AspectInstruct, the first instruction tuning dataset tailored for multi-aspect
    NLG evaluation spanning 27 diverse evaluation aspects with 65 tasks. To enhance
    task diversity, we devise an augmentation strategy that converts human rating
    annotations into diverse forms of NLG evaluation tasks, including scoring, comparison,
    ranking, and Boolean question answering. Extensive experiments across three essential
    categories of NLG tasks: dialogue generation, summarization, and data-to-text
    coupled with 21 aspects in meta-evaluation, demonstrate that X-Eval enables even
    a lightweight language model to achieve a comparable if not higher correlation
    with human judgments compared to the state-of-the-art NLG evaluators like GPT-4.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - dblp_id: https://dblp.org/pid/193/2086
    emails: '****@vt.edu'
    first_name: Minqian
    google_scholar_id: https://scholar.google.com/citations?user=xCR8nrwAAAAJ&hl=en
    homepage: https://mqianliu.github.io/
    institution: Virginia Polytechnic Institute and State University
    last_name: Liu
    name: Minqian Liu
    semantic_scholar_id: https://www.semanticscholar.org/author/Minqian-Liu/2123130842
    username: ~Minqian_Liu2
  - dblp_id: https://dblp.org/pid/01/8558
    emails: '****@vt.edu'
    first_name: Ying
    google_scholar_id: https://scholar.google.com/citations?user=NytpXgwAAAAJ&hl=en
    homepage: https://yingshen-ys.github.io/
    last_name: Shen
    name: Ying Shen
    semantic_scholar_id: https://www.semanticscholar.org/author/Ying-Shen/2115382990
    username: ~Ying_Shen4
  - dblp_id: https://dblp.org/pid/267/2280
    emails: '****@vt.edu'
    first_name: Zhiyang
    google_scholar_id: https://scholar.google.com/citations?user=11zbVUAAAAAJ&hl=en&oi=sra
    last_name: Xu
    name: Zhiyang Xu
    semantic_scholar_id: https://www.semanticscholar.org/author/Z.-Xu/4230728
    username: ~Zhiyang_Xu1
  - dblp_id: https://dblp.org/pid/20/8038-2
    emails: '****@gmail.com'
    first_name: Yixin
    google_scholar_id: https://scholar.google.co.uk/citations?user=CnhTvdoAAAAJ
    homepage: https://sites.google.com/view/yixin-homepage
    institution: Singapore Management University
    last_name: Cao
    name: Yixin Cao
    username: ~Yixin_Cao1
  - dblp_id: https://dblp.org/pid/126/8748
    emails: '****@amazon.com'
    first_name: Eunah
    google_scholar_id: https://scholar.google.com/citations?user=QHOk-eUAAAAJ&hl=en
    last_name: Cho
    name: Eunah Cho
    username: ~Eunah_Cho1
  - emails: '****@gmail.com'
    first_name: Vaibhav
    institution: School of Computer Science, Carnegie Mellon University
    last_name: Kumar
    name: Vaibhav Kumar
    username: ~Vaibhav_Kumar1
  - dblp_id: https://dblp.org/pid/50/5680
    emails: '****@ieee.org'
    first_name: Reza
    google_scholar_id: https://scholar.google.com/citations?user=00ncu3cAAAAJ&hl=en
    institution: Amazon
    last_name: Ghanadan
    name: Reza Ghanadan
    username: ~Reza_Ghanadan1
  - dblp_id: https://dblp.org/pid/127/0072
    emails: '****@gmail.com'
    first_name: Lifu
    google_scholar_id: https://scholar.google.com/citations?user=76IEGtYAAAAJ&hl=en
    homepage: https://wilburone.github.io/
    institution: Virginia Tech
    last_name: Huang
    name: Lifu Huang
    username: ~Lifu_Huang1
  decision: toMainConference
  end_page: 9352
  file: 1071.pdf
  id: 1071
  num_pages: 20
  openreview_id: FcLIanNZLC
  pdf_file: 335fe58a866e615a3ef833ccd5aab487308dfc27.pdf
  start_page: 9333
  title: 'X-Eval: Generalizable Multi-aspect Text Evaluation via Augmented Instruction
    Tuning with Auxiliary Evaluation Aspects'
- abstract: The majority of automatic metrics for evaluating NLG systems are reference-based.
    However, the challenge of collecting human annotation results in a lack of reliable
    references in numerous application scenarios. Despite recent advancements in reference-free
    metrics, it has not been well understood when and where they can be used as an
    alternative to reference-based metrics. In this study, by employing diverse analytical
    approaches, we comprehensively assess the performance of both metrics across a
    wide range of NLG tasks, encompassing eight datasets and eight evaluation models.
    Based on solid experiments, the results show that reference-free metrics exhibit
    a higher correlation with human judgment and greater sensitivity to deficiencies
    in language quality. However, their effectiveness varies across tasks and is influenced
    by the quality of candidate texts. Therefore, it's important to assess the performance
    of reference-free metrics before applying them to a new task, especially when
    inputs are in uncommon form or when the answer space is highly variable. Our study
    can provide insight into the appropriate application of automatic metrics and
    the impact of metric choice on evaluation performance.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - dblp_id: https://dblp.org/pid/348/9082
    emails: '****@sjtu.edu.cn'
    first_name: Shuqian
    google_scholar_id: https://scholar.google.com/citations?user=oBlp0yQAAAAJ&hl=en
    last_name: Sheng
    name: Shuqian Sheng
    username: ~Shuqian_Sheng1
  - dblp_id: https://dblp.org/pid/14/5580-4
    emails: '****@sjtu.edu.cn'
    first_name: Yi
    google_scholar_id: https://scholar.google.com/citations?user=E-VwoYEAAAAJ&hl=en
    homepage: https://cv.omegaxyz.com/
    last_name: Xu
    name: Yi Xu
    orcid: https://orcid.org/0000-0002-5280-6132
    username: ~Yi_Xu13
  - emails: '****@sjtu.edu.cn'
    first_name: Luoyi
    google_scholar_id: https://scholar.google.com.tw/citations?user=xHs9mCUAAAAJ
    homepage: http://www.cs.sjtu.edu.cn/~fu-ly/index.html
    last_name: Fu
    name: Luoyi Fu
    username: ~Luoyi_Fu1
  - emails: '****@sjtu.edu.cn'
    first_name: Jiaxin
    google_scholar_id: https://scholar.google.com/citations?user=bF_VBEAAAAAJ&hl=en
    homepage: https://jhc.sjtu.edu.cn/~jiaxinding/
    institution: Shanghai Jiaotong University
    last_name: Ding
    name: Jiaxin Ding
    username: ~Jiaxin_Ding1
  - dblp_id: https://dblp.org/pid/72/5749
    emails: '****@sjtu.edu.cn'
    first_name: Lei
    institution: Shanghai Jiaotong University
    last_name: Zhou
    name: Lei Zhou
    username: ~Lei_Zhou3
  - dblp_id: https://dblp.dagstuhl.de/pid/96/1149.html
    emails: '****@sjtu.edu.cn'
    first_name: Xinbing
    google_scholar_id: https://scholar.google.com.tw/citations?user=CT5yZbwAAAAJ
    homepage: http://www.cs.sjtu.edu.cn/~wang-xb/
    institution: Shanghai Jiao Tong University
    last_name: Wang
    name: Xinbing Wang
    username: ~Xinbing_Wang1
  - dblp_id: https://dblp.org/pid/85/1324.html
    emails: '****@outlook.com'
    first_name: Chenghu
    homepage: http://www.igsnrr.cas.cn/gkjj/ysfc/ysfc_zhouchenghu/
    institution: IGSNRR, Chinese Academy of Sciences, Beijing, China
    last_name: Zhou
    name: Chenghu Zhou
    username: ~Chenghu_Zhou3
  decision: toMainConference
  end_page: 9369
  file: 1073.pdf
  id: 1073
  num_pages: 17
  openreview_id: pMEeSY4w9t
  pdf_file: e5dce6075c94864b5ccbc25a4a6af99f795c2331.pdf
  start_page: 9353
  title: Is Reference Necessary in the Evaluation of NLG Systems? When and Where?
- abstract: 'An important open question in the use of large language models for knowledge-intensive
    tasks is how to effectively integrate knowledge from three sources: the model''s
    parametric memory, external structured knowledge, and external unstructured knowledge.
    Most existing prompting methods either rely on one or two of these sources, or
    require repeatedly invoking large language models to generate similar or identical
    content. In this work, we overcome these limitations by introducing a novel semi-structured
    prompting approach that seamlessly integrates the model''s parametric memory with
    unstructured knowledge from text documents and structured knowledge from knowledge
    graphs. Experimental results on open-domain multi-hop question answering datasets
    demonstrate that our prompting method significantly surpasses existing techniques,
    even exceeding those that require fine-tuning.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Learning for NLP
  authors:
  - dblp_id: https://dblp.org/pid/54/3643
    emails: '****@arizona.edu'
    first_name: Xin
    google_scholar_id: https://scholar.google.com/citations?user=4YlcxsoAAAAJ&hl=en
    homepage: https://xinsu.name/
    institution: University of Arizona
    last_name: Su
    name: Xin Su
    orcid: https://orcid.org/0000-0002-2712-2804
    semantic_scholar_id: https://www.semanticscholar.org/author/2087722611
    username: ~Xin_Su2
  - dblp_id: https://dblp.org/pid/118/3873
    emails: '****@intel.com'
    first_name: Tiep
    google_scholar_id: https://scholar.google.com/citations?user=3RV_GAwAAAAJ&hl=en
    institution: Intel
    last_name: Le
    name: Tiep Le
    username: ~Tiep_Le2
  - dblp_id: https://dblp.org/pid/52/5246
    emails: '****@arizona.edu'
    first_name: Steven
    google_scholar_id: https://scholar.google.com.tw/citations?user=sXM8J5EAAAAJ
    homepage: http://bethard.faculty.arizona.edu/
    institution: University of Arizona
    last_name: Bethard
    name: Steven Bethard
    orcid: https://orcid.org/0000-0001-9560-6491
    semantic_scholar_id: https://www.semanticscholar.org/author/Steven-Bethard/2105138
    username: ~Steven_Bethard1
  - emails: '****@intel.com'
    first_name: Phillip
    google_scholar_id: https://scholar.google.com/citations?user=EKh822gAAAAJ&hl=en
    institution: Intel
    last_name: Howard
    name: Phillip Howard
    username: ~Phillip_Howard1
  decision: toMainConference
  end_page: 9386
  file: 1078.pdf
  id: 1078
  num_pages: 17
  openreview_id: y4lMKbFwKW
  pdf_file: a7f12afb7b719f08f83762bd9304f4dd6551d544.pdf
  start_page: 9370
  title: 'Semi-Structured Chain-of-Thought: Integrating Multiple Sources of Knowledge
    for Improved Language Model Reasoning'
- abstract: The development of highly fluent large language models (LLMs) has prompted
    increased interest in assessing their reasoning and problem-solving capabilities.
    We investigate whether several LLMs can solve a classic type of deductive reasoning
    problem from the cognitive science literature. The tested LLMs have limited abilities
    to solve these problems in their conventional form. We performed follow up experiments
    to investigate if changes to the presentation format and content improve model
    performance. We do find performance differences between conditions; however, they
    do not improve overall performance. Moreover, we find that performance interacts
    with presentation format and content in unexpected ways that differ from human
    performance. Overall, our results suggest that LLMs have unique reasoning biases
    that are only partially predicted from human reasoning performance and the human-generated
    language corpora that informs them.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - emails: '****@outlook.com'
    first_name: S
    google_scholar_id: https://scholar.google.com/citations?user=42TI1qYAAAAJ&hl=en&oi=ao
    last_name: Seals
    middle_name: M
    name: S M Seals
    orcid: https://orcid.org/0000-0002-3175-2681
    semantic_scholar_id: https://www.semanticscholar.org/author/Savannah-M.-Seals/2048147669
    username: ~S_M_Seals1
  - dblp_id: https://dblp.org/pid/26/6109.html
    emails: '****@wright.edu'
    first_name: Valerie
    google_scholar_id: https://scholar.google.com/citations?user=trFx5GIAAAAJ&hl=en&oi=ao
    homepage: https://people.wright.edu/valerie.shalin
    institution: University of South Carolina and Wright State University
    last_name: Shalin
    name: Valerie Shalin
    semantic_scholar_id: https://www.semanticscholar.org/author/V.-Shalin/2890773
    username: ~Valerie_Shalin1
  decision: toMainConference
  end_page: 9403
  file: 1079.pdf
  id: 1079
  num_pages: 17
  openreview_id: 5j6KoLDQWI
  pdf_file: d205364b7613cbe3e9a5309102d7546037dd035b.pdf
  start_page: 9387
  title: Evaluating the Deductive Competence of Large Language Models
- abstract: Estimating causal effects from non-randomized data requires assumptions
    about the underlying data-generating process. To achieve unbiased estimates of
    the causal effect of a treatment on an outcome, we typically adjust for any confounding
    variables that influence both treatment and outcome. When such confounders include
    text data, existing causal inference methods struggle due to the high dimensionality
    of the text. The simple statistical models which have sufficient convergence criteria
    for causal estimation are not well-equipped to handle noisy unstructured text,
    but flexible large language models that excel at predictive tasks with text data
    do not meet the statistical assumptions necessary for causal estimation. Our method
    enables theoretically consistent estimation of causal effects using LLM-based
    nuisance models by incorporating them within the framework of Double Machine Learning.
    On the best available dataset for evaluating such methods, we obtain a 10.4% reduction
    in the relative absolute error for the estimated causal effect over existing methods.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Learning for NLP
  authors:
  - emails: '****@u.northwestern.edu'
    first_name: Marko
    last_name: Veljanovski
    name: Marko Veljanovski
    username: ~Marko_Veljanovski1
  - dblp_id: https://dblp.org/pid/214/1328
    emails: '****@northwestern.edu'
    first_name: Zach
    google_scholar_id: https://scholar.google.com/citations?user=JMWsXSMAAAAJ&hl=en
    homepage: https://zachwd.com
    institution: Northwestern University
    last_name: Wood-Doughty
    name: Zach Wood-Doughty
    orcid: https://orcid.org/0000-0003-2386-4840
    semantic_scholar_id: https://www.semanticscholar.org/author/Zach-Wood-Doughty/1411379613
    username: ~Zach_Wood-Doughty1
  decision: toMainConference
  end_page: 9412
  file: 1080.pdf
  id: 1080
  num_pages: 9
  openreview_id: yVu8zkRFhh
  pdf_file: 4606e53ac0bbf5a383cbf3737f5f19e77165eca9.pdf
  start_page: 9404
  title: 'DoubleLingo: Causal Estimation with Large Language Models'
- abstract: 'As research in human-centered NLP advances, there is a growing recognition
    of the importance of incorporating human and social factors into NLP models. At
    the same time, our NLP systems have become heavily reliant on LLMs, most of which
    do not model authors. To build NLP systems that can truly understand human language,
    we must better integrate human contexts into LLMs. This brings to the fore a range
    of design considerations and challenges in terms of what human aspects to capture,
    how to represent them, and what modeling strategies to pursue. To address these,
    we advocate for three positions toward creating large human language models (LHLMs)
    using concepts from psychological and behavioral sciences: First, LM training
    should include the human context. Second, LHLMs should recognize that people are
    more than their group(s). Third, LHLMs should be able to account for the dynamic
    and temporally-dependent nature of the human context. We refer to relevant advances
    and present open challenges that need to be addressed and their possible solutions
    in realizing these goals.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Computational Social Science and Cultural Analytics
  authors:
  - dblp_id: https://dblp.org/pid/239/9312-2
    emails: '****@cs.stonybrook.edu'
    first_name: Nikita
    google_scholar_id: https://scholar.google.com/citations?user=1w2rduoAAAAJ&hl=en&authuser=2
    homepage: https://www3.cs.stonybrook.edu/~nisoni/
    last_name: Soni
    name: Nikita Soni
    semantic_scholar_id: https://www.semanticscholar.org/author/Nikita-Soni/145297996
    username: ~Nikita_Soni1
  - dblp_id: https://dblp.org/pid/46/3430
    emails: '****@cs.stonybrook.edu'
    first_name: H.
    google_scholar_id: https://scholar.google.com.tw/citations?user=Na16PsUAAAAJ
    homepage: http://www3.cs.stonybrook.edu/~has/
    institution: Stony Brook University (SUNY)
    last_name: Schwartz
    name: H. Schwartz
    orcid: https://orcid.org/0000-0002-6383-3339
    semantic_scholar_id: https://www.semanticscholar.org/author/H.-A.-Schwartz/145035129
    username: ~H._Schwartz1
  - emails: '****@stern.nyu.edu'
    first_name: "Jo\xE3o"
    google_scholar_id: https://scholar.google.com/citations?user=vv355NgAAAAJ&hl=en&oi=sra
    institution: New York University
    last_name: Sedoc
    name: "Jo\xE3o Sedoc"
    username: "~Jo\xE3o_Sedoc1"
  - dblp_id: https://dblp.org/pid/40/1931
    emails: '****@cs.stonybrook.edu'
    first_name: Niranjan
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=X3JCGVIAAAAJ&view_op=list_works&sortby=pubdate
    homepage: http://www3.cs.stonybrook.edu/~niranjan/
    institution: State University of New York, Stony Brook
    last_name: Balasubramanian
    name: Niranjan Balasubramanian
    semantic_scholar_id: https://www.semanticscholar.org/author/Niranjan-Balasubramanian/35217367
    username: ~Niranjan_Balasubramanian1
  decision: toMainConference
  end_page: 9428
  file: 1084.pdf
  id: 1084
  num_pages: 16
  openreview_id: CspeJrT7dd
  pdf_file: 7ccbb02210b5869f0569ca6647758df0180fd645.pdf
  start_page: 9413
  title: 'Large Human Language Models: A Need and the Challenges'
- abstract: Recent studies have found that summaries generated by large language models
    (LLMs) are favored by human annotators over the original reference summaries in
    commonly used summarization datasets. Therefore, we study an LLM-as-reference
    learning setting for smaller text summarization models to investigate whether
    their performance can be substantially improved. To this end, we use LLMs as both
    oracle summary generators for standard supervised fine-tuning and oracle summary
    evaluators for efficient contrastive learning that leverages the LLMs' supervision
    signals. We conduct comprehensive experiments with source news articles and find
    that (1) summarization models trained under the LLM-as-reference setting achieve
    significant performance improvement in both LLM and human evaluations; (2) contrastive
    learning outperforms standard supervised fine-tuning under both low and high resource
    settings. Our experimental results also enable a meta-analysis of LLMs' summary
    evaluation capacities under a challenging setting, showing that LLMs are not well-aligned
    with human evaluators. Particularly, our expert human evaluation reveals remaining
    nuanced performance gaps between LLMs and our fine-tuned models, which LLMs fail
    to capture. Thus, we call for further studies into both the potential and challenges
    of using LLMs in summarization model development.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Summarization
  authors:
  - dblp_id: https://dblp.org/pid/140/7348.html
    emails: '****@yale.edu'
    first_name: Yixin
    google_scholar_id: https://scholar.google.com/citations?user=sFtxaMkAAAAJ&hl=en
    homepage: https://yixinl7.github.io/
    institution: Yale University
    last_name: Liu
    name: Yixin Liu
    semantic_scholar_id: https://www.semanticscholar.org/author/Yixin-Liu/2108176413
    username: ~Yixin_Liu2
  - emails: '****@nyu.edu'
    first_name: Kejian
    google_scholar_id: https://scholar.google.com/citations?user=KZbOHrUAAAAJ&hl=en
    last_name: Shi
    name: Kejian Shi
    username: ~Kejian_Shi2
  - emails: '****@yale.edu'
    first_name: Katherine
    last_name: He
    middle_name: S
    name: Katherine S He
    username: ~Katherine_S_He1
  - emails: '****@ad.unc.edu'
    first_name: Longtian
    last_name: Ye
    name: Longtian Ye
    username: ~Longtian_Ye2
  - dblp_id: https://dblp.org/pid/203/8539
    emails: '****@salesforce.com'
    first_name: Alexander
    google_scholar_id: https://scholar.google.com/citations?user=GgfJdhwAAAAJ&hl=en
    homepage: https://alex-fabbri.github.io
    institution: SalesForce.com
    last_name: Fabbri
    name: Alexander Fabbri
    username: ~Alexander_Fabbri1
  - dblp_id: https://dblp.org/pid/34/3381-3
    emails: '****@gmail.com'
    first_name: Pengfei
    google_scholar_id: https://scholar.google.com/citations?user=oIz_CYEAAAAJ&hl=en
    homepage: http://pfliu.com/
    last_name: Liu
    name: Pengfei Liu
    semantic_scholar_id: https://www.semanticscholar.org/author/Pengfei-Liu/144118452
    username: ~Pengfei_Liu1
  - dblp_id: https://dblp.org/pid/r/DragomirRRadev
    emails: '****@yale.edu'
    first_name: Dragomir
    google_scholar_id: https://scholar.google.com/citations?user=vIqWvgwAAAAJ&hl=en
    homepage: http://www.cs.yale.edu/~radev
    institution: Yale University
    last_name: Radev
    name: Dragomir Radev
    orcid: https://orcid.org/0000-0002-0213-7487
    semantic_scholar_id: https://www.semanticscholar.org/author/Dragomir-R.-Radev/9215251
    username: ~Dragomir_Radev2
  - dblp_id: https://dblp.org/pid/160/1727
    emails: '****@yale.edu'
    first_name: Arman
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=baI7IY0AAAAJ
    homepage: http://www.armancohan.com
    institution: Yale University and Allen Institute for Artificial Intelligence
    last_name: Cohan
    name: Arman Cohan
    semantic_scholar_id: https://www.semanticscholar.org/author/Arman-Cohan/2527954
    username: ~Arman_Cohan1
  decision: toMainConference
  end_page: 9446
  file: 1088.pdf
  id: 1088
  num_pages: 18
  openreview_id: WXNajiGIy8
  pdf_file: a4ec85f266c6c60ce8cdfc9b98407d5c182d666b.pdf
  start_page: 9429
  title: On Learning to Summarize with Large Language Models as References
- abstract: Large Language Models (LLMs) have shown propensity to generate hallucinated
    outputs, i.e., texts that are factually incorrect or unsupported. Existing methods
    for alleviating hallucinations typically require costly human annotations to identify
    and correct hallucinations in LLM outputs. Moreover, most of these methods focus
    on a specific type of hallucination, e.g., entity or token errors, which limits
    their effectiveness in addressing various types of hallucinations exhibited in
    LLM outputs. To our best knowledge, in this paper we propose the first active
    learning framework to alleviate LLM hallucinations, reducing costly human annotations
    of hallucination needed. By measuring fine-grained hallucinations from errors
    in semantic frame, discourse and content verifiability in text summarization,
    we propose HAllucination Diversity-Aware Sampling (HADAS) to select diverse hallucinations
    for annotations in active learning for LLM finetuning. Extensive experiments on
    three datasets and different backbone models demonstrate advantages of our method
    in effectively and efficiently mitigating LLM hallucinations.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Machine Learning for NLP
  authors:
  - emails: '****@umich.edu'
    first_name: Yu
    google_scholar_id: https://scholar.google.com/citations?user=sTVqEUMAAAAJ&hl=en
    homepage: https://andree-9.github.io/
    last_name: Xia
    name: Yu Xia
    username: ~Yu_Xia9
  - emails: '****@sjtu.edu.cn'
    first_name: Xu
    homepage: https://github.com/LiuX41
    institution: Shanghai Jiaotong University
    last_name: Liu
    name: Xu Liu
    username: ~Xu_Liu12
  - dblp_id: https://dblp.org/pid/32/1593-1
    emails: '****@gmail.com'
    first_name: Tong
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=6-ARmXsAAAAJ&view_op=list_works&sortby=pubdate
    institution: Adobe Research
    last_name: Yu
    name: Tong Yu
    semantic_scholar_id: https://www.semanticscholar.org/author/Tong-Yu/1500399016
    username: ~Tong_Yu3
  - dblp_id: https://dblp.org/pid/61/1573
    emails: '****@live.co.kr'
    first_name: Sungchul
    institution: Adobe Systems
    last_name: Kim
    name: Sungchul Kim
    username: ~Sungchul_Kim1
  - dblp_id: https://dblp.org/pid/17/5085
    emails: '****@gmail.com'
    first_name: Ryan
    google_scholar_id: https://scholar.google.com/citations?user=_Dc6lbQAAAAJ&hl=en
    homepage: http://ryanrossi.com
    institution: Adobe Research
    last_name: Rossi
    middle_name: A.
    name: Ryan A. Rossi
    semantic_scholar_id: https://www.semanticscholar.org/author/Ryan-A.-Rossi/1862090
    username: ~Ryan_A._Rossi2
  - dblp_id: https://dblp.org/pid/63/6846
    emails: '****@gmail.com'
    first_name: Anup
    google_scholar_id: https://scholar.google.com/citations?user=pkwXPU0AAAAJ&hl=en
    institution: Adobe Systems
    last_name: Rao
    name: Anup Rao
    username: ~Anup_Rao1
  - dblp_id: https://dblp.org/pid/177/8902.html
    emails: '****@adobe.com'
    first_name: Tung
    google_scholar_id: https://scholar.google.com/citations?user=eUt8nlIAAAAJ&hl=en
    institution: Adobe
    last_name: Mai
    name: Tung Mai
    username: ~Tung_Mai1
  - dblp_id: https://dblp.org/pid/57/2281-10
    emails: '****@sjtu.edu.cn'
    first_name: Shuai
    google_scholar_id: https://scholar.google.com.hk/citations?user=kMZgQxcAAAAJ&hl=zh-CN
    homepage: http://shuaili8.github.io
    institution: John Hopcroft Center, Shanghai Jiao Tong University
    last_name: Li
    name: Shuai Li
    username: ~Shuai_Li3
  decision: toMainConference
  end_page: 9459
  file: 1100.pdf
  id: 1100
  num_pages: 13
  openreview_id: fRwYeYhcyt
  pdf_file: 7a9482a56676e063ff66ebe5dd6de2909c61d3a4.pdf
  start_page: 9447
  title: Hallucination Diversity-Aware Active Learning for Text Summarization
- abstract: Authorship obfuscation techniques hold the promise of helping people protect
    their privacy in online communications by automatically rewriting text to hide
    the identity of the original author. However, obfuscation has been evaluated in
    narrow settings in the NLP literature and has primarily been addressed with superficial
    edit operations that can lead to unnatural outputs. In this work, we introduce
    an automatic text privatization framework that fine-tunes a large language model
    via reinforcement learning to produce rewrites that balance soundness, sense,
    and privacy. We evaluate it extensively on a large-scale test set of English Reddit
    posts by 68k authors composed of short-medium length texts. We study how the performance
    changes among evaluative conditions including authorial profile length and authorship
    detection strategy. Our method maintains high text quality according to both automated
    metrics and human evaluation, and successfully evades several automated authorship
    attacks.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Generation
  authors:
  - dblp_id: https://dblp.org/pid/283/4064
    emails: '****@umd.edu'
    first_name: Calvin
    institution: University of Maryland, College Park
    last_name: Bao
    name: Calvin Bao
    username: ~Calvin_Bao1
  - dblp_id: https://dblp.org/pid/71/1827
    emails: '****@umd.edu'
    first_name: Marine
    google_scholar_id: https://scholar.google.com/citations?user=iPAX6jcAAAAJ
    homepage: http://www.cs.umd.edu/~marine/
    institution: University of Maryland, College Park
    last_name: Carpuat
    name: Marine Carpuat
    username: ~Marine_Carpuat1
  decision: toMainConference
  end_page: 9475
  file: 1105.pdf
  id: 1105
  num_pages: 16
  openreview_id: u8yEkDoofP
  pdf_file: b9ce38e2fe3599f09faffb645c3d4fe9dfbd24f1.pdf
  start_page: 9460
  title: '{K}eep it {P}rivate: Unsupervised Privatization of Online Text'
- abstract: "Emotion detection in textual data has received growing interest in recent\
    \ years, as it is pivotal for developing empathetic human-computer interaction\
    \ systems.\nThis paper introduces a method for categorizing emotions from text,\
    \ which acknowledges and differentiates between the diversified similarities and\
    \ distinctions of various emotions.\nInitially, we establish a baseline by training\
    \ a transformer-based model for standard emotion classification, achieving state-of-the-art\
    \ performance. \nWe argue that not all misclassifications are of the same importance,\
    \ as there are perceptual similarities among emotional classes.\nWe thus redefine\
    \ the emotion labeling problem by shifting it from a traditional classification\
    \ model to an ordinal classification one, where discrete emotions are arranged\
    \ in a sequential order according to their valence levels.\nFinally, we propose\
    \ a method that performs ordinal classification in the two-dimensional emotion\
    \ space, considering both valence and arousal scales.\nThe results show that our\
    \ approach not only preserves high accuracy in emotion prediction but also significantly\
    \ reduces the magnitude of errors in cases of misclassification."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Sentiment Analysis, Stylistic Analysis, and Argument Mining
  authors:
  - emails: '****@gmail.com'
    first_name: Michail
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=7ds7iXwAAAAJ
    homepage: https://mikemitsios.github.io/
    last_name: Mitsios
    name: Michail Mitsios
    username: ~Michail_Mitsios1
  - emails: '****@samsung.com'
    first_name: Georgios
    google_scholar_id: https://scholar.google.com/citations?user=5c0xhxUAAAAJ&hl=en
    homepage: https://www.linkedin.com/in/george-vamvoukakis-75a58b122/
    institution: Samsung
    last_name: Vamvoukakis
    name: Georgios Vamvoukakis
    username: ~Georgios_Vamvoukakis1
  - emails: '****@samsung.com'
    first_name: Georgia
    google_scholar_id: https://scholar.google.com/citations?user=zGfrqKMAAAAJ
    institution: Samsung
    last_name: Maniati
    name: Georgia Maniati
    username: ~Georgia_Maniati1
  - emails: '****@samsung.com'
    first_name: Nikolaos
    google_scholar_id: https://scholar.google.com/citations?user=y329tukAAAAJ&hl=en
    institution: Samsung
    last_name: Ellinas
    name: Nikolaos Ellinas
    username: ~Nikolaos_Ellinas1
  - emails: '****@samsung.com'
    first_name: Georgios
    institution: Samsung
    last_name: Dimitriou
    name: Georgios Dimitriou
    username: ~Georgios_Dimitriou2
  - emails: '****@samsung.com'
    first_name: Konstantinos
    google_scholar_id: https://scholar.google.com/citations?user=hl16m14AAAAJ&hl=el
    homepage: https://scholar.google.com/citations?user=hl16m14AAAAJ&hl=el
    last_name: Markopoulos
    name: Konstantinos Markopoulos
    username: ~Konstantinos_Markopoulos1
  - emails: '****@samsung.com'
    first_name: Panos
    institution: University of Athens
    last_name: Kakoulidis
    name: Panos Kakoulidis
    username: ~Panos_Kakoulidis1
  - emails: '****@samsung.com'
    first_name: Alexandra
    google_scholar_id: https://scholar.google.com/citations?user=Qhp4-EwAAAAJ&hl=en
    institution: Innoetics, Samsung Electronics
    last_name: Vioni
    name: Alexandra Vioni
    username: ~Alexandra_Vioni1
  - emails: '****@samsung.com'
    first_name: Myrsini
    last_name: Christidou
    name: Myrsini Christidou
    username: ~Myrsini_Christidou1
  - emails: '****@samsung.com'
    first_name: Junkwang
    homepage: https://www.linkedin.com/in/junkwangoh/
    last_name: Oh
    name: Junkwang Oh
    username: ~Junkwang_Oh1
  - emails: '****@samsung.com'
    first_name: Gunu
    homepage: https://m.facebook.com/?paipv=0&eav=AfaR5fJGaMxs0oNnQ2qQElTdslVY-utGCzbBsxt_6k7wmvOcSRV7XunWhKqdAqmUrRU&_rdr
    last_name: Jho
    name: Gunu Jho
    username: ~Gunu_Jho1
  - dblp_id: https://dblp.org/pid/151/4034
    emails: '****@samsung.com'
    first_name: Inchul
    google_scholar_id: https://scholar.google.com/citations?user=T93CjwEAAAAJ
    homepage: https://scholar.google.com/citations?user=T93CjwEAAAAJ
    last_name: Hwang
    name: Inchul Hwang
    username: ~Inchul_Hwang1
  - emails: '****@samsung.com'
    first_name: Georgios
    homepage: https://www.innoetics.com/about
    last_name: Vardaxoglou
    name: Georgios Vardaxoglou
    username: ~Georgios_Vardaxoglou1
  - emails: '****@samsung.com'
    first_name: Aimilios
    homepage: https://www.samsung.com
    last_name: Chalamandaris
    name: Aimilios Chalamandaris
    username: ~Aimilios_Chalamandaris1
  - emails: '****@samsung.com'
    first_name: Pirros
    last_name: Tsiakoulis
    name: Pirros Tsiakoulis
    username: ~Pirros_Tsiakoulis1
  - emails: '****@samsung.com'
    first_name: Spyros
    last_name: Raptis
    name: Spyros Raptis
    username: ~Spyros_Raptis1
  decision: toMainConference
  end_page: 9481
  file: 1108.pdf
  id: 1108
  num_pages: 6
  openreview_id: KG4ZOZnmzm
  pdf_file: 7fe8b8687b4eeba2ad0128256ab80c34dc4ee081.pdf
  start_page: 9476
  title: Improved Text Emotion Prediction Using Combined Valence and Arousal Ordinal
    Classification
- abstract: "We introduce Tied-LoRA, a novel paradigm leveraging weight tying and\
    \ selective training to enhance the parameter efficiency of Low-rank Adaptation\
    \ (LoRA). Our exploration encompasses different plausible combinations of parameter\
    \ training and freezing, coupled with weight tying, aimed at identifying the optimal\
    \ trade-off between performance and the count of trainable parameters. Across\
    \ \n$5$ diverse tasks and two foundational language models with different parameter\
    \ counts, our experiments provide comprehensive insights into the inherent trade-offs\
    \ between efficiency and performance.\n\nOur findings reveal a specific Tied-LoRA\
    \ configuration that distinguishes itself by showcasing comparable performance\
    \ to LoRA across multiple tasks while utilizing only a fraction of the parameters\
    \ employed by the standard LoRA method, particularly at elevated ranks. This underscores\
    \ the efficacy of Tied-LoRA in achieving impressive results with significantly\
    \ reduced model complexity."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Efficient/Low-Resource Methods for NLP
  authors:
  - emails: '****@nvidia.com'
    first_name: Adithya
    homepage: https://arendu.github.io/
    institution: NVIDIA
    last_name: Renduchintala
    name: Adithya Renduchintala
    orcid: https://orcid.org/0000-0002-1896-3679
    username: ~Adithya_Renduchintala2
  - emails: '****@gmail.com'
    first_name: Tugrul
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=wnNdBZMAAAAJ
    institution: NVIDIA
    last_name: Konuk
    name: Tugrul Konuk
    orcid: https://orcid.org/0000-0001-6351-8593
    username: ~Tugrul_Konuk1
  - emails: '****@nvidia.com'
    first_name: Oleksii
    google_scholar_id: https://scholar.google.com/citations?user=qmmIGnwAAAAJ&hl
    homepage: http://www.kuchaev.com
    institution: NVIDIA
    last_name: Kuchaiev
    name: Oleksii Kuchaiev
    username: ~Oleksii_Kuchaiev1
  decision: toMainConference
  end_page: 9493
  file: 1110.pdf
  id: 1110
  num_pages: 12
  openreview_id: MeBvsUdrcl
  pdf_file: 336471f134ec2dba23fc67e1a203418120d1e3c8.pdf
  start_page: 9482
  title: 'Tied-LoRA: Enhancing parameter efficiency of LoRA with Weight Tying'
- abstract: Recent observations have underscored a disparity between the inflated
    benchmark scores and the actual performance of LLMs, raising concerns about potential
    contamination of evaluation benchmarks. This issue is especially critical for
    closed-source models and certain open-source models where training data transparency
    is lacking. In this paper we study data contamination by proposing two methods
    tailored for both open-source and proprietary LLMs. We first introduce a retrieval-based
    system to explore potential overlaps between evaluation benchmarks and pretraining
    corpora. We further present a novel investigation protocol named Testset Slot
    Guessing (TS-Guessing), applicable to both open and proprietary models. This approach
    entails masking a wrong answer in a multiple-choice question and prompting the
    model to fill in the gap. Additionally, it involves obscuring an unlikely word
    in an evaluation example and asking the model to produce it. We find that certain
    commercial LLMs could surprisingly guess the missing option in various test sets.
    Specifically, in the MMLU benchmark, ChatGPT and GPT-4 demonstrated an exact match
    rate of 52\% and 57\%, respectively, in guessing the missing options in benchmark
    test data. We hope these results underscore the need for more robust evaluation
    methodologies and benchmarks in the field.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Ethics, Bias, and Fairness
  authors:
  - emails: '****@gatech.edu'
    first_name: Chunyuan
    google_scholar_id: https://scholar.google.com/citations?user=g7Y0RHcAAAAJ&hl=en
    homepage: https://charlesdddd.github.io/
    last_name: Deng
    name: Chunyuan Deng
    semantic_scholar_id: https://www.semanticscholar.org/author/Chunyuan-Deng/2266840268
    username: ~Chunyuan_Deng1
  - dblp_id: https://dblp.org/pid/271/8391
    emails: '****@yale.edu'
    first_name: Yilun
    google_scholar_id: https://scholar.google.com/citations?hl=zh-CN&user=yAQAXrIAAAAJ
    homepage: https://yilunzhao.github.io/
    institution: Yale University
    last_name: Zhao
    name: Yilun Zhao
    semantic_scholar_id: https://www.semanticscholar.org/author/Yilun-Zhao/46316984
    username: ~Yilun_Zhao1
  - dblp_id: https://dblp.org/pid/246/8064
    emails: '****@gmail.com'
    first_name: Xiangru
    homepage: https://xiangrutang.github.io/
    institution: Yale University
    last_name: Tang
    name: Xiangru Tang
    semantic_scholar_id: https://www.semanticscholar.org/author/Xiangru-Tang/47274259
    username: ~Xiangru_Tang2
  - dblp_id: https://dblp.org/pid/67/5132
    emails: '****@gersteinlab.org'
    first_name: Mark
    google_scholar_id: https://scholar.google.com/citations?user=YvjuUugAAAAJ&hl=en
    homepage: http://www.gersteinlab.org/
    institution: Yale University
    last_name: Gerstein
    name: Mark Gerstein
    orcid: https://orcid.org/0000-0002-9746-3719
    username: ~Mark_Gerstein2
  - dblp_id: https://dblp.org/pid/160/1727
    emails: '****@yale.edu'
    first_name: Arman
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=baI7IY0AAAAJ
    homepage: http://www.armancohan.com
    institution: Yale University and Allen Institute for Artificial Intelligence
    last_name: Cohan
    name: Arman Cohan
    semantic_scholar_id: https://www.semanticscholar.org/author/Arman-Cohan/2527954
    username: ~Arman_Cohan1
  decision: toMainConference
  end_page: 9506
  file: 1116.pdf
  id: 1116
  num_pages: 13
  openreview_id: EAFfBk3OQB
  pdf_file: 8d5a3510d4abcb3585267785fa0d9701d8d65b64.pdf
  start_page: 9494
  title: Investigating Data Contamination in Modern Benchmarks for Large Language
    Models
- abstract: 'Entity Resolution (ER) is an essential task in data integration and its
    goal is to find records that represent the same entity in a dataset. Deep learning
    models, especially large pre-trained language models, have achieved state-of-the-art
    results on this task. A typical ER pipeline consists of Entity Blocking and Entity
    Matching: Entity Blocking finds candidate record pairs that potentially match
    and Entity Matching determines if the pairs match. The goal of the entity blocking
    step is to include as many matching pairs as possible while including as few non-matching
    pairs as possible. On the other hand, the blocking task can also be considered
    as an Information Retrieval (IR) task. However, state-of-the-art neural IR models
    that are based on large language models have not been evaluated on the ER task.
    What''s more, the generalization ability of state-of-the-art methods for entity
    blocking is not well-studied but an import aspect in real-world applications.
    In this work, we evaluate state-of-the-art models for Entity Blocking along with
    neural IR models on a wide range of real-world datasets, and also study their
    in-distribution and out-of-distribution generalization abilities.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - dblp_id: https://dblp.org/pid/247/9289
    emails: '****@gmail.com'
    first_name: Runhui
    google_scholar_id: https://scholar.google.com/citations?user=GDSiOPsAAAAJ&hl=en
    homepage: https://runhuiwang.github.io/
    last_name: Wang
    name: Runhui Wang
    username: ~Runhui_Wang1
  - dblp_id: https://dblp.org/pid/82/7829
    emails: '****@rutgers.edu'
    first_name: Yongfeng
    google_scholar_id: https://scholar.google.com/citations?user=A66WefUAAAAJ&hl=en
    homepage: http://www.yongfeng.me
    institution: Rutgers University
    last_name: Zhang
    name: Yongfeng Zhang
    username: ~Yongfeng_Zhang1
  decision: toMainConference
  end_page: 9517
  file: 1119.pdf
  id: 1119
  num_pages: 11
  openreview_id: T1uajcUjjs
  pdf_file: b8ed821c2493e741197f17d81058196ef543dfa7.pdf
  start_page: 9507
  title: 'Pre-trained Language Models for Entity Blocking: A Reproducibility Study'
- abstract: Narrative Question Answering is an important task for evaluating and improving
    reading comprehension abilities in both humans and machines. However, there is
    a lack of consensus on the skill taxonomy that would enable systematic and comprehensive
    assessment and learning of the various aspects of Narrative Question Answering.
    Existing task-level skill views oversimplify the multidimensional nature of tasks,
    while question-level taxonomies face issues in evaluation and methodology. To
    address these challenges, we introduce a more inclusive skill taxonomy that synthesizes
    and redefines narrative understanding skills from previous taxonomies and includes
    a generation skill dimension from the answering perspective.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Question Answering
  authors:
  - emails: '****@ut.ee'
    first_name: Emil
    homepage: https://emilkalbaliyev.github.io/
    last_name: Kalbaliyev
    name: Emil Kalbaliyev
    username: ~Emil_Kalbaliyev1
  - dblp_id: https://dblp.uni-trier.de/pid/116/0450.html
    emails: '****@gmail.com'
    first_name: Kairit
    google_scholar_id: https://scholar.google.com/citations?user=YOrsSssAAAAJ&hl=en
    institution: institute of computer science, University of Tartu
    last_name: Sirts
    name: Kairit Sirts
    semantic_scholar_id: https://www.semanticscholar.org/author/Kairit-Sirts/2132289
    username: ~Kairit_Sirts2
  decision: toMainConference
  end_page: 9524
  file: 1120.pdf
  id: 1120
  num_pages: 7
  openreview_id: 14mu6sqjtw
  pdf_file: 45c2180c7f409da2dd3258e79fa304606ceaf24d.pdf
  start_page: 9518
  title: On Narrative Question Answering Skills
- abstract: Current research in form understanding predominantly relies on large pre-trained
    language models, necessitating extensive data for pre-training. However, the importance
    of layout structure (i.e., the spatial relationship between the entity blocks
    in the visually rich document) to relation extraction has been overlooked. In
    this paper, we propose $\textbf{RE}$gion-Aware $\textbf{R}$elation $\textbf{E}$xtraction
    ($\bf{RE^2}$) that leverages region-level spatial structure among the entity blocks
    to improve their relation prediction. We design an edge-aware graph attention
    network to learn the interaction between entities while considering their spatial
    relationship defined by their region-level representations. We also introduce
    a constraint objective to regularize the model towards consistency with the inherent
    constraints of the relation extraction task. To support the research on relation
    extraction from visually rich documents and demonstrate the generalizability of
    $\bf{RE^2}$, we build a new benchmark dataset, ${DiverseForm}$, that covers a
    wide range of domains. Extensive experiments on ${DiverseForm}$ and several public
    benchmark datasets demonstrate significant superiority and transferability of
    $\bf{RE^2}$ across various domains and languages, with up to 18.88\% absolute
    F-score gain over all high-performing baselines
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Information Extraction
  authors:
  - emails: '****@gmail.com'
    first_name: Pritika
    institution: Adobe Research
    last_name: Ramu
    name: Pritika Ramu
    username: ~Pritika_Ramu1
  - emails: '****@vt.edu'
    first_name: Sijia
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=raDkYB8AAAAJ
    last_name: Wang
    name: Sijia Wang
    semantic_scholar_id: https://www.semanticscholar.org/author/Sijia-Wang/2116423012
    username: ~Sijia_Wang1
  - emails: '****@intuit.com'
    first_name: Lalla
    homepage: https://www.google.com/
    last_name: Mouatadid
    name: Lalla Mouatadid
    username: ~Lalla_Mouatadid1
  - emails: '****@gmail.com'
    first_name: Joy
    homepage: https://www.linkedin.com/in/joytafty
    last_name: Rimchala
    name: Joy Rimchala
    username: ~Joy_Rimchala1
  - dblp_id: https://dblp.org/pid/127/0072
    emails: '****@gmail.com'
    first_name: Lifu
    google_scholar_id: https://scholar.google.com/citations?user=76IEGtYAAAAJ&hl=en
    homepage: https://wilburone.github.io/
    institution: Virginia Tech
    last_name: Huang
    name: Lifu Huang
    username: ~Lifu_Huang1
  decision: toMainConference
  end_page: 9541
  file: 1123.pdf
  id: 1123
  num_pages: 17
  openreview_id: IhZ6KTkFkE
  pdf_file: 49f3b5e19a7fb09f22b9545c59f756df4e551741.pdf
  start_page: 9525
  title: '$RE^2$: Region-Aware Relation Extraction from Visually Rich Documents'
- abstract: Mixed initiative serves as one of the key factors in controlling conversation
    directions. For a speaker, responding passively or leading proactively would result
    in rather different responses. However, most dialogue systems focus on training
    a holistic response generation model without any distinction among different initiatives.
    It leads to the cross-contamination problem, where the model confuses different
    initiatives and generates inappropriate responses. Moreover, obtaining plenty
    of human annotations for initiative labels can be expensive. To address this issue,
    we propose a general mix-Initiative Dynamic Prefix Tuning framework (IDPT) to
    decouple different initiatives from the generation model, which learns initiative-aware
    prefixes in both supervised and unsupervised settings. Specifically, IDPT decouples
    initiative factors into different prefix parameters and uses the attention mechanism
    to adjust the selection of initiatives in guiding generation dynamically. The
    prefix parameters can be tuned towards accurate initiative prediction as well
    as mix-initiative response generation. Extensive experiments on two public dialogue
    datasets show that the proposed IDPT outperforms previous baselines on both automatic
    metrics and human evaluations. It also manages to generate appropriate responses
    with manipulated initiatives.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Dialogue and Interactive Systems
  authors:
  - dblp_id: https://dblp.org/pid/247/9594
    emails: '****@gmail.com'
    first_name: Yuxiang
    google_scholar_id: https://scholar.google.com/citations?hl=zh-CN&user=-SkEwyYAAAAJ
    homepage: https://github.com/JerrryNie
    institution: Hong Kong University of Science and Technology
    last_name: Nie
    name: Yuxiang Nie
    semantic_scholar_id: https://www.semanticscholar.org/author/Y.-Nie/2003588344
    username: ~Yuxiang_Nie1
  - dblp_id: https://dblp.org/pid/27/8686
    emails: '****@bit.edu.cn'
    first_name: Heyan
    homepage: https://cs.bit.edu.cn/szdw/jsml/js/hhy/index.htm
    institution: Beijing Institute of Technology
    last_name: Huang
    name: Heyan Huang
    orcid: https://orcid.org/0000-0002-0320-7520
    semantic_scholar_id: https://www.semanticscholar.org/author/Heyan-Huang/4590286
    username: ~Heyan_Huang1
  - dblp_id: https://dblp.org/pid/46/9687.html
    emails: '****@bit.edu.cn'
    first_name: Xian-Ling
    google_scholar_id: https://scholar.google.com/citations?user=b2DzFF8AAAAJ&hl=en
    homepage: https://cs.bit.edu.cn/szdw/jsml/js/mxl/index.htm
    institution: Beijing Institute of Technology
    last_name: Mao
    name: Xian-Ling Mao
    semantic_scholar_id: https://www.semanticscholar.org/author/Xian-Ling-Mao/134880677
    username: ~Xian-Ling_Mao1
  - dblp_id: https://dblp.org/pid/149/1249
    emails: '****@gmail.com'
    first_name: Lizi
    google_scholar_id: https://scholar.google.com.sg/citations?user=W2b08EUAAAAJ&hl=en
    homepage: https://liziliao.github.io/
    institution: Singapore Management University
    last_name: Liao
    name: Lizi Liao
    username: ~Lizi_Liao1
  decision: toMainConference
  end_page: 9555
  file: 1124.pdf
  id: 1124
  num_pages: 14
  openreview_id: KP8l4UtujC
  pdf_file: 12449f7609a5b4c5fad24d93618c013c43b1ae0a.pdf
  start_page: 9542
  title: Mix-Initiative Response Generation with Dynamic Prefix Tuning
- abstract: 'Value alignment is crucial for the responsible development of Large Language
    Models (LLMs). However, how to define values in this context remains largely unexplored.
    Existing work mainly specifies values as risk criteria formulated in the AI community,
    e.g., fairness and privacy protection, suffering from poor clarity, adaptability
    and transparency. Leveraging basic values established in humanity and social science
    that are compatible with values across cultures, this paper introduces a novel
    value space spanned by multiple basic value dimensions and proposes BaseAlign,
    a corresponding value alignment paradigm. Applying the representative Schwartz''s
    Theory of Basic Values as an instantiation, we construct FULCRA, a dataset consisting
    of 20k $($LLM output, value vector$)$ pairs. LLMs'' outputs are mapped into the
    $K$-dim value space beyond simple binary labels, by identifying their underlying
    priorities for these value dimensions. Extensive analysis and experiments on FULCRA:
    (1) reveal the essential relation between basic values and LLMs'' behaviors, (2)
    demonstrate that our paradigm with basic values not only covers existing risks
    but also anticipates the unidentified ones, and (3) manifest BaseAlign''s superiority
    in alignment performance with less data, paving the way for addressing the above
    three challenges.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Resources and Evaluation
  authors:
  - dblp_id: https://dblp.org/pid/24/5678.html
    emails: '****@microsoft.com'
    first_name: Jing
    google_scholar_id: https://scholar.google.jp/citations?user=2FH8EjkAAAAJ&hl=en&oi=sra
    institution: Microsoft
    last_name: Yao
    name: Jing Yao
    username: ~Jing_Yao4
  - dblp_id: https://dblp.uni-trier.de/pid/179/2248
    emails: '****@microsoft.com'
    first_name: Xiaoyuan
    google_scholar_id: https://scholar.google.com/citations?user=BdpXcLgAAAAJ&hl=zh-CN
    homepage: https://xiaoyuanyi.github.io/
    institution: Microsoft Research
    last_name: Yi
    name: Xiaoyuan Yi
    semantic_scholar_id: https://www.semanticscholar.org/author/Xiaoyuan-Yi/3393196
    username: ~Xiaoyuan_Yi1
  - emails: '****@qq.com'
    first_name: Yifan
    homepage: https://github.com/paraGONG
    last_name: Gong
    name: Yifan Gong
    username: ~Yifan_Gong3
  - dblp_id: https://dblp.org/pid/134/4020
    emails: '****@gmail.com'
    first_name: Xiting
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=urC8meQAAAAJ
    homepage: https://www.microsoft.com/en-us/research/people/xitwan/
    institution: Renmin University of China
    last_name: Wang
    name: Xiting Wang
    username: ~Xiting_Wang2
  - dblp_id: https://dblp.org/pid/08/6809-1
    emails: '****@microsoft.com'
    first_name: Xing
    google_scholar_id: https://scholar.google.com/citations?user=5EQfAFIAAAAJ
    homepage: http://research.microsoft.com/en-us/people/xingx/
    institution: Microsoft
    last_name: Xie
    name: Xing Xie
    username: ~Xing_Xie3
  decision: toMainConference
  end_page: 9579
  file: 1126.pdf
  id: 1126
  num_pages: 24
  openreview_id: AJLCEjHmqk
  pdf_file: 6bcd0fd048a5bfc10db47b56a51c74e55e5272da.pdf
  start_page: 9556
  title: 'Value FULCRA: Mapping Large Language Models to the Multidimensional Spectrum
    of Basic Human Value'
- abstract: 'In this paper, we propose sequence-based pre-training methods to enhance
    procedural understanding in natural language processing. Procedural text, containing
    sequential instructions to accomplish a task, is difficult to understand due to
    the changing attributes of entities in the context. We focus on recipes as they
    are commonly represented as ordered instructions, and use this order as a supervision
    signal. Our work is one of the first to compare several `order-as-supervision''
    transformer pre-training methods, including Permutation Classification, Embedding
    Regression, and Skip-Clip, and show that these methods give improved results compared
    to baselines and SoTA LLMs on two downstream Entity-Tracking datasets: NPN-Cooking
    dataset in recipe domain and ProPara dataset in open domain. Our proposed methods
    address the non-trivial Entity Tracking Task that requires prediction of entity
    states across procedure steps, which requires understanding the order of steps.
    These methods show an improvement over the best baseline by $1.6$\% and $7$-$9$\%
    on NPN-Cooking and ProPara Datasets respectively across metrics.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: NLP Applications
  authors:
  - dblp_id: https://dblp.org/pid/237/2393.html
    emails: '****@gmail.com'
    first_name: Abhilash
    google_scholar_id: https://scholar.google.com/citations?user=vJhwesAAAAAJ&hl=en
    homepage: https://sites.google.com/view/abhilashnandy
    institution: Indian Institute of Technology Kharagpur
    last_name: Nandy
    name: Abhilash Nandy
    semantic_scholar_id: https://www.semanticscholar.org/author/Abhilash-Nandy/82742429
    username: ~Abhilash_Nandy1
  - emails: '****@gmail.com'
    first_name: Yash
    last_name: Kulkarni
    name: Yash Kulkarni
    username: ~Yash_Kulkarni1
  - dblp_id: https://dblp.org/pid/77/2307-2
    emails: '****@cse.iitkgp.ac.in'
    first_name: Pawan
    google_scholar_id: https://scholar.google.com.tw/citations?user=F14FHsIAAAAJ
    homepage: http://cse.iitkgp.ac.in/~pawang/
    institution: IIT Kharagpur
    last_name: Goyal
    name: Pawan Goyal
    semantic_scholar_id: https://www.semanticscholar.org/author/P.-Goyal/2111980452
    username: ~Pawan_Goyal1
  - dblp_id: https://dblp.org/pers/hd/g/Ganguly:Niloy
    emails: '****@cse.iitkgp.ac.in'
    first_name: Niloy
    google_scholar_id: https://scholar.google.com/citations?user=hCbFmUUAAAAJ&hl=en
    homepage: http://www.facweb.iitkgp.ac.in/~niloy/
    institution: Indian Institute of Technology Kharagpur,
    last_name: Ganguly
    name: Niloy Ganguly
    username: ~Niloy_Ganguly1
  decision: toMainConference
  end_page: 9587
  file: 1127.pdf
  id: 1127
  num_pages: 8
  openreview_id: 9m1H2wihfP
  pdf_file: 2bcdfdb46d465c334cfe15b7f33f5c32bcb3b20c.pdf
  start_page: 9580
  title: Order-Based Pre-training Strategies for Procedural Text Understanding
- abstract: Large language models hold significant promise in multilingual applications.
    However, inherent biases stemming from predominantly English-centric pre-training
    have led to the widespread practice of pre-translation, i.e., translating non-English
    inputs to English before inference, leading to complexity and information loss.
    This study re-evaluates the need for pre-translation in the context of PaLM2 models,
    which have been established as highly performant in multilingual tasks. We offer
    a comprehensive investigation across 108 languages and 6 diverse benchmarks, including
    open-end generative tasks, which were excluded from previous similar studies.
    Our findings challenge the pre-translation paradigm established in prior research,
    highlighting the advantages of direct inference in PaLM2. Specifically, PaLM2-L
    consistently outperforms pre-translation in 94 out of 108 languages. These findings
    pave the way for more efficient and effective multilingual applications, alleviating
    the limitations associated with pre-translation and unlocking linguistic authenticity.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Multilinguality and Language Diversity
  authors:
  - emails: '****@verily.com'
    first_name: Yotam
    google_scholar_id: https://scholar.google.com/citations?user=UvL5S2sAAAAJ&hl=en&oi=ao
    last_name: Intrator
    name: Yotam Intrator
    username: ~Yotam_Intrator1
  - emails: '****@mail.huji.ac.il'
    first_name: Matan
    last_name: Halfon
    name: Matan Halfon
    username: ~Matan_Halfon1
  - dblp_id: https://dblp.org/pid/99/3512.html
    emails: '****@gmail.com'
    first_name: Roman
    google_scholar_id: https://scholar.google.com/citations?user=yggUoWYAAAAJ&hl=en
    last_name: Goldenberg
    name: Roman Goldenberg
    orcid: https://orcid.org/0000-0002-5538-772X
    username: ~Roman_Goldenberg1
  - dblp_id: https://dblp.org/pid/21/3716
    emails: '****@biu.ac.il'
    first_name: Reut
    institution: Google and Bar-Ilan University, Technion
    last_name: Tsarfaty
    name: Reut Tsarfaty
    username: ~Reut_Tsarfaty1
  - dblp_id: https://dblp.org/pid/213/8040
    emails: '****@gmail.com'
    first_name: Matan
    google_scholar_id: https://scholar.google.com/citations?user=Dfb3aV8AAAAJ&hl=en
    institution: Allen Institute for Artificial Intelligence
    last_name: Eyal
    name: Matan Eyal
    semantic_scholar_id: https://www.semanticscholar.org/author/Matan-Eyal/35298844
    username: ~Matan_Eyal1
  - emails: '****@google.com'
    first_name: Ehud
    homepage: http://www.cs.technion.ac.il/~ehudr/
    institution: Technion, Technion
    last_name: Rivlin
    name: Ehud Rivlin
    username: ~Ehud_Rivlin2
  - dblp_id: https://dblp.org/pid/m/YossiMatias
    emails: '****@gmail.com'
    first_name: Yossi
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=IwSe1-MAAAAJ
    homepage: https://research.google/people/YossiMatias/
    institution: Google and Tel Aviv University
    last_name: Matias
    name: Yossi Matias
    semantic_scholar_id: https://www.semanticscholar.org/author/Y.-Matias/1745572
    username: ~Yossi_Matias2
  - dblp_id: https://dblp.org/pid/19/11191
    emails: '****@gmail.com'
    first_name: Natalia
    google_scholar_id: https://scholar.google.com/citations?user=m7dz9gwAAAAJ&hl=en
    last_name: Aizenberg
    name: Natalia Aizenberg
    username: ~Natalia_Aizenberg1
  decision: toMainConference
  end_page: 9603
  file: 1137.pdf
  id: 1137
  num_pages: 16
  openreview_id: V47MRKYiWH
  pdf_file: 9e2d6336f15faf5d46603c923881218756a141c2.pdf
  start_page: 9588
  title: 'Breaking the Language Barrier: Can Direct Inference Outperform Pre-Translation
    in Multilingual LLM Applications?'
- abstract: "The pervasive influence of social biases in language data has sparked\
    \ the need for benchmark datasets that capture and evaluate these biases in Large\
    \ Language Models (LLMs). Existing efforts predominantly focus on English language\
    \ and the Western context, leaving a void for a reliable dataset that encapsulates\
    \ India\u2019s unique socio-cultural nuances. To bridge this gap, we introduce\
    \ IndiBias, a comprehensive benchmarking dataset designed specifically for evaluating\
    \ social biases in the Indian context. We filter and translate the existing CrowS-Pairs\
    \ dataset to create a benchmark dataset suited to the Indian context in Hindi\
    \ language. Additionally, we leverage LLMs including ChatGPT and InstructGPT to\
    \ augment our dataset with diverse societal biases and stereotypes prevalent in\
    \ India. The included bias dimensions encompass gender, religion, caste, age,\
    \ region, physical appearance, and occupation. We also build a resource to address\
    \ intersectional biases along three intersectional dimensions. Our dataset contains\
    \ 800 sentence pairs and 300 tuples for bias measurement across different demographics.\
    \ The dataset is available in English and Hindi, providing a size comparable to\
    \ existing benchmark datasets. Furthermore, using IndiBias we compare ten different\
    \ language models on multiple bias measurement metrics. We observed that the language\
    \ models exhibit more bias across a majority of the intersectional groups. All\
    \ the scripts utilized and datasets created in this study are publicly available."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: Ethics, Bias, and Fairness
  authors:
  - emails: '****@gmail.com'
    first_name: Nihar
    google_scholar_id: https://scholar.google.com/citations?hl=en&view_op=list_works&gmla=AHoSzlUiX7_u2K61tL0j-o3Yf9hudfC2eqGs6_2i_gxvSXjhNg-upi_xQHIpDZbOKggm10H10vU8Pnudn1Wafe_UeAD4ERkle_V9iTtv_O0&user=uoIh14MAAAAJ
    homepage: https://sahoonihar.github.io/
    last_name: Sahoo
    middle_name: Ranjan
    name: Nihar Ranjan Sahoo
    username: ~Nihar_Ranjan_Sahoo1
  - emails: '****@ee.iitb.ac.in'
    first_name: Pranamya
    last_name: Kulkarni
    middle_name: Prashant
    name: Pranamya Prashant Kulkarni
    username: ~Pranamya_Prashant_Kulkarni1
  - emails: '****@cse.iitb.ac.in'
    first_name: Arif
    homepage: https://arifahmad-py.github.io/
    last_name: Ahmad
    name: Arif Ahmad
    username: ~Arif_Ahmad1
  - emails: '****@gmail.com'
    first_name: Tanu
    last_name: Goyal
    name: Tanu Goyal
    username: ~Tanu_Goyal1
  - emails: '****@cse.iitb.ac.in'
    first_name: Narjis
    last_name: Asad
    name: Narjis Asad
    username: ~Narjis_Asad1
  - dblp_id: https://dblp.org/pid/183/5034.html
    emails: '****@adobe.com'
    first_name: Aparna
    google_scholar_id: https://scholar.google.com/citations?user=Q4PJyXIAAAAJ&hl=en
    homepage: https://research.adobe.com/person/aparna-garimella/
    institution: Adobe Research
    last_name: Garimella
    name: Aparna Garimella
    semantic_scholar_id: https://www.semanticscholar.org/author/Aparna-Garimella/31099365
    username: ~Aparna_Garimella1
  - dblp_id: https://dblp.org/pid/p/PushpakBhattacharyya
    emails: '****@cse.iitb.ac.in'
    first_name: Pushpak
    google_scholar_id: https://scholar.google.com.tw/citations?user=vvg-pAkAAAAJ
    homepage: https://www.cse.iitb.ac.in/~pb/
    institution: Indian Institute of Technology, Bombay, Dhirubhai Ambani Institute
      Of Information and Communication Technology
    last_name: Bhattacharyya
    name: Pushpak Bhattacharyya
    semantic_scholar_id: https://www.semanticscholar.org/author/P.-Bhattacharyya/145532184
    username: ~Pushpak_Bhattacharyya1
  decision: toMainConference
  end_page: 9624
  file: 1154.pdf
  id: 1154
  num_pages: 21
  openreview_id: rfSGDL4fee
  pdf_file: 650fa7b54bf1e8c00b0c4f1037a9e3fa7ef44e65.pdf
  start_page: 9604
  title: 'IndiBias: A Benchmark Dataset to Measure Social Biases in Language Models
    for Indian Context'
